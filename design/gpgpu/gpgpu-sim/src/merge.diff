diff --git a/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.cc b/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.cc
index 127240a0bc..c1280c7650 100644
--- a/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.cc
+++ b/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.cc
@@ -28,1159 +28,1241 @@
 
 
 #include "abstract_hardware_model.h"
+#include <sys/stat.h>
+#include <algorithm>
+#include <iostream>
+#include <sstream>
+#include "../libcuda_sim/gpgpu_context.h"
+#include "cuda-sim/cuda-sim.h"
 #include "cuda-sim/memory.h"
-#include "cuda-sim/ptx_ir.h"
 #include "cuda-sim/ptx-stats.h"
-#include "cuda-sim/cuda-sim.h"
+#include "cuda-sim/ptx_ir.h"
 #include "gpgpu-sim/gpu-sim.h"
-
+#include "gpgpusim_entrypoint.h"
 // TODO schi
 #include "gpu/gpgpu-sim/cuda_gpu.hh"
 #include "option_parser.h"
-#include <algorithm>
-#include <sys/stat.h>
-#include <sstream>
-#include <iostream>
 
-extern gpgpu_sim *g_the_gpu;
+void mem_access_t::init(gpgpu_context *ctx) {
+  gpgpu_ctx = ctx;
+  m_uid = ++(gpgpu_ctx->sm_next_access_uid);
+  m_addr = 0;
+  m_req_size = 0;
+}
+void warp_inst_t::issue(const active_mask_t &mask, unsigned warp_id,
+                        unsigned long long cycle, int dynamic_warp_id,
+                        int sch_id) {
+  m_warp_active_mask = mask;
+  m_warp_issued_mask = mask;
+  m_uid = ++(m_config->gpgpu_ctx->warp_inst_sm_next_uid);
+  m_warp_id = warp_id;
+  m_dynamic_warp_id = dynamic_warp_id;
+  issue_cycle = cycle;
+  cycles = initiation_interval;
+  m_cache_hit = false;
+  m_empty = false;
+  m_scheduler_id = sch_id;
+}
 
 unsigned mem_access_t::sm_next_access_uid = 0;   
 unsigned warp_inst_t::sm_next_uid = 0;
 
-checkpoint::checkpoint()
-{
-
-    struct stat st = {0};
+checkpoint::checkpoint() {
+  struct stat st = {0};
 
-    if (stat("checkpoint_files", &st) == -1) {
-        mkdir("checkpoint_files", 0777);
+  if (stat("checkpoint_files", &st) == -1) {
+    mkdir("checkpoint_files", 0777);
+  }
+}
+void checkpoint::load_global_mem(class memory_space *temp_mem, char *f1name) {
+  FILE *fp2 = fopen(f1name, "r");
+  assert(fp2 != NULL);
+  char line[128]; /* or other suitable maximum line size */
+  unsigned int offset;
+  while (fgets(line, sizeof line, fp2) != NULL) /* read a line */
+  {
+    unsigned int index;
+    char *pch;
+    pch = strtok(line, " ");
+    if (pch[0] == 'g' || pch[0] == 's' || pch[0] == 'l') {
+      pch = strtok(NULL, " ");
+
+      std::stringstream ss;
+      ss << std::hex << pch;
+      ss >> index;
+
+      offset = 0;
+    } else {
+      unsigned int data;
+      std::stringstream ss;
+      ss << std::hex << pch;
+      ss >> data;
+      temp_mem->write_only(offset, index, 4, &data);
+      offset = offset + 4;
     }
-
+    // fputs ( line, stdout ); /* write the line */
+  }
+  fclose(fp2);
 }
-void checkpoint::load_global_mem(class memory_space *temp_mem, char * f1name)
-{
 
-    FILE * fp2 = fopen(f1name, "r");
-    assert(fp2!=NULL);
-      char line [ 128 ]; /* or other suitable maximum line size */
-      unsigned int offset = 0;
-      while ( fgets ( line, sizeof line, fp2 ) != NULL ) /* read a line */
-      {
-         unsigned int index;
-         char * pch;
-         pch = strtok (line," ");
-         if (pch[0]=='g' || pch[0]=='s' || pch[0]=='l')
-         {
-
-           pch = strtok (NULL, " ");
-           
-           std::stringstream ss;
-            ss << std::hex << pch;
-            ss >> index;
-
-           offset=0;
-         }
-         else {
-            unsigned int  data;
-            std::stringstream ss;
-            ss << std::hex << pch;
-            ss >> data;
-            temp_mem->write_only(offset,index, 4,&data);
-            offset= offset+4;
-         }
-         //fputs ( line, stdout ); /* write the line */
-      }
-      fclose ( fp2 );
+void checkpoint::store_global_mem(class memory_space *mem, char *fname,
+                                  char *format) {
+  FILE *fp3 = fopen(fname, "w");
+  assert(fp3 != NULL);
+  mem->print(format, fp3);
+  fclose(fp3);
 }
 
-void checkpoint::store_global_mem(class memory_space * mem, char *fname, char * format)
-{
-
-      FILE * fp3 = fopen(fname, "w");
-      assert(fp3!=NULL);
-      mem->print(format,fp3);
-      fclose(fp3);
-}
-
-void move_warp( warp_inst_t *&dst, warp_inst_t *&src )
-{
-   assert( dst->empty() );
-   warp_inst_t* temp = dst;
-   dst = src;
-   src = temp;
-   src->clear();
+void move_warp(warp_inst_t *&dst, warp_inst_t *&src) {
+  assert(dst->empty());
+  warp_inst_t *temp = dst;
+  dst = src;
+  src = temp;
+  src->clear();
 }
 
 
-void gpgpu_functional_sim_config::reg_options(class OptionParser * opp)
-{
-	option_parser_register(opp, "-gpgpu_ptx_use_cuobjdump", OPT_BOOL,
-                 &m_ptx_use_cuobjdump,
-                 "Use cuobjdump to extract ptx and sass from binaries",
+void gpgpu_functional_sim_config::reg_options(class OptionParser *opp) {
+  option_parser_register(opp, "-gpgpu_ptx_use_cuobjdump", OPT_BOOL,
+                         &m_ptx_use_cuobjdump,
+                         "Use cuobjdump to extract ptx and sass from binaries",
 #if (CUDART_VERSION >= 4000)
-                 "1"
+                         "1"
 #else
-                 "0"
+                         "0"
 #endif
-                 );
-	option_parser_register(opp, "-gpgpu_experimental_lib_support", OPT_BOOL,
-	                 &m_experimental_lib_support,
-	                 "Try to extract code from cuda libraries [Broken because of unknown cudaGetExportTable]",
-	                 "0");
-  option_parser_register(opp, "-checkpoint_option", OPT_INT32, &checkpoint_option, 
-               " checkpointing flag (0 = no checkpoint)",
-               "0");
-  option_parser_register(opp, "-checkpoint_kernel", OPT_INT32, &checkpoint_kernel, 
-               " checkpointing during execution of which kernel (1- 1st kernel)",
-               "1");
-  option_parser_register(opp, "-checkpoint_CTA", OPT_INT32, &checkpoint_CTA, 
-               " checkpointing after # of CTA (< less than total CTA)",
-               "0");
-  option_parser_register(opp, "-resume_option", OPT_INT32, &resume_option, 
-               " resume flag (0 = no resume)",
-               "0");
-  option_parser_register(opp, "-resume_kernel", OPT_INT32, &resume_kernel, 
-               " Resume from which kernel (1= 1st kernel)",
-               "0");
-   option_parser_register(opp, "-resume_CTA", OPT_INT32, &resume_CTA, 
-               " resume from which CTA ",
-               "0");
-      option_parser_register(opp, "-checkpoint_CTA_t", OPT_INT32, &checkpoint_CTA_t, 
-               " resume from which CTA ",
-               "0");
-         option_parser_register(opp, "-checkpoint_insn_Y", OPT_INT32, &checkpoint_insn_Y, 
-               " resume from which CTA ",
-               "0");
-
-    option_parser_register(opp, "-gpgpu_ptx_convert_to_ptxplus", OPT_BOOL,
-                 &m_ptx_convert_to_ptxplus,
-                 "Convert SASS (native ISA) to ptxplus and run ptxplus",
-                 "0");
-    option_parser_register(opp, "-gpgpu_ptx_force_max_capability", OPT_UINT32,
-                 &m_ptx_force_max_capability,
-                 "Force maximum compute capability",
-                 "0");
-   option_parser_register(opp, "-gpgpu_ptx_inst_debug_to_file", OPT_BOOL, 
-                &g_ptx_inst_debug_to_file, 
-                "Dump executed instructions' debug information to file", 
-                "0");
-   option_parser_register(opp, "-gpgpu_ptx_inst_debug_file", OPT_CSTR, &g_ptx_inst_debug_file, 
-                  "Executed instructions' debug output file",
-                  "inst_debug.txt");
-   option_parser_register(opp, "-gpgpu_ptx_inst_debug_thread_uid", OPT_INT32, &g_ptx_inst_debug_thread_uid, 
-               "Thread UID for executed instructions' debug output", 
-               "1");
+  );
+  option_parser_register(opp, "-gpgpu_experimental_lib_support", OPT_BOOL,
+                         &m_experimental_lib_support,
+                         "Try to extract code from cuda libraries [Broken "
+                         "because of unknown cudaGetExportTable]",
+                         "0");
+  option_parser_register(opp, "-checkpoint_option", OPT_INT32,
+                         &checkpoint_option,
+                         " checkpointing flag (0 = no checkpoint)", "0");
+  option_parser_register(
+      opp, "-checkpoint_kernel", OPT_INT32, &checkpoint_kernel,
+      " checkpointing during execution of which kernel (1- 1st kernel)", "1");
+  option_parser_register(
+      opp, "-checkpoint_CTA", OPT_INT32, &checkpoint_CTA,
+      " checkpointing after # of CTA (< less than total CTA)", "0");
+  option_parser_register(opp, "-resume_option", OPT_INT32, &resume_option,
+                         " resume flag (0 = no resume)", "0");
+  option_parser_register(opp, "-resume_kernel", OPT_INT32, &resume_kernel,
+                         " Resume from which kernel (1= 1st kernel)", "0");
+  option_parser_register(opp, "-resume_CTA", OPT_INT32, &resume_CTA,
+                         " resume from which CTA ", "0");
+  option_parser_register(opp, "-checkpoint_CTA_t", OPT_INT32, &checkpoint_CTA_t,
+                         " resume from which CTA ", "0");
+  option_parser_register(opp, "-checkpoint_insn_Y", OPT_INT32,
+                         &checkpoint_insn_Y, " resume from which CTA ", "0");
+
+  option_parser_register(
+      opp, "-gpgpu_ptx_convert_to_ptxplus", OPT_BOOL, &m_ptx_convert_to_ptxplus,
+      "Convert SASS (native ISA) to ptxplus and run ptxplus", "0");
+  option_parser_register(opp, "-gpgpu_ptx_force_max_capability", OPT_UINT32,
+                         &m_ptx_force_max_capability,
+                         "Force maximum compute capability", "0");
+  option_parser_register(
+      opp, "-gpgpu_ptx_inst_debug_to_file", OPT_BOOL, &g_ptx_inst_debug_to_file,
+      "Dump executed instructions' debug information to file", "0");
+  option_parser_register(
+      opp, "-gpgpu_ptx_inst_debug_file", OPT_CSTR, &g_ptx_inst_debug_file,
+      "Executed instructions' debug output file", "inst_debug.txt");
+  option_parser_register(opp, "-gpgpu_ptx_inst_debug_thread_uid", OPT_INT32,
+                         &g_ptx_inst_debug_thread_uid,
+                         "Thread UID for executed instructions' debug output",
+                         "1");
 }
 
-void gpgpu_functional_sim_config::ptx_set_tex_cache_linesize(unsigned linesize)
-{
-   m_texcache_linesize = linesize;
+void gpgpu_functional_sim_config::ptx_set_tex_cache_linesize(
+    unsigned linesize) {
+  m_texcache_linesize = linesize;
 }
 
 // TODO schi gpgpu_t::gpgpu_t( const gpgpu_functional_sim_config &config )
 //    : m_function_model_config(config)
-gpgpu_t::gpgpu_t( const gpgpu_functional_sim_config &config, gem5::CudaGPU *cuda_gpu )
+gpgpu_t::gpgpu_t( const gpgpu_functional_sim_config &config, gpgpu_context *ctx, gem5::CudaGPU *cuda_gpu )
     : gem5CudaGPU(cuda_gpu), m_function_model_config(config)
 {
-   // m_global_mem = new memory_space_impl<8192>("global",64*1024);
-   m_global_mem = NULL; // Accesses to global memory should go through gem5-gpu
-   
-   m_tex_mem = new memory_space_impl<8192>("tex",64*1024);
-   m_surf_mem = new memory_space_impl<8192>("surf",64*1024);
-
-   m_dev_malloc=GLOBAL_HEAP_START; 
-   checkpoint_option = m_function_model_config.get_checkpoint_option();
-   checkpoint_kernel = m_function_model_config.get_checkpoint_kernel();
-   checkpoint_CTA = m_function_model_config.get_checkpoint_CTA();
-   resume_option = m_function_model_config.get_resume_option();
-   resume_kernel = m_function_model_config.get_resume_kernel();
-   resume_CTA = m_function_model_config.get_resume_CTA();
-   checkpoint_CTA_t = m_function_model_config.get_checkpoint_CTA_t();
-   checkpoint_insn_Y = m_function_model_config.get_checkpoint_insn_Y();
-
-   // initialize texture mappings to empty
-   m_NameToTextureInfo.clear();
-   m_NameToCudaArray.clear();
-   m_TextureRefToName.clear();
-   m_NameToAttribute.clear();
-
-   if(m_function_model_config.get_ptx_inst_debug_to_file() != 0) 
-      ptx_inst_debug_file = fopen(m_function_model_config.get_ptx_inst_debug_file(), "w");
+  gpgpu_ctx = ctx;
+  // m_global_mem = new memory_space_impl<8192>("global",64*1024);
+  m_global_mem = NULL; // Accesses to global memory should go through gem5-gpu
+
+  m_tex_mem = new memory_space_impl<8192>("tex", 64 * 1024);
+  m_surf_mem = new memory_space_impl<8192>("surf", 64 * 1024);
+
+  m_dev_malloc = GLOBAL_HEAP_START;
+  checkpoint_option = m_function_model_config.get_checkpoint_option();
+  checkpoint_kernel = m_function_model_config.get_checkpoint_kernel();
+  checkpoint_CTA = m_function_model_config.get_checkpoint_CTA();
+  resume_option = m_function_model_config.get_resume_option();
+  resume_kernel = m_function_model_config.get_resume_kernel();
+  resume_CTA = m_function_model_config.get_resume_CTA();
+  checkpoint_CTA_t = m_function_model_config.get_checkpoint_CTA_t();
+  checkpoint_insn_Y = m_function_model_config.get_checkpoint_insn_Y();
+
+  // initialize texture mappings to empty
+  m_NameToTextureInfo.clear();
+  m_NameToCudaArray.clear();
+  m_TextureRefToName.clear();
+  m_NameToAttribute.clear();
+
+  if (m_function_model_config.get_ptx_inst_debug_to_file() != 0)
+    ptx_inst_debug_file =
+        fopen(m_function_model_config.get_ptx_inst_debug_file(), "w");
+
+  gpu_sim_cycle = 0;
+  gpu_tot_sim_cycle = 0;
 }
 
-address_type line_size_based_tag_func(new_addr_type address, new_addr_type line_size)
-{
-   //gives the tag for an address based on a given line size
-   return address & ~(line_size-1);
+new_addr_type line_size_based_tag_func(new_addr_type address,
+                                       new_addr_type line_size) {
+  // gives the tag for an address based on a given line size
+  return address & ~(line_size - 1);
 }
 
-const char * mem_access_type_str(enum mem_access_type access_type)
-{
-   #define MA_TUP_BEGIN(X) static const char* access_type_str[] = {
-   #define MA_TUP(X) #X
-   #define MA_TUP_END(X) };
-   MEM_ACCESS_TYPE_TUP_DEF
-   #undef MA_TUP_BEGIN
-   #undef MA_TUP
-   #undef MA_TUP_END
+const char *mem_access_type_str(enum mem_access_type access_type) {
+#define MA_TUP_BEGIN(X) static const char *access_type_str[] = {
+#define MA_TUP(X) #X
+#define MA_TUP_END(X) \
+  }                   \
+  ;
+  MEM_ACCESS_TYPE_TUP_DEF
+#undef MA_TUP_BEGIN
+#undef MA_TUP
+#undef MA_TUP_END
 
-   assert(access_type < NUM_MEM_ACCESS_TYPE); 
+  assert(access_type < NUM_MEM_ACCESS_TYPE);
 
-   return access_type_str[access_type]; 
+  return access_type_str[access_type];
 }
 
-
-void warp_inst_t::clear_active( const active_mask_t &inactive ) {
-    active_mask_t test = m_warp_active_mask;
-    test &= inactive;
-    assert( test == inactive ); // verify threads being disabled were active
-    m_warp_active_mask &= ~inactive;
+void warp_inst_t::clear_active(const active_mask_t &inactive) {
+  active_mask_t test = m_warp_active_mask;
+  test &= inactive;
+  assert(test == inactive);  // verify threads being disabled were active
+  m_warp_active_mask &= ~inactive;
 }
 
-void warp_inst_t::set_not_active( unsigned lane_id ) {
-    m_warp_active_mask.reset(lane_id);
+void warp_inst_t::set_not_active(unsigned lane_id) {
+  m_warp_active_mask.reset(lane_id);
 }
 
-void warp_inst_t::set_active( const active_mask_t &active ) {
-   m_warp_active_mask = active;
-   if( m_isatomic ) {
-      for( unsigned i=0; i < m_config->warp_size; i++ ) {
-         if( !m_warp_active_mask.test(i) ) {
-             m_per_scalar_thread[i].callback.function = NULL;
-             m_per_scalar_thread[i].callback.instruction = NULL;
-             m_per_scalar_thread[i].callback.thread = NULL;
-         }
+void warp_inst_t::set_active(const active_mask_t &active) {
+  m_warp_active_mask = active;
+  if (m_isatomic) {
+    for (unsigned i = 0; i < m_config->warp_size; i++) {
+      if (!m_warp_active_mask.test(i)) {
+        m_per_scalar_thread[i].callback.function = NULL;
+        m_per_scalar_thread[i].callback.instruction = NULL;
+        m_per_scalar_thread[i].callback.thread = NULL;
       }
-   }
+    }
+  }
 }
 
 void warp_inst_t::do_atomic(bool forceDo) {
-    do_atomic( m_warp_active_mask,forceDo );
+  do_atomic(m_warp_active_mask, forceDo);
 }
 
-
-void warp_inst_t::do_atomic( const active_mask_t& access_mask,bool forceDo ) {
-    assert( m_isatomic && (!m_empty||forceDo) );
-    for( unsigned i=0; i < m_config->warp_size; i++ )
-    {
-        if( access_mask.test(i) )
-        {
-            dram_callback_t &cb = m_per_scalar_thread[i].callback;
-            if( cb.thread )
-                cb.function(cb.instruction, cb.thread);
-        }
+void warp_inst_t::do_atomic(const active_mask_t &access_mask, bool forceDo) {
+  assert(m_isatomic && (!m_empty || forceDo));
+  if (!should_do_atomic) return;
+  for (unsigned i = 0; i < m_config->warp_size; i++) {
+    if (access_mask.test(i)) {
+      dram_callback_t &cb = m_per_scalar_thread[i].callback;
+      if (cb.thread) cb.function(cb.instruction, cb.thread);
     }
+  }
 }
 
-void warp_inst_t::broadcast_barrier_reduction(const active_mask_t& access_mask)
-{
-	for( unsigned i=0; i < m_config->warp_size; i++ )
-    {
-        if( access_mask.test(i) )
-        {
-            dram_callback_t &cb = m_per_scalar_thread[i].callback;
-            if( cb.thread ){
-                cb.function(cb.instruction, cb.thread);
-            }
-        }
+void warp_inst_t::broadcast_barrier_reduction(
+    const active_mask_t &access_mask) {
+  for (unsigned i = 0; i < m_config->warp_size; i++) {
+    if (access_mask.test(i)) {
+      dram_callback_t &cb = m_per_scalar_thread[i].callback;
+      if (cb.thread) {
+        cb.function(cb.instruction, cb.thread);
+      }
     }
+  }
 }
 
-void warp_inst_t::generate_mem_accesses()
-{
-    if( empty() || op == MEMORY_BARRIER_OP || m_mem_accesses_created ) 
-        return;
-    if (!((op == LOAD_OP) || (op==TENSOR_CORE_LOAD_OP)   || (op == STORE_OP)||(op==TENSOR_CORE_STORE_OP)))
-        return; 
-    if( m_warp_active_mask.count() == 0 ) 
-        return; // predicated off
+void warp_inst_t::generate_mem_accesses() {
+  if (empty() || op == MEMORY_BARRIER_OP || m_mem_accesses_created) return;
+  if (!((op == LOAD_OP) || (op == TENSOR_CORE_LOAD_OP) || (op == STORE_OP) ||
+        (op == TENSOR_CORE_STORE_OP) ))
+    return;
+  if (m_warp_active_mask.count() == 0) return;  // predicated off
     // In gem5-gpu, global, const and local references go through the gem5-gpu LSQ
     if ( space.get_type() == global_space || space.get_type() == const_space || space.get_type() == local_space )
         return;
 
-    const size_t starting_queue_size = m_accessq.size();
+  const size_t starting_queue_size = m_accessq.size();
+
+  assert(is_load() || is_store());
 
-    assert( is_load() || is_store() );
-    assert( m_per_scalar_thread_valid ); // need address information per thread
+  //if((space.get_type() != tex_space) && (space.get_type() != const_space))
+    assert(m_per_scalar_thread_valid);  // need address information per thread
 
-    bool is_write = is_store();
+  bool is_write = is_store();
 
-    mem_access_type access_type;
-    switch (space.get_type()) {
+  mem_access_type access_type;
+  switch (space.get_type()) {
     case const_space:
     case param_space_kernel: 
-        access_type = CONST_ACC_R; 
-        break;
-    case tex_space: 
-        access_type = TEXTURE_ACC_R;   
-        break;
-    case global_space:       
-        access_type = is_write? GLOBAL_ACC_W: GLOBAL_ACC_R;   
-        break;
+      access_type = CONST_ACC_R;
+      break;
+    case tex_space:
+      access_type = TEXTURE_ACC_R;
+      break;
+    case global_space:
+      access_type = is_write ? GLOBAL_ACC_W : GLOBAL_ACC_R;
+      break;
     case local_space:
-    case param_space_local:  
-        access_type = is_write? LOCAL_ACC_W: LOCAL_ACC_R;   
-        break;
-    case shared_space: break;
-    case sstarr_space: break;
-    default: assert(0); break; 
-    }
+    case param_space_local:
+      access_type = is_write ? LOCAL_ACC_W : LOCAL_ACC_R;
+      break;
+    case shared_space:
+      break;
+    case sstarr_space:
+      break;
+    default:
+      assert(0);
+      break;
+  }
 
-    // Calculate memory accesses generated by this warp
-    new_addr_type cache_block_size = 0; // in bytes 
+  // Calculate memory accesses generated by this warp
+  new_addr_type cache_block_size = 0;  // in bytes
 
-    switch( space.get_type() ) {
+  switch (space.get_type()) {
     case shared_space:
     case sstarr_space: {
-        unsigned subwarp_size = m_config->warp_size / m_config->mem_warp_parts;
-        unsigned total_accesses=0;
-        for( unsigned subwarp=0; subwarp <  m_config->mem_warp_parts; subwarp++ ) {
-
-            // data structures used per part warp 
-            std::map<unsigned,std::map<new_addr_type,unsigned> > bank_accs; // bank -> word address -> access count
-
-            // step 1: compute accesses to words in banks
-            for( unsigned thread=subwarp*subwarp_size; thread < (subwarp+1)*subwarp_size; thread++ ) {
-                if( !active(thread) ) 
-                    continue;
-                new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];
-                //FIXME: deferred allocation of shared memory should not accumulate across kernel launches
-                //assert( addr < m_config->gpgpu_shmem_size ); 
-                unsigned bank = m_config->shmem_bank_func(addr);
-                new_addr_type word = line_size_based_tag_func(addr,m_config->WORD_SIZE);
-                bank_accs[bank][word]++;
-            }
+      unsigned subwarp_size = m_config->warp_size / m_config->mem_warp_parts;
+      unsigned total_accesses = 0;
+      for (unsigned subwarp = 0; subwarp < m_config->mem_warp_parts;
+           subwarp++) {
+        // data structures used per part warp
+        std::map<unsigned, std::map<new_addr_type, unsigned> >
+            bank_accs;  // bank -> word address -> access count
+
+        // step 1: compute accesses to words in banks
+        for (unsigned thread = subwarp * subwarp_size;
+             thread < (subwarp + 1) * subwarp_size; thread++) {
+          if (!active(thread)) continue;
+          new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];
+          // FIXME: deferred allocation of shared memory should not accumulate
+          // across kernel launches assert( addr < m_config->gpgpu_shmem_size );
+          unsigned bank = m_config->shmem_bank_func(addr);
+          new_addr_type word =
+              line_size_based_tag_func(addr, m_config->WORD_SIZE);
+          bank_accs[bank][word]++;
+        }
 
-            if (m_config->shmem_limited_broadcast) {
-                // step 2: look for and select a broadcast bank/word if one occurs
-                bool broadcast_detected = false;
-                new_addr_type broadcast_word=(new_addr_type)-1;
-                unsigned broadcast_bank=(unsigned)-1;
-                std::map<unsigned,std::map<new_addr_type,unsigned> >::iterator b;
-                for( b=bank_accs.begin(); b != bank_accs.end(); b++ ) {
-                    unsigned bank = b->first;
-                    std::map<new_addr_type,unsigned> &access_set = b->second;
-                    std::map<new_addr_type,unsigned>::iterator w;
-                    for( w=access_set.begin(); w != access_set.end(); ++w ) {
-                        if( w->second > 1 ) {
-                            // found a broadcast
-                            broadcast_detected=true;
-                            broadcast_bank=bank;
-                            broadcast_word=w->first;
-                            break;
-                        }
-                    }
-                    if( broadcast_detected ) 
-                        break;
-                }
+        if (m_config->shmem_limited_broadcast) {
+          // step 2: look for and select a broadcast bank/word if one occurs
+          bool broadcast_detected = false;
+          new_addr_type broadcast_word = (new_addr_type)-1;
+          unsigned broadcast_bank = (unsigned)-1;
+          std::map<unsigned, std::map<new_addr_type, unsigned> >::iterator b;
+          for (b = bank_accs.begin(); b != bank_accs.end(); b++) {
+            unsigned bank = b->first;
+            std::map<new_addr_type, unsigned> &access_set = b->second;
+            std::map<new_addr_type, unsigned>::iterator w;
+            for (w = access_set.begin(); w != access_set.end(); ++w) {
+              if (w->second > 1) {
+                // found a broadcast
+                broadcast_detected = true;
+                broadcast_bank = bank;
+                broadcast_word = w->first;
+                break;
+              }
+            }
+            if (broadcast_detected) break;
+          }
             
-                // step 3: figure out max bank accesses performed, taking account of broadcast case
-                unsigned max_bank_accesses=0;
-                for( b=bank_accs.begin(); b != bank_accs.end(); b++ ) {
-                    unsigned bank_accesses=0;
-                    std::map<new_addr_type,unsigned> &access_set = b->second;
-                    std::map<new_addr_type,unsigned>::iterator w;
-                    for( w=access_set.begin(); w != access_set.end(); ++w ) 
-                        bank_accesses += w->second;
-                    if( broadcast_detected && broadcast_bank == b->first ) {
-                        for( w=access_set.begin(); w != access_set.end(); ++w ) {
-                            if( w->first == broadcast_word ) {
-                                unsigned n = w->second;
-                                assert(n > 1); // or this wasn't a broadcast
-                                assert(bank_accesses >= (n-1));
-                                bank_accesses -= (n-1);
-                                break;
-                            }
-                        }
-                    }
-                    if( bank_accesses > max_bank_accesses ) 
-                        max_bank_accesses = bank_accesses;
+          // step 3: figure out max bank accesses performed, taking account of
+          // broadcast case
+          unsigned max_bank_accesses = 0;
+          for (b = bank_accs.begin(); b != bank_accs.end(); b++) {
+            unsigned bank_accesses = 0;
+            std::map<new_addr_type, unsigned> &access_set = b->second;
+            std::map<new_addr_type, unsigned>::iterator w;
+            for (w = access_set.begin(); w != access_set.end(); ++w)
+              bank_accesses += w->second;
+            if (broadcast_detected && broadcast_bank == b->first) {
+              for (w = access_set.begin(); w != access_set.end(); ++w) {
+                if (w->first == broadcast_word) {
+                  unsigned n = w->second;
+                  assert(n > 1);  // or this wasn't a broadcast
+                  assert(bank_accesses >= (n - 1));
+                  bank_accesses -= (n - 1);
+                  break;
                 }
-
-                // step 4: accumulate
-                total_accesses+= max_bank_accesses;
-            } else {
-                // step 2: look for the bank with the maximum number of access to different words 
-                unsigned max_bank_accesses=0;
-                std::map<unsigned,std::map<new_addr_type,unsigned> >::iterator b;
-                for( b=bank_accs.begin(); b != bank_accs.end(); b++ ) {
-                    max_bank_accesses = std::max(max_bank_accesses, (unsigned)b->second.size());
-                }
-
-                // step 3: accumulate
-                total_accesses+= max_bank_accesses;
+              }
             }
+            if (bank_accesses > max_bank_accesses)
+              max_bank_accesses = bank_accesses;
+          }
+
+          // step 4: accumulate
+          total_accesses += max_bank_accesses;
+        } else {
+          // step 2: look for the bank with the maximum number of access to
+          // different words
+          unsigned max_bank_accesses = 0;
+          std::map<unsigned, std::map<new_addr_type, unsigned> >::iterator b;
+          for (b = bank_accs.begin(); b != bank_accs.end(); b++) {
+            max_bank_accesses =
+                std::max(max_bank_accesses, (unsigned)b->second.size());
+          }
+
+          // step 3: accumulate
+          total_accesses += max_bank_accesses;
         }
-        assert( total_accesses > 0 && total_accesses <= m_config->warp_size );
-        cycles = total_accesses * g_the_gpu->sharedMemDelay; // shared memory conflicts modeled as larger initiation interval 
-        ptx_file_line_stats_add_smem_bank_conflict( pc, total_accesses );
-        break;
+      }
+      assert(total_accesses > 0 && total_accesses <= m_config->warp_size);
+      cycles = total_accesses;  // shared memory conflicts modeled as larger
+                                // initiation interval
+      m_config->gpgpu_ctx->stats->ptx_file_line_stats_add_smem_bank_conflict(
+          pc, total_accesses);
+      break;
     }
 
-    case tex_space: 
-        cache_block_size = m_config->gpgpu_cache_texl1_linesize;
-        break;
-    case const_space:  case param_space_kernel:
-        cache_block_size = m_config->gpgpu_cache_constl1_linesize; 
-        break;
-
-    case global_space: case local_space: case param_space_local:
-    	 if( m_config->gpgpu_coalesce_arch >= 13) {
-            if(isatomic())
-                memory_coalescing_arch_atomic(is_write, access_type);
-            else
-                memory_coalescing_arch(is_write, access_type);
-         } else abort();
-
-        break;
+    case tex_space:
+      cache_block_size = m_config->gpgpu_cache_texl1_linesize;
+      break;
+    case const_space:
+    case param_space_kernel:
+      cache_block_size = m_config->gpgpu_cache_constl1_linesize;
+      break;
 
-    default:
+    case global_space:
+    case local_space:
+    case param_space_local:
+      if (m_config->gpgpu_coalesce_arch >= 13) {
+        if (isatomic())
+          memory_coalescing_arch_atomic(is_write, access_type);
+        else
+          memory_coalescing_arch(is_write, access_type);
+      } else
         abort();
-    }
 
-    if( cache_block_size ) {
-        assert( m_accessq.empty() );
-        mem_access_byte_mask_t byte_mask; 
-        std::map<new_addr_type,active_mask_t> accesses; // block address -> set of thread offsets in warp
-        std::map<new_addr_type,active_mask_t>::iterator a;
-        for( unsigned thread=0; thread < m_config->warp_size; thread++ ) {
-            if( !active(thread) ) 
-                continue;
-            new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];
-            unsigned block_address = line_size_based_tag_func(addr,cache_block_size);
-            accesses[block_address].set(thread);
-            unsigned idx = addr-block_address; 
-            for( unsigned i=0; i < data_size; i++ ) 
-                byte_mask.set(idx+i);
-        }
-        for( a=accesses.begin(); a != accesses.end(); ++a ) 
-            m_accessq.push_back( mem_access_t(access_type,a->first,cache_block_size,is_write,a->second, byte_mask, mem_access_sector_mask_t()));
-    }
+      break;
 
-    if ( space.get_type() == global_space ) {
-        ptx_file_line_stats_add_uncoalesced_gmem( pc, m_accessq.size() - starting_queue_size );
+    default:
+      abort();
+  }
+
+  if (cache_block_size) {
+    assert(m_accessq.empty());
+    mem_access_byte_mask_t byte_mask;
+    std::map<new_addr_type, active_mask_t>
+        accesses;  // block address -> set of thread offsets in warp
+    std::map<new_addr_type, active_mask_t>::iterator a;
+    for (unsigned thread = 0; thread < m_config->warp_size; thread++) {
+      if (!active(thread)) continue;
+      new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];
+      new_addr_type block_address =
+          line_size_based_tag_func(addr, cache_block_size);
+      accesses[block_address].set(thread);
+      unsigned idx = addr - block_address;
+      for (unsigned i = 0; i < data_size; i++) byte_mask.set(idx + i);
     }
-    m_mem_accesses_created=true;
+    for (a = accesses.begin(); a != accesses.end(); ++a)
+      m_accessq.push_back(mem_access_t(
+          access_type, a->first, cache_block_size, is_write, a->second,
+          byte_mask, mem_access_sector_mask_t(), m_config->gpgpu_ctx));
+  }
+
+  if (space.get_type() == global_space) {
+    m_config->gpgpu_ctx->stats->ptx_file_line_stats_add_uncoalesced_gmem(
+        pc, m_accessq.size() - starting_queue_size);
+  }
+  m_mem_accesses_created = true;
 }
 
-void warp_inst_t::memory_coalescing_arch( bool is_write, mem_access_type access_type )
-{
-    // see the CUDA manual where it discusses coalescing rules before reading this
-    unsigned segment_size = 0;
-    unsigned warp_parts = m_config->mem_warp_parts;
-    bool sector_segment_size = false;
-
-    if(m_config->gpgpu_coalesce_arch >= 20 && m_config->gpgpu_coalesce_arch < 39)
-    {
-    	//Fermi and Kepler, L1 is normal and L2 is sector
-    	if(m_config->gmem_skip_L1D || cache_op == CACHE_GLOBAL)
-    		sector_segment_size = true;
-    	else
-    		sector_segment_size = false;
-    }
-    else if(m_config->gpgpu_coalesce_arch >= 40)
-    {
-    	//Maxwell, Pascal and Volta, L1 and L2 are sectors
-    	//all requests should be 32 bytes
-    	sector_segment_size = true;
-    }
-
-    switch( data_size ) {
-    case 1: segment_size = 32; break;
-    case 2: segment_size = sector_segment_size? 32 : 64; break;
-    case 4: case 8: case 16: segment_size = sector_segment_size? 32 : 128; break;
-    }
-    unsigned subwarp_size = m_config->warp_size / warp_parts;
-
-    for( unsigned subwarp=0; subwarp <  warp_parts; subwarp++ ) {
-        std::map<new_addr_type,transaction_info> subwarp_transactions;
-
-        // step 1: find all transactions generated by this subwarp
-        for( unsigned thread=subwarp*subwarp_size; thread<subwarp_size*(subwarp+1); thread++ ) {
-            if( !active(thread) )
-                continue;
-
-            unsigned data_size_coales = data_size;
-            unsigned num_accesses = 1;
-
-            if( space.get_type() == local_space || space.get_type() == param_space_local ) {
-               // Local memory accesses >4B were split into 4B chunks
-               if(data_size >= 4) {
-                  data_size_coales = 4;
-                  num_accesses = data_size/4;
-               }
-               // Otherwise keep the same data_size for sub-4B access to local memory
-            }
-
-
-            assert(num_accesses <= MAX_ACCESSES_PER_INSN_PER_THREAD);
-
-//            for(unsigned access=0; access<num_accesses; access++) {
-            for(unsigned access=0; (access<MAX_ACCESSES_PER_INSN_PER_THREAD)&&(m_per_scalar_thread[thread].memreqaddr[access]!=0); access++) {
-                new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[access];
-                unsigned block_address = line_size_based_tag_func(addr,segment_size);
-                unsigned chunk = (addr&127)/32; // which 32-byte chunk within in a 128-byte chunk does this thread access?
-                transaction_info &info = subwarp_transactions[block_address];
-
-                // can only write to one segment
-                assert(block_address == line_size_based_tag_func(addr+data_size_coales-1,segment_size));
-
-                info.chunks.set(chunk);
-                info.active.set(thread);
-                unsigned idx = (addr&127);
-                for( unsigned i=0; i < data_size_coales; i++ )
-                    info.bytes.set(idx+i);
-            }
+void warp_inst_t::memory_coalescing_arch(bool is_write,
+                                         mem_access_type access_type) {
+  // see the CUDA manual where it discusses coalescing rules before reading this
+  unsigned segment_size = 0;
+  unsigned warp_parts = m_config->mem_warp_parts;
+  bool sector_segment_size = false;
+
+  if (m_config->gpgpu_coalesce_arch >= 20 &&
+      m_config->gpgpu_coalesce_arch < 39) {
+    // Fermi and Kepler, L1 is normal and L2 is sector
+    if (m_config->gmem_skip_L1D || cache_op == CACHE_GLOBAL)
+      sector_segment_size = true;
+    else
+      sector_segment_size = false;
+  } else if (m_config->gpgpu_coalesce_arch >= 40) {
+    // Maxwell, Pascal and Volta, L1 and L2 are sectors
+    // all requests should be 32 bytes
+    sector_segment_size = true;
+  }
+
+  switch (data_size) {
+    case 1:
+      segment_size = 32;
+      break;
+    case 2:
+      segment_size = sector_segment_size ? 32 : 64;
+      break;
+    case 4:
+    case 8:
+    case 16:
+      segment_size = sector_segment_size ? 32 : 128;
+      break;
+  }
+  unsigned subwarp_size = m_config->warp_size / warp_parts;
+
+  for (unsigned subwarp = 0; subwarp < warp_parts; subwarp++) {
+    std::map<new_addr_type, transaction_info> subwarp_transactions;
+
+    // step 1: find all transactions generated by this subwarp
+    for (unsigned thread = subwarp * subwarp_size;
+         thread < subwarp_size * (subwarp + 1); thread++) {
+      if (!active(thread)) continue;
+
+      unsigned data_size_coales = data_size;
+      unsigned num_accesses = 1;
+
+      if (space.get_type() == local_space ||
+          space.get_type() == param_space_local) {
+        // Local memory accesses >4B were split into 4B chunks
+        if (data_size >= 4) {
+          data_size_coales = 4;
+          num_accesses = data_size / 4;
         }
+        // Otherwise keep the same data_size for sub-4B access to local memory
+      }
 
-        // step 2: reduce each transaction size, if possible
-        std::map< new_addr_type, transaction_info >::iterator t;
-        for( t=subwarp_transactions.begin(); t !=subwarp_transactions.end(); t++ ) {
-            new_addr_type addr = t->first;
-            const transaction_info &info = t->second;
-
-            memory_coalescing_arch_reduce_and_send(is_write, access_type, info, addr, segment_size);
-
+      assert(num_accesses <= MAX_ACCESSES_PER_INSN_PER_THREAD);
+
+      //            for(unsigned access=0; access<num_accesses; access++) {
+      for (unsigned access = 0;
+           (access < MAX_ACCESSES_PER_INSN_PER_THREAD) &&
+           (m_per_scalar_thread[thread].memreqaddr[access] != 0);
+           access++) {
+        new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[access];
+        new_addr_type block_address =
+            line_size_based_tag_func(addr, segment_size);
+        unsigned chunk =
+            (addr & 127) / 32;  // which 32-byte chunk within in a 128-byte
+                                // chunk does this thread access?
+        transaction_info &info = subwarp_transactions[block_address];
+
+        // can only write to one segment
+        // it seems like in trace driven, a thread can write to more than one
+        // segment assert(block_address ==
+        // line_size_based_tag_func(addr+data_size_coales-1,segment_size));
+
+        info.chunks.set(chunk);
+        info.active.set(thread);
+        unsigned idx = (addr & 127);
+        for (unsigned i = 0; i < data_size_coales; i++)
+          if ((idx + i) < MAX_MEMORY_ACCESS_SIZE) info.bytes.set(idx + i);
+
+        // it seems like in trace driven, a thread can write to more than one
+        // segment handle this special case
+        if (block_address != line_size_based_tag_func(
+                                 addr + data_size_coales - 1, segment_size)) {
+          addr = addr + data_size_coales - 1;
+          new_addr_type block_address =
+              line_size_based_tag_func(addr, segment_size);
+          unsigned chunk = (addr & 127) / 32;
+          transaction_info &info = subwarp_transactions[block_address];
+          info.chunks.set(chunk);
+          info.active.set(thread);
+          unsigned idx = (addr & 127);
+          for (unsigned i = 0; i < data_size_coales; i++)
+            if ((idx + i) < MAX_MEMORY_ACCESS_SIZE) info.bytes.set(idx + i);
         }
+      }
     }
-}
-
-void warp_inst_t::memory_coalescing_arch_atomic( bool is_write, mem_access_type access_type )
-{
 
-   assert(space.get_type() == global_space); // Atomics allowed only for global memory
+    // step 2: reduce each transaction size, if possible
+    std::map<new_addr_type, transaction_info>::iterator t;
+    for (t = subwarp_transactions.begin(); t != subwarp_transactions.end();
+         t++) {
+      new_addr_type addr = t->first;
+      const transaction_info &info = t->second;
 
-   // see the CUDA manual where it discusses coalescing rules before reading this
-   unsigned segment_size = 0;
-   unsigned warp_parts = m_config->mem_warp_parts;
-   bool sector_segment_size = false;
+      memory_coalescing_arch_reduce_and_send(is_write, access_type, info, addr,
+                                             segment_size);
+    }
+  }
+}
 
-   if(m_config->gpgpu_coalesce_arch >= 20 && m_config->gpgpu_coalesce_arch < 39)
-   {
-	//Fermi and Kepler, L1 is normal and L2 is sector
-	if(m_config->gmem_skip_L1D || cache_op == CACHE_GLOBAL)
-		sector_segment_size = true;
-	else
-		sector_segment_size = false;
-   }
-   else if(m_config->gpgpu_coalesce_arch >= 40)
-   {
-	//Maxwell, Pascal and Volta, L1 and L2 are sectors
-	//all requests should be 32 bytes
-	sector_segment_size = true;
-   }
+void warp_inst_t::memory_coalescing_arch_atomic(bool is_write,
+                                                mem_access_type access_type) {
+  assert(space.get_type() ==
+         global_space);  // Atomics allowed only for global memory
+
+  // see the CUDA manual where it discusses coalescing rules before reading this
+  unsigned segment_size = 0;
+  unsigned warp_parts = m_config->mem_warp_parts;
+  bool sector_segment_size = false;
+
+  if (m_config->gpgpu_coalesce_arch >= 20 &&
+      m_config->gpgpu_coalesce_arch < 39) {
+    // Fermi and Kepler, L1 is normal and L2 is sector
+    if (m_config->gmem_skip_L1D || cache_op == CACHE_GLOBAL)
+      sector_segment_size = true;
+    else
+      sector_segment_size = false;
+  } else if (m_config->gpgpu_coalesce_arch >= 40) {
+    // Maxwell, Pascal and Volta, L1 and L2 are sectors
+    // all requests should be 32 bytes
+    sector_segment_size = true;
+  }
 
    switch( data_size ) {
    case 1: segment_size = 32; break;
    case 2: segment_size = sector_segment_size? 32 : 64; break;
    case 4: case 8: case 16: segment_size = sector_segment_size? 32 : 128; break;
    }
-   unsigned subwarp_size = m_config->warp_size / warp_parts;
-
-   for( unsigned subwarp=0; subwarp <  warp_parts; subwarp++ ) {
-       std::map<new_addr_type,std::list<transaction_info> > subwarp_transactions; // each block addr maps to a list of transactions
-
-       // step 1: find all transactions generated by this subwarp
-       for( unsigned thread=subwarp*subwarp_size; thread<subwarp_size*(subwarp+1); thread++ ) {
-           if( !active(thread) )
-               continue;
-
-           new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];
-           unsigned block_address = line_size_based_tag_func(addr,segment_size);
-           unsigned chunk = (addr&127)/32; // which 32-byte chunk within in a 128-byte chunk does this thread access?
-
-           // can only write to one segment
-           assert(block_address == line_size_based_tag_func(addr+data_size-1,segment_size));
-
-           // Find a transaction that does not conflict with this thread's accesses
-           bool new_transaction = true;
-           std::list<transaction_info>::iterator it;
-           transaction_info* info;
-           for(it=subwarp_transactions[block_address].begin(); it!=subwarp_transactions[block_address].end(); it++) {
-              unsigned idx = (addr&127);
-              if(not it->test_bytes(idx,idx+data_size-1)) {
-                 new_transaction = false;
-                 info = &(*it);
-                 break;
-              }
-           }
-           if(new_transaction) {
-              // Need a new transaction
-              subwarp_transactions[block_address].push_back(transaction_info());
-              info = &subwarp_transactions[block_address].back();
-           }
-           assert(info);
-
-           info->chunks.set(chunk);
-           info->active.set(thread);
-           unsigned idx = (addr&127);
-           for( unsigned i=0; i < data_size; i++ ) {
-               assert(!info->bytes.test(idx+i));
-               info->bytes.set(idx+i);
-           }
-       }
-
-       // step 2: reduce each transaction size, if possible
-       std::map< new_addr_type, std::list<transaction_info> >::iterator t_list;
-       for( t_list=subwarp_transactions.begin(); t_list !=subwarp_transactions.end(); t_list++ ) {
-           // For each block addr
-           new_addr_type addr = t_list->first;
-           const std::list<transaction_info>& transaction_list = t_list->second;
-
-           std::list<transaction_info>::const_iterator t;
-           for(t=transaction_list.begin(); t!=transaction_list.end(); t++) {
-               // For each transaction
-               const transaction_info &info = *t;
-               memory_coalescing_arch_reduce_and_send(is_write, access_type, info, addr, segment_size);
-           }
-       }
-   }
+  unsigned subwarp_size = m_config->warp_size / warp_parts;
+
+  for (unsigned subwarp = 0; subwarp < warp_parts; subwarp++) {
+    std::map<new_addr_type, std::list<transaction_info> >
+        subwarp_transactions;  // each block addr maps to a list of transactions
+
+    // step 1: find all transactions generated by this subwarp
+    for (unsigned thread = subwarp * subwarp_size;
+         thread < subwarp_size * (subwarp + 1); thread++) {
+      if (!active(thread)) continue;
+
+      new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];
+      new_addr_type block_address =
+          line_size_based_tag_func(addr, segment_size);
+      unsigned chunk =
+          (addr & 127) / 32;  // which 32-byte chunk within in a 128-byte chunk
+                              // does this thread access?
+
+      // can only write to one segment
+      assert(block_address ==
+             line_size_based_tag_func(addr + data_size - 1, segment_size));
+
+      // Find a transaction that does not conflict with this thread's accesses
+      bool new_transaction = true;
+      std::list<transaction_info>::iterator it;
+      transaction_info *info;
+      for (it = subwarp_transactions[block_address].begin();
+           it != subwarp_transactions[block_address].end(); it++) {
+        unsigned idx = (addr & 127);
+        if (not it->test_bytes(idx, idx + data_size - 1)) {
+          new_transaction = false;
+          info = &(*it);
+          break;
+        }
+      }
+      if (new_transaction) {
+        // Need a new transaction
+        subwarp_transactions[block_address].push_back(transaction_info());
+        info = &subwarp_transactions[block_address].back();
+      }
+      assert(info);
+
+      info->chunks.set(chunk);
+      info->active.set(thread);
+      unsigned idx = (addr & 127);
+      for (unsigned i = 0; i < data_size; i++) {
+        assert(!info->bytes.test(idx + i));
+        info->bytes.set(idx + i);
+      }
+    }
+
+    // step 2: reduce each transaction size, if possible
+    std::map<new_addr_type, std::list<transaction_info> >::iterator t_list;
+    for (t_list = subwarp_transactions.begin();
+         t_list != subwarp_transactions.end(); t_list++) {
+      // For each block addr
+      new_addr_type addr = t_list->first;
+      const std::list<transaction_info> &transaction_list = t_list->second;
+
+      std::list<transaction_info>::const_iterator t;
+      for (t = transaction_list.begin(); t != transaction_list.end(); t++) {
+        // For each transaction
+        const transaction_info &info = *t;
+        memory_coalescing_arch_reduce_and_send(is_write, access_type, info,
+                                               addr, segment_size);
+      }
+    }
+  }
 }
 
-void warp_inst_t::memory_coalescing_arch_reduce_and_send( bool is_write, mem_access_type access_type, const transaction_info &info, new_addr_type addr, unsigned segment_size )
-{
-   assert( (addr & (segment_size-1)) == 0 );
-
-   const std::bitset<4> &q = info.chunks;
-   assert( q.count() >= 1 );
-   std::bitset<2> h; // halves (used to check if 64 byte segment can be compressed into a single 32 byte segment)
-
-   unsigned size=segment_size;
-   if( segment_size == 128 ) {
-       bool lower_half_used = q[0] || q[1];
-       bool upper_half_used = q[2] || q[3];
-       if( lower_half_used && !upper_half_used ) {
-           // only lower 64 bytes used
-           size = 64;
-           if(q[0]) h.set(0);
-           if(q[1]) h.set(1);
-       } else if ( (!lower_half_used) && upper_half_used ) {
-           // only upper 64 bytes used
-           addr = addr+64;
-           size = 64;
-           if(q[2]) h.set(0);
-           if(q[3]) h.set(1);
-       } else {
-           assert(lower_half_used && upper_half_used);
-       }
-   } else if( segment_size == 64 ) {
-       // need to set halves
-       if( (addr % 128) == 0 ) {
-           if(q[0]) h.set(0);
-           if(q[1]) h.set(1);
-       } else {
-           assert( (addr % 128) == 64 );
-           if(q[2]) h.set(0);
-           if(q[3]) h.set(1);
-       }
-   }
-   if( size == 64 ) {
-       bool lower_half_used = h[0];
-       bool upper_half_used = h[1];
-       if( lower_half_used && !upper_half_used ) {
-           size = 32;
-       } else if ( (!lower_half_used) && upper_half_used ) {
-           addr = addr+32;
-           size = 32;
-       } else {
-           assert(lower_half_used && upper_half_used);
-       }
-   }
-   m_accessq.push_back( mem_access_t(access_type,addr,size,is_write,info.active,info.bytes, info.chunks) );
+void warp_inst_t::memory_coalescing_arch_reduce_and_send(
+    bool is_write, mem_access_type access_type, const transaction_info &info,
+    new_addr_type addr, unsigned segment_size) {
+  assert((addr & (segment_size - 1)) == 0);
+
+  const std::bitset<4> &q = info.chunks;
+  assert(q.count() >= 1);
+  std::bitset<2> h;  // halves (used to check if 64 byte segment can be
+                     // compressed into a single 32 byte segment)
+
+  unsigned size = segment_size;
+  if (segment_size == 128) {
+    bool lower_half_used = q[0] || q[1];
+    bool upper_half_used = q[2] || q[3];
+    if (lower_half_used && !upper_half_used) {
+      // only lower 64 bytes used
+      size = 64;
+      if (q[0]) h.set(0);
+      if (q[1]) h.set(1);
+    } else if ((!lower_half_used) && upper_half_used) {
+      // only upper 64 bytes used
+      addr = addr + 64;
+      size = 64;
+      if (q[2]) h.set(0);
+      if (q[3]) h.set(1);
+    } else {
+      assert(lower_half_used && upper_half_used);
+    }
+  } else if (segment_size == 64) {
+    // need to set halves
+    if ((addr % 128) == 0) {
+      if (q[0]) h.set(0);
+      if (q[1]) h.set(1);
+    } else {
+      assert((addr % 128) == 64);
+      if (q[2]) h.set(0);
+      if (q[3]) h.set(1);
+    }
+  }
+  if (size == 64) {
+    bool lower_half_used = h[0];
+    bool upper_half_used = h[1];
+    if (lower_half_used && !upper_half_used) {
+      size = 32;
+    } else if ((!lower_half_used) && upper_half_used) {
+      addr = addr + 32;
+      size = 32;
+    } else {
+      assert(lower_half_used && upper_half_used);
+    }
+  }
+  m_accessq.push_back(mem_access_t(access_type, addr, size, is_write,
+                                   info.active, info.bytes, info.chunks,
+                                   m_config->gpgpu_ctx));
 }
 
-void warp_inst_t::completed( unsigned long long cycle ) const 
-{
-   unsigned long long latency = cycle - issue_cycle; 
-   assert(latency <= cycle); // underflow detection 
-   ptx_file_line_stats_add_latency(pc, latency * active_count());  
+void warp_inst_t::completed(unsigned long long cycle) const {
+  unsigned long long latency = cycle - issue_cycle;
+  assert(latency <= cycle);  // underflow detection
+  m_config->gpgpu_ctx->stats->ptx_file_line_stats_add_latency(
+      pc, latency * active_count());
+}
+
+kernel_info_t::kernel_info_t(dim3 gridDim, dim3 blockDim,
+                             class function_info *entry) {
+  m_kernel_entry = entry;
+  m_grid_dim = gridDim;
+  m_block_dim = blockDim;
+  m_next_cta.x = 0;
+  m_next_cta.y = 0;
+  m_next_cta.z = 0;
+  m_next_tid = m_next_cta;
+  m_num_cores_running = 0;
+  m_uid = (entry->gpgpu_ctx->kernel_info_m_next_uid)++;
+  m_param_mem = new memory_space_impl<8192>("param", 64 * 1024);
+
+  // Jin: parent and child kernel management for CDP
+  m_parent_kernel = NULL;
+
+  // Jin: launch latency management
+  m_launch_latency = entry->gpgpu_ctx->device_runtime->g_kernel_launch_latency;
+
+  //    entry->gpgpu_ctx->device_runtime->g_kernel_launch_latency +
+  m_kernel_TB_latency =
+      num_blocks() * entry->gpgpu_ctx->device_runtime->g_TB_launch_latency;
+
+  cache_config_set = false;
 }
 
 //Jin: CDP support
-bool g_cdp_enabled;
-unsigned g_kernel_launch_latency;
+// bool g_cdp_enabled;
+// unsigned g_kernel_launch_latency;
 
-unsigned kernel_info_t::m_next_uid = 1;
+// unsigned kernel_info_t::m_next_uid = 1;
 
 /*A snapshot of the texture mappings needs to be stored in the kernel's info as 
 kernels should use the texture bindings seen at the time of launch and textures
  can be bound/unbound asynchronously with respect to streams. */
-kernel_info_t::kernel_info_t( dim3 gridDim, dim3 blockDim, class function_info *entry, std::map<std::string, const struct cudaArray*> nameToCudaArray, std::map<std::string, const struct textureInfo*> nameToTextureInfo)   
-{
-    m_kernel_entry=entry;
-    m_grid_dim=gridDim;
-    m_block_dim=blockDim;
-    m_next_cta.x=0;
-    m_next_cta.y=0;
-    m_next_cta.z=0;
-    m_next_tid=m_next_cta;
-    m_num_cores_running=0;
-    m_uid = m_next_uid++;
-    m_param_mem = new memory_space_impl<8192>("param",64*1024);
-
-    //Jin: parent and child kernel management for CDP
-    m_parent_kernel = NULL;
-   
-    //Jin: launch latency management
-    m_launch_latency = g_kernel_launch_latency;
-
-    volta_cache_config_set=false;
-    m_NameToCudaArray = nameToCudaArray;
-    m_NameToTextureInfo = nameToTextureInfo;
+kernel_info_t::kernel_info_t(
+    dim3 gridDim, dim3 blockDim, class function_info *entry,
+    std::map<std::string, const struct cudaArray *> nameToCudaArray,
+    std::map<std::string, const struct textureInfo *> nameToTextureInfo) {
+  m_kernel_entry = entry;
+  m_grid_dim = gridDim;
+  m_block_dim = blockDim;
+  m_next_cta.x = 0;
+  m_next_cta.y = 0;
+  m_next_cta.z = 0;
+  m_next_tid = m_next_cta;
+  m_num_cores_running = 0;
+  m_uid = (entry->gpgpu_ctx->kernel_info_m_next_uid)++;
+  m_param_mem = new memory_space_impl<8192>("param", 64 * 1024);
+
+  // Jin: parent and child kernel management for CDP
+  m_parent_kernel = NULL;
+
+  // Jin: launch latency management
+  m_launch_latency = entry->gpgpu_ctx->device_runtime->g_kernel_launch_latency;
+
+  //    entry->gpgpu_ctx->device_runtime->g_kernel_launch_latency +
+  m_kernel_TB_latency =
+      num_blocks() * entry->gpgpu_ctx->device_runtime->g_TB_launch_latency;
+
+  cache_config_set = false;
+  m_NameToCudaArray = nameToCudaArray;
+  m_NameToTextureInfo = nameToTextureInfo;
 }
 
-kernel_info_t::~kernel_info_t()
-{
-    assert( m_active_threads.empty() );
-    destroy_cta_streams();
-    delete m_param_mem;
+kernel_info_t::~kernel_info_t() {
+  assert(m_active_threads.empty());
+  destroy_cta_streams();
+  delete m_param_mem;
 }
 
-std::string kernel_info_t::name() const
-{
-    return m_kernel_entry->get_name();
-}
+std::string kernel_info_t::name() const { return m_kernel_entry->get_name(); }
 
-//Jin: parent and child kernel management for CDP
-void kernel_info_t::set_parent(kernel_info_t * parent, 
-    dim3 parent_ctaid, dim3 parent_tid) {
-    m_parent_kernel = parent;
-    m_parent_ctaid = parent_ctaid;
-    m_parent_tid = parent_tid;
-    parent->set_child(this);
+// Jin: parent and child kernel management for CDP
+void kernel_info_t::set_parent(kernel_info_t *parent, dim3 parent_ctaid,
+                               dim3 parent_tid) {
+  m_parent_kernel = parent;
+  m_parent_ctaid = parent_ctaid;
+  m_parent_tid = parent_tid;
+  parent->set_child(this);
 }
 
-void kernel_info_t::set_child(kernel_info_t * child) {
-    m_child_kernels.push_back(child);
+void kernel_info_t::set_child(kernel_info_t *child) {
+  m_child_kernels.push_back(child);
 }
 
-void kernel_info_t::remove_child(kernel_info_t * child) {
-    assert(std::find(m_child_kernels.begin(), m_child_kernels.end(), child)
-        != m_child_kernels.end());
-    m_child_kernels.remove(child);
+void kernel_info_t::remove_child(kernel_info_t *child) {
+  assert(std::find(m_child_kernels.begin(), m_child_kernels.end(), child) !=
+         m_child_kernels.end());
+  m_child_kernels.remove(child);
 }
 
 bool kernel_info_t::is_finished() {
-  if(done() && children_all_finished())
-     return true;
+  if (done() && children_all_finished())
+    return true;
   else
-     return false;
+    return false;
 }
 
 bool kernel_info_t::children_all_finished() {
-   if(!m_child_kernels.empty())
-         return false;
-   
-   return true;
+  if (!m_child_kernels.empty()) return false;
+
+  return true;
 }
 
 void kernel_info_t::notify_parent_finished() {
-   if(m_parent_kernel) {
-       extern unsigned long long g_total_param_size;
-       g_total_param_size -= ((m_kernel_entry->get_args_aligned_size() + 255)/256*256);
-       m_parent_kernel->remove_child(this);
-       g_stream_manager->register_finished_kernel(m_parent_kernel->get_uid());
-   }
+  if (m_parent_kernel) {
+    m_kernel_entry->gpgpu_ctx->device_runtime->g_total_param_size -=
+        ((m_kernel_entry->get_args_aligned_size() + 255) / 256 * 256);
+    m_parent_kernel->remove_child(this);
+    m_kernel_entry->gpgpu_ctx->the_gpgpusim->g_stream_manager
+        ->register_finished_kernel(m_parent_kernel->get_uid());
+  }
 }
 
-CUstream_st * kernel_info_t::create_stream_cta(dim3 ctaid) {
-    assert(get_default_stream_cta(ctaid));
-    CUstream_st * stream = new CUstream_st();
-    g_stream_manager->add_stream(stream);
-    assert(m_cta_streams.find(ctaid) != m_cta_streams.end());
-    assert(m_cta_streams[ctaid].size() >= 1); //must have default stream
-    m_cta_streams[ctaid].push_back(stream);
+CUstream_st *kernel_info_t::create_stream_cta(dim3 ctaid) {
+  assert(get_default_stream_cta(ctaid));
+  CUstream_st *stream = new CUstream_st();
+  m_kernel_entry->gpgpu_ctx->the_gpgpusim->g_stream_manager->add_stream(stream);
+  assert(m_cta_streams.find(ctaid) != m_cta_streams.end());
+  assert(m_cta_streams[ctaid].size() >= 1);  // must have default stream
+  m_cta_streams[ctaid].push_back(stream);
 
-    return stream;
+  return stream;
 }
 
-CUstream_st * kernel_info_t::get_default_stream_cta(dim3 ctaid) {
-    if(m_cta_streams.find(ctaid) != m_cta_streams.end()) {
-       assert(m_cta_streams[ctaid].size() >= 1); //already created, must have default stream
-       return *(m_cta_streams[ctaid].begin());
-    }
-    else {
-      m_cta_streams[ctaid] = std::list<CUstream_st *>();
-      CUstream_st * stream = new CUstream_st();
-      g_stream_manager->add_stream(stream);
-      m_cta_streams[ctaid].push_back(stream);
-      return stream;
-    }
+CUstream_st *kernel_info_t::get_default_stream_cta(dim3 ctaid) {
+  if (m_cta_streams.find(ctaid) != m_cta_streams.end()) {
+    assert(m_cta_streams[ctaid].size() >=
+           1);  // already created, must have default stream
+    return *(m_cta_streams[ctaid].begin());
+  } else {
+    m_cta_streams[ctaid] = std::list<CUstream_st *>();
+    CUstream_st *stream = new CUstream_st();
+    m_kernel_entry->gpgpu_ctx->the_gpgpusim->g_stream_manager->add_stream(
+        stream);
+    m_cta_streams[ctaid].push_back(stream);
+    return stream;
+  }
 }
 
-bool kernel_info_t::cta_has_stream(dim3 ctaid, CUstream_st* stream) {
-    if(m_cta_streams.find(ctaid) == m_cta_streams.end())
-       return false;
+bool kernel_info_t::cta_has_stream(dim3 ctaid, CUstream_st *stream) {
+  if (m_cta_streams.find(ctaid) == m_cta_streams.end()) return false;
 
-    std::list<CUstream_st *> &stream_list = m_cta_streams[ctaid];
-    if(std::find(stream_list.begin(), stream_list.end(), stream) 
-         == stream_list.end())
-       return false;
-    else
-       return true;
+  std::list<CUstream_st *> &stream_list = m_cta_streams[ctaid];
+  if (std::find(stream_list.begin(), stream_list.end(), stream) ==
+      stream_list.end())
+    return false;
+  else
+    return true;
 }
 
 void kernel_info_t::print_parent_info() {
-    if(m_parent_kernel) {
-        printf("Parent %d: \'%s\', Block (%d, %d, %d), Thread (%d, %d, %d)\n", 
-            m_parent_kernel->get_uid(), m_parent_kernel->name().c_str(), 
-            m_parent_ctaid.x, m_parent_ctaid.y, m_parent_ctaid.z,
-            m_parent_tid.x, m_parent_tid.y, m_parent_tid.z);
-    }
+  if (m_parent_kernel) {
+    printf("Parent %d: \'%s\', Block (%d, %d, %d), Thread (%d, %d, %d)\n",
+           m_parent_kernel->get_uid(), m_parent_kernel->name().c_str(),
+           m_parent_ctaid.x, m_parent_ctaid.y, m_parent_ctaid.z, m_parent_tid.x,
+           m_parent_tid.y, m_parent_tid.z);
+  }
 }
 
 void kernel_info_t::destroy_cta_streams() {
-     printf("Destroy streams for kernel %d: ", get_uid()); size_t stream_size = 0;
-     for(auto s = m_cta_streams.begin(); s != m_cta_streams.end(); s++) {
-        stream_size += s->second.size();
-        for(auto ss = s->second.begin(); ss != s->second.end(); ss++)
-        g_stream_manager->destroy_stream(*ss);
-        s->second.clear();
-     }
-     printf("size %lu\n", stream_size);
-     m_cta_streams.clear();
+  printf("Destroy streams for kernel %d: ", get_uid());
+  size_t stream_size = 0;
+  for (auto s = m_cta_streams.begin(); s != m_cta_streams.end(); s++) {
+    stream_size += s->second.size();
+    for (auto ss = s->second.begin(); ss != s->second.end(); ss++)
+      m_kernel_entry->gpgpu_ctx->the_gpgpusim->g_stream_manager->destroy_stream(
+          *ss);
+    s->second.clear();
+  }
+  printf("size %lu\n", stream_size);
+  m_cta_streams.clear();
 }
 
-simt_stack::simt_stack( unsigned wid, unsigned warpSize)
-{
-    m_warp_id=wid;
-    m_warp_size = warpSize;
-    reset();
+simt_stack::simt_stack(unsigned wid, unsigned warpSize, class gpgpu_sim *gpu) {
+  m_warp_id = wid;
+  m_warp_size = warpSize;
+  m_gpu = gpu;
+  reset();
 }
 
-void simt_stack::reset()
-{
-    m_stack.clear();
-}
+void simt_stack::reset() { m_stack.clear(); }
 
-void simt_stack::launch( address_type start_pc, const simt_mask_t &active_mask )
-{
-    reset();
-    simt_stack_entry new_stack_entry;
-    new_stack_entry.m_pc = start_pc;
-    new_stack_entry.m_calldepth = 1;
-    new_stack_entry.m_active_mask = active_mask;
-    new_stack_entry.m_type = STACK_ENTRY_TYPE_NORMAL;
-    m_stack.push_back(new_stack_entry);
+void simt_stack::launch(address_type start_pc, const simt_mask_t &active_mask) {
+  reset();
+  simt_stack_entry new_stack_entry;
+  new_stack_entry.m_pc = start_pc;
+  new_stack_entry.m_calldepth = 1;
+  new_stack_entry.m_active_mask = active_mask;
+  new_stack_entry.m_type = STACK_ENTRY_TYPE_NORMAL;
+  m_stack.push_back(new_stack_entry);
 }
 
-void simt_stack::resume( char * fname )
-{
-    reset();    
-
-
-
-      FILE * fp2 = fopen(fname, "r");
-      assert(fp2!=NULL);
-
-      char line [ 200 ]; /* or other suitable maximum line size */
-
-      while ( fgets ( line, sizeof line, fp2 ) != NULL ) /* read a line */
-      {
-          simt_stack_entry new_stack_entry;
-          char * pch;
-          pch = strtok (line," ");
-          for (unsigned j=0; j<m_warp_size; j++)
-          {
-                if (pch[0]=='1')
-                    new_stack_entry.m_active_mask.set(j);
-                else
-                    new_stack_entry.m_active_mask.reset(j);
-                pch = strtok (NULL," ");
-                
-          }  
-          
-         new_stack_entry.m_pc=atoi(pch);
-         pch = strtok (NULL," "); 
-         new_stack_entry.m_calldepth=atoi(pch);
-         pch = strtok (NULL," "); 
-         new_stack_entry.m_recvg_pc=atoi(pch);
-         pch = strtok (NULL," "); 
-         new_stack_entry.m_branch_div_cycle=atoi(pch);
-         pch = strtok (NULL," "); 
-         if(pch[0]=='0')
-            new_stack_entry.m_type= STACK_ENTRY_TYPE_NORMAL;
-         else
-            new_stack_entry.m_type= STACK_ENTRY_TYPE_CALL;
-         m_stack.push_back(new_stack_entry);
-      }
-      fclose ( fp2 );
+void simt_stack::resume(char *fname) {
+  reset();
 
-    
-}
+  FILE *fp2 = fopen(fname, "r");
+  assert(fp2 != NULL);
 
-const simt_mask_t &simt_stack::get_active_mask() const
-{
-    assert(m_stack.size() > 0);
-    return m_stack.back().m_active_mask;
-}
+  char line[200]; /* or other suitable maximum line size */
 
-void simt_stack::get_pdom_stack_top_info( unsigned *pc, unsigned *rpc ) const
-{
-   assert(m_stack.size() > 0);
-   *pc = m_stack.back().m_pc;
-   *rpc = m_stack.back().m_recvg_pc;
+  while (fgets(line, sizeof line, fp2) != NULL) /* read a line */
+  {
+    simt_stack_entry new_stack_entry;
+    char *pch;
+    pch = strtok(line, " ");
+    for (unsigned j = 0; j < m_warp_size; j++) {
+      if (pch[0] == '1')
+        new_stack_entry.m_active_mask.set(j);
+      else
+        new_stack_entry.m_active_mask.reset(j);
+      pch = strtok(NULL, " ");
+    }
+
+    new_stack_entry.m_pc = atoi(pch);
+    pch = strtok(NULL, " ");
+    new_stack_entry.m_calldepth = atoi(pch);
+    pch = strtok(NULL, " ");
+    new_stack_entry.m_recvg_pc = atoi(pch);
+    pch = strtok(NULL, " ");
+    new_stack_entry.m_branch_div_cycle = atoi(pch);
+    pch = strtok(NULL, " ");
+    if (pch[0] == '0')
+      new_stack_entry.m_type = STACK_ENTRY_TYPE_NORMAL;
+    else
+      new_stack_entry.m_type = STACK_ENTRY_TYPE_CALL;
+    m_stack.push_back(new_stack_entry);
+  }
+  fclose(fp2);
 }
 
-unsigned simt_stack::get_rp() const 
-{ 
-    assert(m_stack.size() > 0);
-    return m_stack.back().m_recvg_pc;
+const simt_mask_t &simt_stack::get_active_mask() const {
+  assert(m_stack.size() > 0);
+  return m_stack.back().m_active_mask;
 }
 
-void simt_stack::print (FILE *fout) const
-{
-    for ( unsigned k=0; k < m_stack.size(); k++ ) {
-        simt_stack_entry stack_entry = m_stack[k];
-        if ( k==0 ) {
-            fprintf(fout, "w%02d %1u ", m_warp_id, k );
-        } else {
-            fprintf(fout, "    %1u ", k );
-        }
-        for (unsigned j=0; j<m_warp_size; j++)
-            fprintf(fout, "%c", (stack_entry.m_active_mask.test(j)?'1':'0') );
-        fprintf(fout, " pc: 0x%03x", stack_entry.m_pc );
-        if ( stack_entry.m_recvg_pc == (unsigned)-1 ) {
-            fprintf(fout," rp: ---- tp: %s cd: %2u ", (stack_entry.m_type==STACK_ENTRY_TYPE_CALL?"C":"N"), stack_entry.m_calldepth );
-        } else {
-            fprintf(fout," rp: %4u tp: %s cd: %2u ", stack_entry.m_recvg_pc, (stack_entry.m_type==STACK_ENTRY_TYPE_CALL?"C":"N"), stack_entry.m_calldepth );
-        }
-        if ( stack_entry.m_branch_div_cycle != 0 ) {
-            fprintf(fout," bd@%6u ", (unsigned) stack_entry.m_branch_div_cycle );
-        } else {
-            fprintf(fout," " );
-        }
-        ptx_print_insn( stack_entry.m_pc, fout );
-        fprintf(fout,"\n");
-    }
+void simt_stack::get_pdom_stack_top_info(unsigned *pc, unsigned *rpc) const {
+  assert(m_stack.size() > 0);
+  *pc = m_stack.back().m_pc;
+  *rpc = m_stack.back().m_recvg_pc;
+}
 
+unsigned simt_stack::get_rp() const {
+  assert(m_stack.size() > 0);
+  return m_stack.back().m_recvg_pc;
 }
 
-void simt_stack::print_checkpoint (FILE *fout) const
-{
-    for ( unsigned k=0; k < m_stack.size(); k++ ) {
-        simt_stack_entry stack_entry = m_stack[k];
-       
-        for (unsigned j=0; j<m_warp_size; j++)
-            fprintf(fout, "%c ", (stack_entry.m_active_mask.test(j)?'1':'0') );
-        fprintf(fout, "%d %d %d %lld %d ", stack_entry.m_pc,stack_entry.m_calldepth,stack_entry.m_recvg_pc,stack_entry.m_branch_div_cycle,stack_entry.m_type );
-        fprintf(fout, "%d %d\n",m_warp_id, m_warp_size );
-        
+void simt_stack::print(FILE *fout) const {
+  for (unsigned k = 0; k < m_stack.size(); k++) {
+    simt_stack_entry stack_entry = m_stack[k];
+    if (k == 0) {
+      fprintf(fout, "w%02d %1u ", m_warp_id, k);
+    } else {
+      fprintf(fout, "    %1u ", k);
+    }
+    for (unsigned j = 0; j < m_warp_size; j++)
+      fprintf(fout, "%c", (stack_entry.m_active_mask.test(j) ? '1' : '0'));
+    fprintf(fout, " pc: 0x%03x", stack_entry.m_pc);
+    if (stack_entry.m_recvg_pc == (unsigned)-1) {
+      fprintf(fout, " rp: ---- tp: %s cd: %2u ",
+              (stack_entry.m_type == STACK_ENTRY_TYPE_CALL ? "C" : "N"),
+              stack_entry.m_calldepth);
+    } else {
+      fprintf(fout, " rp: %4u tp: %s cd: %2u ", stack_entry.m_recvg_pc,
+              (stack_entry.m_type == STACK_ENTRY_TYPE_CALL ? "C" : "N"),
+              stack_entry.m_calldepth);
     }
+    if (stack_entry.m_branch_div_cycle != 0) {
+      fprintf(fout, " bd@%6u ", (unsigned)stack_entry.m_branch_div_cycle);
+    } else {
+      fprintf(fout, " ");
+    }
+    m_gpu->gpgpu_ctx->func_sim->ptx_print_insn(stack_entry.m_pc, fout);
+    fprintf(fout, "\n");
+  }
 }
 
-void simt_stack::update( simt_mask_t &thread_done, addr_vector_t &next_pc, address_type recvg_pc, op_type next_inst_op,unsigned next_inst_size, address_type next_inst_pc )
-{
-    assert(m_stack.size() > 0);
-
-    assert( next_pc.size() == m_warp_size );
-
-    simt_mask_t  top_active_mask = m_stack.back().m_active_mask;
-    address_type top_recvg_pc = m_stack.back().m_recvg_pc;
-    address_type top_pc = m_stack.back().m_pc; // the pc of the instruction just executed
-    stack_entry_type top_type = m_stack.back().m_type;
-    assert(top_pc==next_inst_pc);
-    assert(top_active_mask.any());
-
-    const address_type null_pc = -1;
-    bool warp_diverged = false;
-    address_type new_recvg_pc = null_pc;
-    unsigned num_divergent_paths=0;
-
-    std::map<address_type,simt_mask_t> divergent_paths;
-    while (top_active_mask.any()) {
-
-        // extract a group of threads with the same next PC among the active threads in the warp
-        address_type tmp_next_pc = null_pc;
-        simt_mask_t tmp_active_mask;
-        for (int i = m_warp_size - 1; i >= 0; i--) {
-            if ( top_active_mask.test(i) ) { // is this thread active?
-                if (thread_done.test(i)) {
-                    top_active_mask.reset(i); // remove completed thread from active mask
-                } else if (tmp_next_pc == null_pc) {
-                    tmp_next_pc = next_pc[i];
-                    tmp_active_mask.set(i);
-                    top_active_mask.reset(i);
-                } else if (tmp_next_pc == next_pc[i]) {
-                    tmp_active_mask.set(i);
-                    top_active_mask.reset(i);
-                }
-            }
-        }
+void simt_stack::print_checkpoint(FILE *fout) const {
+  for (unsigned k = 0; k < m_stack.size(); k++) {
+    simt_stack_entry stack_entry = m_stack[k];
+
+    for (unsigned j = 0; j < m_warp_size; j++)
+      fprintf(fout, "%c ", (stack_entry.m_active_mask.test(j) ? '1' : '0'));
+    fprintf(fout, "%d %d %d %lld %d ", stack_entry.m_pc,
+            stack_entry.m_calldepth, stack_entry.m_recvg_pc,
+            stack_entry.m_branch_div_cycle, stack_entry.m_type);
+    fprintf(fout, "%d %d\n", m_warp_id, m_warp_size);
+  }
+}
 
-        if(tmp_next_pc == null_pc) {
-            assert(!top_active_mask.any()); // all threads done
-            continue;
+void simt_stack::update(simt_mask_t &thread_done, addr_vector_t &next_pc,
+                        address_type recvg_pc, op_type next_inst_op,
+                        unsigned next_inst_size, address_type next_inst_pc) {
+  assert(m_stack.size() > 0);
+
+  assert(next_pc.size() == m_warp_size);
+
+  simt_mask_t top_active_mask = m_stack.back().m_active_mask;
+  address_type top_recvg_pc = m_stack.back().m_recvg_pc;
+  address_type top_pc =
+      m_stack.back().m_pc;  // the pc of the instruction just executed
+  stack_entry_type top_type = m_stack.back().m_type;
+  assert(top_pc == next_inst_pc);
+  assert(top_active_mask.any());
+
+  const address_type null_pc = -1;
+  bool warp_diverged = false;
+  address_type new_recvg_pc = null_pc;
+  unsigned num_divergent_paths = 0;
+
+  std::map<address_type, simt_mask_t> divergent_paths;
+  while (top_active_mask.any()) {
+    // extract a group of threads with the same next PC among the active threads
+    // in the warp
+    address_type tmp_next_pc = null_pc;
+    simt_mask_t tmp_active_mask;
+    for (int i = m_warp_size - 1; i >= 0; i--) {
+      if (top_active_mask.test(i)) {  // is this thread active?
+        if (thread_done.test(i)) {
+          top_active_mask.reset(i);  // remove completed thread from active mask
+        } else if (tmp_next_pc == null_pc) {
+          tmp_next_pc = next_pc[i];
+          tmp_active_mask.set(i);
+          top_active_mask.reset(i);
+        } else if (tmp_next_pc == next_pc[i]) {
+          tmp_active_mask.set(i);
+          top_active_mask.reset(i);
         }
-
-        divergent_paths[tmp_next_pc]=tmp_active_mask;
-        num_divergent_paths++;
+      }
     }
 
+    if (tmp_next_pc == null_pc) {
+      assert(!top_active_mask.any());  // all threads done
+      continue;
+    }
 
-    address_type not_taken_pc = next_inst_pc+next_inst_size;
-    assert(num_divergent_paths<=2);
-    for(unsigned i=0; i<num_divergent_paths; i++){
-    	address_type tmp_next_pc = null_pc;
-    	simt_mask_t tmp_active_mask;
-    	tmp_active_mask.reset();
-    	if(divergent_paths.find(not_taken_pc)!=divergent_paths.end()){
-    		assert(i==0);
-    		tmp_next_pc=not_taken_pc;
-    		tmp_active_mask=divergent_paths[tmp_next_pc];
-    		divergent_paths.erase(tmp_next_pc);
-    	}else{
-    		std::map<address_type,simt_mask_t>:: iterator it=divergent_paths.begin();
-    		tmp_next_pc=it->first;
-    		tmp_active_mask=divergent_paths[tmp_next_pc];
-    		divergent_paths.erase(tmp_next_pc);
-    	}
-
-        // HANDLE THE SPECIAL CASES FIRST
-    	if (next_inst_op== CALL_OPS){
-    		// Since call is not a divergent instruction, all threads should have executed a call instruction
-    		assert(num_divergent_paths == 1);
-
-    		simt_stack_entry new_stack_entry;
-    		new_stack_entry.m_pc = tmp_next_pc;
-    		new_stack_entry.m_active_mask = tmp_active_mask;
-    		new_stack_entry.m_branch_div_cycle = gpu_sim_cycle+gpu_tot_sim_cycle;
-    		new_stack_entry.m_type = STACK_ENTRY_TYPE_CALL;
-    		m_stack.push_back(new_stack_entry);
-    		return;
-    	}else if(next_inst_op == RET_OPS && top_type==STACK_ENTRY_TYPE_CALL){
-    		// pop the CALL Entry
-    		assert(num_divergent_paths == 1);
-    		m_stack.pop_back();
-
-    		assert(m_stack.size() > 0);
-    		m_stack.back().m_pc=tmp_next_pc;// set the PC of the stack top entry to return PC from  the call stack;
-            // Check if the New top of the stack is reconverging
-            if (tmp_next_pc == m_stack.back().m_recvg_pc && m_stack.back().m_type!=STACK_ENTRY_TYPE_CALL){
-            	assert(m_stack.back().m_type==STACK_ENTRY_TYPE_NORMAL);
-            	m_stack.pop_back();
-            }
-            return;
-    	}
-
-        // discard the new entry if its PC matches with reconvergence PC
-        // that automatically reconverges the entry
-        // If the top stack entry is CALL, dont reconverge.
-        if (tmp_next_pc == top_recvg_pc && (top_type != STACK_ENTRY_TYPE_CALL)) continue;
-
-        // this new entry is not converging
-        // if this entry does not include thread from the warp, divergence occurs
-        if ((num_divergent_paths>1) && !warp_diverged ) {
-            warp_diverged = true;
-            // modify the existing top entry into a reconvergence entry in the pdom stack
-            new_recvg_pc = recvg_pc;
-            if (new_recvg_pc != top_recvg_pc) {
-                m_stack.back().m_pc = new_recvg_pc;
-                m_stack.back().m_branch_div_cycle = gpu_sim_cycle+gpu_tot_sim_cycle;
-
-                m_stack.push_back(simt_stack_entry());
-            }
-        }
+    divergent_paths[tmp_next_pc] = tmp_active_mask;
+    num_divergent_paths++;
+  }
+
+  address_type not_taken_pc = next_inst_pc + next_inst_size;
+  assert(num_divergent_paths <= 2);
+  for (unsigned i = 0; i < num_divergent_paths; i++) {
+    address_type tmp_next_pc = null_pc;
+    simt_mask_t tmp_active_mask;
+    tmp_active_mask.reset();
+    if (divergent_paths.find(not_taken_pc) != divergent_paths.end()) {
+      assert(i == 0);
+      tmp_next_pc = not_taken_pc;
+      tmp_active_mask = divergent_paths[tmp_next_pc];
+      divergent_paths.erase(tmp_next_pc);
+    } else {
+      std::map<address_type, simt_mask_t>::iterator it =
+          divergent_paths.begin();
+      tmp_next_pc = it->first;
+      tmp_active_mask = divergent_paths[tmp_next_pc];
+      divergent_paths.erase(tmp_next_pc);
+    }
 
-        // discard the new entry if its PC matches with reconvergence PC
-        if (warp_diverged && tmp_next_pc == new_recvg_pc) continue;
+    // HANDLE THE SPECIAL CASES FIRST
+    if (next_inst_op == CALL_OPS) {
+      // Since call is not a divergent instruction, all threads should have
+      // executed a call instruction
+      assert(num_divergent_paths == 1);
+
+      simt_stack_entry new_stack_entry;
+      new_stack_entry.m_pc = tmp_next_pc;
+      new_stack_entry.m_active_mask = tmp_active_mask;
+      new_stack_entry.m_branch_div_cycle =
+          m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle;
+      new_stack_entry.m_type = STACK_ENTRY_TYPE_CALL;
+      m_stack.push_back(new_stack_entry);
+      return;
+    } else if (next_inst_op == RET_OPS && top_type == STACK_ENTRY_TYPE_CALL) {
+      // pop the CALL Entry
+      assert(num_divergent_paths == 1);
+      m_stack.pop_back();
+
+      assert(m_stack.size() > 0);
+      m_stack.back().m_pc = tmp_next_pc;  // set the PC of the stack top entry
+                                          // to return PC from  the call stack;
+      // Check if the New top of the stack is reconverging
+      if (tmp_next_pc == m_stack.back().m_recvg_pc &&
+          m_stack.back().m_type != STACK_ENTRY_TYPE_CALL) {
+        assert(m_stack.back().m_type == STACK_ENTRY_TYPE_NORMAL);
+        m_stack.pop_back();
+      }
+      return;
+    }
 
-        // update the current top of pdom stack
-        m_stack.back().m_pc = tmp_next_pc;
-        m_stack.back().m_active_mask = tmp_active_mask;
-        if (warp_diverged) {
-            m_stack.back().m_calldepth = 0;
-            m_stack.back().m_recvg_pc = new_recvg_pc;
-        } else {
-            m_stack.back().m_recvg_pc = top_recvg_pc;
-        }
+    // discard the new entry if its PC matches with reconvergence PC
+    // that automatically reconverges the entry
+    // If the top stack entry is CALL, dont reconverge.
+    if (tmp_next_pc == top_recvg_pc && (top_type != STACK_ENTRY_TYPE_CALL))
+      continue;
+
+    // this new entry is not converging
+    // if this entry does not include thread from the warp, divergence occurs
+    if ((num_divergent_paths > 1) && !warp_diverged) {
+      warp_diverged = true;
+      // modify the existing top entry into a reconvergence entry in the pdom
+      // stack
+      new_recvg_pc = recvg_pc;
+      if (new_recvg_pc != top_recvg_pc) {
+        m_stack.back().m_pc = new_recvg_pc;
+        m_stack.back().m_branch_div_cycle =
+            m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle;
 
         m_stack.push_back(simt_stack_entry());
+      }
     }
-    assert(m_stack.size() > 0);
-    m_stack.pop_back();
 
+    // discard the new entry if its PC matches with reconvergence PC
+    if (warp_diverged && tmp_next_pc == new_recvg_pc) continue;
 
+    // update the current top of pdom stack
+    m_stack.back().m_pc = tmp_next_pc;
+    m_stack.back().m_active_mask = tmp_active_mask;
     if (warp_diverged) {
-        ptx_file_line_stats_add_warp_divergence(top_pc, 1); 
+      m_stack.back().m_calldepth = 0;
+      m_stack.back().m_recvg_pc = new_recvg_pc;
+    } else {
+      m_stack.back().m_recvg_pc = top_recvg_pc;
     }
+
+    m_stack.push_back(simt_stack_entry());
+  }
+  assert(m_stack.size() > 0);
+  m_stack.pop_back();
+
+  if (warp_diverged) {
+    m_gpu->gpgpu_ctx->stats->ptx_file_line_stats_add_warp_divergence(top_pc, 1);
+  }
 }
 
-void core_t::execute_warp_inst_t(warp_inst_t &inst, unsigned warpId)
-{
-    for ( unsigned t=0; t < m_warp_size; t++ ) {
-        if( inst.active(t) ) {
-            if(warpId==(unsigned (-1)))
-                warpId = inst.warp_id();
-            unsigned tid=m_warp_size*warpId+t;
-            m_thread[tid]->ptx_exec_inst(inst,t);
-            
-            //virtual function
-            checkExecutionStatusAndUpdate(inst,t,tid);
-        }
-    } 
+void core_t::execute_warp_inst_t(warp_inst_t &inst, unsigned warpId) {
+  for (unsigned t = 0; t < m_warp_size; t++) {
+    if (inst.active(t)) {
+      if (warpId == (unsigned(-1))) warpId = inst.warp_id();
+      unsigned tid = m_warp_size * warpId + t;
+      m_thread[tid]->ptx_exec_inst(inst, t);
+
+      // virtual function
+      checkExecutionStatusAndUpdate(inst, t, tid);
+    }
+  }
 }
 
 // TODO schi add
 void core_t::writeRegister(const warp_inst_t &inst, unsigned warpSize, unsigned lane_id, char *data) {
     assert(inst.active(lane_id));
     int warpId = inst.warp_id();
+    assert(false);
     m_thread[warpSize*warpId+lane_id]->writeRegister(inst, lane_id, data);
 }
 
-bool  core_t::ptx_thread_done( unsigned hw_thread_id ) const  
-{
-    return ((m_thread[ hw_thread_id ]==NULL) || m_thread[ hw_thread_id ]->is_done());
+bool core_t::ptx_thread_done(unsigned hw_thread_id) const {
+  return ((m_thread[hw_thread_id] == NULL) ||
+          m_thread[hw_thread_id]->is_done());
 }
-  
-void core_t::updateSIMTStack(unsigned warpId, warp_inst_t * inst)
-{
-    simt_mask_t thread_done;
-    addr_vector_t next_pc;
-    unsigned wtid = warpId * m_warp_size;
-    for (unsigned i = 0; i < m_warp_size; i++) {
-        if( ptx_thread_done(wtid+i) ) {
-            thread_done.set(i);
-            next_pc.push_back( (address_type)-1 );
-        } else {
-            if( inst->reconvergence_pc == RECONVERGE_RETURN_PC ) 
-                inst->reconvergence_pc = get_return_pc(m_thread[wtid+i]);
-            next_pc.push_back( m_thread[wtid+i]->get_pc() );
-        }
+
+void core_t::updateSIMTStack(unsigned warpId, warp_inst_t *inst) {
+  simt_mask_t thread_done;
+  addr_vector_t next_pc;
+  unsigned wtid = warpId * m_warp_size;
+  for (unsigned i = 0; i < m_warp_size; i++) {
+    if (ptx_thread_done(wtid + i)) {
+      thread_done.set(i);
+      next_pc.push_back((address_type)-1);
+    } else {
+      if (inst->reconvergence_pc == RECONVERGE_RETURN_PC)
+        inst->reconvergence_pc = get_return_pc(m_thread[wtid + i]);
+      next_pc.push_back(m_thread[wtid + i]->get_pc());
     }
-    m_simt_stack[warpId]->update(thread_done,next_pc,inst->reconvergence_pc, inst->op,inst->isize,inst->pc);
+  }
+  m_simt_stack[warpId]->update(thread_done, next_pc, inst->reconvergence_pc,
+                               inst->op, inst->isize, inst->pc);
 }
 
 //! Get the warp to be executed using the data taken form the SIMT stack
-warp_inst_t core_t::getExecuteWarp(unsigned warpId)
-{
-    unsigned pc,rpc;
-    m_simt_stack[warpId]->get_pdom_stack_top_info(&pc,&rpc);
-    warp_inst_t wi= *ptx_fetch_inst(pc);
-    wi.set_active(m_simt_stack[warpId]->get_active_mask());
-    return wi;
+warp_inst_t core_t::getExecuteWarp(unsigned warpId) {
+  unsigned pc, rpc;
+  m_simt_stack[warpId]->get_pdom_stack_top_info(&pc, &rpc);
+  warp_inst_t wi = *(m_gpu->gpgpu_ctx->ptx_fetch_inst(pc));
+  wi.set_active(m_simt_stack[warpId]->get_active_mask());
+  return wi;
 }
 
-void core_t::deleteSIMTStack()
-{
-    if ( m_simt_stack ) {
-        for (unsigned i = 0; i < m_warp_count; ++i) 
-            delete m_simt_stack[i];
-        delete[] m_simt_stack;
-        m_simt_stack = NULL;
-    }
+void core_t::deleteSIMTStack() {
+  if (m_simt_stack) {
+    for (unsigned i = 0; i < m_warp_count; ++i) delete m_simt_stack[i];
+    delete[] m_simt_stack;
+    m_simt_stack = NULL;
+  }
 }
 
-void core_t::initilizeSIMTStack(unsigned warp_count, unsigned warp_size)
-{ 
-    m_simt_stack = new simt_stack*[warp_count];
-    for (unsigned i = 0; i < warp_count; ++i) 
-        m_simt_stack[i] = new simt_stack(i,warp_size);
-    m_warp_size = warp_size;
-    m_warp_count = warp_count;
+void core_t::initilizeSIMTStack(unsigned warp_count, unsigned warp_size) {
+  m_simt_stack = new simt_stack *[warp_count];
+  for (unsigned i = 0; i < warp_count; ++i)
+    m_simt_stack[i] = new simt_stack(i, warp_size, m_gpu);
+  m_warp_size = warp_size;
+  m_warp_count = warp_count;
 }
 
-void core_t::get_pdom_stack_top_info( unsigned warpId, unsigned *pc, unsigned *rpc ) const
-{
-    m_simt_stack[warpId]->get_pdom_stack_top_info(pc,rpc);
+void core_t::get_pdom_stack_top_info(unsigned warpId, unsigned *pc,
+                                     unsigned *rpc) const {
+  m_simt_stack[warpId]->get_pdom_stack_top_info(pc, rpc);
 }
diff --git a/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.h b/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.h
index d5464f3832..dd701ed385 100644
--- a/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.h
+++ b/design/gpgpu/gpgpu-sim/src/abstract_hardware_model.h
@@ -1,38 +1,39 @@
-// Copyright (c) 2009-2011, Tor M. Aamodt, Inderpreet Singh,
-// The University of British Columbia
+// Copyright (c) 2009-2021, Tor M. Aamodt, Inderpreet Singh, Vijay Kandiah, Nikos Hardavellas
+// The University of British Columbia, Northwestern University
 // All rights reserved.
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
 //
-// Redistributions of source code must retain the above copyright notice, this
-// list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// 1. Redistributions of source code must retain the above copyright notice, this
+//    list of conditions and the following disclaimer;
+// 2. Redistributions in binary form must reproduce the above copyright notice,
+//    this list of conditions and the following disclaimer in the documentation
+//    and/or other materials provided with the distribution;
+// 3. Neither the names of The University of British Columbia, Northwestern 
+//    University nor the names of their contributors may be used to
+//    endorse or promote products derived from this software without specific
+//    prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
 
 #ifndef ABSTRACT_HARDWARE_MODEL_INCLUDED
 #define ABSTRACT_HARDWARE_MODEL_INCLUDED
 
 // Forward declarations
-// class gpgpu_sim;
-// class kernel_info_t;
 struct gpgpu_sim;
 struct kernel_info_t;
+struct gpgpu_context;
 
 //Set a hard limit of 32 CTAs per shader [cuda only has 8]
 #define MAX_CTA_PER_SHADER 32
@@ -43,162 +44,182 @@ struct kernel_info_t;
 #define MAX_OUTPUT_VALUES 8
 
 enum _memory_space_t {
-   undefined_space=0,
-   reg_space,
-   local_space,
-   shared_space,
-   sstarr_space,
-   param_space_unclassified,
-   param_space_kernel,  /* global to all threads in a kernel : read-only */
-   param_space_local,   /* local to a thread : read-writable */
-   const_space,
-   tex_space,
-   surf_space,
-   global_space,
-   generic_space,
-   instruction_space
+  undefined_space = 0,
+  reg_space,
+  local_space,
+  shared_space,
+  sstarr_space,
+  param_space_unclassified,
+  param_space_kernel, /* global to all threads in a kernel : read-only */
+  param_space_local,  /* local to a thread : read-writable */
+  const_space,
+  tex_space,
+  surf_space,
+  global_space,
+  generic_space,
+  instruction_space
 };
 
+#ifndef COEFF_STRUCT
+#define COEFF_STRUCT
+
+struct PowerscalingCoefficients{
+    double int_coeff;
+    double int_mul_coeff;
+    double int_mul24_coeff;
+    double int_mul32_coeff;
+    double int_div_coeff;
+    double fp_coeff;
+    double dp_coeff;
+    double fp_mul_coeff;
+    double fp_div_coeff;
+    double dp_mul_coeff;
+    double dp_div_coeff;
+    double sqrt_coeff;
+    double log_coeff;
+    double sin_coeff;
+    double exp_coeff;
+    double tensor_coeff;
+    double tex_coeff;
+};
+#endif
 
-enum FuncCache
-{
+enum FuncCache {
   FuncCachePreferNone = 0,
   FuncCachePreferShared = 1,
   FuncCachePreferL1 = 2
 };
 
+enum AdaptiveCache { FIXED = 0, ADAPTIVE_CACHE = 1 };
 
 #ifdef __cplusplus
 
-#include <string.h>
 #include <stdio.h>
+#include <string.h>
 #include <set>
 
 typedef unsigned long long new_addr_type;
 typedef unsigned long long cudaTextureObject_t;
-typedef unsigned address_type;
-typedef unsigned addr_t;
+typedef unsigned long long address_type;
+typedef unsigned long long addr_t;
 
-// the following are operations the timing model can see 
+// the following are operations the timing model can see
+#define SPECIALIZED_UNIT_NUM 8
+#define SPEC_UNIT_START_ID 100
 
 enum uarch_op_t {
-   NO_OP=-1,
-   ALU_OP=1,
-   SFU_OP,
-   TENSOR_CORE_OP,
-   DP_OP,
-   SP_OP,
-   INTP_OP,
-   ALU_SFU_OP,
-   LOAD_OP,
-   TENSOR_CORE_LOAD_OP,
-   TENSOR_CORE_STORE_OP,
-   STORE_OP,
-   BRANCH_OP,
-   BARRIER_OP,
-   MEMORY_BARRIER_OP,
-   CALL_OPS,
-   RET_OPS
+  NO_OP = -1,
+  ALU_OP = 1,
+  SFU_OP,
+  TENSOR_CORE_OP,
+  DP_OP,
+  SP_OP,
+  INTP_OP,
+  ALU_SFU_OP,
+  LOAD_OP,
+  TENSOR_CORE_LOAD_OP,
+  TENSOR_CORE_STORE_OP,
+  STORE_OP,
+  BRANCH_OP,
+  BARRIER_OP,
+  MEMORY_BARRIER_OP,
+  CALL_OPS,
+  RET_OPS,
+  EXIT_OPS,
+  SPECIALIZED_UNIT_1_OP = SPEC_UNIT_START_ID,
+  SPECIALIZED_UNIT_2_OP,
+  SPECIALIZED_UNIT_3_OP,
+  SPECIALIZED_UNIT_4_OP,
+  SPECIALIZED_UNIT_5_OP,
+  SPECIALIZED_UNIT_6_OP,
+  SPECIALIZED_UNIT_7_OP,
+  SPECIALIZED_UNIT_8_OP
 };
 typedef enum uarch_op_t op_type;
 
-
-enum uarch_bar_t {
-   NOT_BAR=-1,
-   SYNC=1,
-   ARRIVE,
-   RED
-};
+enum uarch_bar_t { NOT_BAR = -1, SYNC = 1, ARRIVE, RED };
 typedef enum uarch_bar_t barrier_type;
 
-enum uarch_red_t {
-   NOT_RED=-1,
-   POPC_RED=1,
-   AND_RED,
-   OR_RED
-};
+enum uarch_red_t { NOT_RED = -1, POPC_RED = 1, AND_RED, OR_RED };
 typedef enum uarch_red_t reduction_type;
 
-
-enum uarch_operand_type_t {
-	UN_OP=-1,
-    INT_OP,
-    FP_OP
-};
+enum uarch_operand_type_t { UN_OP = -1, INT_OP, FP_OP };
 typedef enum uarch_operand_type_t types_of_operands;
 
 enum special_operations_t {
-    OTHER_OP,
-    INT__OP,
-	INT_MUL24_OP,
-	INT_MUL32_OP,
-	INT_MUL_OP,
-    INT_DIV_OP,
-    FP_MUL_OP,
-    FP_DIV_OP,
-    FP__OP,
-	FP_SQRT_OP,
-	FP_LG_OP,
-	FP_SIN_OP,
-	FP_EXP_OP
+  OTHER_OP,
+  INT__OP,
+  INT_MUL24_OP,
+  INT_MUL32_OP,
+  INT_MUL_OP,
+  INT_DIV_OP,
+  FP_MUL_OP,
+  FP_DIV_OP,
+  FP__OP,
+  FP_SQRT_OP,
+  FP_LG_OP,
+  FP_SIN_OP,
+  FP_EXP_OP,
+  DP_MUL_OP,
+  DP_DIV_OP,
+  DP___OP,
+  TENSOR__OP,
+  TEX__OP
 };
-typedef enum special_operations_t special_ops; // Required to identify for the power model
+typedef enum special_operations_t
+    special_ops;  // Required to identify for the power model
 enum operation_pipeline_t {
-    UNKOWN_OP,
-    SP__OP,
-	DP__OP,
-	INTP__OP,
-    SFU__OP,
-    TENSOR_CORE__OP,
-    MEM__OP
+  UNKOWN_OP,
+  SP__OP,
+  DP__OP,
+  INTP__OP,
+  SFU__OP,
+  TENSOR_CORE__OP,
+  MEM__OP,
+  SPECIALIZED__OP,
 };
 typedef enum operation_pipeline_t operation_pipeline;
-enum mem_operation_t {
-    NOT_TEX,
-    TEX
-};
+enum mem_operation_t { NOT_TEX, TEX };
 typedef enum mem_operation_t mem_operation;
 
-enum _memory_op_t {
-	no_memory_op = 0,
-	memory_load,
-	memory_store
-};
+enum _memory_op_t { no_memory_op = 0, memory_load, memory_store };
 
-#include <bitset>
-#include <list>
-#include <vector>
 #include <assert.h>
 #include <stdlib.h>
-#include <map>
-#include <deque>
 #include <algorithm>
+#include <bitset>
+#include <deque>
+#include <list>
+#include <map>
+#include <vector>
 
 #if !defined(__VECTOR_TYPES_H__)
+#include "vector_types.h"
+/*
 struct dim3 {
    unsigned int x, y, z;
 };
+*/
 #endif
 struct dim3comp {
-    bool operator() (const dim3 & a, const dim3 & b) const
-    {    
-        if(a.z < b.z)
-            return true;
-        else if(a.y < b.y)
-            return true;
-        else if (a.x < b.x)
-            return true;
-        else
-            return false;
-    }
+  bool operator()(const dim3 &a, const dim3 &b) const {
+    if (a.z < b.z)
+      return true;
+    else if (a.y < b.y)
+      return true;
+    else if (a.x < b.x)
+      return true;
+    else
+      return false;
+  }
 };
 
 void increment_x_then_y_then_z( dim3 &i, const dim3 &bound);
 
 //Jin: child kernel information for CDP
 #include "stream_manager.h"
-// class stream_manager;
-struct stream_manager;
+class stream_manager;
+// struct stream_manager;
 
 struct CUstream_st;
 extern stream_manager * g_stream_manager;
@@ -216,108 +237,114 @@ public:
 //      m_num_cores_running=0;
 //      m_param_mem=NULL;
 //   }
-   kernel_info_t( dim3 gridDim, dim3 blockDim, class function_info *entry, std::map<std::string, const struct cudaArray*> nameToCudaArray, std::map<std::string, const struct textureInfo*> nameToTextureInfo);
-   ~kernel_info_t();
-
-   void inc_running() { m_num_cores_running++; }
-   void dec_running()
-   {
-       assert( m_num_cores_running > 0 );
-       m_num_cores_running--; 
-   }
-   bool running() const { return m_num_cores_running>0; }
-   bool done() const 
-   {
-       return no_more_ctas_to_run() && !running();
-   }
-   class function_info *entry() { return m_kernel_entry; }
-   const class function_info *entry() const { return m_kernel_entry; }
-
-   size_t num_blocks() const
-   {
-      return m_grid_dim.x * m_grid_dim.y * m_grid_dim.z;
-   }
-
-   size_t threads_per_cta() const
-   {
-      return m_block_dim.x * m_block_dim.y * m_block_dim.z;
-   } 
-
-   dim3 get_grid_dim() const { return m_grid_dim; }
-   dim3 get_cta_dim() const { return m_block_dim; }
-
-   void increment_cta_id() 
-   { 
-      increment_x_then_y_then_z(m_next_cta,m_grid_dim); 
-      m_next_tid.x=0;
-      m_next_tid.y=0;
-      m_next_tid.z=0;
-   }
+  // kernel_info_t( dim3 gridDim, dim3 blockDim, class function_info *entry, std::map<std::string, const struct cudaArray*> nameToCudaArray, std::map<std::string, const struct textureInfo*> nameToTextureInfo);
+  kernel_info_t(dim3 gridDim, dim3 blockDim, class function_info *entry);
+  kernel_info_t(
+      dim3 gridDim, dim3 blockDim, class function_info *entry,
+      std::map<std::string, const struct cudaArray *> nameToCudaArray,
+      std::map<std::string, const struct textureInfo *> nameToTextureInfo);
+  ~kernel_info_t();
+
+  void inc_running() { m_num_cores_running++; }
+  void dec_running() {
+    assert(m_num_cores_running > 0);
+    m_num_cores_running--;
+  }
+  bool running() const { return m_num_cores_running > 0; }
+  bool done() const { return no_more_ctas_to_run() && !running(); }
+  class function_info *entry() {
+    return m_kernel_entry;
+  }
+  const class function_info *entry() const { return m_kernel_entry; }
+
+  size_t num_blocks() const {
+    return m_grid_dim.x * m_grid_dim.y * m_grid_dim.z;
+  }
+
+  size_t threads_per_cta() const {
+    return m_block_dim.x * m_block_dim.y * m_block_dim.z;
+  }
+
+  dim3 get_grid_dim() const { return m_grid_dim; }
+  dim3 get_cta_dim() const { return m_block_dim; }
+
+  void increment_cta_id() {
+    increment_x_then_y_then_z(m_next_cta, m_grid_dim);
+    m_next_tid.x = 0;
+    m_next_tid.y = 0;
+    m_next_tid.z = 0;
+  }
    dim3 get_next_cta_id() const { return m_next_cta; }
-   unsigned get_next_cta_id_single() const 
-   {
-      return m_next_cta.x + m_grid_dim.x*m_next_cta.y + m_grid_dim.x*m_grid_dim.y*m_next_cta.z;
-    }
-   bool no_more_ctas_to_run() const 
-   {
-      return (m_next_cta.x >= m_grid_dim.x || m_next_cta.y >= m_grid_dim.y || m_next_cta.z >= m_grid_dim.z );
-   }
-
-   void increment_thread_id() { increment_x_then_y_then_z(m_next_tid,m_block_dim); }
-   dim3 get_next_thread_id_3d() const  { return m_next_tid; }
-   unsigned get_next_thread_id() const 
-   { 
-      return m_next_tid.x + m_block_dim.x*m_next_tid.y + m_block_dim.x*m_block_dim.y*m_next_tid.z;
-   }
-   bool more_threads_in_cta() const 
-   {
-      return m_next_tid.z < m_block_dim.z && m_next_tid.y < m_block_dim.y && m_next_tid.x < m_block_dim.x;
-   }
-   unsigned get_uid() const { return m_uid; }
-   std::string name() const;
-
-   std::list<class ptx_thread_info *> &active_threads() { return m_active_threads; }
-   class memory_space *get_param_memory() { return m_param_mem; }
-
-   
-   //The following functions access texture bindings present at the kernel's launch
-   
-   const struct cudaArray* get_texarray( const std::string &texname ) const
-   {
-      std::map<std::string,const struct cudaArray*>::const_iterator t=m_NameToCudaArray.find(texname);
-      assert(t != m_NameToCudaArray.end());
-      return t->second;
-   }
-
-   const struct textureInfo* get_texinfo( const std::string &texname ) const
-   {
-      std::map<std::string, const struct textureInfo*>::const_iterator t=m_NameToTextureInfo.find(texname);
-      assert(t != m_NameToTextureInfo.end());
-      return t->second;
-   }
+  unsigned get_next_cta_id_single() const {
+    return m_next_cta.x + m_grid_dim.x * m_next_cta.y +
+           m_grid_dim.x * m_grid_dim.y * m_next_cta.z;
+  }
+  bool no_more_ctas_to_run() const {
+    return (m_next_cta.x >= m_grid_dim.x || m_next_cta.y >= m_grid_dim.y ||
+            m_next_cta.z >= m_grid_dim.z);
+  }
+
+  void increment_thread_id() {
+    increment_x_then_y_then_z(m_next_tid, m_block_dim);
+  }
+  dim3 get_next_thread_id_3d() const { return m_next_tid; }
+  unsigned get_next_thread_id() const {
+    return m_next_tid.x + m_block_dim.x * m_next_tid.y +
+           m_block_dim.x * m_block_dim.y * m_next_tid.z;
+  }
+  bool more_threads_in_cta() const {
+    return m_next_tid.z < m_block_dim.z && m_next_tid.y < m_block_dim.y &&
+           m_next_tid.x < m_block_dim.x;
+  }
+  unsigned get_uid() const { return m_uid; }
+  std::string name() const;
+
+  std::list<class ptx_thread_info *> &active_threads() {
+    return m_active_threads;
+  }
+  class memory_space *get_param_memory() {
+    return m_param_mem;
+  }
+
+  // The following functions access texture bindings present at the kernel's
+  // launch
+
+  const struct cudaArray *get_texarray(const std::string &texname) const {
+    std::map<std::string, const struct cudaArray *>::const_iterator t =
+        m_NameToCudaArray.find(texname);
+    assert(t != m_NameToCudaArray.end());
+    return t->second;
+  }
+
+  const struct textureInfo *get_texinfo(const std::string &texname) const {
+    std::map<std::string, const struct textureInfo *>::const_iterator t =
+        m_NameToTextureInfo.find(texname);
+    assert(t != m_NameToTextureInfo.end());
+    return t->second;
+  }
 
 private:
-   kernel_info_t( const kernel_info_t & ); // disable copy constructor
-   void operator=( const kernel_info_t & ); // disable copy operator
+  kernel_info_t(const kernel_info_t &);   // disable copy constructor
+  void operator=(const kernel_info_t &);  // disable copy operator
+
+  class function_info *m_kernel_entry;
 
-   class function_info *m_kernel_entry;
+  unsigned m_uid;
+  static unsigned m_next_uid;
 
-   unsigned m_uid;
-   static unsigned m_next_uid;
-   
-   //These maps contain the snapshot of the texture mappings at kernel launch
-   std::map<std::string, const struct cudaArray*> m_NameToCudaArray;
-   std::map<std::string, const struct textureInfo*> m_NameToTextureInfo;
+  // These maps contain the snapshot of the texture mappings at kernel launch
+  std::map<std::string, const struct cudaArray *> m_NameToCudaArray;
+  std::map<std::string, const struct textureInfo *> m_NameToTextureInfo;
 
-   dim3 m_grid_dim;
-   dim3 m_block_dim;
-   dim3 m_next_cta;
-   dim3 m_next_tid;
+  dim3 m_grid_dim;
+  dim3 m_block_dim;
+  dim3 m_next_cta;
+  dim3 m_next_tid;
 
-   unsigned m_num_cores_running;
+  unsigned m_num_cores_running;
 
-   std::list<class ptx_thread_info *> m_active_threads;
-   class memory_space *m_param_mem;
+  std::list<class ptx_thread_info *> m_active_threads;
+  class memory_space *m_param_mem;
 
 public:
    // TODO schi
@@ -325,26 +352,27 @@ public:
    address_type get_inst_base_vaddr() { return m_inst_text_base_vaddr; };
    void set_inst_base_vaddr(address_type addr) { m_inst_text_base_vaddr = addr; };
 
-   //Jin: parent and child kernel management for CDP
-   void set_parent(kernel_info_t * parent, dim3 parent_ctaid, dim3 parent_tid);
-   void set_child(kernel_info_t * child);
-   void remove_child(kernel_info_t * child);
-   bool is_finished();
-   bool children_all_finished();
-   void notify_parent_finished();
-   CUstream_st * create_stream_cta(dim3 ctaid);
-   CUstream_st * get_default_stream_cta(dim3 ctaid);
-   bool cta_has_stream(dim3 ctaid, CUstream_st* stream);
-   void destroy_cta_streams();
-   void print_parent_info();
-   kernel_info_t * get_parent() { return m_parent_kernel; }
-
-private:
-   kernel_info_t * m_parent_kernel;
-   dim3 m_parent_ctaid;
-   dim3 m_parent_tid;
-   std::list<kernel_info_t *> m_child_kernels; //child kernel launched
-   std::map< dim3, std::list<CUstream_st *>, dim3comp > m_cta_streams; //streams created in each CTA
+  // Jin: parent and child kernel management for CDP
+  void set_parent(kernel_info_t *parent, dim3 parent_ctaid, dim3 parent_tid);
+  void set_child(kernel_info_t *child);
+  void remove_child(kernel_info_t *child);
+  bool is_finished();
+  bool children_all_finished();
+  void notify_parent_finished();
+  CUstream_st *create_stream_cta(dim3 ctaid);
+  CUstream_st *get_default_stream_cta(dim3 ctaid);
+  bool cta_has_stream(dim3 ctaid, CUstream_st *stream);
+  void destroy_cta_streams();
+  void print_parent_info();
+  kernel_info_t *get_parent() { return m_parent_kernel; }
+
+ private:
+  kernel_info_t *m_parent_kernel;
+  dim3 m_parent_ctaid;
+  dim3 m_parent_tid;
+  std::list<kernel_info_t *> m_child_kernels;  // child kernel launched
+  std::map<dim3, std::list<CUstream_st *>, dim3comp>
+      m_cta_streams;  // streams created in each CTA
 
 //Jin: kernel timing
 public:
@@ -353,50 +381,56 @@ public:
    unsigned long long end_cycle;
    unsigned m_launch_latency;
 
-   mutable bool volta_cache_config_set;
+  mutable bool cache_config_set;
+  unsigned m_kernel_TB_latency;  // this used for any CPU-GPU kernel latency and
+                                 // counted in the gpu_cycle
 };
 
-struct core_config {
-    core_config() 
-    { 
-        m_valid = false; 
-        num_shmem_bank=16; 
-        shmem_limited_broadcast = false; 
-        gpgpu_shmem_sizeDefault=(unsigned)-1;
-        gpgpu_shmem_sizePrefL1=(unsigned)-1;
-        gpgpu_shmem_sizePrefShared=(unsigned)-1;
-    }
-    virtual void init() = 0;
-
-    bool m_valid;
-    unsigned warp_size;
-
-    // off-chip memory request architecture parameters
-    int gpgpu_coalesce_arch;
-
-    // shared memory bank conflict checking parameters
-    bool shmem_limited_broadcast;
-    static const address_type WORD_SIZE=4;
-    unsigned num_shmem_bank;
-    unsigned shmem_bank_func(address_type addr) const
-    {
-        return ((addr/WORD_SIZE) % num_shmem_bank);
-    }
-    unsigned mem_warp_parts;  
-    mutable unsigned gpgpu_shmem_size;
-    unsigned gpgpu_shmem_sizeDefault;
-    unsigned gpgpu_shmem_sizePrefL1;
-    unsigned gpgpu_shmem_sizePrefShared;
-    unsigned mem_unit_ports;
-
-    // texture and constant cache line sizes (used to determine number of memory accesses)
-    unsigned gpgpu_cache_texl1_linesize;
-    unsigned gpgpu_cache_constl1_linesize;
-
-	unsigned gpgpu_max_insn_issue_per_warp;
-	bool gmem_skip_L1D; // on = global memory access always skip the L1 cache
-
-	bool adaptive_volta_cache_config;
+class core_config {
+ public:
+  core_config(gpgpu_context *ctx) {
+    gpgpu_ctx = ctx;
+    m_valid = false;
+    num_shmem_bank = 16;
+    shmem_limited_broadcast = false;
+    gpgpu_shmem_sizeDefault = (unsigned)-1;
+    gpgpu_shmem_sizePrefL1 = (unsigned)-1;
+    gpgpu_shmem_sizePrefShared = (unsigned)-1;
+  }
+  virtual void init() = 0;
+
+  bool m_valid;
+  unsigned warp_size;
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+  // off-chip memory request architecture parameters
+  int gpgpu_coalesce_arch;
+
+  // shared memory bank conflict checking parameters
+  bool shmem_limited_broadcast;
+  static const address_type WORD_SIZE = 4;
+  unsigned num_shmem_bank;
+  unsigned shmem_bank_func(address_type addr) const {
+    return ((addr / WORD_SIZE) % num_shmem_bank);
+  }
+  unsigned mem_warp_parts;
+  mutable unsigned gpgpu_shmem_size;
+  char *gpgpu_shmem_option;
+  std::vector<unsigned> shmem_opt_list;
+  unsigned gpgpu_shmem_sizeDefault;
+  unsigned gpgpu_shmem_sizePrefL1;
+  unsigned gpgpu_shmem_sizePrefShared;
+  unsigned mem_unit_ports;
+
+  // texture and constant cache line sizes (used to determine number of memory
+  // accesses)
+  unsigned gpgpu_cache_texl1_linesize;
+  unsigned gpgpu_cache_constl1_linesize;
+
+  unsigned gpgpu_max_insn_issue_per_warp;
+  bool gmem_skip_L1D;  // on = global memory access always skip the L1 cache
+
+  bool adaptive_cache_config;
 };
 
 // bounded stack that implements simt reconvergence using pdom mechanism from MICRO'07 paper
@@ -407,59 +441,81 @@ typedef std::bitset<MAX_WARP_SIZE_SIMT_STACK> simt_mask_t;
 typedef std::vector<address_type> addr_vector_t;
 
 class simt_stack {
-public:
-    simt_stack( unsigned wid,  unsigned warpSize);
-
-    void reset();
-    void launch( address_type start_pc, const simt_mask_t &active_mask );
-    void update( simt_mask_t &thread_done, addr_vector_t &next_pc, address_type recvg_pc, op_type next_inst_op,unsigned next_inst_size, address_type next_inst_pc );
-
-    const simt_mask_t &get_active_mask() const;
-    void     get_pdom_stack_top_info( unsigned *pc, unsigned *rpc ) const;
-    unsigned get_rp() const;
-    void     print(FILE *fp) const;
-    void     resume(char * fname) ;
-    void    print_checkpoint (FILE *fout) const;
-
-protected:
-    unsigned m_warp_id;
-    unsigned m_warp_size;
-
-    enum stack_entry_type {
-        STACK_ENTRY_TYPE_NORMAL = 0,
-        STACK_ENTRY_TYPE_CALL
-    };
-
-    struct simt_stack_entry {
-        address_type m_pc;
-        unsigned int m_calldepth;
-        simt_mask_t m_active_mask;
-        address_type m_recvg_pc;
-        unsigned long long m_branch_div_cycle;
-        stack_entry_type m_type;
-        simt_stack_entry() :
-            m_pc(-1), m_calldepth(0), m_active_mask(), m_recvg_pc(-1), m_branch_div_cycle(0), m_type(STACK_ENTRY_TYPE_NORMAL) { };
-    };
-
-    std::deque<simt_stack_entry> m_stack;
+ public:
+  simt_stack(unsigned wid, unsigned warpSize, class gpgpu_sim *gpu);
+
+  void reset();
+  void launch(address_type start_pc, const simt_mask_t &active_mask);
+  void update(simt_mask_t &thread_done, addr_vector_t &next_pc,
+              address_type recvg_pc, op_type next_inst_op,
+              unsigned next_inst_size, address_type next_inst_pc);
+
+  const simt_mask_t &get_active_mask() const;
+  void get_pdom_stack_top_info(unsigned *pc, unsigned *rpc) const;
+  unsigned get_rp() const;
+  void print(FILE *fp) const;
+  void resume(char *fname);
+  void print_checkpoint(FILE *fout) const;
+
+ protected:
+  unsigned m_warp_id;
+  unsigned m_warp_size;
+
+  enum stack_entry_type { STACK_ENTRY_TYPE_NORMAL = 0, STACK_ENTRY_TYPE_CALL };
+
+  struct simt_stack_entry {
+    address_type m_pc;
+    unsigned int m_calldepth;
+    simt_mask_t m_active_mask;
+    address_type m_recvg_pc;
+    unsigned long long m_branch_div_cycle;
+    stack_entry_type m_type;
+    simt_stack_entry()
+        : m_pc(-1),
+          m_calldepth(0),
+          m_active_mask(),
+          m_recvg_pc(-1),
+          m_branch_div_cycle(0),
+          m_type(STACK_ENTRY_TYPE_NORMAL){};
+  };
+
+  std::deque<simt_stack_entry> m_stack;
+
+  class gpgpu_sim *m_gpu;
 };
 
-#define GLOBAL_HEAP_START 0xC0000000
-   // start allocating from this address (lower values used for allocating globals in .ptx file)
-#define SHARED_MEM_SIZE_MAX (64*1024)
-#define LOCAL_MEM_SIZE_MAX (8*1024)
-#define MAX_STREAMING_MULTIPROCESSORS 64
-#define MAX_THREAD_PER_SM 2048
-#define MAX_WARP_PER_SM 64
-#define TOTAL_LOCAL_MEM_PER_SM (MAX_THREAD_PER_SM*LOCAL_MEM_SIZE_MAX)
-#define TOTAL_SHARED_MEM (MAX_STREAMING_MULTIPROCESSORS*SHARED_MEM_SIZE_MAX)
-#define TOTAL_LOCAL_MEM (MAX_STREAMING_MULTIPROCESSORS*MAX_THREAD_PER_SM*LOCAL_MEM_SIZE_MAX)
-#define SHARED_GENERIC_START (GLOBAL_HEAP_START-TOTAL_SHARED_MEM)
-#define LOCAL_GENERIC_START (SHARED_GENERIC_START-TOTAL_LOCAL_MEM)
-#define STATIC_ALLOC_LIMIT (GLOBAL_HEAP_START - (TOTAL_LOCAL_MEM+TOTAL_SHARED_MEM))
-
-#if !defined(__CUDA_RUNTIME_API_H__)
-
+// Let's just upgrade to C++11 so we can use constexpr here...
+// start allocating from this address (lower values used for allocating globals
+// in .ptx file)
+const unsigned long long GLOBAL_HEAP_START = 0xC0000000;
+// Volta max shmem size is 96kB
+const unsigned long long SHARED_MEM_SIZE_MAX = 96 * (1 << 10);
+// Volta max local mem is 16kB
+const unsigned long long LOCAL_MEM_SIZE_MAX = 1 << 14;
+// Volta Titan V has 80 SMs
+const unsigned MAX_STREAMING_MULTIPROCESSORS = 80;
+// Max 2048 threads / SM
+const unsigned MAX_THREAD_PER_SM = 1 << 11;
+// MAX 64 warps / SM
+const unsigned MAX_WARP_PER_SM = 1 << 6;
+const unsigned long long TOTAL_LOCAL_MEM_PER_SM =
+    MAX_THREAD_PER_SM * LOCAL_MEM_SIZE_MAX;
+const unsigned long long TOTAL_SHARED_MEM =
+    MAX_STREAMING_MULTIPROCESSORS * SHARED_MEM_SIZE_MAX;
+const unsigned long long TOTAL_LOCAL_MEM =
+    MAX_STREAMING_MULTIPROCESSORS * MAX_THREAD_PER_SM * LOCAL_MEM_SIZE_MAX;
+const unsigned long long SHARED_GENERIC_START =
+    GLOBAL_HEAP_START - TOTAL_SHARED_MEM;
+const unsigned long long LOCAL_GENERIC_START =
+    SHARED_GENERIC_START - TOTAL_LOCAL_MEM;
+const unsigned long long STATIC_ALLOC_LIMIT =
+    GLOBAL_HEAP_START - (TOTAL_LOCAL_MEM + TOTAL_SHARED_MEM);
+
+// #if !defined(__CUDA_RUNTIME_API_H__)
+#if !defined(__DRIVER_TYPES_H__)
+#include "driver_types.h"
+//#include "builtin_types.h"
+/*
 enum cudaChannelFormatKind {
    cudaChannelFormatKindSigned,
    cudaChannelFormatKindUnsigned,
@@ -473,6 +529,7 @@ struct cudaChannelFormatDesc {
    int                        w;
    enum cudaChannelFormatKind f;
 };
+*/
 
 struct cudaArray {
    void *devPtr;
@@ -483,7 +540,7 @@ struct cudaArray {
    int size; //in bytes
    unsigned dimensions;
 };
-
+/*
 enum cudaTextureAddressMode {
    cudaAddressModeWrap,
    cudaAddressModeClamp
@@ -505,215 +562,255 @@ struct textureReference {
    enum cudaTextureAddressMode   addressMode[3];
    struct cudaChannelFormatDesc  channelDesc;
 };
+*/
+#endif
 
+#if !defined(__TEXTURE_TYPES_H__)
+#include "texture_types.h"
 #endif
 
 // Struct that record other attributes in the textureReference declaration 
 // - These attributes are passed thru __cudaRegisterTexture()
 struct textureReferenceAttr {
-    const struct textureReference *m_texref; 
-    int m_dim; 
-    enum cudaTextureReadMode m_readmode; 
-    int m_ext; 
-    textureReferenceAttr(const struct textureReference *texref, 
-                         int dim, 
-                         enum cudaTextureReadMode readmode, 
-                         int ext)
-    : m_texref(texref), m_dim(dim), m_readmode(readmode), m_ext(ext) 
-    {  }
+  const struct textureReference *m_texref;
+  int m_dim;
+  enum cudaTextureReadMode m_readmode;
+  int m_ext;
+  textureReferenceAttr(const struct textureReference *texref, int dim,
+                       enum cudaTextureReadMode readmode, int ext)
+      : m_texref(texref), m_dim(dim), m_readmode(readmode), m_ext(ext) {}
 };
 
-class gpgpu_functional_sim_config 
-{
-public:
-    void reg_options(class OptionParser * opp);
-
-    void ptx_set_tex_cache_linesize(unsigned linesize);
-
-    unsigned get_forced_max_capability() const { return m_ptx_force_max_capability; }
-    bool convert_to_ptxplus() const { return m_ptx_convert_to_ptxplus; }
-    bool use_cuobjdump() const { return m_ptx_use_cuobjdump; }
-    bool experimental_lib_support() const { return m_experimental_lib_support; }
-
-    int         get_ptx_inst_debug_to_file() const { return g_ptx_inst_debug_to_file; }
-    const char* get_ptx_inst_debug_file() const  { return g_ptx_inst_debug_file; }
-    int         get_ptx_inst_debug_thread_uid() const { return g_ptx_inst_debug_thread_uid; }
-    unsigned    get_texcache_linesize() const { return m_texcache_linesize; }
-    int get_checkpoint_option() const {return checkpoint_option; }
-    int get_checkpoint_kernel() const {return checkpoint_kernel; }
-    int get_checkpoint_CTA() const {return checkpoint_CTA; }
-    int get_resume_option() const {return resume_option; }
-    int get_resume_kernel() const {return resume_kernel; }
-    int get_resume_CTA() const {return resume_CTA; }
-    int get_checkpoint_CTA_t() const {return checkpoint_CTA_t; }
-    int get_checkpoint_insn_Y() const {return checkpoint_insn_Y; }
-private:
-    // PTX options
-    int m_ptx_convert_to_ptxplus;
-    int m_ptx_use_cuobjdump;
-    int m_experimental_lib_support;
-    unsigned m_ptx_force_max_capability;
-    int checkpoint_option;
-    int checkpoint_kernel;
-    int checkpoint_CTA;
-    int resume_option;
-    int resume_kernel;
-    int resume_CTA;
-    int checkpoint_CTA_t;
-    int checkpoint_insn_Y;
-    int   g_ptx_inst_debug_to_file;
-    char* g_ptx_inst_debug_file;
-    int   g_ptx_inst_debug_thread_uid;
-
-    unsigned m_texcache_linesize;
+class gpgpu_functional_sim_config {
+ public:
+  void reg_options(class OptionParser *opp);
+
+  void ptx_set_tex_cache_linesize(unsigned linesize);
+
+  unsigned get_forced_max_capability() const {
+    return m_ptx_force_max_capability;
+  }
+  bool convert_to_ptxplus() const { return m_ptx_convert_to_ptxplus; }
+  bool use_cuobjdump() const { return m_ptx_use_cuobjdump; }
+  bool experimental_lib_support() const { return m_experimental_lib_support; }
+
+  int get_ptx_inst_debug_to_file() const { return g_ptx_inst_debug_to_file; }
+  const char *get_ptx_inst_debug_file() const { return g_ptx_inst_debug_file; }
+  int get_ptx_inst_debug_thread_uid() const {
+    return g_ptx_inst_debug_thread_uid;
+  }
+  unsigned get_texcache_linesize() const { return m_texcache_linesize; }
+  int get_checkpoint_option() const { return checkpoint_option; }
+  int get_checkpoint_kernel() const { return checkpoint_kernel; }
+  int get_checkpoint_CTA() const { return checkpoint_CTA; }
+  int get_resume_option() const { return resume_option; }
+  int get_resume_kernel() const { return resume_kernel; }
+  int get_resume_CTA() const { return resume_CTA; }
+  int get_checkpoint_CTA_t() const { return checkpoint_CTA_t; }
+  int get_checkpoint_insn_Y() const { return checkpoint_insn_Y; }
+
+ private:
+  // PTX options
+  int m_ptx_convert_to_ptxplus;
+  int m_ptx_use_cuobjdump;
+  int m_experimental_lib_support;
+  unsigned m_ptx_force_max_capability;
+  int checkpoint_option;
+  int checkpoint_kernel;
+  int checkpoint_CTA;
+  unsigned resume_option;
+  unsigned resume_kernel;
+  unsigned resume_CTA;
+  unsigned checkpoint_CTA_t;
+  int checkpoint_insn_Y;
+  int g_ptx_inst_debug_to_file;
+  char *g_ptx_inst_debug_file;
+  int g_ptx_inst_debug_thread_uid;
+
+  unsigned m_texcache_linesize;
 };
 
 namespace gem5 {
     struct CudaGPU;
 }
 
-
 class gpgpu_t {
-public:
-    gpgpu_t( const gpgpu_functional_sim_config &config, gem5::CudaGPU *cuda_gpu );
-    int checkpoint_option;
-    int checkpoint_kernel;
-    int checkpoint_CTA;
-    int resume_option;
-    int resume_kernel;
-    int resume_CTA;
-    int checkpoint_CTA_t;
-    int checkpoint_insn_Y;
-    void* gpu_malloc( size_t size );
-    void* gpu_mallocarray( size_t count );
-    void  gpu_memset( size_t dst_start_addr, int c, size_t count );
-    void  memcpy_to_gpu( size_t dst_start_addr, const void *src, size_t count );
-    void  memcpy_from_gpu( void *dst, size_t src_start_addr, size_t count );
-    void  memcpy_gpu_to_gpu( size_t dst, size_t src, size_t count );
-    
-    class memory_space *get_global_memory() { return m_global_mem; }
-    class memory_space *get_tex_memory() { return m_tex_mem; }
-    class memory_space *get_surf_memory() { return m_surf_mem; }
-
-    void gpgpu_ptx_sim_bindTextureToArray(const struct textureReference* texref, const struct cudaArray* array);
-    void gpgpu_ptx_sim_bindNameToTexture(const char* name, const struct textureReference* texref, int dim, int readmode, int ext);
-    void gpgpu_ptx_sim_unbindTexture(const struct textureReference* texref);
-    const char* gpgpu_ptx_sim_findNamefromTexture(const struct textureReference* texref);
-
-    const struct textureReference* get_texref( const std::string &texname ) const
-    {
-        std::map<std::string, std::set<const struct textureReference*> >::const_iterator t=m_NameToTextureRef.find(texname);
-        assert( t != m_NameToTextureRef.end() );
-        return *(t->second.begin());
-    }
-
-    const struct cudaArray* get_texarray( const std::string &texname ) const
-    {
-        std::map<std::string,const struct cudaArray*>::const_iterator t=m_NameToCudaArray.find(texname);
-        assert(t != m_NameToCudaArray.end());
-        return t->second;
-    }
-
-    const struct textureInfo* get_texinfo( const std::string &texname ) const
-    {
-        std::map<std::string, const struct textureInfo*>::const_iterator t=m_NameToTextureInfo.find(texname);
-        assert(t != m_NameToTextureInfo.end());
-        return t->second;
-    }
-
-    const struct textureReferenceAttr* get_texattr( const std::string &texname ) const
-    {
-        std::map<std::string, const struct textureReferenceAttr*>::const_iterator t=m_NameToAttribute.find(texname);
-        assert(t != m_NameToAttribute.end());
-        return t->second;
-    }
-
-    const gpgpu_functional_sim_config &get_config() const { return m_function_model_config; }
-    FILE* get_ptx_inst_debug_file() { return ptx_inst_debug_file; }
-    
-    //  These maps return the current texture mappings for the GPU at any given time.
-    std::map<std::string, const struct cudaArray*> getNameArrayMapping() {return m_NameToCudaArray;}
-    std::map<std::string, const struct textureInfo*> getNameInfoMapping() {return m_NameToTextureInfo;}
-
-    // gem5 stuff
-    gem5::CudaGPU *gem5CudaGPU;
-    int sharedMemDelay;
-
-
-protected:
-    const gpgpu_functional_sim_config &m_function_model_config;
-    FILE* ptx_inst_debug_file;
-
-    class memory_space *m_global_mem;
-    class memory_space *m_tex_mem;
-    class memory_space *m_surf_mem;
-
-    unsigned long long m_dev_malloc;
-    //  These maps contain the current texture mappings for the GPU at any given time. 
-    std::map<std::string, std::set<const struct textureReference*> > m_NameToTextureRef;
-    std::map<const struct textureReference*, std::string> m_TextureRefToName;
-    std::map<std::string, const struct cudaArray*> m_NameToCudaArray;
-    std::map<std::string, const struct textureInfo*> m_NameToTextureInfo;
-    std::map<std::string, const struct textureReferenceAttr*> m_NameToAttribute;
+ public:
+  gpgpu_t(const gpgpu_functional_sim_config &config, gpgpu_context *ctx, gem5::CudaGPU *cuda_gpu );
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+  int checkpoint_option;
+  int checkpoint_kernel;
+  int checkpoint_CTA;
+  unsigned resume_option;
+  unsigned resume_kernel;
+  unsigned resume_CTA;
+  unsigned checkpoint_CTA_t;
+  int checkpoint_insn_Y;
+  // Move some cycle core stats here instead of being global
+  unsigned long long gpu_sim_cycle;
+  unsigned long long gpu_tot_sim_cycle;
+
+  void *gpu_malloc(size_t size);
+  void *gpu_mallocarray(size_t count);
+  void gpu_memset(size_t dst_start_addr, int c, size_t count);
+  void memcpy_to_gpu(size_t dst_start_addr, const void *src, size_t count);
+  void memcpy_from_gpu(void *dst, size_t src_start_addr, size_t count);
+  void memcpy_gpu_to_gpu(size_t dst, size_t src, size_t count);
+
+  class memory_space *get_global_memory() {
+    return m_global_mem;
+  }
+  class memory_space *get_tex_memory() {
+    return m_tex_mem;
+  }
+  class memory_space *get_surf_memory() {
+    return m_surf_mem;
+  }
+
+  void gpgpu_ptx_sim_bindTextureToArray(const struct textureReference *texref,
+                                        const struct cudaArray *array);
+  void gpgpu_ptx_sim_bindNameToTexture(const char *name,
+                                       const struct textureReference *texref,
+                                       int dim, int readmode, int ext);
+  void gpgpu_ptx_sim_unbindTexture(const struct textureReference *texref);
+  const char *gpgpu_ptx_sim_findNamefromTexture(
+      const struct textureReference *texref);
+
+  const struct textureReference *get_texref(const std::string &texname) const {
+    std::map<std::string,
+             std::set<const struct textureReference *> >::const_iterator t =
+        m_NameToTextureRef.find(texname);
+    assert(t != m_NameToTextureRef.end());
+    return *(t->second.begin());
+  }
+
+  const struct cudaArray *get_texarray(const std::string &texname) const {
+    std::map<std::string, const struct cudaArray *>::const_iterator t =
+        m_NameToCudaArray.find(texname);
+    assert(t != m_NameToCudaArray.end());
+    return t->second;
+  }
+
+  const struct textureInfo *get_texinfo(const std::string &texname) const {
+    std::map<std::string, const struct textureInfo *>::const_iterator t =
+        m_NameToTextureInfo.find(texname);
+    assert(t != m_NameToTextureInfo.end());
+    return t->second;
+  }
+
+  const struct textureReferenceAttr *get_texattr(
+      const std::string &texname) const {
+    std::map<std::string, const struct textureReferenceAttr *>::const_iterator
+        t = m_NameToAttribute.find(texname);
+    assert(t != m_NameToAttribute.end());
+    return t->second;
+  }
+
+  const gpgpu_functional_sim_config &get_config() const {
+    return m_function_model_config;
+  }
+  FILE *get_ptx_inst_debug_file() { return ptx_inst_debug_file; }
+
+  //  These maps return the current texture mappings for the GPU at any given
+  //  time.
+  std::map<std::string, const struct cudaArray *> getNameArrayMapping() {
+    return m_NameToCudaArray;
+  }
+  std::map<std::string, const struct textureInfo *> getNameInfoMapping() {
+    return m_NameToTextureInfo;
+  }
+
+  virtual ~gpgpu_t() {}
+
+  // gem5 stuff
+  gem5::CudaGPU *gem5CudaGPU;
+  int sharedMemDelay;
+
+ protected:
+  const gpgpu_functional_sim_config &m_function_model_config;
+  FILE *ptx_inst_debug_file;
+
+  class memory_space *m_global_mem;
+  class memory_space *m_tex_mem;
+  class memory_space *m_surf_mem;
+
+  unsigned long long m_dev_malloc;
+  //  These maps contain the current texture mappings for the GPU at any given
+  //  time.
+  std::map<std::string, std::set<const struct textureReference *> >
+      m_NameToTextureRef;
+  std::map<const struct textureReference *, std::string> m_TextureRefToName;
+  std::map<std::string, const struct cudaArray *> m_NameToCudaArray;
+  std::map<std::string, const struct textureInfo *> m_NameToTextureInfo;
+  std::map<std::string, const struct textureReferenceAttr *> m_NameToAttribute;
 };
 
-struct gpgpu_ptx_sim_info
-{
-   // Holds properties of the kernel (Kernel's resource use). 
-   // These will be set to zero if a ptxinfo file is not present.
-   int lmem;
-   int smem;
-   int cmem;
-   int gmem;
-   int regs;
-   unsigned maxthreads;
-   unsigned ptx_version;
-   unsigned sm_target;
+struct gpgpu_ptx_sim_info {
+  // Holds properties of the kernel (Kernel's resource use). 
+  // These will be set to zero if a ptxinfo file is not present.
+  int lmem;
+  int smem;
+  int cmem;
+  int gmem;
+  int regs;
+  unsigned maxthreads;
+  unsigned ptx_version;
+  unsigned sm_target;
 };
 
-
 struct gpgpu_ptx_sim_arg {
-   gpgpu_ptx_sim_arg() { m_start=NULL; }
-   gpgpu_ptx_sim_arg(const void *arg, size_t size, size_t offset)
-   {
-      m_start=arg;
-      m_nbytes=size;
-      m_offset=offset;
-   }
-   const void *m_start;
-   size_t m_nbytes;
-   size_t m_offset;
+  gpgpu_ptx_sim_arg() { m_start = NULL; }
+  gpgpu_ptx_sim_arg(const void *arg, size_t size, size_t offset) {
+    m_start = arg;
+    m_nbytes = size;
+    m_offset = offset;
+  }
+  const void *m_start;
+  size_t m_nbytes;
+  size_t m_offset;
 };
 
 typedef std::list<gpgpu_ptx_sim_arg> gpgpu_ptx_sim_arg_list_t;
 
 class memory_space_t {
 public:
-   memory_space_t() { m_type = undefined_space; m_bank=0; }
-   memory_space_t( const enum _memory_space_t &from ) { m_type = from; m_bank = 0; }
-   bool operator==( const memory_space_t &x ) const { return (m_bank == x.m_bank) && (m_type == x.m_type); }
-   bool operator!=( const memory_space_t &x ) const { return !(*this == x); }
-   bool operator<( const memory_space_t &x ) const 
-   { 
-      if(m_type < x.m_type)
-         return true;
-      else if(m_type > x.m_type)
-         return false;
-      else if( m_bank < x.m_bank )
-         return true; 
+  memory_space_t() {
+    m_type = undefined_space;
+    m_bank = 0;
+  }
+  memory_space_t(const enum _memory_space_t &from) {
+    m_type = from;
+    m_bank = 0;
+  }
+  bool operator==(const memory_space_t &x) const {
+    return (m_bank == x.m_bank) && (m_type == x.m_type);
+  }
+  bool operator!=(const memory_space_t &x) const { return !(*this == x); }
+  bool operator<(const memory_space_t &x) const {
+    if (m_type < x.m_type)
+      return true;
+    else if (m_type > x.m_type)
       return false;
-   }
-   enum _memory_space_t get_type() const { return m_type; }
-   void set_type( enum _memory_space_t t ) { m_type = t; }
-   unsigned get_bank() const { return m_bank; }
-   void set_bank( unsigned b ) { m_bank = b; }
-   bool is_const() const { return (m_type == const_space) || (m_type == param_space_kernel); }
-   bool is_local() const { return (m_type == local_space) || (m_type == param_space_local); }
-   bool is_global() const { return (m_type == global_space); }
+    else if (m_bank < x.m_bank)
+      return true;
+    return false;
+  }
+  enum _memory_space_t get_type() const { return m_type; }
+  void set_type(enum _memory_space_t t ) { m_type = t; }
+  unsigned get_bank() const { return m_bank; }
+  void set_bank(unsigned b ) { m_bank = b; }
+  bool is_const() const {
+    return (m_type == const_space) || (m_type == param_space_kernel);
+  }
+  bool is_local() const {
+    return (m_type == local_space) || (m_type == param_space_local);
+  }
+  bool is_global() const { return (m_type == global_space); }
 
 private:
-   enum _memory_space_t m_type;
-   unsigned m_bank; // n in ".const[n]"; note .const == .const[0] (see PTX 2.1 manual, sec. 5.1.3)
+  enum _memory_space_t m_type;
+  unsigned m_bank; // n in ".const[n]"; note .const == .const[0] (see PTX 2.1 manual, sec. 5.1.3)
 };
 
 const unsigned MAX_MEMORY_ACCESS_SIZE = 128;
@@ -747,217 +844,284 @@ MEM_ACCESS_TYPE_TUP_DEF
 #undef MA_TUP
 #undef MA_TUP_END
 
-const char * mem_access_type_str(enum mem_access_type access_type); 
+const char *mem_access_type_str(enum mem_access_type access_type);
 
 enum cache_operator_type {
-    CACHE_UNDEFINED, 
-
-    // loads
-    CACHE_ALL,          // .ca
-    CACHE_LAST_USE,     // .lu
-    CACHE_VOLATILE,     // .cv
-    CACHE_L1,     // .nc
-                       
-    // loads and stores 
-    CACHE_STREAMING,    // .cs
-    CACHE_GLOBAL,       // .cg
-
-    // stores
-    CACHE_WRITE_BACK,   // .wb
-    CACHE_WRITE_THROUGH // .wt
+  CACHE_UNDEFINED,
+
+  // loads
+  CACHE_ALL,       // .ca
+  CACHE_LAST_USE,  // .lu
+  CACHE_VOLATILE,  // .cv
+  CACHE_L1,        // .nc
+
+  // loads and stores
+  CACHE_STREAMING,  // .cs
+  CACHE_GLOBAL,     // .cg
+
+  // stores
+  CACHE_WRITE_BACK,    // .wb
+  CACHE_WRITE_THROUGH  // .wt
 };
 
 class mem_access_t {
-public:
-   mem_access_t() { init(); }
-   mem_access_t( mem_access_type type, 
-                 new_addr_type address, 
-                 unsigned size,
-                 bool wr )
-   {
-       init();
-       m_type = type;
-       m_addr = address;
-       m_req_size = size;
-       m_write = wr;
-   }
-   mem_access_t( mem_access_type type, 
-                 new_addr_type address, 
-                 unsigned size, 
-                 bool wr, 
-                 const active_mask_t &active_mask,
-                 const mem_access_byte_mask_t &byte_mask,
-		 const mem_access_sector_mask_t &sector_mask)
-    : m_warp_mask(active_mask), m_byte_mask(byte_mask), m_sector_mask(sector_mask)
-   {
-      init();
-      m_type = type;
-      m_addr = address;
-      m_req_size = size;
-      m_write = wr;
-   }
-
-   new_addr_type get_addr() const { return m_addr; }
-   void set_addr(new_addr_type addr) {m_addr=addr;}
-   unsigned get_size() const { return m_req_size; }
-   const active_mask_t &get_warp_mask() const { return m_warp_mask; }
-   bool is_write() const { return m_write; }
-   enum mem_access_type get_type() const { return m_type; }
-   mem_access_byte_mask_t get_byte_mask() const { return m_byte_mask; }
-   mem_access_sector_mask_t get_sector_mask() const { return m_sector_mask; }
+ public:
+  mem_access_t(gpgpu_context *ctx) { init(ctx); }
+  mem_access_t(mem_access_type type, new_addr_type address, unsigned size,
+               bool wr, gpgpu_context *ctx) {
+    init(ctx);
+    m_type = type;
+    m_addr = address;
+    m_req_size = size;
+    m_write = wr;
+  }
+  mem_access_t(mem_access_type type, new_addr_type address, unsigned size,
+               bool wr, const active_mask_t &active_mask,
+               const mem_access_byte_mask_t &byte_mask,
+               const mem_access_sector_mask_t &sector_mask, gpgpu_context *ctx)
+      : m_warp_mask(active_mask),
+        m_byte_mask(byte_mask),
+        m_sector_mask(sector_mask) {
+    init(ctx);
+    m_type = type;
+    m_addr = address;
+    m_req_size = size;
+    m_write = wr;
+  }
+
+  new_addr_type get_addr() const { return m_addr; }
+  void set_addr(new_addr_type addr) { m_addr = addr; }
+  unsigned get_size() const { return m_req_size; }
+  const active_mask_t &get_warp_mask() const { return m_warp_mask; }
+  bool is_write() const { return m_write; }
+  enum mem_access_type get_type() const { return m_type; }
+  mem_access_byte_mask_t get_byte_mask() const { return m_byte_mask; }
+  mem_access_sector_mask_t get_sector_mask() const { return m_sector_mask; }
+
+  void print(FILE *fp) const {
+    fprintf(fp, "addr=0x%llx, %s, size=%u, ", m_addr,
+            m_write ? "store" : "load ", m_req_size);
+    switch (m_type) {
+      case GLOBAL_ACC_R:
+        fprintf(fp, "GLOBAL_R");
+        break;
+      case LOCAL_ACC_R:
+        fprintf(fp, "LOCAL_R ");
+        break;
+      case CONST_ACC_R:
+        fprintf(fp, "CONST   ");
+        break;
+      case TEXTURE_ACC_R:
+        fprintf(fp, "TEXTURE ");
+        break;
+      case GLOBAL_ACC_W:
+        fprintf(fp, "GLOBAL_W");
+        break;
+      case LOCAL_ACC_W:
+        fprintf(fp, "LOCAL_W ");
+        break;
+      case L2_WRBK_ACC:
+        fprintf(fp, "L2_WRBK ");
+        break;
+      case INST_ACC_R:
+        fprintf(fp, "INST    ");
+        break;
+      case L1_WRBK_ACC:
+        fprintf(fp, "L1_WRBK ");
+        break;
+      default:
+        fprintf(fp, "unknown ");
+        break;
+    }
+  }
 
-   void print(FILE *fp) const
-   {
-       fprintf(fp,"addr=0x%llx, %s, size=%u, ", m_addr, m_write?"store":"load ", m_req_size );
-       switch(m_type) {
-       case GLOBAL_ACC_R:  fprintf(fp,"GLOBAL_R"); break;
-       case LOCAL_ACC_R:   fprintf(fp,"LOCAL_R "); break;
-       case CONST_ACC_R:   fprintf(fp,"CONST   "); break;
-       case TEXTURE_ACC_R: fprintf(fp,"TEXTURE "); break;
-       case GLOBAL_ACC_W:  fprintf(fp,"GLOBAL_W"); break;
-       case LOCAL_ACC_W:   fprintf(fp,"LOCAL_W "); break;
-       case L2_WRBK_ACC:   fprintf(fp,"L2_WRBK "); break;
-       case INST_ACC_R:    fprintf(fp,"INST    "); break;
-       case L1_WRBK_ACC:   fprintf(fp,"L1_WRBK "); break;
-       default:            fprintf(fp,"unknown "); break;
-       }
-   }
+  gpgpu_context *gpgpu_ctx;
 
 private:
+  void init(gpgpu_context *ctx);
+  /*
    void init() 
    {
       m_uid=++sm_next_access_uid;
       m_addr=0;
       m_req_size=0;
    }
-
-   unsigned      m_uid;
-   new_addr_type m_addr;     // request address
-   bool          m_write;
-   unsigned      m_req_size; // bytes
-   mem_access_type m_type;
-   active_mask_t m_warp_mask;
-   mem_access_byte_mask_t m_byte_mask;
-   mem_access_sector_mask_t m_sector_mask;
-
-   static unsigned sm_next_access_uid;
+   */
+
+  unsigned m_uid;
+  new_addr_type m_addr;  // request address
+  bool m_write;
+  unsigned m_req_size;  // bytes
+  mem_access_type m_type;
+  active_mask_t m_warp_mask;
+  mem_access_byte_mask_t m_byte_mask;
+  mem_access_sector_mask_t m_sector_mask;
+
+  static unsigned sm_next_access_uid;
 };
 
 class mem_fetch;
 
 class mem_fetch_interface {
-public:
-    virtual bool full( unsigned size, bool write ) const = 0;
-    virtual void push( mem_fetch *mf ) = 0;
+ public:
+  virtual bool full(unsigned size, bool write) const = 0;
+  virtual void push(mem_fetch *mf) = 0;
 };
 
 class mem_fetch_allocator {
 public:
-    virtual mem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const = 0;
-    virtual mem_fetch *alloc( const class warp_inst_t &inst, const mem_access_t &access ) const = 0;
+  /*
+  virtual mem_fetch *alloc( new_addr_type addr, mem_access_type type,
+                                unsigned size, bool wr ) const = 0;
+  virtual mem_fetch *alloc( const class warp_inst_t &inst,
+                                const mem_access_t &access ) const = 0;
+                                */
+  virtual mem_fetch *alloc(new_addr_type addr, mem_access_type type,
+                           unsigned size, bool wr,
+                           unsigned long long cycle) const = 0;
+  virtual mem_fetch *alloc(const class warp_inst_t &inst,
+                           const mem_access_t &access,
+                           unsigned long long cycle) const = 0;
+  virtual mem_fetch *alloc(new_addr_type addr, mem_access_type type,
+                           const active_mask_t &active_mask,
+                           const mem_access_byte_mask_t &byte_mask,
+                           const mem_access_sector_mask_t &sector_mask,
+                           unsigned size, bool wr, unsigned long long cycle,
+                           unsigned wid, unsigned sid, unsigned tpc,
+                           mem_fetch *original_mf) const = 0;
 };
 
-// the maximum number of destination, source, or address uarch operands in a instruction
-#define MAX_REG_OPERANDS 32 
+// the maximum number of destination, source, or address uarch operands in a
+// instruction
+#define MAX_REG_OPERANDS 32
 
 struct dram_callback_t {
-   dram_callback_t() { function=NULL; instruction=NULL; thread=NULL; }
-   void (*function)(const class inst_t*, class ptx_thread_info*);
-
-   const class inst_t* instruction;
-   class ptx_thread_info *thread;
+  dram_callback_t() {
+    function = NULL;
+    instruction = NULL;
+    thread = NULL;
+  }
+  void (*function)(const class inst_t *, class ptx_thread_info *);
+
+  const class inst_t *instruction;
+  class ptx_thread_info *thread;
 };
 
 class inst_t {
-public:
-    inst_t()
-    {
-        m_decoded=false;
-        pc=(address_type)-1;
-        reconvergence_pc=(address_type)-1;
-        op=NO_OP;
-        bar_type=NOT_BAR;
-        red_type=NOT_RED;
-        bar_id=(unsigned)-1;
-        bar_count=(unsigned)-1;
-        oprnd_type=UN_OP;
-        sp_op=OTHER_OP;
-        op_pipe=UNKOWN_OP;
-        mem_op=NOT_TEX;
-        num_operands=0;
-        num_regs=0;
-        memset(out, 0, sizeof(unsigned)); 
-        memset(in, 0, sizeof(unsigned)); 
-        is_vectorin=0; 
-        is_vectorout=0;
-        space = memory_space_t();
-        cache_op = CACHE_UNDEFINED;
-        latency = 1;
-        initiation_interval = 1;
-        for( unsigned i=0; i < MAX_REG_OPERANDS; i++ ) {
-            arch_reg.src[i] = -1;
-            arch_reg.dst[i] = -1;
-        }
-        isize=0;
-    }
-    bool valid() const { return m_decoded; }
-    virtual void print_insn( FILE *fp ) const 
-    {
-        fprintf(fp," [inst @ pc=0x%04x] ", pc );
+ public:
+  inst_t() {
+    m_decoded = false;
+    pc = (address_type)-1;
+    reconvergence_pc = (address_type)-1;
+    op = NO_OP;
+    bar_type = NOT_BAR;
+    red_type = NOT_RED;
+    bar_id = (unsigned)-1;
+    bar_count = (unsigned)-1;
+    oprnd_type = UN_OP;
+    sp_op = OTHER_OP;
+    op_pipe = UNKOWN_OP;
+    mem_op = NOT_TEX;
+    const_cache_operand = 0;
+    num_operands = 0;
+    num_regs = 0;
+    memset(out, 0, sizeof(unsigned));
+    memset(in, 0, sizeof(unsigned));
+    is_vectorin = 0;
+    is_vectorout = 0;
+    space = memory_space_t();
+    cache_op = CACHE_UNDEFINED;
+    latency = 1;
+    initiation_interval = 1;
+    for (unsigned i = 0; i < MAX_REG_OPERANDS; i++) {
+      arch_reg.src[i] = -1;
+      arch_reg.dst[i] = -1;
     }
-    bool is_load() const { return (op == LOAD_OP ||op==TENSOR_CORE_LOAD_OP || memory_op == memory_load); }
-    bool is_store() const { return (op == STORE_OP ||op==TENSOR_CORE_STORE_OP || memory_op == memory_store); }
-    unsigned get_num_operands() const {return num_operands;}
-    unsigned get_num_regs() const {return num_regs;}
-    void set_num_regs(unsigned num) {num_regs=num;}
-    void set_num_operands(unsigned num) {num_operands=num;}
-    void set_bar_id(unsigned id) {bar_id=id;}
-    void set_bar_count(unsigned count) {bar_count=count;}
-
-    address_type pc;        // program counter address of instruction
-    unsigned isize;         // size of instruction in bytes 
-    op_type op;             // opcode (uarch visible)
-
-    barrier_type bar_type;
-    reduction_type red_type;
-    unsigned bar_id;
-    unsigned bar_count;
-
-    types_of_operands oprnd_type;     // code (uarch visible) identify if the operation is an interger or a floating point
-    special_ops sp_op;           // code (uarch visible) identify if int_alu, fp_alu, int_mul ....
-    operation_pipeline op_pipe;  // code (uarch visible) identify the pipeline of the operation (SP, SFU or MEM)
-    mem_operation mem_op;        // code (uarch visible) identify memory type
-    _memory_op_t memory_op; // memory_op used by ptxplus 
-    unsigned num_operands;
-    unsigned num_regs; // count vector operand as one register operand
-
-    address_type reconvergence_pc; // -1 => not a branch, -2 => use function return address
-    
-    unsigned out[8];
-    unsigned outcount;
-    unsigned in[24];
-    unsigned incount;
-    unsigned char is_vectorin;
-    unsigned char is_vectorout;
-    int pred; // predicate register number
-    int ar1, ar2;
-    // register number for bank conflict evaluation
-    struct {
-        int dst[MAX_REG_OPERANDS];
-        int src[MAX_REG_OPERANDS];
-    } arch_reg;
-    //int arch_reg[MAX_REG_OPERANDS]; // register number for bank conflict evaluation
-    unsigned latency; // operation latency
-    unsigned initiation_interval;
-
-    unsigned data_size; // what is the size of the word being operated on?
-    int data_type;      // TODO schi add
-    memory_space_t space;
-    cache_operator_type cache_op;
+    isize = 0;
+  }
+  bool valid() const { return m_decoded; }
+  virtual void print_insn(FILE *fp) const {
+    fprintf(fp, " [inst @ pc=0x%04x] ", pc);
+  }
+  bool is_load() const {
+    return (op == LOAD_OP || op == TENSOR_CORE_LOAD_OP ||
+            memory_op == memory_load);
+  }
+  bool is_store() const {
+    return (op == STORE_OP || op == TENSOR_CORE_STORE_OP ||
+            memory_op == memory_store);
+  }
+
+  bool is_fp() const { return ((sp_op == FP__OP));}    //VIJAY
+  bool is_fpdiv() const { return ((sp_op == FP_DIV_OP));} 
+  bool is_fpmul() const { return ((sp_op == FP_MUL_OP));} 
+  bool is_dp() const { return ((sp_op == DP___OP));}    
+  bool is_dpdiv() const { return ((sp_op == DP_DIV_OP));} 
+  bool is_dpmul() const { return ((sp_op == DP_MUL_OP));}
+  bool is_imul() const { return ((sp_op == INT_MUL_OP));} 
+  bool is_imul24() const { return ((sp_op == INT_MUL24_OP));} 
+  bool is_imul32() const { return ((sp_op == INT_MUL32_OP));} 
+  bool is_idiv() const { return ((sp_op == INT_DIV_OP));}   
+  bool is_sfu() const {return ((sp_op == FP_SQRT_OP) || (sp_op == FP_LG_OP)  || (sp_op == FP_SIN_OP)  || (sp_op == FP_EXP_OP) || (sp_op == TENSOR__OP));}
+  bool is_alu() const {return (sp_op == INT__OP);}
+
+  unsigned get_num_operands() const { return num_operands; }
+  unsigned get_num_regs() const { return num_regs; }
+  void set_num_regs(unsigned num) { num_regs = num; }
+  void set_num_operands(unsigned num) { num_operands = num; }
+  void set_bar_id(unsigned id) { bar_id = id; }
+  void set_bar_count(unsigned count) { bar_count = count; }
+
+  address_type pc;  // program counter address of instruction
+  unsigned isize;   // size of instruction in bytes
+  op_type op;       // opcode (uarch visible)
+
+  barrier_type bar_type;
+  reduction_type red_type;
+  unsigned bar_id;
+  unsigned bar_count;
+
+  types_of_operands oprnd_type;  // code (uarch visible) identify if the
+                                 // operation is an interger or a floating point
+  special_ops
+      sp_op;  // code (uarch visible) identify if int_alu, fp_alu, int_mul ....
+  operation_pipeline op_pipe;  // code (uarch visible) identify the pipeline of
+                               // the operation (SP, SFU or MEM)
+  mem_operation mem_op;        // code (uarch visible) identify memory type
+  bool const_cache_operand;   // has a load from constant memory as an operand
+  _memory_op_t memory_op;      // memory_op used by ptxplus
+  unsigned num_operands;
+  unsigned num_regs; // count vector operand as one register operand
+
+  address_type reconvergence_pc;  // -1 => not a branch, -2 => use function
+                                  // return address
+
+  unsigned out[8];
+  unsigned outcount;
+  unsigned in[24];
+  unsigned incount;
+  unsigned char is_vectorin;
+  unsigned char is_vectorout;
+  int pred;  // predicate register number
+  int ar1, ar2;
+  // register number for bank conflict evaluation
+  struct {
+    int dst[MAX_REG_OPERANDS];
+    int src[MAX_REG_OPERANDS];
+  } arch_reg;
+  // int arch_reg[MAX_REG_OPERANDS]; 
+  // register number for bank conflict evaluation
+  unsigned latency; // operation latency
+  unsigned initiation_interval;
+
+  unsigned data_size; // what is the size of the word being operated on?
+  int data_type;      // TODO schi add
+  memory_space_t space;
+  cache_operator_type cache_op;
 
 protected:
-    bool m_decoded;
-    virtual void pre_decode() {}
+  bool m_decoded;
+  virtual void pre_decode() {}
 };
 
 enum divergence_support_t {
@@ -972,36 +1136,34 @@ const unsigned MAX_DATA_BYTES_PER_INSN_PER_THREAD = 16;
 class warp_inst_t: public inst_t {
 public:
     // constructors
-    warp_inst_t() 
-    {
-        m_uid=0;
-        m_empty=true; 
-        m_config=NULL; 
-    }
-    warp_inst_t( const core_config *config ) 
-    { 
-        m_uid=0;
-        assert(config->warp_size<=MAX_WARP_SIZE); 
-        m_config=config;
-        m_empty=true; 
-        m_isatomic=false;
-        m_per_scalar_thread_valid=false;
-        m_mem_accesses_created=false;
-        m_cache_hit=false;
-        m_is_printf=false;
-        m_is_cdp = 0;
-    }
-    virtual ~warp_inst_t(){
-    }
-
-    // modifiers
-    void broadcast_barrier_reduction( const active_mask_t& access_mask);
-    void do_atomic(bool forceDo=false);
-    void do_atomic( const active_mask_t& access_mask, bool forceDo=false );
-    void clear() 
-    { 
-        m_empty=true; 
-    }
+  warp_inst_t() {
+    m_uid = 0;
+    m_empty = true;
+    m_config = NULL;
+  }
+  warp_inst_t(const core_config *config) {
+    m_uid = 0;
+    assert(config->warp_size <= MAX_WARP_SIZE);
+    m_config = config;
+    m_empty = true;
+    m_isatomic = false;
+    m_per_scalar_thread_valid = false;
+    m_mem_accesses_created = false;
+    m_cache_hit = false;
+    m_is_printf = false;
+    m_is_cdp = 0;
+    should_do_atomic = true;
+  }
+  virtual ~warp_inst_t() {}
+
+  // modifiers
+  void broadcast_barrier_reduction( const active_mask_t& access_mask);
+  void do_atomic(bool forceDo = false);
+  void do_atomic(const active_mask_t& access_mask, bool forceDo=false );
+  void clear() { m_empty=true; }
+  void issue(const active_mask_t &mask, unsigned warp_id,
+             unsigned long long cycle, int dynamic_warp_id, int sch_id);
+/*
     void issue( const active_mask_t &mask, unsigned warp_id, unsigned long long cycle, int dynamic_warp_id, int sch_id )
     {
         m_warp_active_mask = mask;
@@ -1015,30 +1177,27 @@ public:
         m_empty=false;
         m_scheduler_id=sch_id;
     }
-    const active_mask_t & get_active_mask() const
-    {
-    	return m_warp_active_mask;
+*/
+  const active_mask_t &get_active_mask() const { return m_warp_active_mask; }
+  void completed(unsigned long long cycle)
+      const;  // stat collection: called when the instruction is completed
+
+  void set_addr(unsigned n, new_addr_type addr) {
+    if (!m_per_scalar_thread_valid) {
+      m_per_scalar_thread.resize(m_config->warp_size);
+      m_per_scalar_thread_valid = true;
     }
-    void completed( unsigned long long cycle ) const;  // stat collection: called when the instruction is completed  
-
-    void set_addr( unsigned n, new_addr_type addr ) 
-    {
-        if( !m_per_scalar_thread_valid ) {
-            m_per_scalar_thread.resize(m_config->warp_size);
-            m_per_scalar_thread_valid=true;
-        }
-        m_per_scalar_thread[n].memreqaddr[0] = addr;
-    }
-    void set_addr( unsigned n, new_addr_type* addr, unsigned num_addrs )
-    {
-        if( !m_per_scalar_thread_valid ) {
-            m_per_scalar_thread.resize(m_config->warp_size);
-            m_per_scalar_thread_valid=true;
-        }
-        assert(num_addrs <= MAX_ACCESSES_PER_INSN_PER_THREAD);
-        for(unsigned i=0; i<num_addrs; i++)
-            m_per_scalar_thread[n].memreqaddr[i] = addr[i];
+    m_per_scalar_thread[n].memreqaddr[0] = addr;
+  }
+  void set_addr(unsigned n, new_addr_type *addr, unsigned num_addrs) {
+    if (!m_per_scalar_thread_valid) {
+      m_per_scalar_thread.resize(m_config->warp_size);
+      m_per_scalar_thread_valid = true;
     }
+    assert(num_addrs <= MAX_ACCESSES_PER_INSN_PER_THREAD);
+    for (unsigned i = 0; i < num_addrs; i++)
+      m_per_scalar_thread[n].memreqaddr[i] = addr[i];
+  }
 
     // TODO schi add
     void set_data( unsigned n, const uint8_t *_data )
@@ -1052,91 +1211,93 @@ public:
         memcpy(&m_per_scalar_thread[n].data, _data, MAX_DATA_BYTES_PER_INSN_PER_THREAD);
     }
 
-    void print_m_accessq(){
-    		
-		if(accessq_empty())
-			return;
-		else{
-			printf("Printing mem access generated\n");
-			std::list<mem_access_t>::iterator it;	
-			for (it = m_accessq.begin(); it != m_accessq.end(); ++it){
-   				 printf("MEM_TXN_GEN:%s:%llx, Size:%d \n",mem_access_type_str(it->get_type()), it->get_addr(),it->get_size());
-			}	
-		}
-    }   
-    struct transaction_info {
-        std::bitset<4> chunks; // bitmask: 32-byte chunks accessed
-        mem_access_byte_mask_t bytes;
-        active_mask_t active; // threads in this transaction
-
-        bool test_bytes(unsigned start_bit, unsigned end_bit) {
-           for( unsigned i=start_bit; i<=end_bit; i++ )
-              if(bytes.test(i))
-                 return true;
-           return false;
-        }
-    };
-
-    void generate_mem_accesses();
-    void memory_coalescing_arch( bool is_write, mem_access_type access_type );
-    void memory_coalescing_arch_atomic( bool is_write, mem_access_type access_type );
-    void memory_coalescing_arch_reduce_and_send( bool is_write, mem_access_type access_type, const transaction_info &info, new_addr_type addr, unsigned segment_size );
-
-    void add_callback( unsigned lane_id, 
-                       void (*function)(const class inst_t*, class ptx_thread_info*),
-                       const inst_t *inst, 
-                       class ptx_thread_info *thread,
-                       bool atomic)
-    {
-        if( !m_per_scalar_thread_valid ) {
-            m_per_scalar_thread.resize(m_config->warp_size);
-            m_per_scalar_thread_valid=true;
-            if(atomic) m_isatomic=true;
-        }
-        m_per_scalar_thread[lane_id].callback.function = function;
-        m_per_scalar_thread[lane_id].callback.instruction = inst;
-        m_per_scalar_thread[lane_id].callback.thread = thread;
+  void print_m_accessq() {
+    if (accessq_empty())
+      return;
+    else {
+      printf("Printing mem access generated\n");
+      std::list<mem_access_t>::iterator it;
+      for (it = m_accessq.begin(); it != m_accessq.end(); ++it) {
+        printf("MEM_TXN_GEN:%s:%llx, Size:%d \n",
+               mem_access_type_str(it->get_type()), it->get_addr(),
+               it->get_size());
+      }
     }
-    void set_active( const active_mask_t &active );
-
-    void clear_active( const active_mask_t &inactive );
-    void set_not_active( unsigned lane_id );
-
-    // accessors
-    virtual void print_insn(FILE *fp) const 
-    {
-        fprintf(fp," [inst @ pc=0x%04x] ", pc );
-        for (int i=(int)m_config->warp_size-1; i>=0; i--)
-            fprintf(fp, "%c", ((m_warp_active_mask[i])?'1':'0') );
-    }
-    bool active( unsigned thread ) const { return m_warp_active_mask.test(thread); }
-    unsigned active_count() const { return m_warp_active_mask.count(); }
-    unsigned issued_count() const { assert(m_empty == false); return m_warp_issued_mask.count(); }  // for instruction counting 
-    bool empty() const { return m_empty; }
-    unsigned warp_id() const 
-    { 
-        assert( !m_empty );
-        return m_warp_id; 
-    }
-    unsigned warp_id_func() const // to be used in functional simulations only
-    { 
-        return m_warp_id; 
-    }
-    unsigned dynamic_warp_id() const 
-    { 
-        assert( !m_empty );
-        return m_dynamic_warp_id; 
-    }
-    bool has_callback( unsigned n ) const
-    {
-        return m_warp_active_mask[n] && m_per_scalar_thread_valid && 
-            (m_per_scalar_thread[n].callback.function!=NULL);
+  }
+  struct transaction_info {
+    std::bitset<4> chunks;  // bitmask: 32-byte chunks accessed
+    mem_access_byte_mask_t bytes;
+    active_mask_t active;  // threads in this transaction
+
+    bool test_bytes(unsigned start_bit, unsigned end_bit) {
+      for (unsigned i = start_bit; i <= end_bit; i++)
+        if (bytes.test(i)) return true;
+      return false;
     }
-    new_addr_type get_addr( unsigned n ) const
-    {
-        assert( m_per_scalar_thread_valid );
-        return m_per_scalar_thread[n].memreqaddr[0];
+  };
+
+  void generate_mem_accesses();
+  void memory_coalescing_arch(bool is_write, mem_access_type access_type);
+  void memory_coalescing_arch_atomic(bool is_write,
+                                     mem_access_type access_type);
+  void memory_coalescing_arch_reduce_and_send(bool is_write,
+                                              mem_access_type access_type,
+                                              const transaction_info &info,
+                                              new_addr_type addr,
+                                              unsigned segment_size);
+
+  void add_callback(unsigned lane_id,
+                    void (*function)(const class inst_t *,
+                                     class ptx_thread_info *),
+                    const inst_t *inst, class ptx_thread_info *thread,
+                    bool atomic) {
+    if (!m_per_scalar_thread_valid) {
+      m_per_scalar_thread.resize(m_config->warp_size);
+      m_per_scalar_thread_valid = true;
+      if (atomic) m_isatomic = true;
     }
+    m_per_scalar_thread[lane_id].callback.function = function;
+    m_per_scalar_thread[lane_id].callback.instruction = inst;
+    m_per_scalar_thread[lane_id].callback.thread = thread;
+  }
+  void set_active(const active_mask_t &active);
+
+  void clear_active(const active_mask_t &inactive);
+  void set_not_active(unsigned lane_id);
+
+  // accessors
+  virtual void print_insn(FILE *fp) const {
+    fprintf(fp, " [inst @ pc=0x%04x] ", pc);
+    for (int i = (int)m_config->warp_size - 1; i >= 0; i--)
+      fprintf(fp, "%c", ((m_warp_active_mask[i]) ? '1' : '0'));
+  }
+  bool active(unsigned thread) const { return m_warp_active_mask.test(thread); }
+  unsigned active_count() const { return m_warp_active_mask.count(); }
+  unsigned issued_count() const {
+    assert(m_empty == false);
+    return m_warp_issued_mask.count();
+  }  // for instruction counting
+  bool empty() const { return m_empty; }
+  unsigned warp_id() const {
+    assert(!m_empty);
+    return m_warp_id;
+  }
+  unsigned warp_id_func() const  // to be used in functional simulations only
+  {
+    return m_warp_id;
+  }
+  unsigned dynamic_warp_id() const {
+    assert(!m_empty);
+    return m_dynamic_warp_id;
+  }
+  bool has_callback(unsigned n) const {
+    return m_warp_active_mask[n] && m_per_scalar_thread_valid &&
+           (m_per_scalar_thread[n].callback.function != NULL);
+  }
+  new_addr_type get_addr(unsigned n) const {
+    assert(m_per_scalar_thread_valid);
+    return m_per_scalar_thread[n].memreqaddr[0];
+  }
 
     // TODO schi add
     const uint8_t *get_data( unsigned n ) const
@@ -1146,267 +1307,325 @@ public:
         return &(m_per_scalar_thread[n].data[0]);
     }
 
-    bool isatomic() const { return m_isatomic; }
+  bool isatomic() const { return m_isatomic; }
 
-    unsigned warp_size() const { return m_config->warp_size; }
+  unsigned warp_size() const { return m_config->warp_size; }
 
-    bool accessq_empty() const { return m_accessq.empty(); }
-    unsigned accessq_count() const { return m_accessq.size(); }
-    const mem_access_t &accessq_back() { return m_accessq.back(); }
-    void accessq_pop_back() { m_accessq.pop_back(); }
+  bool accessq_empty() const { return m_accessq.empty(); }
+  unsigned accessq_count() const { return m_accessq.size(); }
+  const mem_access_t &accessq_back() { return m_accessq.back(); }
+  void accessq_pop_back() { m_accessq.pop_back(); }
 
-    bool dispatch_delay()
-    { 
-        if( cycles > 0 ) 
-            cycles--;
-        return cycles > 0;
-    }
-
-    bool has_dispatch_delay(){
-    	return cycles > 0;
-    }
+  bool dispatch_delay() {
+    if (cycles > 0) cycles--;
+    return cycles > 0;
+  }
 
-    void print( FILE *fout ) const;
-    unsigned get_uid() const { return m_uid; }
-    unsigned get_schd_id() const { return m_scheduler_id; }
+  bool has_dispatch_delay() { return cycles > 0; }
 
-    // TODO schi add fro gpu/gpgpu-sim/cuda_core use it
-    int vectorLength;
-    int get_atomic() const { return m_atomic_spec; }
+  void print(FILE *fout) const;
+  unsigned get_uid() const { return m_uid; }
+  unsigned get_schd_id() const { return m_scheduler_id; }
+  active_mask_t get_warp_active_mask() const { return m_warp_active_mask; }
 
+  // TODO schi add fro gpu/gpgpu-sim/cuda_core use it
+  int vectorLength;
+  int get_atomic() const { return m_atomic_spec; }
 
-protected:
-
-    unsigned m_uid;
-    bool m_empty;
-    bool m_cache_hit;
-    unsigned long long issue_cycle;
-    unsigned cycles; // used for implementing initiation interval delay
-    bool m_isatomic;
+ protected:
+  unsigned m_uid;
+  bool m_empty;
+  bool m_cache_hit;
+  unsigned long long issue_cycle;
+  unsigned cycles;  // used for implementing initiation interval delay
+  bool m_isatomic;
+  bool should_do_atomic;
     int m_atomic_spec; // TODO schi add 
-    bool m_is_printf;
-    unsigned m_warp_id;
-    unsigned m_dynamic_warp_id; 
-    const core_config *m_config; 
-    active_mask_t m_warp_active_mask; // dynamic active mask for timing model (after predication)
-    active_mask_t m_warp_issued_mask; // active mask at issue (prior to predication test) -- for instruction counting
-
-    struct per_thread_info {
-        per_thread_info() {
-            for(unsigned i=0; i<MAX_ACCESSES_PER_INSN_PER_THREAD; i++)
-                memreqaddr[i] = 0;
-            // TODO schi add
-            data_valid = false;
-        }
-        dram_callback_t callback;
-        new_addr_type memreqaddr[MAX_ACCESSES_PER_INSN_PER_THREAD]; // effective address, upto 8 different requests (to support 32B access in 8 chunks of 4B each)
-
+  bool m_is_printf;
+  unsigned m_warp_id;
+  unsigned m_dynamic_warp_id; 
+  const core_config *m_config; 
+  active_mask_t m_warp_active_mask;  // dynamic active mask for timing model
+                                     // (after predication)
+  active_mask_t
+      m_warp_issued_mask;  // active mask at issue (prior to predication test)
+                           // -- for instruction counting
+
+  struct per_thread_info {
+    per_thread_info() {
+      for (unsigned i = 0; i < MAX_ACCESSES_PER_INSN_PER_THREAD; i++)
+        memreqaddr[i] = 0;
         // TODO schi add
-        bool data_valid;
-        uint8_t data[MAX_DATA_BYTES_PER_INSN_PER_THREAD];
-    };
-    bool m_per_scalar_thread_valid;
-    std::vector<per_thread_info> m_per_scalar_thread;
-    bool m_mem_accesses_created;
-    std::list<mem_access_t> m_accessq;
-
-    static unsigned sm_next_uid;
-
-    unsigned m_scheduler_id;  //the scheduler that issues this inst
-
-    //Jin: cdp support
+        data_valid = false;
+    }
+    dram_callback_t callback;
+    new_addr_type
+        memreqaddr[MAX_ACCESSES_PER_INSN_PER_THREAD];  // effective address,
+                                                       // upto 8 different
+                                                       // requests (to support
+                                                       // 32B access in 8 chunks
+                                                       // of 4B each)
+    // TODO schi add
+    bool data_valid;
+    uint8_t data[MAX_DATA_BYTES_PER_INSN_PER_THREAD];
+  };
+  bool m_per_scalar_thread_valid;
+  std::vector<per_thread_info> m_per_scalar_thread;
+  bool m_mem_accesses_created;
+  std::list<mem_access_t> m_accessq;
+
+  static unsigned sm_next_uid;
+  unsigned m_scheduler_id;  //the scheduler that issues this inst
+
+  // Jin: cdp support
 public:
-    int m_is_cdp;
-    
+  int m_is_cdp;
 };
 
-void move_warp( warp_inst_t *&dst, warp_inst_t *&src );
-
-size_t get_kernel_code_size( class function_info *entry );
-class checkpoint
-{    
-public:
-
-     checkpoint();
-    ~checkpoint(){
-      printf("clasfsfss destructed\n");
-    }
-
-    void load_global_mem(class memory_space *temp_mem, char * f1name);
-    void store_global_mem(class memory_space *mem, char * fname , char * format);
-    unsigned radnom;
+void move_warp(warp_inst_t *&dst, warp_inst_t *&src);
 
+size_t get_kernel_code_size(class function_info *entry);
+class checkpoint {
+ public:
+  checkpoint();
+  ~checkpoint() { printf("clasfsfss destructed\n"); }
 
+  void load_global_mem(class memory_space *temp_mem, char *f1name);
+  void store_global_mem(class memory_space *mem, char *fname, char *format);
+  unsigned radnom;
 };
 /*
- * This abstract class used as a base for functional and performance and simulation, it has basic functional simulation
- * data structures and procedures. 
+ * This abstract class used as a base for functional and performance and
+ * simulation, it has basic functional simulation data structures and
+ * procedures.
  */
 class core_t {
-    public:
-        core_t( gpgpu_sim *gpu, 
-                kernel_info_t *kernel,
-                unsigned warp_size,
-                unsigned threads_per_shader )
-            : m_gpu( gpu ),
-              m_kernel( kernel ),
-              m_simt_stack( NULL ),
-              m_thread( NULL ),
-              m_warp_size( warp_size )
-        {
-            m_warp_count = threads_per_shader/m_warp_size;
-            // Handle the case where the number of threads is not a
-            // multiple of the warp size
-            if ( threads_per_shader % m_warp_size != 0 ) {
-                m_warp_count += 1;
-            }
-            assert( m_warp_count * m_warp_size > 0 );
-            m_thread = ( ptx_thread_info** )
-                     calloc( m_warp_count * m_warp_size,
-                             sizeof( ptx_thread_info* ) );
-            initilizeSIMTStack(m_warp_count,m_warp_size);
-
-            for(unsigned i=0; i<MAX_CTA_PER_SHADER; i++){
-            	for(unsigned j=0; j<MAX_BARRIERS_PER_CTA; j++){
-            		reduction_storage[i][j]=0;
-            	}
-            }
-
-        }
-        virtual ~core_t() { free(m_thread); }
-        virtual void warp_exit( unsigned warp_id ) = 0;
-        virtual bool warp_waiting_at_barrier( unsigned warp_id ) const = 0;
-        virtual void checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t, unsigned tid)=0;
-        class gpgpu_sim * get_gpu() {return m_gpu;}
-        void execute_warp_inst_t(warp_inst_t &inst, unsigned warpId =(unsigned)-1);
-        bool  ptx_thread_done( unsigned hw_thread_id ) const ;
-        void updateSIMTStack(unsigned warpId, warp_inst_t * inst);
-        void initilizeSIMTStack(unsigned warp_count, unsigned warps_size);
-        void deleteSIMTStack();
-        warp_inst_t getExecuteWarp(unsigned warpId);
-        void get_pdom_stack_top_info( unsigned warpId, unsigned *pc, unsigned *rpc ) const;
-        kernel_info_t * get_kernel_info(){ return m_kernel;}
-        class ptx_thread_info ** get_thread_info() { return m_thread; }
-        unsigned get_warp_size() const { return m_warp_size; }
+ public:
+  core_t(gpgpu_sim *gpu, kernel_info_t *kernel, unsigned warp_size,
+         unsigned threads_per_shader)
+      : m_gpu(gpu),
+        m_kernel(kernel),
+        m_simt_stack(NULL),
+        m_thread(NULL),
+        m_warp_size(warp_size) {
+    m_warp_count = threads_per_shader / m_warp_size;
+    // Handle the case where the number of threads is not a
+    // multiple of the warp size
+    if (threads_per_shader % m_warp_size != 0) {
+      m_warp_count += 1;
+    }
+    assert(m_warp_count * m_warp_size > 0);
+    m_thread = (ptx_thread_info **)calloc(m_warp_count * m_warp_size,
+                                          sizeof(ptx_thread_info *));
+    initilizeSIMTStack(m_warp_count, m_warp_size);
+
+    for (unsigned i = 0; i < MAX_CTA_PER_SHADER; i++) {
+      for (unsigned j = 0; j < MAX_BARRIERS_PER_CTA; j++) {
+        reduction_storage[i][j] = 0;
+      }
+    }
+  }
+  virtual ~core_t() { free(m_thread); }
+  virtual void warp_exit(unsigned warp_id) = 0;
+  virtual bool warp_waiting_at_barrier(unsigned warp_id) const = 0;
+  virtual void checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t,
+                                             unsigned tid) = 0;
+  class gpgpu_sim *get_gpu() {
+    return m_gpu;
+  }
+  void execute_warp_inst_t(warp_inst_t &inst, unsigned warpId = (unsigned)-1);
+  bool ptx_thread_done(unsigned hw_thread_id) const;
+  virtual void updateSIMTStack(unsigned warpId, warp_inst_t *inst);
+  void initilizeSIMTStack(unsigned warp_count, unsigned warps_size);
+  void deleteSIMTStack();
+  warp_inst_t getExecuteWarp(unsigned warpId);
+  void get_pdom_stack_top_info(unsigned warpId, unsigned *pc,
+                               unsigned *rpc) const;
+  kernel_info_t *get_kernel_info() { return m_kernel; }
+  class ptx_thread_info **get_thread_info() {
+    return m_thread;
+  }
+  unsigned get_warp_size() const { return m_warp_size; }
 
         // TODO schi add
         void writeRegister(const warp_inst_t &inst, unsigned warpSize, unsigned lane_id, char* data);
 
-        void and_reduction(unsigned ctaid, unsigned barid, bool value) { reduction_storage[ctaid][barid] &= value; }
-        void or_reduction(unsigned ctaid, unsigned barid, bool value) { reduction_storage[ctaid][barid] |= value; }
-        void popc_reduction(unsigned ctaid, unsigned barid, bool value) { reduction_storage[ctaid][barid] += value;}
-        unsigned get_reduction_value(unsigned ctaid, unsigned barid) {return reduction_storage[ctaid][barid];}
-    protected:
-        class gpgpu_sim *m_gpu;
-        kernel_info_t *m_kernel;
-        simt_stack  **m_simt_stack; // pdom based reconvergence context for each warp
-        class ptx_thread_info ** m_thread;
-        unsigned m_warp_size;
-        unsigned m_warp_count;
-        unsigned reduction_storage[MAX_CTA_PER_SHADER][MAX_BARRIERS_PER_CTA];
+  void and_reduction(unsigned ctaid, unsigned barid, bool value) {
+    reduction_storage[ctaid][barid] &= value;
+  }
+  void or_reduction(unsigned ctaid, unsigned barid, bool value) {
+    reduction_storage[ctaid][barid] |= value;
+  }
+  void popc_reduction(unsigned ctaid, unsigned barid, bool value) {
+    reduction_storage[ctaid][barid] += value;
+  }
+  unsigned get_reduction_value(unsigned ctaid, unsigned barid) {
+    return reduction_storage[ctaid][barid];
+  }
+ protected:
+  class gpgpu_sim *m_gpu;
+  kernel_info_t *m_kernel;
+  simt_stack **m_simt_stack;  // pdom based reconvergence context for each warp
+  class ptx_thread_info **m_thread;
+  unsigned m_warp_size;
+  unsigned m_warp_count;
+  unsigned reduction_storage[MAX_CTA_PER_SHADER][MAX_BARRIERS_PER_CTA];
 };
 
-
 //register that can hold multiple instructions.
 class register_set {
 public:
-	register_set(unsigned num, const char* name){
-		for( unsigned i = 0; i < num; i++ ) {
-			regs.push_back(new warp_inst_t());
-		}
-		m_name = name;
-	}
-	bool has_free(){
-		for( unsigned i = 0; i < regs.size(); i++ ) {
-			if( regs[i]->empty() ) {
-				return true;
-			}
-		}
-		return false;
-	}
-	bool has_free(bool sub_core_model, unsigned reg_id){
-		//in subcore model, each sched has a one specific reg to use (based on sched id)
-		if(!sub_core_model)
-			return has_free();
-
-		assert(reg_id < regs.size());
-		return regs[reg_id]->empty();
-	}
-	bool has_ready(){
-		for( unsigned i = 0; i < regs.size(); i++ ) {
-			if( not regs[i]->empty() ) {
-				return true;
-			}
-		}
-		return false;
-	}
-
-	void move_in( warp_inst_t *&src ){
-		warp_inst_t** free = get_free();
-		move_warp(*free, src);
-	}
-	//void copy_in( warp_inst_t* src ){
-		//   src->copy_contents_to(*get_free());
-		//}
-	void move_out_to( warp_inst_t *&dest ){
-		warp_inst_t **ready=get_ready();
-		move_warp(dest, *ready);
-	}
-
-	warp_inst_t** get_ready(){
-		warp_inst_t** ready;
-		ready = NULL;
-		for( unsigned i = 0; i < regs.size(); i++ ) {
-			if( not regs[i]->empty() ) {
-				if( ready and (*ready)->get_uid() < regs[i]->get_uid() ) {
-					// ready is oldest
-				} else {
-					ready = &regs[i];
-				}
-			}
-		}
-		return ready;
-	}
-
-	void print(FILE* fp) const{
-		fprintf(fp, "%s : @%p\n", m_name, this);
-		for( unsigned i = 0; i < regs.size(); i++ ) {
-			fprintf(fp, "     ");
-			regs[i]->print(fp);
-			fprintf(fp, "\n");
-		}
-	}
-
-	warp_inst_t ** get_free(){
-		for( unsigned i = 0; i < regs.size(); i++ ) {
-			if( regs[i]->empty() ) {
-				return &regs[i];
-			}
-		}
-		assert(0 && "No free registers found");
-		return NULL;
-	}
-
-	warp_inst_t ** get_free(bool sub_core_model, unsigned reg_id){
-		//in subcore model, each sched has a one specific reg to use (based on sched id)
-		if(!sub_core_model)
-			return get_free();
-
-		assert(reg_id < regs.size());
-		if( regs[reg_id]->empty() ) {
-			return &regs[reg_id];
-		}
-		assert(0 && "No free register found");
-		return NULL;
-	}
-
-	unsigned get_size(){
-		return regs.size();
-	}
+  register_set(unsigned num, const char *name) {
+    for (unsigned i = 0; i < num; i++) {
+      regs.push_back(new warp_inst_t());
+    }
+    m_name = name;
+  }
+  const char *get_name() { return m_name; }
+  bool has_free() {
+    for (unsigned i = 0; i < regs.size(); i++) {
+      if (regs[i]->empty()) {
+        return true;
+      }
+    }
+    return false;
+  }
+  bool has_free(bool sub_core_model, unsigned reg_id) {
+    // in subcore model, each sched has a one specific reg to use (based on
+    // sched id)
+    if (!sub_core_model) return has_free();
+
+    assert(reg_id < regs.size());
+    return regs[reg_id]->empty();
+  }
+  bool has_ready() {
+    for (unsigned i = 0; i < regs.size(); i++) {
+      if (not regs[i]->empty()) {
+        return true;
+      }
+    }
+    return false;
+  }
+  bool has_ready(bool sub_core_model, unsigned reg_id) {
+    if (!sub_core_model) return has_ready();
+    assert(reg_id < regs.size());
+    return (not regs[reg_id]->empty());
+  }
+
+  unsigned get_ready_reg_id() {
+    // for sub core model we need to figure which reg_id has the ready warp
+    // this function should only be called if has_ready() was true
+    assert(has_ready());
+    warp_inst_t **ready;
+    ready = NULL;
+    unsigned reg_id;
+    for (unsigned i = 0; i < regs.size(); i++) {
+      if (not regs[i]->empty()) {
+        if (ready and (*ready)->get_uid() < regs[i]->get_uid()) {
+          // ready is oldest
+        } else {
+          ready = &regs[i];
+          reg_id = i;
+        }
+      }
+    }
+    return reg_id;
+  }
+  unsigned get_schd_id(unsigned reg_id) {
+    assert(not regs[reg_id]->empty());
+    return regs[reg_id]->get_schd_id();
+  }
+  void move_in(warp_inst_t *&src) {
+    warp_inst_t **free = get_free();
+    move_warp(*free, src);
+  }
+  // void copy_in( warp_inst_t* src ){
+  //   src->copy_contents_to(*get_free());
+  //}
+  void move_in(bool sub_core_model, unsigned reg_id, warp_inst_t *&src) {
+    warp_inst_t **free;
+    if (!sub_core_model) {
+      free = get_free();
+    } else {
+      assert(reg_id < regs.size());
+      free = get_free(sub_core_model, reg_id);
+    }
+    move_warp(*free, src);
+  }
+
+  void move_out_to(warp_inst_t *&dest) {
+    warp_inst_t **ready = get_ready();
+    move_warp(dest, *ready);
+  }
+  void move_out_to(bool sub_core_model, unsigned reg_id, warp_inst_t *&dest) {
+    if (!sub_core_model) {
+      return move_out_to(dest);
+    }
+    warp_inst_t **ready = get_ready(sub_core_model, reg_id);
+    assert(ready != NULL);
+    move_warp(dest, *ready);
+  }
+
+  warp_inst_t **get_ready() {
+    warp_inst_t **ready;
+    ready = NULL;
+    for (unsigned i = 0; i < regs.size(); i++) {
+      if (not regs[i]->empty()) {
+        if (ready and (*ready)->get_uid() < regs[i]->get_uid()) {
+          // ready is oldest
+        } else {
+          ready = &regs[i];
+        }
+      }
+    }
+    return ready;
+  }
+  warp_inst_t **get_ready(bool sub_core_model, unsigned reg_id) {
+    if (!sub_core_model) return get_ready();
+    warp_inst_t **ready;
+    ready = NULL;
+    assert(reg_id < regs.size());
+    if (not regs[reg_id]->empty()) ready = &regs[reg_id];
+    return ready;
+  }
+
+  void print(FILE *fp) const {
+    fprintf(fp, "%s : @%p\n", m_name, this);
+    for (unsigned i = 0; i < regs.size(); i++) {
+      fprintf(fp, "     ");
+      regs[i]->print(fp);
+      fprintf(fp, "\n");
+    }
+  }
+
+  warp_inst_t **get_free() {
+    for (unsigned i = 0; i < regs.size(); i++) {
+      if (regs[i]->empty()) {
+        return &regs[i];
+      }
+    }
+    assert(0 && "No free registers found");
+    return NULL;
+  }
+
+  warp_inst_t **get_free(bool sub_core_model, unsigned reg_id) {
+    // in subcore model, each sched has a one specific reg to use (based on
+    // sched id)
+    if (!sub_core_model) return get_free();
+
+    assert(reg_id < regs.size());
+    if (regs[reg_id]->empty()) {
+      return &regs[reg_id];
+    }
+    assert(0 && "No free register found");
+    return NULL;
+  }
+
+  unsigned get_size() { return regs.size(); }
 
 private:
-	std::vector<warp_inst_t*> regs;
-	const char* m_name;
+  std::vector<warp_inst_t *> regs;
+  const char *m_name;
 };
 
 #endif // #ifdef __cplusplus
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/SConscript b/design/gpgpu/gpgpu-sim/src/cuda-sim/SConscript
index 499fa75cf0..5d5e376a94 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/SConscript
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/SConscript
@@ -39,8 +39,8 @@ Source('ptx_ir.cc', append={'CXXFLAGS': '-Wno-error=ignored-qualifiers -Wno-erro
 Source('ptx_loader.cc', append={'CXXFLAGS': '-Wno-error=ignored-qualifiers -Wno-error=format -Wno-error=format-truncation='})
 Source('ptx_parser.cc', append={'CXXFLAGS': '-Wno-error=ignored-qualifiers -Wno-error=format -Wno-error=type-limits -Wno-error=maybe-uninitialized'})
 Source('ptx_sim.cc', append={'CXXFLAGS': '-Wno-error=ignored-qualifiers -Wno-error=implicit-fallthrough -Wno-error=unused-function -Wno-error=format -Wno-error=maybe-uninitialized'})
-Source('lex.ptx_.c', append={'CFLAGS': '-Wno-error=ignored-qualifiers -Wno-error=unused-function'})
-Source('lex.ptxinfo_.c', append={'CFLAGS': '-Wno-error=ignored-qualifiers -Wno-error=unused-function'})
-Source('ptxinfo.tab.c', append={'CFLAGS': '-Wno-error=ignored-qualifiers'})
-Source('ptx.tab.c', append={'CFLAGS': '-Wno-error=ignored-qualifiers'})
+Source('lex.ptx_.cc', append={'CFLAGS': '-Wno-error=ignored-qualifiers -Wno-error=unused-function'})
+Source('lex.ptxinfo_.cc', append={'CFLAGS': '-Wno-error=ignored-qualifiers -Wno-error=unused-function'})
+Source('ptxinfo.tab.cc', append={'CFLAGS': '-Wno-error=ignored-qualifiers'})
+Source('ptx.tab.cc', append={'CFLAGS': '-Wno-error=ignored-qualifiers'})
 Source('decuda_pred_table/decuda_pred_table.cc', append={'CXXFLAGS': '-Wno-error=ignored-qualifiers'})
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-math.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-math.h
index a5db3378b0..a064d94ff9 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-math.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-math.h
@@ -91,7 +91,7 @@ namespace cuda_math {
    struct float2 {
       float x, y;
    };
-    
+
 
 // DEVICE_BUILTIN
    typedef struct int4 int4;
@@ -275,15 +275,24 @@ float __ull2float_rd(unsigned long long int a) {
 // float to integer conversion 
 int float2int(float a, enum cudaRoundMode mode)
 {
-   int tmp;
-   switch (mode) {
-   case cuda_math::cudaRoundZero: tmp = truncf(a);     break;
-   case cuda_math::cudaRoundNearest: tmp = nearbyintf(a); break;
-   case cuda_math::cudaRoundMinInf: tmp = floorf(a);     break;
-   case cuda_math::cudaRoundPosInf: tmp = ceilf(a);      break;
-   default: abort();
-   }
-   return tmp; 
+  int tmp;
+  switch (mode) {
+    case cudaRoundZero:
+      tmp = truncf(a);
+      break;
+    case cudaRoundNearest:
+      tmp = nearbyintf(a);
+      break;
+    case cudaRoundMinInf:
+      tmp = floorf(a);
+      break;
+    case cudaRoundPosInf:
+      tmp = ceilf(a);
+      break;
+    default:
+      abort();
+  }
+  return tmp;
 }
 
 int __internal_float2int(float a, enum cudaRoundMode mode) 
@@ -294,15 +303,24 @@ int __internal_float2int(float a, enum cudaRoundMode mode)
 // float to unsigned integer conversion 
 unsigned int float2uint(float a, enum cudaRoundMode mode)
 {
-   unsigned int tmp;
-   switch (mode) {
-   case cuda_math::cudaRoundZero: tmp = truncf(a);     break;
-   case cuda_math::cudaRoundNearest: tmp = nearbyintf(a); break;
-   case cuda_math::cudaRoundMinInf: tmp = floorf(a);     break;
-   case cuda_math::cudaRoundPosInf: tmp = ceilf(a);      break;
-   default: abort();
-   }
-   return tmp; 
+  unsigned int tmp;
+  switch (mode) {
+    case cudaRoundZero:
+      tmp = truncf(a);
+      break;
+    case cudaRoundNearest:
+      tmp = nearbyintf(a);
+      break;
+    case cudaRoundMinInf:
+      tmp = floorf(a);
+      break;
+    case cudaRoundPosInf:
+      tmp = ceilf(a);
+      break;
+    default:
+      abort();
+  }
+  return tmp;
 }
 
 unsigned int __internal_float2uint(float a, enum cudaRoundMode mode) 
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.cc
index 84e306fd2f..da624f3c90 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.cc
@@ -30,300 +30,327 @@
 
 #include "instructions.h"
 #include "ptx_ir.h"
-#include "ptx.tab.h"
-#include "ptx_sim.h"
+class ptx_recognizer;
+typedef void *yyscan_t;
 #include <stdio.h>
+#include <map>
+#include <set>
 #include <sstream>
-#include "opcodes.h"
+#include "../../libcuda_sim/gpgpu_context.h"
+#include "../abstract_hardware_model.h"
+#include "../gpgpu-sim/gpu-sim.h"
+#include "../gpgpusim_entrypoint.h"
 #include "../statwrapper.h"
-#include <set>
-#include <map>
-
+#include "../stream_manager.h"
+#include "cuda_device_runtime.h"
+#include "decuda_pred_table/decuda_pred_table.h"
 #include "memory.h"
+#include "opcodes.h"
 #include "ptx-stats.h"
+#include "ptx.tab.h"
 #include "ptx_loader.h"
 #include "ptx_parser.h"
 #include "ptx_sim.h"
-#include "decuda_pred_table/decuda_pred_table.h"
-
-#ifndef LIBCUDA
 #include "gpu/gpgpu-sim/cuda_gpu.hh"
-#include "../abstract_hardware_model.h"
-#include "../gpgpu-sim/gpu-sim.h"
-#include "../gpgpusim_entrypoint.h"
-#else
-#include "../libcuda/abstract_hardware_model.h"
-#include "../libcuda/gpu-sim.h"
-#include "../libcuda/gpgpusim_entrypoint.h"
-#define gs_min2(a,b) (((a)<(b))?(a):(b))
-#endif
-
-#include "../stream_manager.h"
-#include "cuda_device_runtime.h"
 
-int gpgpu_ptx_instruction_classification;
-void ** g_inst_classification_stat = NULL;
-void ** g_inst_op_classification_stat= NULL;
-int g_ptx_kernel_count = -1; // used for classification stat collection purposes
-int g_debug_execution = 3;
-int g_debug_thread_uid = 0;
-addr_t g_debug_pc = 0xBEEF1518;
+int g_debug_execution = 0;
 // Output debug information to file options
-int cp_count;
-int cp_cta_resume;
-
-unsigned g_ptx_sim_num_insn = 0;
-unsigned gpgpu_param_num_shaders = 0;
-
-char *opcode_latency_int, *opcode_latency_fp, *opcode_latency_dp,*opcode_latency_sfu,*opcode_latency_tensor;
-char *opcode_initiation_int, *opcode_initiation_fp, *opcode_initiation_dp,*opcode_initiation_sfu,*opcode_initiation_tensor;
-char *cdp_latency_str;
-unsigned cdp_latency[5];
-
-void ptx_opcocde_latency_options (option_parser_t opp) {
-	option_parser_register(opp, "-ptx_opcode_latency_int", OPT_CSTR, &opcode_latency_int,
-			"Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>"
-			"Default 1,1,19,25,145,32",
-			"1,1,19,25,145,32");
-	option_parser_register(opp, "-ptx_opcode_latency_fp", OPT_CSTR, &opcode_latency_fp,
-			"Opcode latencies for single precision floating points <ADD,MAX,MUL,MAD,DIV>"
-			"Default 1,1,1,1,30",
-			"1,1,1,1,30");
-	option_parser_register(opp, "-ptx_opcode_latency_dp", OPT_CSTR, &opcode_latency_dp,
-			"Opcode latencies for double precision floating points <ADD,MAX,MUL,MAD,DIV>"
-			"Default 8,8,8,8,335",
-			"8,8,8,8,335");
-	option_parser_register(opp, "-ptx_opcode_latency_sfu", OPT_CSTR, &opcode_latency_sfu,
-			"Opcode latencies for SFU instructions"
-			"Default 8",
-			"8");
-	option_parser_register(opp, "-ptx_opcode_latency_tesnor", OPT_CSTR, &opcode_latency_tensor,
-			"Opcode latencies for Tensor instructions"
-			"Default 64",
-			"64");
-	option_parser_register(opp, "-ptx_opcode_initiation_int", OPT_CSTR, &opcode_initiation_int,
-			"Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>"
-			"Default 1,1,4,4,32,4",
-			"1,1,4,4,32,4");
-	option_parser_register(opp, "-ptx_opcode_initiation_fp", OPT_CSTR, &opcode_initiation_fp,
-			"Opcode initiation intervals for single precision floating points <ADD,MAX,MUL,MAD,DIV>"
-			"Default 1,1,1,1,5",
-			"1,1,1,1,5");
-	option_parser_register(opp, "-ptx_opcode_initiation_dp", OPT_CSTR, &opcode_initiation_dp,
-			"Opcode initiation intervals for double precision floating points <ADD,MAX,MUL,MAD,DIV>"
-			"Default 8,8,8,8,130",
-			"8,8,8,8,130");
-	option_parser_register(opp, "-ptx_opcode_initiation_sfu", OPT_CSTR, &opcode_initiation_sfu,
-			"Opcode initiation intervals for sfu instructions"
-			"Default 8",
-			"8");
-	option_parser_register(opp, "-ptx_opcode_initiation_tensor", OPT_CSTR, &opcode_initiation_tensor,
-			"Opcode initiation intervals for tensor instructions"
-			"Default 64",
-			"64");
-	option_parser_register(opp, "-cdp_latency", OPT_CSTR, &cdp_latency_str,
-			"CDP API latency <cudaStreamCreateWithFlags, \
+
+void cuda_sim::ptx_opcocde_latency_options(option_parser_t opp) {
+  option_parser_register(
+      opp, "-ptx_opcode_latency_int", OPT_CSTR, &opcode_latency_int,
+      "Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>"
+      "Default 1,1,19,25,145,32",
+      "1,1,19,25,145,32");
+  option_parser_register(opp, "-ptx_opcode_latency_fp", OPT_CSTR,
+                         &opcode_latency_fp,
+                         "Opcode latencies for single precision floating "
+                         "points <ADD,MAX,MUL,MAD,DIV>"
+                         "Default 1,1,1,1,30",
+                         "1,1,1,1,30");
+  option_parser_register(opp, "-ptx_opcode_latency_dp", OPT_CSTR,
+                         &opcode_latency_dp,
+                         "Opcode latencies for double precision floating "
+                         "points <ADD,MAX,MUL,MAD,DIV>"
+                         "Default 8,8,8,8,335",
+                         "8,8,8,8,335");
+  option_parser_register(opp, "-ptx_opcode_latency_sfu", OPT_CSTR,
+                         &opcode_latency_sfu,
+                         "Opcode latencies for SFU instructions"
+                         "Default 8",
+                         "8");
+  option_parser_register(opp, "-ptx_opcode_latency_tesnor", OPT_CSTR,
+                         &opcode_latency_tensor,
+                         "Opcode latencies for Tensor instructions"
+                         "Default 64",
+                         "64");
+  option_parser_register(
+      opp, "-ptx_opcode_initiation_int", OPT_CSTR, &opcode_initiation_int,
+      "Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>"
+      "Default 1,1,4,4,32,4",
+      "1,1,4,4,32,4");
+  option_parser_register(opp, "-ptx_opcode_initiation_fp", OPT_CSTR,
+                         &opcode_initiation_fp,
+                         "Opcode initiation intervals for single precision "
+                         "floating points <ADD,MAX,MUL,MAD,DIV>"
+                         "Default 1,1,1,1,5",
+                         "1,1,1,1,5");
+  option_parser_register(opp, "-ptx_opcode_initiation_dp", OPT_CSTR,
+                         &opcode_initiation_dp,
+                         "Opcode initiation intervals for double precision "
+                         "floating points <ADD,MAX,MUL,MAD,DIV>"
+                         "Default 8,8,8,8,130",
+                         "8,8,8,8,130");
+  option_parser_register(opp, "-ptx_opcode_initiation_sfu", OPT_CSTR,
+                         &opcode_initiation_sfu,
+                         "Opcode initiation intervals for sfu instructions"
+                         "Default 8",
+                         "8");
+  option_parser_register(opp, "-ptx_opcode_initiation_tensor", OPT_CSTR,
+                         &opcode_initiation_tensor,
+                         "Opcode initiation intervals for tensor instructions"
+                         "Default 64",
+                         "64");
+  option_parser_register(opp, "-cdp_latency", OPT_CSTR, &cdp_latency_str,
+                         "CDP API latency <cudaStreamCreateWithFlags, \
 cudaGetParameterBufferV2_init_perWarp, cudaGetParameterBufferV2_perKernel, \
 cudaLaunchDeviceV2_init_perWarp, cudaLaunchDevicV2_perKernel>"
-			"Default 7200,8000,100,12000,1600",
-			"7200,8000,100,12000,1600");
-}
-
-static address_type get_converge_point(address_type pc);
-
-// TODO schi add
-void sign_extend( ptx_reg_t &data, unsigned src_size, const operand_info &dst );
-void gpgpu_t::gpgpu_ptx_sim_bindNameToTexture(const char* name, const struct textureReference* texref, int dim, int readmode, int ext)
-{
-   std::string texname(name);
-   if (m_NameToTextureRef.find(texname)==m_NameToTextureRef.end()){
-      m_NameToTextureRef[texname] = std::set<const struct textureReference*>();
-   }else{
-     const struct textureReference* tr = *m_NameToTextureRef[texname].begin();
-     assert(tr!=NULL);
-     //asserts that all texrefs in set have same fields
-     assert(tr->normalized==texref->normalized&&
-            tr->filterMode==texref->filterMode&&
-            tr->addressMode[0]==texref->addressMode[0]&&
-            tr->addressMode[1]==texref->addressMode[1]&&
-            tr->addressMode[2]==texref->addressMode[2]&&
-            tr->channelDesc.x==texref->channelDesc.x&&
-            tr->channelDesc.y==texref->channelDesc.y&&
-            tr->channelDesc.z==texref->channelDesc.z&&
-            tr->channelDesc.w==texref->channelDesc.w&&
-            tr->channelDesc.f==texref->channelDesc.f
-           );
-   }
-   m_NameToTextureRef[texname].insert(texref);
-   m_TextureRefToName[texref] = texname;
-   const textureReferenceAttr *texAttr = new textureReferenceAttr(texref, dim, (enum cudaTextureReadMode)readmode, ext);
-   m_NameToAttribute[texname] = texAttr;
-}
-
-const char* gpgpu_t::gpgpu_ptx_sim_findNamefromTexture(const struct textureReference* texref)
-{
-   std::map<const struct textureReference*, std::string>::const_iterator t=m_TextureRefToName.find(texref);
-   assert( t != m_TextureRefToName.end() );
-   return t->second.c_str();
-}
-
-unsigned int intLOGB2( unsigned int v ) {
-   unsigned int shift;
-   unsigned int r;
-
-   r = 0;
-
-   shift = (( v & 0xFFFF0000) != 0 ) << 4; v >>= shift; r |= shift;
-   shift = (( v & 0xFF00    ) != 0 ) << 3; v >>= shift; r |= shift;
-   shift = (( v & 0xF0      ) != 0 ) << 2; v >>= shift; r |= shift;
-   shift = (( v & 0xC       ) != 0 ) << 1; v >>= shift; r |= shift;
-   shift = (( v & 0x2       ) != 0 ) << 0; v >>= shift; r |= shift;
-
-   return r;
-}
-
-void gpgpu_t::gpgpu_ptx_sim_bindTextureToArray(const struct textureReference* texref, const struct cudaArray* array)
-{
-   std::string texname = gpgpu_ptx_sim_findNamefromTexture(texref);
-
-   std::map<std::string,const struct cudaArray*>::const_iterator t=m_NameToCudaArray.find(texname);
-   //check that there's nothing there first
-   if(t != m_NameToCudaArray.end()){
-      printf("GPGPU-Sim PTX:   Warning: binding to texref associated with %s, which was previously bound.\nImplicitly unbinding texref associated to %s first\n", texname.c_str(), texname.c_str());
-   }
-   m_NameToCudaArray[texname] = array;
-   unsigned int texel_size_bits = array->desc.w + array->desc.x + array->desc.y + array->desc.z;
-   unsigned int texel_size = texel_size_bits/8;
-   unsigned int Tx, Ty;
-   int r;
-
-   printf("GPGPU-Sim PTX:   texel size = %d\n", texel_size);
-   printf("GPGPU-Sim PTX:   texture cache linesize = %d\n", m_function_model_config.get_texcache_linesize());
-   //first determine base Tx size for given linesize
-   switch (m_function_model_config.get_texcache_linesize()) {
-   case 16:  Tx = 4; break;
-   case 32:  Tx = 8; break;
-   case 64:  Tx = 8; break;
-   case 128: Tx = 16; break;
-   case 256: Tx = 16; break;
-   default:
-      printf("GPGPU-Sim PTX:   Line size of %d bytes currently not supported.\n", m_function_model_config.get_texcache_linesize());
+                         "Default 7200,8000,100,12000,1600",
+                         "7200,8000,100,12000,1600");
+}
+
+void gpgpu_t::gpgpu_ptx_sim_bindNameToTexture(
+    const char *name, const struct textureReference *texref, int dim,
+    int readmode, int ext) {
+  std::string texname(name);
+  if (m_NameToTextureRef.find(texname) == m_NameToTextureRef.end()) {
+    m_NameToTextureRef[texname] = std::set<const struct textureReference *>();
+  } else {
+    const struct textureReference *tr = *m_NameToTextureRef[texname].begin();
+    assert(tr != NULL);
+    // asserts that all texrefs in set have same fields
+    assert(tr->normalized == texref->normalized &&
+           tr->filterMode == texref->filterMode &&
+           tr->addressMode[0] == texref->addressMode[0] &&
+           tr->addressMode[1] == texref->addressMode[1] &&
+           tr->addressMode[2] == texref->addressMode[2] &&
+           tr->channelDesc.x == texref->channelDesc.x &&
+           tr->channelDesc.y == texref->channelDesc.y &&
+           tr->channelDesc.z == texref->channelDesc.z &&
+           tr->channelDesc.w == texref->channelDesc.w &&
+           tr->channelDesc.f == texref->channelDesc.f);
+  }
+  m_NameToTextureRef[texname].insert(texref);
+  m_TextureRefToName[texref] = texname;
+  const textureReferenceAttr *texAttr = new textureReferenceAttr(
+      texref, dim, (enum cudaTextureReadMode)readmode, ext);
+  m_NameToAttribute[texname] = texAttr;
+}
+
+const char *gpgpu_t::gpgpu_ptx_sim_findNamefromTexture(
+    const struct textureReference *texref) {
+  std::map<const struct textureReference *, std::string>::const_iterator t =
+      m_TextureRefToName.find(texref);
+  assert(t != m_TextureRefToName.end());
+  return t->second.c_str();
+}
+
+unsigned int intLOGB2(unsigned int v) {
+  unsigned int shift;
+  unsigned int r;
+
+  r = 0;
+
+  shift = ((v & 0xFFFF0000) != 0) << 4;
+  v >>= shift;
+  r |= shift;
+  shift = ((v & 0xFF00) != 0) << 3;
+  v >>= shift;
+  r |= shift;
+  shift = ((v & 0xF0) != 0) << 2;
+  v >>= shift;
+  r |= shift;
+  shift = ((v & 0xC) != 0) << 1;
+  v >>= shift;
+  r |= shift;
+  shift = ((v & 0x2) != 0) << 0;
+  v >>= shift;
+  r |= shift;
+
+  return r;
+}
+
+void gpgpu_t::gpgpu_ptx_sim_bindTextureToArray(
+    const struct textureReference *texref, const struct cudaArray *array) {
+  std::string texname = gpgpu_ptx_sim_findNamefromTexture(texref);
+
+  std::map<std::string, const struct cudaArray *>::const_iterator t =
+      m_NameToCudaArray.find(texname);
+  // check that there's nothing there first
+  if (t != m_NameToCudaArray.end()) {
+    printf(
+        "GPGPU-Sim PTX:   Warning: binding to texref associated with %s, which "
+        "was previously bound.\nImplicitly unbinding texref associated to %s "
+        "first\n",
+        texname.c_str(), texname.c_str());
+  }
+  m_NameToCudaArray[texname] = array;
+  unsigned int texel_size_bits =
+      array->desc.w + array->desc.x + array->desc.y + array->desc.z;
+  unsigned int texel_size = texel_size_bits / 8;
+  unsigned int Tx, Ty;
+  int r;
+
+  printf("GPGPU-Sim PTX:   texel size = %d\n", texel_size);
+  printf("GPGPU-Sim PTX:   texture cache linesize = %d\n",
+         m_function_model_config.get_texcache_linesize());
+  // first determine base Tx size for given linesize
+  switch (m_function_model_config.get_texcache_linesize()) {
+    case 16:
+      Tx = 4;
+      break;
+    case 32:
+      Tx = 8;
+      break;
+    case 64:
+      Tx = 8;
+      break;
+    case 128:
+      Tx = 16;
+      break;
+    case 256:
+      Tx = 16;
+      break;
+    default:
+      printf(
+          "GPGPU-Sim PTX:   Line size of %d bytes currently not supported.\n",
+          m_function_model_config.get_texcache_linesize());
       assert(0);
       break;
-   }
-   r = texel_size >> 2;
-   //modify base Tx size to take into account size of each texel in bytes
-   while (r != 0) {
-      Tx = Tx >> 1;
-      r = r >> 2;
-   }
-   //by now, got the correct Tx size, calculate correct Ty size
-   Ty = m_function_model_config.get_texcache_linesize()/(Tx*texel_size);
-
-   printf("GPGPU-Sim PTX:   Tx = %d; Ty = %d, Tx_numbits = %d, Ty_numbits = %d\n", Tx, Ty, intLOGB2(Tx), intLOGB2(Ty));
-   printf("GPGPU-Sim PTX:   Texel size = %d bytes; texel_size_numbits = %d\n", texel_size, intLOGB2(texel_size));
-   printf("GPGPU-Sim PTX:   Binding texture to array starting at devPtr32 = 0x%x\n", array->devPtr32);
-   printf("GPGPU-Sim PTX:   Texel size = %d bytes\n", texel_size);
-   struct textureInfo* texInfo = (struct textureInfo*) malloc(sizeof(struct textureInfo));
-   texInfo->Tx = Tx;
-   texInfo->Ty = Ty;
-   texInfo->Tx_numbits = intLOGB2(Tx);
-   texInfo->Ty_numbits = intLOGB2(Ty);
-   texInfo->texel_size = texel_size;
-   texInfo->texel_size_numbits = intLOGB2(texel_size);
-   m_NameToTextureInfo[texname] = texInfo;
-}
-
-void gpgpu_t::gpgpu_ptx_sim_unbindTexture(const struct textureReference* texref)
-{
-   //assumes bind-use-unbind-bind-use-unbind pattern
-   std::string texname = gpgpu_ptx_sim_findNamefromTexture(texref);
-   m_NameToCudaArray.erase(texname);
-   m_NameToTextureInfo.erase(texname);
-}
-
-unsigned g_assemble_code_next_pc=0;
-std::map<unsigned,function_info*> g_pc_to_finfo;
-std::vector<ptx_instruction*> function_info::s_g_pc_to_insn;
+  }
+  r = texel_size >> 2;
+  // modify base Tx size to take into account size of each texel in bytes
+  while (r != 0) {
+    Tx = Tx >> 1;
+    r = r >> 2;
+  }
+  // by now, got the correct Tx size, calculate correct Ty size
+  Ty = m_function_model_config.get_texcache_linesize() / (Tx * texel_size);
+
+  printf(
+      "GPGPU-Sim PTX:   Tx = %d; Ty = %d, Tx_numbits = %d, Ty_numbits = %d\n",
+      Tx, Ty, intLOGB2(Tx), intLOGB2(Ty));
+  printf("GPGPU-Sim PTX:   Texel size = %d bytes; texel_size_numbits = %d\n",
+         texel_size, intLOGB2(texel_size));
+  printf(
+      "GPGPU-Sim PTX:   Binding texture to array starting at devPtr32 = 0x%x\n",
+      array->devPtr32);
+  printf("GPGPU-Sim PTX:   Texel size = %d bytes\n", texel_size);
+  struct textureInfo *texInfo =
+      (struct textureInfo *)malloc(sizeof(struct textureInfo));
+  texInfo->Tx = Tx;
+  texInfo->Ty = Ty;
+  texInfo->Tx_numbits = intLOGB2(Tx);
+  texInfo->Ty_numbits = intLOGB2(Ty);
+  texInfo->texel_size = texel_size;
+  texInfo->texel_size_numbits = intLOGB2(texel_size);
+  m_NameToTextureInfo[texname] = texInfo;
+}
+
+void gpgpu_t::gpgpu_ptx_sim_unbindTexture(
+    const struct textureReference *texref) {
+  // assumes bind-use-unbind-bind-use-unbind pattern
+  std::string texname = gpgpu_ptx_sim_findNamefromTexture(texref);
+  m_NameToCudaArray.erase(texname);
+  m_NameToTextureInfo.erase(texname);
+}
 
 #define MAX_INST_SIZE 8 /*bytes*/
 
-void function_info::ptx_assemble()
-{
-   if( m_assembled ) {
-      return;
-   }
-
-   // get the instructions into instruction memory...
-   unsigned num_inst = m_instructions.size();
-   m_instr_mem_size = MAX_INST_SIZE*(num_inst+1);
-   m_instr_mem = new ptx_instruction*[ m_instr_mem_size ];
-
-   printf("GPGPU-Sim PTX: instruction assembly for function \'%s\'... ", m_name.c_str() );
-   fflush(stdout);
-   std::list<ptx_instruction*>::iterator i;
-
-   addr_t PC = g_assemble_code_next_pc; // globally unique address (across functions)
-   // start function on an aligned address
-   for( unsigned i=0; i < (PC%MAX_INST_SIZE); i++ )
-      s_g_pc_to_insn.push_back((ptx_instruction*)NULL);
-   PC += PC%MAX_INST_SIZE;
-   m_start_PC = PC;
-
-   addr_t n=0; // offset in m_instr_mem
-   //Why s_g_pc_to_insn.size() is needed to reserve additional memory for insts? reserve is cumulative.
-   //s_g_pc_to_insn.reserve(s_g_pc_to_insn.size() + MAX_INST_SIZE*m_instructions.size());
-   s_g_pc_to_insn.reserve(MAX_INST_SIZE*m_instructions.size());
-   for ( i=m_instructions.begin(); i != m_instructions.end(); i++ ) {
-      ptx_instruction *pI = *i;
-      if ( pI->is_label() ) {
-         const symbol *l = pI->get_label();
-         labels[l->name()] = n;
-      } else {
-         g_pc_to_finfo[PC] = this;
-         m_instr_mem[n] = pI;
-         s_g_pc_to_insn.push_back(pI);
-         assert(pI == s_g_pc_to_insn[PC]);
-         pI->set_m_instr_mem_index(n);
-         pI->set_PC(PC);
-         assert( pI->inst_size() <= MAX_INST_SIZE );
-         for( unsigned i=1; i < pI->inst_size(); i++ ) {
-            s_g_pc_to_insn.push_back((ptx_instruction*)NULL);
-            m_instr_mem[n+i]=NULL;
-         }
-         n  += pI->inst_size();
-         PC += pI->inst_size();
+void function_info::ptx_assemble() {
+  if (m_assembled) {
+    return;
+  }
+
+  // get the instructions into instruction memory...
+  unsigned num_inst = m_instructions.size();
+  m_instr_mem_size = MAX_INST_SIZE * (num_inst + 1);
+  m_instr_mem = new ptx_instruction *[m_instr_mem_size];
+
+  printf("GPGPU-Sim PTX: instruction assembly for function \'%s\'... ",
+         m_name.c_str());
+  fflush(stdout);
+  std::list<ptx_instruction *>::iterator i;
+
+  addr_t PC =
+      gpgpu_ctx->func_sim->g_assemble_code_next_pc;  // globally unique address
+                                                     // (across functions)
+  // start function on an aligned address
+  for (unsigned i = 0; i < (PC % MAX_INST_SIZE); i++)
+    gpgpu_ctx->s_g_pc_to_insn.push_back((ptx_instruction *)NULL);
+  PC += PC % MAX_INST_SIZE;
+  m_start_PC = PC;
+
+  addr_t n = 0;  // offset in m_instr_mem
+  // Why s_g_pc_to_insn.size() is needed to reserve additional memory for insts?
+  // reserve is cumulative. s_g_pc_to_insn.reserve(s_g_pc_to_insn.size() +
+  // MAX_INST_SIZE*m_instructions.size());
+  gpgpu_ctx->s_g_pc_to_insn.reserve(MAX_INST_SIZE * m_instructions.size());
+  for (i = m_instructions.begin(); i != m_instructions.end(); i++) {
+    ptx_instruction *pI = *i;
+    if (pI->is_label()) {
+      const symbol *l = pI->get_label();
+      labels[l->name()] = n;
+    } else {
+      gpgpu_ctx->func_sim->g_pc_to_finfo[PC] = this;
+      m_instr_mem[n] = pI;
+      gpgpu_ctx->s_g_pc_to_insn.push_back(pI);
+      assert(pI == gpgpu_ctx->s_g_pc_to_insn[PC]);
+      pI->set_m_instr_mem_index(n);
+      pI->set_PC(PC);
+      assert(pI->inst_size() <= MAX_INST_SIZE);
+      for (unsigned i = 1; i < pI->inst_size(); i++) {
+        gpgpu_ctx->s_g_pc_to_insn.push_back((ptx_instruction *)NULL);
+        m_instr_mem[n + i] = NULL;
       }
-   }
-   g_assemble_code_next_pc=PC;
-   for ( unsigned ii=0; ii < n; ii += m_instr_mem[ii]->inst_size() ) { // handle branch instructions
-      ptx_instruction *pI = m_instr_mem[ii];
-      if ( pI->get_opcode() == BRA_OP || pI->get_opcode() == BREAKADDR_OP  || pI->get_opcode() == CALLP_OP) {
-         operand_info &target = pI->dst(); //get operand, e.g. target name
-         if ( labels.find(target.name()) == labels.end() ) {
-            printf("GPGPU-Sim PTX: Loader error (%s:%u): Branch label \"%s\" does not appear in assembly code.",
-                   pI->source_file(),pI->source_line(), target.name().c_str() );
-            abort();
-         }
-         unsigned index = labels[ target.name() ]; //determine address from name
-         unsigned PC = m_instr_mem[index]->get_PC();
-         m_symtab->set_label_address( target.get_symbol(), PC );
-         target.set_type(label_t);
+      n += pI->inst_size();
+      PC += pI->inst_size();
+    }
+  }
+  gpgpu_ctx->func_sim->g_assemble_code_next_pc = PC;
+  for (unsigned ii = 0; ii < n;
+       ii += m_instr_mem[ii]->inst_size()) {  // handle branch instructions
+    ptx_instruction *pI = m_instr_mem[ii];
+    if (pI->get_opcode() == BRA_OP || pI->get_opcode() == BREAKADDR_OP ||
+        pI->get_opcode() == CALLP_OP) {
+      operand_info &target = pI->dst();  // get operand, e.g. target name
+      if (labels.find(target.name()) == labels.end()) {
+        printf(
+            "GPGPU-Sim PTX: Loader error (%s:%u): Branch label \"%s\" does not "
+            "appear in assembly code.",
+            pI->source_file(), pI->source_line(), target.name().c_str());
+        abort();
       }
-   }
-   m_n = n;
-   printf("  done.\n");
-   fflush(stdout);
+      unsigned index = labels[target.name()];  // determine address from name
+      unsigned PC = m_instr_mem[index]->get_PC();
+      m_symtab->set_label_address(target.get_symbol(), PC);
+      target.set_type(label_t);
+    }
+  }
+  m_n = n;
+  printf("  done.\n");
+  fflush(stdout);
 
-   //disable pdom analysis  here and do it at runtime
+  // disable pdom analysis  here and do it at runtime
+#if 0
    printf("GPGPU-Sim PTX: finding reconvergence points for \'%s\'...\n", m_name.c_str() );
    create_basic_blocks();
    connect_basic_blocks();
-   bool modified = false;
+   bool modified = false; 
    do {
       find_dominators();
       find_idominators();
-      modified = connect_break_targets();
+      modified = connect_break_targets(); 
    } while (modified == true);
 
    if ( g_debug_execution>=50 ) {
@@ -350,2480 +377,2580 @@ void function_info::ptx_assemble()
    fflush(stdout);
 
    m_assembled = true;
+#endif
 }
 
-addr_t shared_to_generic( unsigned smid, addr_t addr )
-{
-   assert( addr < SHARED_MEM_SIZE_MAX );
-   return SHARED_GENERIC_START + smid*SHARED_MEM_SIZE_MAX + addr;
-}
-
-addr_t global_to_generic( addr_t addr )
-{
-   return addr;
-}
-
-bool isspace_shared( unsigned smid, addr_t addr )
-{
-   addr_t start = SHARED_GENERIC_START + smid*SHARED_MEM_SIZE_MAX;
-   addr_t end = SHARED_GENERIC_START + (smid+1)*SHARED_MEM_SIZE_MAX;
-   if( (addr >= end) || (addr < start) )
-      return false;
-   return true;
-}
-
-bool isspace_global( addr_t addr )
-{
-   return (addr >= GLOBAL_HEAP_START) || (addr < STATIC_ALLOC_LIMIT);
-}
-
-memory_space_t whichspace( addr_t addr )
-{
-   if( (addr >= GLOBAL_HEAP_START) || (addr < STATIC_ALLOC_LIMIT) ) {
-      return global_space;
-   } else if( addr >= SHARED_GENERIC_START ) {
-      return shared_space;
-   } else {
-      return local_space;
-   }
-}
-
-addr_t generic_to_shared( unsigned smid, addr_t addr )
-{
-   assert(isspace_shared(smid,addr));
-   return addr - (SHARED_GENERIC_START + smid*SHARED_MEM_SIZE_MAX);
-}
-
-addr_t local_to_generic( unsigned smid, unsigned hwtid, addr_t addr )
-{
-   assert(addr < LOCAL_MEM_SIZE_MAX);
-   return LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) + (LOCAL_MEM_SIZE_MAX * hwtid) + addr;
-}
-
-bool isspace_local( unsigned smid, unsigned hwtid, addr_t addr )
-{
-   addr_t start = LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) + (LOCAL_MEM_SIZE_MAX * hwtid);
-   addr_t end   = LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) + (LOCAL_MEM_SIZE_MAX * (hwtid+1));
-   if( (addr >= end) || (addr < start) )
-      return false;
-   return true;
-}
-
-addr_t generic_to_local( unsigned smid, unsigned hwtid, addr_t addr )
-{
-   assert(isspace_local(smid,hwtid,addr));
-   return addr - (LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) + (LOCAL_MEM_SIZE_MAX * hwtid));
-}
-
-addr_t generic_to_global( addr_t addr )
-{
-   return addr;
-}
-
-#ifndef LIBCUDA
-void* gpgpu_t::gpu_malloc( size_t size )
-{
-   unsigned long long result = m_dev_malloc;
-   if(g_debug_execution >= 3) {
-      printf("GPGPU-Sim PTX: allocating %zu bytes on GPU starting at address 0x%Lx\n", size, m_dev_malloc );
-      fflush(stdout);
-   }
-   m_dev_malloc += size;
-   if (size%256) m_dev_malloc += (256 - size%256); //align to 256 byte boundaries
-   return(void*) result;
-}
-
-void* gpgpu_t::gpu_mallocarray( size_t size )
-{
-   unsigned long long result = m_dev_malloc;
-   if(g_debug_execution >= 3) {
-      printf("GPGPU-Sim PTX: allocating %zu bytes on GPU starting at address 0x%Lx\n", size, m_dev_malloc );
-      fflush(stdout);
-   }
-   m_dev_malloc += size;
-   if (size%256) m_dev_malloc += (256 - size%256); //align to 256 byte boundaries
-   return(void*) result;
-}
-
+addr_t shared_to_generic(unsigned smid, addr_t addr) {
+  assert(addr < SHARED_MEM_SIZE_MAX);
+  return SHARED_GENERIC_START + smid * SHARED_MEM_SIZE_MAX + addr;
+}
+
+addr_t global_to_generic(addr_t addr) { return addr; }
 
-void gpgpu_t::memcpy_to_gpu( size_t dst_start_addr, const void *src, size_t count )
-{
-   if(g_debug_execution >= 3) {
-      printf("GPGPU-Sim PTX: copying %zu bytes from CPU[0x%Lx] to GPU[0x%Lx] ... ", count, (unsigned long long) src, (unsigned long long) dst_start_addr );
-      fflush(stdout);
-   }
-   char *src_data = (char*)src;
-   for (unsigned n=0; n < count; n ++ )
-      m_global_mem->write(dst_start_addr+n,1, src_data+n,NULL,NULL);
-
-   // Copy into the performance model.
-   extern gpgpu_sim* g_the_gpu;
-   g_the_gpu->perf_memcpy_to_gpu(dst_start_addr, count);
-   if(g_debug_execution >= 3) {
-      printf( " done.\n");
-      fflush(stdout);
-   }
+bool isspace_shared(unsigned smid, addr_t addr) {
+  addr_t start = SHARED_GENERIC_START + smid * SHARED_MEM_SIZE_MAX;
+  addr_t end = SHARED_GENERIC_START + (smid + 1) * SHARED_MEM_SIZE_MAX;
+  if ((addr >= end) || (addr < start)) return false;
+  return true;
+}
+
+bool isspace_global(addr_t addr) {
+  return (addr >= GLOBAL_HEAP_START) || (addr < STATIC_ALLOC_LIMIT);
+}
+
+memory_space_t whichspace(addr_t addr) {
+  if ((addr >= GLOBAL_HEAP_START) || (addr < STATIC_ALLOC_LIMIT)) {
+    return global_space;
+  } else if (addr >= SHARED_GENERIC_START) {
+    return shared_space;
+  } else {
+    return local_space;
+  }
+}
+
+addr_t generic_to_shared(unsigned smid, addr_t addr) {
+  assert(isspace_shared(smid, addr));
+  return addr - (SHARED_GENERIC_START + smid * SHARED_MEM_SIZE_MAX);
+}
+
+addr_t local_to_generic(unsigned smid, unsigned hwtid, addr_t addr) {
+  assert(addr < LOCAL_MEM_SIZE_MAX);
+  return LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) +
+         (LOCAL_MEM_SIZE_MAX * hwtid) + addr;
+}
+
+bool isspace_local(unsigned smid, unsigned hwtid, addr_t addr) {
+  addr_t start = LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) +
+                 (LOCAL_MEM_SIZE_MAX * hwtid);
+  addr_t end = LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) +
+               (LOCAL_MEM_SIZE_MAX * (hwtid + 1));
+  if ((addr >= end) || (addr < start)) return false;
+  return true;
+}
+
+addr_t generic_to_local(unsigned smid, unsigned hwtid, addr_t addr) {
+  assert(isspace_local(smid, hwtid, addr));
+  return addr - (LOCAL_GENERIC_START + (TOTAL_LOCAL_MEM_PER_SM * smid) +
+                 (LOCAL_MEM_SIZE_MAX * hwtid));
+}
+
+addr_t generic_to_global(addr_t addr) { return addr; }
+
+void *gpgpu_t::gpu_malloc(size_t size) {
+  unsigned long long result = m_dev_malloc;
+  if (g_debug_execution >= 3) {
+    printf(
+        "GPGPU-Sim PTX: allocating %zu bytes on GPU starting at address "
+        "0x%Lx\n",
+        size, m_dev_malloc);
+    fflush(stdout);
+  }
+  m_dev_malloc += size;
+  if (size % 256)
+    m_dev_malloc += (256 - size % 256);  // align to 256 byte boundaries
+  return (void *)result;
+}
+
+void *gpgpu_t::gpu_mallocarray(size_t size) {
+  unsigned long long result = m_dev_malloc;
+  if (g_debug_execution >= 3) {
+    printf(
+        "GPGPU-Sim PTX: allocating %zu bytes on GPU starting at address "
+        "0x%Lx\n",
+        size, m_dev_malloc);
+    fflush(stdout);
+  }
+  m_dev_malloc += size;
+  if (size % 256)
+    m_dev_malloc += (256 - size % 256);  // align to 256 byte boundaries
+  return (void *)result;
+}
+
+void gpgpu_t::memcpy_to_gpu(size_t dst_start_addr, const void *src,
+                            size_t count) {
+  if (g_debug_execution >= 3) {
+    printf(
+        "GPGPU-Sim PTX: copying %zu bytes from CPU[0x%Lx] to GPU[0x%Lx] ... ",
+        count, (unsigned long long)src, (unsigned long long)dst_start_addr);
+    fflush(stdout);
+  }
+  char *src_data = (char *)src;
+  for (unsigned n = 0; n < count; n++)
+    m_global_mem->write(dst_start_addr + n, 1, src_data + n, NULL, NULL);
+
+  // Copy into the performance model.
+  // extern gpgpu_sim* g_the_gpu;
+  gpgpu_ctx->the_gpgpusim->g_the_gpu->perf_memcpy_to_gpu(dst_start_addr, count);
+  if (g_debug_execution >= 3) {
+    printf(" done.\n");
+    fflush(stdout);
+  }
+}
+
+void gpgpu_t::memcpy_from_gpu(void *dst, size_t src_start_addr, size_t count) {
+  if (g_debug_execution >= 3) {
+    printf("GPGPU-Sim PTX: copying %zu bytes from GPU[0x%Lx] to CPU[0x%Lx] ...",
+           count, (unsigned long long)src_start_addr, (unsigned long long)dst);
+    fflush(stdout);
+  }
+  unsigned char *dst_data = (unsigned char *)dst;
+  for (unsigned n = 0; n < count; n++)
+    m_global_mem->read(src_start_addr + n, 1, dst_data + n);
+
+  // Copy into the performance model.
+  // extern gpgpu_sim* g_the_gpu;
+  gpgpu_ctx->the_gpgpusim->g_the_gpu->perf_memcpy_to_gpu(src_start_addr, count);
+  if (g_debug_execution >= 3) {
+    printf(" done.\n");
+    fflush(stdout);
+  }
+}
+
+void gpgpu_t::memcpy_gpu_to_gpu(size_t dst, size_t src, size_t count) {
+  if (g_debug_execution >= 3) {
+    printf("GPGPU-Sim PTX: copying %zu bytes from GPU[0x%Lx] to GPU[0x%Lx] ...",
+           count, (unsigned long long)src, (unsigned long long)dst);
+    fflush(stdout);
+  }
+  for (unsigned n = 0; n < count; n++) {
+    unsigned char tmp;
+    m_global_mem->read(src + n, 1, &tmp);
+    m_global_mem->write(dst + n, 1, &tmp, NULL, NULL);
+  }
+  if (g_debug_execution >= 3) {
+    printf(" done.\n");
+    fflush(stdout);
+  }
+}
+
+void gpgpu_t::gpu_memset(size_t dst_start_addr, int c, size_t count) {
+  if (g_debug_execution >= 3) {
+    printf(
+        "GPGPU-Sim PTX: setting %zu bytes of memory to 0x%x starting at "
+        "0x%Lx... ",
+        count, (unsigned char)c, (unsigned long long)dst_start_addr);
+    fflush(stdout);
+  }
+  unsigned char c_value = (unsigned char)c;
+  for (unsigned n = 0; n < count; n++)
+    m_global_mem->write(dst_start_addr + n, 1, &c_value, NULL, NULL);
+  if (g_debug_execution >= 3) {
+    printf(" done.\n");
+    fflush(stdout);
+  }
+}
+
+void cuda_sim::ptx_print_insn(address_type pc, FILE *fp) {
+  std::map<unsigned, function_info *>::iterator f = g_pc_to_finfo.find(pc);
+  if (f == g_pc_to_finfo.end()) {
+    fprintf(fp, "<no instruction at address 0x%x>", pc);
+    return;
+  }
+  function_info *finfo = f->second;
+  assert(finfo);
+  finfo->print_insn(pc, fp);
+}
+
+std::string cuda_sim::ptx_get_insn_str(address_type pc) {
+  std::map<unsigned, function_info *>::iterator f = g_pc_to_finfo.find(pc);
+  if (f == g_pc_to_finfo.end()) {
+#define STR_SIZE 255
+    char buff[STR_SIZE];
+    buff[STR_SIZE - 1] = '\0';
+    snprintf(buff, STR_SIZE, "<no instruction at address 0x%x>", pc);
+    return std::string(buff);
+  }
+  function_info *finfo = f->second;
+  assert(finfo);
+  return finfo->get_insn_str(pc);
+}
+
+void ptx_instruction::set_fp_or_int_archop() {
+  oprnd_type = UN_OP;
+  if ((m_opcode == MEMBAR_OP) || (m_opcode == SSY_OP) || (m_opcode == BRA_OP) ||
+      (m_opcode == BAR_OP) || (m_opcode == RET_OP) || (m_opcode == RETP_OP) ||
+      (m_opcode == NOP_OP) || (m_opcode == EXIT_OP) || (m_opcode == CALLP_OP) ||
+      (m_opcode == CALL_OP)) {
+    // do nothing
+  } else if ((m_opcode == CVT_OP || m_opcode == SET_OP ||
+              m_opcode == SLCT_OP)) {
+    if (get_type2() == F16_TYPE || get_type2() == F32_TYPE ||
+        get_type2() == F64_TYPE || get_type2() == FF64_TYPE) {
+      oprnd_type = FP_OP;
+    } else
+      oprnd_type = INT_OP;
+
+  } else {
+    if (get_type() == F16_TYPE || get_type() == F32_TYPE ||
+        get_type() == F64_TYPE || get_type() == FF64_TYPE) {
+      oprnd_type = FP_OP;
+    } else
+      oprnd_type = INT_OP;
+  }
 }
 
-void gpgpu_t::memcpy_from_gpu( void *dst, size_t src_start_addr, size_t count )
-{
-   if(g_debug_execution >= 3) {
-      printf("GPGPU-Sim PTX: copying %zu bytes from GPU[0x%Lx] to CPU[0x%Lx] ...", count, (unsigned long long) src_start_addr, (unsigned long long) dst );
-      fflush(stdout);
-   }
-   unsigned char *dst_data = (unsigned char*)dst;
-   for (unsigned n=0; n < count; n ++ )
-      m_global_mem->read(src_start_addr+n,1,dst_data+n);
-
-   // Copy into the performance model.
-   extern gpgpu_sim* g_the_gpu;
-   g_the_gpu->perf_memcpy_to_gpu(src_start_addr, count);
-   if(g_debug_execution >= 3) {
-      printf( " done.\n");
-      fflush(stdout);
-   }
-}
-
-void gpgpu_t::memcpy_gpu_to_gpu( size_t dst, size_t src, size_t count )
-{
-   if(g_debug_execution >= 3) {
-      printf("GPGPU-Sim PTX: copying %zu bytes from GPU[0x%Lx] to GPU[0x%Lx] ...", count,
-          (unsigned long long) src, (unsigned long long) dst );
-      fflush(stdout);
-   }
-   for (unsigned n=0; n < count; n ++ ) {
-      unsigned char tmp;
-      m_global_mem->read(src+n,1,&tmp);
-      m_global_mem->write(dst+n,1, &tmp,NULL,NULL);
-   }
-   if(g_debug_execution >= 3) {
-      printf( " done.\n");
-      fflush(stdout);
-   }
-}
-
-void gpgpu_t::gpu_memset( size_t dst_start_addr, int c, size_t count )
-{
-   if(g_debug_execution >= 3) {
-      printf("GPGPU-Sim PTX: setting %zu bytes of memory to 0x%x starting at 0x%Lx... ",
-          count, (unsigned char) c, (unsigned long long) dst_start_addr );
-      fflush(stdout);
-   }
-   unsigned char c_value = (unsigned char)c;
-   for (unsigned n=0; n < count; n ++ )
-      m_global_mem->write(dst_start_addr+n,1,&c_value,NULL,NULL);
-   if(g_debug_execution >= 3) {
-      printf( " done.\n");
-      fflush(stdout);
-   }
-}
-#endif // LIBCUDA
-
-void ptx_print_insn( address_type pc, FILE *fp )
-{
-   std::map<unsigned,function_info*>::iterator f = g_pc_to_finfo.find(pc);
-   if( f == g_pc_to_finfo.end() ) {
-       fprintf(fp,"<no instruction at address 0x%x>", pc );
-       return;
-   }
-   function_info *finfo = f->second;
-   assert( finfo );
-   finfo->print_insn(pc,fp);
-}
-
-std::string ptx_get_insn_str( address_type pc )
-{
-   std::map<unsigned,function_info*>::iterator f = g_pc_to_finfo.find(pc);
-   if( f == g_pc_to_finfo.end() ) {
-       #define STR_SIZE 255
-       char buff[STR_SIZE];
-       buff[STR_SIZE - 1] = '\0';
-       snprintf(buff, STR_SIZE,"<no instruction at address 0x%x>", pc );
-       return std::string(buff);
-   }
-   function_info *finfo = f->second;
-   assert( finfo );
-   return finfo->get_insn_str(pc);
-}
-
-void ptx_instruction::set_fp_or_int_archop(){
-    oprnd_type=UN_OP;
-	if((m_opcode == MEMBAR_OP)||(m_opcode == SSY_OP )||(m_opcode == BRA_OP) || (m_opcode == BAR_OP) || (m_opcode == RET_OP) || (m_opcode == RETP_OP) || (m_opcode == NOP_OP) || (m_opcode == EXIT_OP) || (m_opcode == CALLP_OP) || (m_opcode == CALL_OP)){
-			// do nothing
-	}else if((m_opcode == CVT_OP || m_opcode == SET_OP || m_opcode == SLCT_OP)){
-		if(get_type2()==F16_TYPE || get_type2()==F32_TYPE || get_type2() == F64_TYPE || get_type2() == FF64_TYPE){
-		    oprnd_type= FP_OP;
-		}else oprnd_type=INT_OP;
-
-	}else{
-		if(get_type()==F16_TYPE || get_type()==F32_TYPE || get_type() == F64_TYPE || get_type() == FF64_TYPE){
-		    oprnd_type= FP_OP;
-		}else oprnd_type=INT_OP;
-	}
-}
 void ptx_instruction::set_mul_div_or_other_archop(){
-    sp_op=OTHER_OP;
-	if((m_opcode != MEMBAR_OP) && (m_opcode != SSY_OP) && (m_opcode != BRA_OP) && (m_opcode != BAR_OP) && (m_opcode != EXIT_OP) && (m_opcode != NOP_OP) && (m_opcode != RETP_OP) && (m_opcode != RET_OP) && (m_opcode != CALLP_OP) && (m_opcode != CALL_OP)){
-		if(get_type()==F32_TYPE || get_type() == F64_TYPE || get_type() == FF64_TYPE){
-			switch(get_opcode()){
-				case MUL_OP:
-				case MAD_OP:
-				    sp_op=FP_MUL_OP;
-					break;
-				case DIV_OP:
-				    sp_op=FP_DIV_OP;
-					break;
-				case LG2_OP:
-				    sp_op=FP_LG_OP;
-					break;
-				case RSQRT_OP:
-				case SQRT_OP:
-				    sp_op=FP_SQRT_OP;
-					break;
-				case RCP_OP:
-				    sp_op=FP_DIV_OP;
-					break;
-				case SIN_OP:
-				case COS_OP:
-				    sp_op=FP_SIN_OP;
-					break;
-				case EX2_OP:
-				    sp_op=FP_EXP_OP;
-					break;
-				default:
-					if((op==ALU_OP)||(op==TENSOR_CORE_OP))
-					    sp_op=FP__OP;
-					break;
-
-			}
-		}else {
-			switch(get_opcode()){
-				case MUL24_OP:
-				case MAD24_OP:
-				    sp_op=INT_MUL24_OP;
-				break;
-				case MUL_OP:
-				case MAD_OP:
-					if(get_type()==U32_TYPE || get_type()==S32_TYPE || get_type()==B32_TYPE)
-					    sp_op=INT_MUL32_OP;
-					else
-					    sp_op=INT_MUL_OP;
-				break;
-				case DIV_OP:
-				    sp_op=INT_DIV_OP;
-				break;
-				default:
-					if((op==ALU_OP))
-					    sp_op=INT__OP;
-					break;
-			}
-		}
-	}
-
-}
-
-
-
-void ptx_instruction::set_bar_type()
-{
-	   if(m_opcode==BAR_OP) {
-		   switch(m_barrier_op){
-		   	   case SYNC_OPTION:
-		   		   bar_type = SYNC;
-		   		   break;
-		   	   case ARRIVE_OPTION:
-		   		   bar_type = ARRIVE;
-		   		   break;
-		   	   case RED_OPTION:
-		   		   bar_type = RED;
-		   		   switch(m_atomic_spec){
-		   		   	   case ATOMIC_POPC:
-				   		   red_type = POPC_RED;
-				   		   break;
-		   		   	   case ATOMIC_AND:
-				   		   red_type = AND_RED;
-				   		   break;
-		   		   	   case ATOMIC_OR:
-				   		   red_type = OR_RED;
-				   		   break;
-		   		   }
-		   		break;
-		   	   default:
-		   		   abort();
-		   }
-	   }
-	   else if(m_opcode==SST_OP) {
-		   bar_type = SYNC;
-	   }
-}
-
-
-void ptx_instruction::set_opcode_and_latency()
-{
-	unsigned int_latency[6];
-	unsigned fp_latency[5];
-	unsigned dp_latency[5];
-	unsigned sfu_latency;
-	unsigned tensor_latency;
-	unsigned int_init[6];
-	unsigned fp_init[5];
-	unsigned dp_init[5];
-	unsigned sfu_init;
-	unsigned tensor_init;
-	/*
-	 * [0] ADD,SUB
-	 * [1] MAX,Min
-	 * [2] MUL
-	 * [3] MAD
-	 * [4] DIV
-	 * [5] SHFL
-	 */
-	sscanf(opcode_latency_int, "%u,%u,%u,%u,%u,%u",
-			&int_latency[0],&int_latency[1],&int_latency[2],
-			&int_latency[3],&int_latency[4],&int_latency[5]);
-	sscanf(opcode_latency_fp, "%u,%u,%u,%u,%u",
-			&fp_latency[0],&fp_latency[1],&fp_latency[2],
-			&fp_latency[3],&fp_latency[4]);
-	sscanf(opcode_latency_dp, "%u,%u,%u,%u,%u",
-			&dp_latency[0],&dp_latency[1],&dp_latency[2],
-			&dp_latency[3],&dp_latency[4]);
-	sscanf(opcode_latency_sfu, "%u",
-			&sfu_latency);
-	sscanf(opcode_latency_tensor, "%u",
-			&tensor_latency);
-	sscanf(opcode_initiation_int, "%u,%u,%u,%u,%u,%u",
-			&int_init[0],&int_init[1],&int_init[2],
-			&int_init[3],&int_init[4],&int_init[5]);
-	sscanf(opcode_initiation_fp, "%u,%u,%u,%u,%u",
-			&fp_init[0],&fp_init[1],&fp_init[2],
-			&fp_init[3],&fp_init[4]);
-	sscanf(opcode_initiation_dp, "%u,%u,%u,%u,%u",
-			&dp_init[0],&dp_init[1],&dp_init[2],
-			&dp_init[3],&dp_init[4]);
-	sscanf(opcode_initiation_sfu, "%u",
-			&sfu_init);
-	sscanf(opcode_initiation_tensor, "%u",
-			&tensor_init);
-	sscanf(cdp_latency_str, "%u,%u,%u,%u,%u",
-			&cdp_latency[0],&cdp_latency[1],&cdp_latency[2],
-            &cdp_latency[3],&cdp_latency[4]);
-
-	if(!m_operands.empty()){
-		std::vector<operand_info>::iterator it;
-	   	for(it=++m_operands.begin();it!=m_operands.end();it++){
-	   		num_operands++;
-	   		if((it->is_reg() || it->is_vector())){
-	   			   num_regs++;
-	   		}
-	   	 }
-	}
-   op = ALU_OP;
-   mem_op= NOT_TEX;
-   initiation_interval = latency = 1;
-   switch( m_opcode ) {
-   case MOV_OP:
-       assert( !(has_memory_read() && has_memory_write()) );
-       if ( has_memory_read() ) op = LOAD_OP;
-       if ( has_memory_write() ) op = STORE_OP;
-       break;
-   case LD_OP: op = LOAD_OP; break;
-   case MMA_LD_OP: op = TENSOR_CORE_LOAD_OP; break;
-   case LDU_OP: op = LOAD_OP; break;
-   case ST_OP: op = STORE_OP; break;
-   case MMA_ST_OP: op = TENSOR_CORE_STORE_OP; break;
-   case BRA_OP: op = BRANCH_OP; break;
-   case BREAKADDR_OP: op = BRANCH_OP; break;
-   case TEX_OP: op = LOAD_OP; mem_op=TEX; break;
-   case ATOM_OP: op = LOAD_OP; break;
-   case BAR_OP: op = BARRIER_OP; break;
-   case SST_OP: op = BARRIER_OP; break;
-   case MEMBAR_OP: op = MEMORY_BARRIER_OP; break;
-   case CALL_OP:
-   {
-       if(m_is_printf || m_is_cdp) {
-           op = ALU_OP;
-       }
-       else
-           op = CALL_OPS;
-       break;
-   }
-   case CALLP_OP:
-   {
-       if(m_is_printf || m_is_cdp) {
-               op = ALU_OP;
-       }
-       else
-           op = CALL_OPS;
-       break;
-   }
-   case RET_OP: case RETP_OP:  op = RET_OPS;break;
-   case ADD_OP: case ADDP_OP: case ADDC_OP: case SUB_OP: case SUBC_OP:
-	   //ADD,SUB latency
-	   switch(get_type()){
-	   case F32_TYPE:
-		   latency = fp_latency[0];
-		   initiation_interval = fp_init[0];
-		   op = SP_OP;
-		   break;
-	   case F64_TYPE:
-	   case FF64_TYPE:
-		   latency = dp_latency[0];
-		   initiation_interval = dp_init[0];
-		   op = DP_OP;
-		   break;
-	   case B32_TYPE:
-	   case U32_TYPE:
-	   case S32_TYPE:
-	   default: //Use int settings for default
-		   latency = int_latency[0];
-		   initiation_interval = int_init[0];
-		   op = INTP_OP;
-		   break;
-	   }
-	   break;
-   case MAX_OP: case MIN_OP:
-	   //MAX,MIN latency
-	   switch(get_type()){
-	   case F32_TYPE:
-		   latency = fp_latency[1];
-		   initiation_interval = fp_init[1];
-		   op = SP_OP;
-		   break;
-	   case F64_TYPE:
-	   case FF64_TYPE:
-		   latency = dp_latency[1];
-		   initiation_interval = dp_init[1];
-		   op = DP_OP;
-		   break;
-	   case B32_TYPE:
-	   case U32_TYPE:
-	   case S32_TYPE:
-	   default: //Use int settings for default
-		   latency = int_latency[1];
-		   initiation_interval = int_init[1];
-		   op = INTP_OP;
-		   break;
-	   }
-	   break;
-   case MUL_OP:
-	   //MUL latency
-	   switch(get_type()){
-	   case F32_TYPE:
-		   latency = fp_latency[2];
-		   initiation_interval = fp_init[2];
-		   op = SP_OP;
-		   break;
-	   case F64_TYPE:
-	   case FF64_TYPE:
-		   latency = dp_latency[2];
-		   initiation_interval = dp_init[2];
-		   op = DP_OP;
-		   break;
-	   case B32_TYPE:
-	   case U32_TYPE:
-	   case S32_TYPE:
-	   default: //Use int settings for default
-		   latency = int_latency[2];
-		   initiation_interval = int_init[2];
-		   op = INTP_OP;
-		   break;
-	   }
-	   break;
-   case MAD_OP: case MADC_OP: case MADP_OP:
-	   //MAD latency
-	   switch(get_type()){
-	   case F32_TYPE:
-		   latency = fp_latency[3];
-		   initiation_interval = fp_init[3];
-		   op = SP_OP;
-		   break;
-	   case F64_TYPE:
-	   case FF64_TYPE:
-		   latency = dp_latency[3];
-		   initiation_interval = dp_init[3];
-		   op = DP_OP;
-		   break;
-	   case B32_TYPE:
-	   case U32_TYPE:
-	   case S32_TYPE:
-	   default: //Use int settings for default
-		   latency = int_latency[3];
-		   initiation_interval = int_init[3];
-		   op = INTP_OP;
-		   break;
-	   }
-	   break;
-   case DIV_OP:
-	   // Floating point only
-	   op = SFU_OP;
-	   switch(get_type()){
-	   case F32_TYPE:
-		   latency = fp_latency[4];
-		   initiation_interval = fp_init[4];
-		   break;
-	   case F64_TYPE:
-	   case FF64_TYPE:
-		   latency = dp_latency[4];
-		   initiation_interval = dp_init[4];
-		   break;
-	   case B32_TYPE:
-	   case U32_TYPE:
-	   case S32_TYPE:
-	   default: //Use int settings for default
-		   latency = int_latency[4];
-		   initiation_interval = int_init[4];
-		   break;
-	   }
-	   break;
-   case SQRT_OP: case SIN_OP: case COS_OP: case EX2_OP: case LG2_OP: case RSQRT_OP: case RCP_OP:
-	  latency = sfu_latency;
-	  initiation_interval = sfu_init;
-      op = SFU_OP;
-      break;
-   case MMA_OP:
-	   latency = tensor_latency;
-	   initiation_interval = tensor_init;
-       op=TENSOR_CORE_OP;
-	   break;
-   case SHFL_OP:
-	   latency = int_latency[5];
-	   initiation_interval = int_init[5];
-	   break;
-   default:
-       break;
-   }
-	set_fp_or_int_archop();
-	set_mul_div_or_other_archop();
-
-}
-
-void ptx_thread_info::ptx_fetch_inst( inst_t &inst ) const
-{
-   addr_t pc = get_pc();
-   const ptx_instruction *pI = m_func_info->get_instruction(pc);
-   inst = (const inst_t&)*pI;
-   assert( inst.valid() );
-}
-
-static unsigned datatype2size( unsigned data_type )
-{
-   unsigned data_size;
-   switch ( data_type ) {
-      case B8_TYPE:
-      case S8_TYPE:
-      case U8_TYPE:
-         data_size = 1; break;
-      case B16_TYPE:
-      case S16_TYPE:
-      case U16_TYPE:
-      case F16_TYPE:
-         data_size = 2; break;
-      case B32_TYPE:
-      case S32_TYPE:
-      case U32_TYPE:
-      case F32_TYPE:
-         data_size = 4; break;
-      case B64_TYPE:
-      case BB64_TYPE:
-      case S64_TYPE:
-      case U64_TYPE:
-      case F64_TYPE:
-      case FF64_TYPE:
-         data_size = 8; break;
-      case BB128_TYPE:
-         data_size = 16; break;
-      default: assert(0); break;
-   }
-   return data_size;
-}
-
-void ptx_instruction::pre_decode()
-{
-   pc = m_PC;
-   isize = m_inst_size;
-   for(unsigned i=0; i<MAX_OUTPUT_VALUES; i++) {
-       out[i] = 0;
-   }
-   for(unsigned i=0; i<MAX_INPUT_VALUES; i++) {
-       in[i] = 0;
-   }
-   incount=0;
-   outcount=0;
-   is_vectorin = 0;
-   is_vectorout = 0;
-   std::fill_n(arch_reg.src, MAX_REG_OPERANDS, -1);
-   std::fill_n(arch_reg.dst, MAX_REG_OPERANDS, -1);
-   pred = 0;
-   ar1 = 0;
-   ar2 = 0;
-   space = m_space_spec;
-   memory_op = no_memory_op;
-   data_size = 0;
-   if ( has_memory_read() || has_memory_write() ) {
-      unsigned to_type = get_type();
-      data_size = datatype2size(to_type);
-      memory_op = has_memory_read() ? memory_load : memory_store;
-   }
-
-   bool has_dst = false ;
-
-   switch ( get_opcode() ) {
-#define OP_DEF(OP,FUNC,STR,DST,CLASSIFICATION) case OP: has_dst = (DST!=0); break;
-#define OP_W_DEF(OP,FUNC,STR,DST,CLASSIFICATION) case OP: has_dst = (DST!=0); break;
-#include "opcodes.def"
-#undef OP_DEF
-#undef OP_W_DEF
-   default:
-      printf( "Execution error: Invalid opcode (0x%x)\n", get_opcode() );
-      break;
-   }
-
-   switch( m_cache_option ) {
-   case CA_OPTION: cache_op = CACHE_ALL; break;
-   case NC_OPTION: cache_op = CACHE_L1; break;
-   case CG_OPTION: cache_op = CACHE_GLOBAL; break;
-   case CS_OPTION: cache_op = CACHE_STREAMING; break;
-   case LU_OPTION: cache_op = CACHE_LAST_USE; break;
-   case CV_OPTION: cache_op = CACHE_VOLATILE; break;
-   case WB_OPTION: cache_op = CACHE_WRITE_BACK; break;
-   case WT_OPTION: cache_op = CACHE_WRITE_THROUGH; break;
-   default:
-      //if( m_opcode == LD_OP || m_opcode == LDU_OP )
-      if(  m_opcode == MMA_LD_OP || m_opcode == LD_OP || m_opcode == LDU_OP )
-         cache_op = CACHE_ALL;
-      //else if( m_opcode == ST_OP )
-      else if( m_opcode == MMA_ST_OP || m_opcode == ST_OP )
-         cache_op = CACHE_WRITE_BACK;
-      else if( m_opcode == ATOM_OP )
-         cache_op = CACHE_GLOBAL;
-      break;
-   }
-
-   set_opcode_and_latency();
-   set_bar_type();
-   // Get register operands
-   int n=0,m=0;
-   ptx_instruction::const_iterator opr=op_iter_begin();
-   for ( ; opr != op_iter_end(); opr++, n++ ) { //process operands
-      const operand_info &o = *opr;
-      if ( has_dst && n==0 ) {
-         // Do not set the null register "_" as an architectural register
-         if ( o.is_reg() && !o.is_non_arch_reg() ) {
-            out[0] = o.reg_num();
-            arch_reg.dst[0] = o.arch_reg_num();
-         } else if ( o.is_vector() ) {
-            is_vectorin = 1;
-            unsigned num_elem = o.get_vect_nelem();
-            if( num_elem >= 1 ) out[0] = o.reg1_num();
-            if( num_elem >= 2 ) out[1] = o.reg2_num();
-            if( num_elem >= 3 ) out[2] = o.reg3_num();
-            if( num_elem >= 4 ) out[3] = o.reg4_num();
-            if( num_elem >= 5 ) out[4] = o.reg5_num();
-            if( num_elem >= 6 ) out[5] = o.reg6_num();
-            if( num_elem >= 7 ) out[6] = o.reg7_num();
-            if( num_elem >= 8 ) out[7] = o.reg8_num();
-            for (int i = 0; i < num_elem; i++)
-               arch_reg.dst[i] = o.arch_reg_num(i);
-         }
-      } else {
-         if ( o.is_reg() && !o.is_non_arch_reg() ) {
-            int reg_num = o.reg_num();
-            arch_reg.src[m] = o.arch_reg_num();
-            switch ( m ) {
-            case 0: in[0] = reg_num; break;
-            case 1: in[1] = reg_num; break;
-            case 2: in[2] = reg_num; break;
-            default: break;
-            }
-            m++;
-         } else if ( o.is_vector() ) {
-            //assert(m == 0); //only support 1 vector operand (for textures) right now
-            is_vectorout = 1;
-            unsigned num_elem = o.get_vect_nelem();
-            if( num_elem >= 1 ) in[m+0] = o.reg1_num();
-            if( num_elem >= 2 ) in[m+1] = o.reg2_num();
-            if( num_elem >= 3 ) in[m+2] = o.reg3_num();
-            if( num_elem >= 4 ) in[m+3] = o.reg4_num();
-            if( num_elem >= 5 ) in[m+4] = o.reg5_num();
-            if( num_elem >= 6 ) in[m+5] = o.reg6_num();
-            if( num_elem >= 7 ) in[m+6] = o.reg7_num();
-            if( num_elem >= 8 ) in[m+7] = o.reg8_num();
-            for (int i = 0; i < num_elem; i++)
-               arch_reg.src[m+i] = o.arch_reg_num(i);
-            m+=num_elem;
-         }
-      }
-   }
-
-   //Setting number of input and output operands which is required for scoreboard check
-   for(int i=0;i<MAX_OUTPUT_VALUES;i++)
-	if(out[i]>0)
-		outcount++;
-
-   for(int i=0;i<MAX_INPUT_VALUES;i++)
-	if(in[i]>0)
-		incount++;
-
-   // Get predicate
-   if(has_pred()) {
-	   const operand_info &p = get_pred();
-	   pred = p.reg_num();
-   }
-
-   // Get address registers inside memory operands.
-   // Assuming only one memory operand per instruction,
-   //  and maximum of two address registers for one memory operand.
-   if( has_memory_read() || has_memory_write() ) {
-      ptx_instruction::const_iterator op=op_iter_begin();
-      for ( ; op != op_iter_end(); op++, n++ ) { //process operands
-         const operand_info &o = *op;
-
-         if(o.is_memory_operand()) {
-             // We do not support the null register as a memory operand
-             assert( !o.is_non_arch_reg() );
-
-            // Check PTXPlus-type operand
-            // memory operand with addressing (ex. s[0x4] or g[$r1])
-            if(o.is_memory_operand2()) {
-
-               // memory operand with one address register (ex. g[$r1+0x4] or s[$r2+=0x4])
-               if(o.get_double_operand_type() == 0 || o.get_double_operand_type() == 3){
-                  ar1 = o.reg_num();
-                  arch_reg.src[4] = o.arch_reg_num();
-                  // TODO: address register in $r2+=0x4 should be an output register as well
-               }
-               // memory operand with two address register (ex. s[$r1+$r1] or g[$r1+=$r2])
-               else if(o.get_double_operand_type() == 1 || o.get_double_operand_type() == 2) {
-                  ar1 = o.reg1_num();
-                  arch_reg.src[4] = o.arch_reg_num();
-                  ar2 = o.reg2_num();
-                  arch_reg.src[5] = o.arch_reg_num();
-                  // TODO: first address register in $r1+=$r2 should be an output register as well
-               }
-            }
-            else if(o.is_immediate_address()){
-
-            }
-            // Regular PTX operand
-            else if (o.get_symbol()->type()->get_key().is_reg()) { // Memory operand contains a register
-              ar1 = o.reg_num();
-              arch_reg.src[4] = o.arch_reg_num();
-            }
-
+  sp_op=OTHER_OP;
+  if((m_opcode != MEMBAR_OP) && (m_opcode != SSY_OP) && (m_opcode != BRA_OP) && (m_opcode != BAR_OP) && (m_opcode != EXIT_OP) && (m_opcode != NOP_OP) && (m_opcode != RETP_OP) && (m_opcode != RET_OP) && (m_opcode != CALLP_OP) && (m_opcode != CALL_OP)){
+    if(get_type() == F64_TYPE || get_type() == FF64_TYPE){
+         switch(get_opcode()){
+            case MUL_OP:
+            case MAD_OP:
+            case FMA_OP:
+                sp_op=DP_MUL_OP;
+               break;
+            case DIV_OP:
+            case REM_OP:
+                sp_op=DP_DIV_OP;
+               break;
+            case RCP_OP:
+                sp_op=DP_DIV_OP;
+               break;
+            case LG2_OP:
+                sp_op=FP_LG_OP;
+               break;
+            case RSQRT_OP:
+            case SQRT_OP:
+                sp_op=FP_SQRT_OP;
+               break;            
+            case SIN_OP:
+            case COS_OP:
+                sp_op=FP_SIN_OP;
+               break;
+            case EX2_OP:
+                sp_op=FP_EXP_OP;
+               break;
+            case MMA_OP:
+                sp_op=TENSOR__OP;
+            break;
+            case TEX_OP:
+                sp_op=TEX__OP;
+            break;
+            default:
+               if((op==DP_OP) || (op==ALU_OP))
+                  sp_op=DP___OP;
+               break;
          }
       }
-   }
-
-   // get reconvergence pc
-   reconvergence_pc = get_converge_point(pc);
-
-   m_decoded=true;
-}
-
-void function_info::add_param_name_type_size( unsigned index, std::string name, int type, size_t size, bool ptr, memory_space_t space )
-{
-   unsigned parsed_index;
-   char buffer[2048];
-   snprintf(buffer,2048,"%s_param_%%u", m_name.c_str() );
-   int ntokens = sscanf(name.c_str(),buffer,&parsed_index);
-   if( ntokens == 1 ) {
-      assert( m_ptx_kernel_param_info.find(parsed_index) == m_ptx_kernel_param_info.end() );
-      m_ptx_kernel_param_info[parsed_index] = param_info(name, type, size, ptr, space);
-   } else {
-      assert( m_ptx_kernel_param_info.find(index) == m_ptx_kernel_param_info.end() );
-      m_ptx_kernel_param_info[index] = param_info(name, type, size, ptr, space);
-   }
-}
-
-void function_info::add_param_data( unsigned argn, struct gpgpu_ptx_sim_arg *args )
-{
-   const void *data = args->m_start;
-
-   bool scratchpad_memory_param = false; // Is this parameter in CUDA shared memory or OpenCL local memory
-
-   std::map<unsigned,param_info>::iterator i=m_ptx_kernel_param_info.find(argn);
-   if( i != m_ptx_kernel_param_info.end() ) {
-      if (i->second.is_ptr_shared()) {
-         assert(args->m_start == NULL && "OpenCL parameter pointer to local memory must have NULL as value");
-         scratchpad_memory_param = true;
-      } else {
-         param_t tmp;
-         tmp.pdata = args->m_start;
-         tmp.size = args->m_nbytes;
-         tmp.offset = args->m_offset;
-         tmp.type = 0;
-         i->second.add_data(tmp);
-         i->second.add_offset((unsigned) args->m_offset);
-      }
-   } else {
-      scratchpad_memory_param = true;
-   }
-
-   if (scratchpad_memory_param) {
-      // This should only happen for OpenCL:
-      //
-      // The LLVM PTX compiler in NVIDIA's driver (version 190.29)
-      // does not generate an argument in the function declaration
-      // for __constant arguments.
-      //
-      // The associated constant memory space can be allocated in two
-      // ways. It can be explicitly initialized in the .ptx file where
-      // it is declared.  Or, it can be allocated using the clCreateBuffer
-      // on the host. In this later case, the .ptx file will contain
-      // a global declaration of the parameter, but it will have an unknown
-      // array size.  Thus, the symbol's address will not be set and we need
-      // to set it here before executing the PTX.
-
-      char buffer[2048];
-      snprintf(buffer,2048,"%s_param_%u",m_name.c_str(),argn);
-
-      symbol *p = m_symtab->lookup(buffer);
-      if( p == NULL ) {
-         printf("GPGPU-Sim PTX: ERROR ** could not locate symbol for \'%s\' : cannot bind buffer\n", buffer);
-         abort();
-      }
-      if( data )
-         p->set_address((addr_t)*(size_t*)data);
-      else {
-         // clSetKernelArg was passed NULL pointer for data...
-         // this is used for dynamically sized shared memory on NVIDIA platforms
-         bool is_ptr_shared = false;
-         if( i != m_ptx_kernel_param_info.end() ) {
-            is_ptr_shared = i->second.is_ptr_shared();
+      else if(get_type()==F16_TYPE || get_type()==F32_TYPE){
+         switch(get_opcode()){
+            case MUL_OP:
+            case MAD_OP:
+            case FMA_OP:
+                sp_op=FP_MUL_OP;
+               break;
+            case DIV_OP:
+            case REM_OP:
+                sp_op=FP_DIV_OP;
+               break;
+            case RCP_OP:
+                sp_op=FP_DIV_OP;
+               break;
+            case LG2_OP:
+                sp_op=FP_LG_OP;
+               break;
+            case RSQRT_OP:
+            case SQRT_OP:
+                sp_op=FP_SQRT_OP;
+               break;            
+            case SIN_OP:
+            case COS_OP:
+                sp_op=FP_SIN_OP;
+               break;
+            case EX2_OP:
+                sp_op=FP_EXP_OP;
+               break;
+            case MMA_OP:
+                sp_op=TENSOR__OP;
+            break;
+            case TEX_OP:
+                sp_op=TEX__OP;
+            break;
+            default:
+               if((op==SP_OP) || (op==ALU_OP))
+                  sp_op=FP__OP;
+               break;
          }
-
-         if( !is_ptr_shared and !p->is_shared() ) {
-            printf("GPGPU-Sim PTX: ERROR ** clSetKernelArg passed NULL but arg not shared memory\n");
-            abort();
+      }else {
+         switch(get_opcode()){
+            case MUL24_OP:
+            case MAD24_OP:
+                sp_op=INT_MUL24_OP;
+            break;
+            case MUL_OP:
+            case MAD_OP:
+            case FMA_OP:
+               if(get_type()==U32_TYPE || get_type()==S32_TYPE || get_type()==B32_TYPE)
+                   sp_op=INT_MUL32_OP;
+               else
+                   sp_op=INT_MUL_OP;
+            break;
+            case DIV_OP:
+            case REM_OP:
+                sp_op=INT_DIV_OP;
+            break;
+            case MMA_OP:
+                sp_op=TENSOR__OP;
+            break;
+            case TEX_OP:
+                sp_op=TEX__OP;
+            break;
+            default:
+               if((op==INTP_OP) || (op==ALU_OP))
+                   sp_op=INT__OP;
+               break;
          }
-         unsigned num_bits = 8*args->m_nbytes;
-         printf("GPGPU-Sim PTX: deferred allocation of shared region for \"%s\" from 0x%x to 0x%x (shared memory space)\n",
-                p->name().c_str(),
-                m_symtab->get_shared_next(),
-                m_symtab->get_shared_next() + num_bits/8 );
-         fflush(stdout);
-         assert( (num_bits%8) == 0  );
-         addr_t addr = m_symtab->get_shared_next();
-         addr_t addr_pad = num_bits ? (((num_bits/8) - (addr % (num_bits/8))) % (num_bits/8)) : 0;
-         p->set_address( addr+addr_pad );
-         m_symtab->alloc_shared( num_bits/8 + addr_pad );
-      }
-   }
-}
-
-unsigned function_info::get_args_aligned_size() {
-
-   if(m_args_aligned_size >= 0)
-      return m_args_aligned_size;
-
-   unsigned param_address = 0;
-   unsigned int total_size = 0;
-   for( std::map<unsigned,param_info>::iterator i=m_ptx_kernel_param_info.begin(); i!=m_ptx_kernel_param_info.end(); i++ ) {
-      param_info &p = i->second;
-      std::string name = p.get_name();
-      symbol *param = m_symtab->lookup(name.c_str());
-
-      size_t arg_size = p.get_size() / 8; // size of param in bytes
-      total_size = (total_size + arg_size - 1) / arg_size * arg_size; //aligned
-      p.add_offset(total_size);
-      param->set_address(param_address + total_size);
-      total_size += arg_size;
-   }
-
-   m_args_aligned_size = (total_size + 3) / 4 * 4; //final size aligned to word
-
-   return m_args_aligned_size;
-
-}
-
-
-void function_info::finalize( memory_space *param_mem )
-{
-   unsigned param_address = 0;
-   for( std::map<unsigned,param_info>::iterator i=m_ptx_kernel_param_info.begin(); i!=m_ptx_kernel_param_info.end(); i++ ) {
-      param_info &p = i->second;
-      if (p.is_ptr_shared()) continue; // Pointer to local memory: Should we pass the allocated shared memory address to the param memory space?
-      std::string name = p.get_name();
-      int type = p.get_type();
-      param_t param_value = p.get_value();
-      param_value.type = type;
-      symbol *param = m_symtab->lookup(name.c_str());
-      unsigned xtype = param->type()->get_key().scalar_type();
-      assert(xtype==(unsigned)type);
-      size_t size;
-      size = param_value.size; // size of param in bytes
-      // assert(param_value.offset == param_address);
-      if( size != p.get_size() / 8) {
-         printf("GPGPU-Sim PTX: WARNING actual kernel paramter size = %zu bytes vs. formal size = %zu (using smaller of two)\n",
-                size, p.get_size()/8);
-         size = (size<(p.get_size()/8))?size:(p.get_size()/8);
       }
-      // copy the parameter over word-by-word so that parameter that crosses a memory page can be copied over
-      //Jin: copy parameter using aligned rules
-      const type_info *paramtype = param->type();
-      int align_amount = paramtype->get_key().get_alignment_spec();
-      align_amount = (align_amount == -1) ? size : align_amount;
-      param_address = (param_address + align_amount - 1) / align_amount * align_amount; //aligned
-
-      const size_t word_size = 4;
-      //param_address = (param_address + size - 1) / size * size; //aligned with size
-      for (size_t idx = 0; idx < size; idx += word_size) {
-         const char *pdata = reinterpret_cast<const char*>(param_value.pdata) + idx; // cast to char * for ptr arithmetic
-         param_mem->write(param_address + idx, word_size, pdata,NULL,NULL);
-      }
-      unsigned offset = p.get_offset();
-      assert(offset == param_address);
-      param->set_address(param_address);
-      param_address += size;
-   }
-}
-
-void function_info::param_to_shared( memory_space *shared_mem, symbol_table *symtab )
-{
-   // TODO: call this only for PTXPlus with GT200 models
-   extern gpgpu_sim* g_the_gpu;
-   if (not g_the_gpu->get_config().convert_to_ptxplus()) return;
-
-   // copies parameters into simulated shared memory
-   for( std::map<unsigned,param_info>::iterator i=m_ptx_kernel_param_info.begin(); i!=m_ptx_kernel_param_info.end(); i++ ) {
-      param_info &p = i->second;
-      if (p.is_ptr_shared()) continue; // Pointer to local memory: Should we pass the allocated shared memory address to the param memory space?
-      std::string name = p.get_name();
-      int type = p.get_type();
-      param_t value = p.get_value();
-      value.type = type;
-      symbol *param = symtab->lookup(name.c_str());
-      unsigned xtype = param->type()->get_key().scalar_type();
-      assert(xtype==(unsigned)type);
-
-      int tmp;
-      size_t size;
-      unsigned offset = p.get_offset();
-      type_info_key::type_decode(xtype,size,tmp);
-
-      // Write to shared memory - offset + 0x10
-      shared_mem->write(offset+0x10,size/8,value.pdata,NULL,NULL);
-   }
-}
-
-
-void function_info::list_param( FILE *fout ) const
-{
-   for( std::map<unsigned,param_info>::const_iterator i=m_ptx_kernel_param_info.begin(); i!=m_ptx_kernel_param_info.end(); i++ ) {
-      const param_info &p = i->second;
-      std::string name = p.get_name();
-      symbol *param = m_symtab->lookup(name.c_str());
-      addr_t param_addr = param->get_address();
-      fprintf(fout, "%s: %#08x\n", name.c_str(), param_addr);
-   }
-   fflush(fout);
-}
-
-void function_info::ptx_jit_config(std::map<unsigned long long, size_t> mallocPtr_Size, memory_space *param_mem, gpgpu_t* gpu, dim3 gridDim, dim3 blockDim)
-{
-    static unsigned long long counter = 0;
-    std::vector< std::pair<size_t, unsigned char*> > param_data;
-    std::vector<unsigned> offsets;
-    std::vector<bool> paramIsPointer;
-
-    char * gpgpusim_path = getenv("GPGPUSIM_ROOT");
-    assert(gpgpusim_path!=NULL);
-    char * wys_exec_path = getenv("WYS_EXEC_PATH");
-    assert(wys_exec_path!=NULL);
-    std::string command = std::string("mkdir ") + gpgpusim_path + "/debug_tools/WatchYourStep/data";
-    std::string filename(std::string(gpgpusim_path) + "/debug_tools/WatchYourStep/data/params.config" + std::to_string(counter));
-
-    //initialize paramList
-    char buff[1024];
-    std::string filename_c(filename+"_c");
-    snprintf(buff,1024,"c++filt %s > %s", get_name().c_str(), filename_c.c_str());
-    system(buff);
-    FILE *fp = fopen(filename_c.c_str(), "r");
-    fgets(buff, 1024, fp);
-    fclose(fp);
-    std::string fn(buff);
-    size_t pos1, pos2;
-    pos1 = fn.find_last_of("(");
-    pos2 = fn.find(")", pos1);
-    assert(pos2>pos1&&pos1>0);
-    strcpy(buff, fn.substr(pos1 + 1, pos2 - pos1 - 1).c_str());
-    char *tok;
-    tok = strtok(buff, ",");
-    std::string tmp;
-    while(tok!=NULL){
-        std::string param(tok);
-        if(param.find("<")!=std::string::npos){
-            assert(param.find(">")==std::string::npos);
-            assert(param.find("*")==std::string::npos);
-            tmp = param;
-        } else {
-            if (tmp.length()>0){
-                tmp = "";
-                assert(param.find(">")!=std::string::npos);
-                assert(param.find("<")==std::string::npos);
-                assert(param.find("*")==std::string::npos);
-            }
-            printf("%s\n", param.c_str());
-            if(param.find("*")!=std::string::npos){
-                paramIsPointer.push_back(true);
-            }else{
-                paramIsPointer.push_back(false);
-            }
-        }
-        tok = strtok(NULL, ",");
-    }
-
-
-    for( std::map<unsigned,param_info>::iterator i=m_ptx_kernel_param_info.begin(); i!=m_ptx_kernel_param_info.end(); i++ ) {
-        param_info &p = i->second;
-        std::string name = p.get_name();
-        symbol *param = m_symtab->lookup(name.c_str());
-        addr_t param_addr = param->get_address();
-        param_t param_value = p.get_value();
-        offsets.push_back((unsigned)p.get_offset());
-
-        if (paramIsPointer[i->first] && (*(unsigned long long*)param_value.pdata != 0)){
-            //is pointer
-            assert(param_value.size==sizeof(void*)&&"MisID'd this param as pointer");
-            size_t array_size = 0;
-            unsigned long long param_pointer = *(unsigned long long*)param_value.pdata;
-            if(mallocPtr_Size.find(param_pointer)!=mallocPtr_Size.end()){
-                array_size = mallocPtr_Size[param_pointer];
-            }else{
-                for( std::map<unsigned long long, size_t>::iterator j=mallocPtr_Size.begin(); j!=mallocPtr_Size.end(); j++ ) {
-                    if(param_pointer>j->first&&param_pointer<j->first + j->second){
-                        array_size = j->first + j->second - param_pointer;
-                        break;
-                    }
-                }
-                assert(array_size>0&&"pointer was not previously malloc'd");
-            }
-
-            unsigned char* val = (unsigned char*) malloc(param_value.size);
-            param_mem->read(param_addr,param_value.size,(void*)val);
-            unsigned char* array_val = (unsigned char*) malloc(array_size);
-            gpu->get_global_memory()->read(*(unsigned*)((void*)val),array_size,(void*)array_val);
-            param_data.push_back(std::pair<size_t, unsigned char*>(array_size,array_val));
-            paramIsPointer.push_back(true);
-        }else{
-            unsigned char* val = (unsigned char*) malloc(param_value.size);
-            param_mem->read(param_addr,param_value.size,(void*)val);
-            param_data.push_back(std::pair<size_t, unsigned char*>(param_value.size,val));
-            paramIsPointer.push_back(false);
-        }
-    }
-
-    FILE *fout  = fopen (filename.c_str(), "w");
-    printf("Writing data to %s ...\n", filename.c_str());
-    fprintf(fout, "%s\n", get_name().c_str());
-    fprintf(fout, "%u,%u,%u %u,%u,%u\n", gridDim.x, gridDim.y, gridDim.z, blockDim.x, blockDim.y, blockDim.z);
-    size_t index = 0;
-    for( std::vector< std::pair<size_t,unsigned char*> >::const_iterator i=param_data.begin(); i!=param_data.end(); i++ ) {
-        if (paramIsPointer[index]){
-            fprintf(fout, "*");
-        }
-        fprintf(fout, "%lu :", i->first);
-        for (size_t j = 0; j<i->first; j++){
-            fprintf(fout, " %u", i->second[j]);
+  }
+}
+
+void ptx_instruction::set_bar_type() {
+  if (m_opcode == BAR_OP) {
+    switch (m_barrier_op) {
+      case SYNC_OPTION:
+        bar_type = SYNC;
+        break;
+      case ARRIVE_OPTION:
+        bar_type = ARRIVE;
+        break;
+      case RED_OPTION:
+        bar_type = RED;
+        switch (m_atomic_spec) {
+          case ATOMIC_POPC:
+            red_type = POPC_RED;
+            break;
+          case ATOMIC_AND:
+            red_type = AND_RED;
+            break;
+          case ATOMIC_OR:
+            red_type = OR_RED;
+            break;
         }
-        fprintf(fout, " : %u", offsets[index]);
-        free (i->second);
-        fprintf(fout, "\n");
-        index++;
-    }
-    fflush(fout);
-    fclose(fout);
-
-    //ptx config
-    std::string ptx_config_fn(std::string(gpgpusim_path) + "/debug_tools/WatchYourStep/data/ptx.config" + std::to_string(counter));
-    snprintf(buff, 1024, "grep -rn \".entry %s\" %s/*.ptx | cut -d \":\" -f 1-2 > %s", get_name().c_str(), wys_exec_path, ptx_config_fn.c_str());
-    if (system(buff)!=0){
-        printf("WARNING: Failed to execute grep to find ptx source \n");
-        printf("Problematic call: %s", buff);
+        break;
+      default:
         abort();
     }
-    FILE *fin = fopen(ptx_config_fn.c_str(), "r");
-    char ptx_source[256];
-    unsigned line_number;
-    int numscanned = fscanf(fin, "%[^:]:%u", ptx_source, &line_number);
-    assert(numscanned == 2);
-    fclose(fin);
-    snprintf(buff, 1024, "grep -rn \".version\" %s | cut -d \":\" -f 1 | xargs -I \"{}\" awk \"NR>={}&&NR<={}+2\" %s > %s", ptx_source, ptx_source, ptx_config_fn.c_str());
-    if (system(buff)!=0){
-        printf("WARNING: Failed to execute grep to find ptx header \n");
-        printf("Problematic call: %s", buff);
-        abort();
+  } else if (m_opcode == SST_OP) {
+    bar_type = SYNC;
+  }
+}
+
+void ptx_instruction::set_opcode_and_latency() {
+  unsigned int_latency[6];
+  unsigned fp_latency[5];
+  unsigned dp_latency[5];
+  unsigned sfu_latency;
+  unsigned tensor_latency;
+  unsigned int_init[6];
+  unsigned fp_init[5];
+  unsigned dp_init[5];
+  unsigned sfu_init;
+  unsigned tensor_init;
+  /*
+   * [0] ADD,SUB
+   * [1] MAX,Min
+   * [2] MUL
+   * [3] MAD
+   * [4] DIV
+   * [5] SHFL
+   */
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_int, "%u,%u,%u,%u,%u,%u",
+         &int_latency[0], &int_latency[1], &int_latency[2], &int_latency[3],
+         &int_latency[4], &int_latency[5]);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_fp, "%u,%u,%u,%u,%u",
+         &fp_latency[0], &fp_latency[1], &fp_latency[2], &fp_latency[3],
+         &fp_latency[4]);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_dp, "%u,%u,%u,%u,%u",
+         &dp_latency[0], &dp_latency[1], &dp_latency[2], &dp_latency[3],
+         &dp_latency[4]);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_sfu, "%u", &sfu_latency);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_tensor, "%u", &tensor_latency);
+  sscanf(gpgpu_ctx->func_sim->opcode_initiation_int, "%u,%u,%u,%u,%u,%u",
+         &int_init[0], &int_init[1], &int_init[2], &int_init[3], &int_init[4],
+         &int_init[5]);
+  sscanf(gpgpu_ctx->func_sim->opcode_initiation_fp, "%u,%u,%u,%u,%u",
+         &fp_init[0], &fp_init[1], &fp_init[2], &fp_init[3], &fp_init[4]);
+  sscanf(gpgpu_ctx->func_sim->opcode_initiation_dp, "%u,%u,%u,%u,%u",
+         &dp_init[0], &dp_init[1], &dp_init[2], &dp_init[3], &dp_init[4]);
+  sscanf(gpgpu_ctx->func_sim->opcode_initiation_sfu, "%u", &sfu_init);
+  sscanf(gpgpu_ctx->func_sim->opcode_initiation_tensor, "%u", &tensor_init);
+  sscanf(gpgpu_ctx->func_sim->cdp_latency_str, "%u,%u,%u,%u,%u",
+         &gpgpu_ctx->func_sim->cdp_latency[0],
+         &gpgpu_ctx->func_sim->cdp_latency[1],
+         &gpgpu_ctx->func_sim->cdp_latency[2],
+         &gpgpu_ctx->func_sim->cdp_latency[3],
+         &gpgpu_ctx->func_sim->cdp_latency[4]);
+
+  if (!m_operands.empty()) {
+    std::vector<operand_info>::iterator it;
+    for (it = ++m_operands.begin(); it != m_operands.end(); it++) {
+      num_operands++;
+      if ((it->is_reg() || it->is_vector())) {
+        num_regs++;
+      }
     }
-    fin = fopen(ptx_source, "r");
-    assert(fin!=NULL);
-    printf("Writing data to %s ...\n", ptx_config_fn.c_str());
-    fout = fopen(ptx_config_fn.c_str(), "a");
-    assert(fout!=NULL);
-    for (unsigned i = 0; i<line_number; i++){
-        fgets(buff, 1024, fin);
-        assert(!feof(fin));
+  }
+  op = ALU_OP;
+  mem_op = NOT_TEX;
+  initiation_interval = latency = 1;
+  switch (m_opcode) {
+    case MOV_OP:
+      assert(!(has_memory_read() && has_memory_write()));
+      if (has_memory_read()) op = LOAD_OP;
+      if (has_memory_write()) op = STORE_OP;
+      break;
+    case LD_OP:
+      op = LOAD_OP;
+      break;
+    case MMA_LD_OP:
+      op = TENSOR_CORE_LOAD_OP;
+      break;
+    case LDU_OP:
+      op = LOAD_OP;
+      break;
+    case ST_OP:
+      op = STORE_OP;
+      break;
+    case MMA_ST_OP:
+      op = TENSOR_CORE_STORE_OP;
+      break;
+    case BRA_OP:
+      op = BRANCH_OP;
+      break;
+    case BREAKADDR_OP:
+      op = BRANCH_OP;
+      break;
+    case TEX_OP:
+      op = LOAD_OP;
+      mem_op = TEX;
+      break;
+    case ATOM_OP:
+      op = LOAD_OP;
+      break;
+    case BAR_OP:
+      op = BARRIER_OP;
+      break;
+    case SST_OP:
+      op = BARRIER_OP;
+      break;
+    case MEMBAR_OP:
+      op = MEMORY_BARRIER_OP;
+      break;
+    case CALL_OP: {
+      if (m_is_printf || m_is_cdp) {
+        op = ALU_OP;
+      } else
+        op = CALL_OPS;
+      break;
     }
-    fprintf(fout, "\n\n");
-    do{
-        fprintf(fout, "%s", buff);
-        fgets(buff, 1024, fin);
-        if(feof(fin)){
-            break;
-        }
-    } while(strstr(buff, "entry")==NULL);
-
-    fclose(fin);
-    fflush(fout);
-    fclose(fout);
-    counter++;
-}
-
-template<int activate_level>
-bool ptx_debug_exec_dump_cond(int thd_uid, addr_t pc)
-{
-   if (g_debug_execution >= activate_level) {
-      // check each type of debug dump constraint to filter out dumps
-      if ( (g_debug_thread_uid != 0) && (thd_uid != (unsigned)g_debug_thread_uid) ) {
-         return false;
+    case CALLP_OP: {
+      if (m_is_printf || m_is_cdp) {
+        op = ALU_OP;
+      } else
+        op = CALL_OPS;
+      break;
+    }
+    case RET_OP:
+    case RETP_OP:
+      op = RET_OPS;
+      break;
+    case ADD_OP:
+    case ADDP_OP:
+    case ADDC_OP:
+    case SUB_OP:
+    case SUBC_OP:
+      // ADD,SUB latency
+      switch (get_type()) {
+        case F32_TYPE:
+          latency = fp_latency[0];
+          initiation_interval = fp_init[0];
+          op = SP_OP;
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          latency = dp_latency[0];
+          initiation_interval = dp_init[0];
+          op = DP_OP;
+          break;
+        case B32_TYPE:
+        case U32_TYPE:
+        case S32_TYPE:
+        default:  // Use int settings for default
+          latency = int_latency[0];
+          initiation_interval = int_init[0];
+          op = INTP_OP;
+          break;
       }
-      if ( (g_debug_pc != 0xBEEF1518) && (pc != g_debug_pc) ) {
-         return false;
+      break;
+    case MAX_OP:
+    case MIN_OP:
+      // MAX,MIN latency
+      switch (get_type()) {
+        case F32_TYPE:
+          latency = fp_latency[1];
+          initiation_interval = fp_init[1];
+          op = SP_OP;
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          latency = dp_latency[1];
+          initiation_interval = dp_init[1];
+          op = DP_OP;
+          break;
+        case B32_TYPE:
+        case U32_TYPE:
+        case S32_TYPE:
+        default:  // Use int settings for default
+          latency = int_latency[1];
+          initiation_interval = int_init[1];
+          op = INTP_OP;
+          break;
       }
-
-      return true;
-   }
-
-   return false;
-}
-
-void init_inst_classification_stat()
-{
-   static std::set<unsigned> init;
-   if( init.find(g_ptx_kernel_count) != init.end() )
-      return;
-   init.insert(g_ptx_kernel_count);
-#ifndef LIBCUDA
-   #define MAX_CLASS_KER 1024
-   char kernelname[MAX_CLASS_KER] ="";
-   if (!g_inst_classification_stat) g_inst_classification_stat = (void**)calloc(MAX_CLASS_KER, sizeof(void*));
-   snprintf(kernelname, MAX_CLASS_KER, "Kernel %d Classification\n",g_ptx_kernel_count  );
-   assert( g_ptx_kernel_count < MAX_CLASS_KER ) ; // a static limit on number of kernels increase it if it fails!
-   g_inst_classification_stat[g_ptx_kernel_count] = StatCreate(kernelname,1,20);
-   if (!g_inst_op_classification_stat) g_inst_op_classification_stat = (void**)calloc(MAX_CLASS_KER, sizeof(void*));
-   snprintf(kernelname, MAX_CLASS_KER, "Kernel %d OP Classification\n",g_ptx_kernel_count  );
-   g_inst_op_classification_stat[g_ptx_kernel_count] = StatCreate(kernelname,1,100);
-#endif
-}
-
-static unsigned get_tex_datasize( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-   const operand_info &src1 = pI->src1(); //the name of the texture
-   std::string texname = src1.name();
-
-   // TODO schi gpu is unsed
-   // gpgpu_t *gpu = thread->get_gpu();
-   /*
-     For programs with many streams, textures can be bound and unbound
-     asynchronously.  This means we need to use the kernel's "snapshot" of
-     the state of the texture mappings when it was launched (so that we
-     don't try to access the incorrect texture mapping if it's been updated,
-     or that we don't access a mapping that has been unbound).
-    */
-   kernel_info_t& k = thread->get_kernel();
-
-   // FIXME
-   const struct textureInfo* texInfo = NULL; // k.get_texinfo(texname);
-
-   unsigned data_size = texInfo->texel_size;
-   return data_size;
-}
-int ptx_thread_info::readRegister(const warp_inst_t &inst, unsigned lane_id, char *data, unsigned reg_id)
-{
-   const ptx_instruction *pI = m_func_info->get_instruction(inst.pc);
-
-   const operand_info &dst = pI->dst();
-   const operand_info &src = pI->operand_lookup(reg_id);
-   unsigned type = pI->get_type();
-   unsigned vector_spec = pI->get_vector();
-
-   // SIZE IS IN BITS
-   size_t size;
-   int basic_type;
-   type_info_key::type_decode(pI->get_type(), size, basic_type);
-
-   // NOTE: converting the register values like below (casting to ull) may not
-   // work. It might keep some upper bits that are stale. see ptx_sim.h line 56
-
-   int offset = 0;
-   int bytes = size/8;
-
-   if (vector_spec) {
-      if (vector_spec == V2_TYPE) {
-         ptx_reg_t ptx_regs[2];
-         get_vector_operand_values(src, ptx_regs, 2);
-         memcpy(data+offset, &ptx_regs[0], bytes);
-         offset += bytes;
-         memcpy(data+offset, &ptx_regs[1], bytes);
-         return 2;
+      break;
+    case MUL_OP:
+      // MUL latency
+      switch (get_type()) {
+        case F32_TYPE:
+          latency = fp_latency[2];
+          initiation_interval = fp_init[2];
+          op = SP_OP;
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          latency = dp_latency[2];
+          initiation_interval = dp_init[2];
+          op = DP_OP;
+          break;
+        case B32_TYPE:
+        case U32_TYPE:
+        case S32_TYPE:
+        default:  // Use int settings for default
+          latency = int_latency[2];
+          initiation_interval = int_init[2];
+          op = INTP_OP;
+          break;
       }
-      else if (vector_spec == V3_TYPE) {
-         ptx_reg_t ptx_regs[3];
-         get_vector_operand_values(src, ptx_regs, 3);
-         memcpy(data+offset, &ptx_regs[0], bytes);
-         offset += bytes;
-         memcpy(data+offset, &ptx_regs[1], bytes);
-         offset += bytes;
-         memcpy(data+offset, &ptx_regs[2], bytes);
-         offset += bytes;
-         return 3;
+      break;
+    case MAD_OP:
+    case MADC_OP:
+    case MADP_OP:
+    case FMA_OP:
+      // MAD latency
+      switch (get_type()) {
+        case F32_TYPE:
+          latency = fp_latency[3];
+          initiation_interval = fp_init[3];
+          op = SP_OP;
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          latency = dp_latency[3];
+          initiation_interval = dp_init[3];
+          op = DP_OP;
+          break;
+        case B32_TYPE:
+        case U32_TYPE:
+        case S32_TYPE:
+        default:  // Use int settings for default
+          latency = int_latency[3];
+          initiation_interval = int_init[3];
+          op = INTP_OP;
+          break;
       }
-      else {
-         assert(vector_spec == V4_TYPE);
-         ptx_reg_t ptx_regs[4];
-         get_vector_operand_values(src, ptx_regs, 4);
-         memcpy(data+offset, &ptx_regs[0], bytes);
-         offset += bytes;
-         memcpy(data+offset, &ptx_regs[1], bytes);
-         offset += bytes;
-         memcpy(data+offset, &ptx_regs[2], bytes);
-         offset += bytes;
-         memcpy(data+offset, &ptx_regs[3], bytes);
-         offset += bytes;
-         return 4;
+      break;
+    case MUL24_OP: //MUL24 is performed on mul32 units (with additional instructions for bitmasking) on devices with compute capability >1.x
+      latency = int_latency[2]+1;
+      initiation_interval = int_init[2]+1;
+      op = INTP_OP;
+      break;
+    case MAD24_OP:
+      latency = int_latency[3]+1;
+      initiation_interval = int_init[3]+1;
+      op = INTP_OP;
+      break;
+    case DIV_OP:
+    case REM_OP:
+      // Floating point only
+      op = SFU_OP;
+      switch (get_type()) {
+        case F32_TYPE:
+          latency = fp_latency[4];
+          initiation_interval = fp_init[4];
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          latency = dp_latency[4];
+          initiation_interval = dp_init[4];
+          break;
+        case B32_TYPE:
+        case U32_TYPE:
+        case S32_TYPE:
+        default:  // Use int settings for default
+          latency = int_latency[4];
+          initiation_interval = int_init[4];
+          break;
       }
-   } else {
-      ptx_reg_t ptx_reg = this->get_operand_value(src, dst, type, this, 1);
-      memcpy(data+offset, &ptx_reg, bytes);
-      offset += bytes;
-      return 1;
-   }
+      break;
+    case SQRT_OP:
+    case SIN_OP:
+    case COS_OP:
+    case EX2_OP:
+    case LG2_OP:
+    case RSQRT_OP:
+    case RCP_OP:
+      latency = sfu_latency;
+      initiation_interval = sfu_init;
+      op = SFU_OP;
+      break;
+    case MMA_OP:
+      latency = tensor_latency;
+      initiation_interval = tensor_init;
+      op = TENSOR_CORE_OP;
+      break;
+    case SHFL_OP:
+      latency = int_latency[5];
+      initiation_interval = int_init[5];
+      break;
+    default:
+      break;
+  }
+  set_fp_or_int_archop();
+  set_mul_div_or_other_archop();
 }
 
-void ptx_thread_info::writeRegister(const warp_inst_t &inst, unsigned lane_id, char *data)
-{
-   const ptx_instruction *pI = m_func_info->get_instruction(inst.pc);
-
-   const operand_info &dst = pI->dst();
-
-   unsigned type = pI->get_type();
-
-   ptx_reg_t reg;
-   memory_space_t space = pI->get_space();
-   unsigned vector_spec = pI->get_vector();
-
-   size_t size;
-   int t;
-   type_info_key::type_decode(type,size,t);
-
-   // NOTE: converting the register values like below (casting to ull) may not
-   // work. It might keep some upper bits that are stale. see ptx_sim.h line 56
-
-   int offset = 0;
-   int bytes = size/8;
+void ptx_thread_info::ptx_fetch_inst(inst_t &inst) const {
+  addr_t pc = get_pc();
+  const ptx_instruction *pI = m_func_info->get_instruction(pc);
+  inst = (const inst_t &)*pI;
+  assert(inst.valid());
+}
 
-   //reg.u64 = data[0];
-   memcpy(&reg, data, bytes);
+static unsigned datatype2size(unsigned data_type) {
+  unsigned data_size;
+  switch (data_type) {
+    case B8_TYPE:
+    case S8_TYPE:
+    case U8_TYPE:
+      data_size = 1;
+      break;
+    case B16_TYPE:
+    case S16_TYPE:
+    case U16_TYPE:
+    case F16_TYPE:
+      data_size = 2;
+      break;
+    case B32_TYPE:
+    case S32_TYPE:
+    case U32_TYPE:
+    case F32_TYPE:
+      data_size = 4;
+      break;
+    case B64_TYPE:
+    case BB64_TYPE:
+    case S64_TYPE:
+    case U64_TYPE:
+    case F64_TYPE:
+    case FF64_TYPE:
+      data_size = 8;
+      break;
+    case BB128_TYPE:
+      data_size = 16;
+      break;
+    default:
+      assert(0);
+      break;
+  }
+  return data_size;
+}
+
+void ptx_instruction::pre_decode() {
+  pc = m_PC;
+  isize = m_inst_size;
+  for (unsigned i = 0; i < MAX_OUTPUT_VALUES; i++) {
+    out[i] = 0;
+  }
+  for (unsigned i = 0; i < MAX_INPUT_VALUES; i++) {
+    in[i] = 0;
+  }
+  incount = 0;
+  outcount = 0;
+  is_vectorin = 0;
+  is_vectorout = 0;
+  std::fill_n(arch_reg.src, MAX_REG_OPERANDS, -1);
+  std::fill_n(arch_reg.dst, MAX_REG_OPERANDS, -1);
+  pred = 0;
+  ar1 = 0;
+  ar2 = 0;
+  space = m_space_spec;
+  memory_op = no_memory_op;
+  data_size = 0;
+  if (has_memory_read() || has_memory_write()) {
+    unsigned to_type = get_type();
+    data_size = datatype2size(to_type);
+    memory_op = has_memory_read() ? memory_load : memory_store;
+  }
+
+  bool has_dst = false;
+
+  switch (get_opcode()) {
+#define OP_DEF(OP, FUNC, STR, DST, CLASSIFICATION) \
+  case OP:                                         \
+    has_dst = (DST != 0);                          \
+    break;
+#define OP_W_DEF(OP, FUNC, STR, DST, CLASSIFICATION) \
+  case OP:                                           \
+    has_dst = (DST != 0);                            \
+    break;
+#include "opcodes.def"
+#undef OP_DEF
+#undef OP_W_DEF
+    default:
+      printf("Execution error: Invalid opcode (0x%x)\n", get_opcode());
+      break;
+  }
 
-   if (!vector_spec) {
-      if ( type == S16_TYPE || type == S32_TYPE ) {
-         sign_extend(reg,size,dst);
+  switch (m_cache_option) {
+    case CA_OPTION:
+      cache_op = CACHE_ALL;
+      break;
+    case NC_OPTION:
+      cache_op = CACHE_L1;
+      break;
+    case CG_OPTION:
+      cache_op = CACHE_GLOBAL;
+      break;
+    case CS_OPTION:
+      cache_op = CACHE_STREAMING;
+      break;
+    case LU_OPTION:
+      cache_op = CACHE_LAST_USE;
+      break;
+    case CV_OPTION:
+      cache_op = CACHE_VOLATILE;
+      break;
+    case WB_OPTION:
+      cache_op = CACHE_WRITE_BACK;
+      break;
+    case WT_OPTION:
+      cache_op = CACHE_WRITE_THROUGH;
+      break;
+    default:
+      // if( m_opcode == LD_OP || m_opcode == LDU_OP )
+      if (m_opcode == MMA_LD_OP || m_opcode == LD_OP || m_opcode == LDU_OP)
+        cache_op = CACHE_ALL;
+      // else if( m_opcode == ST_OP )
+      else if (m_opcode == MMA_ST_OP || m_opcode == ST_OP)
+        cache_op = CACHE_WRITE_BACK;
+      else if (m_opcode == ATOM_OP)
+        cache_op = CACHE_GLOBAL;
+      break;
+  }
+
+  set_opcode_and_latency();
+  set_bar_type();
+  // Get register operands
+  int n = 0, m = 0;
+  ptx_instruction::const_iterator opr = op_iter_begin();
+  for (; opr != op_iter_end(); opr++, n++) {  // process operands
+    const operand_info &o = *opr;
+    if (has_dst && n == 0) {
+      // Do not set the null register "_" as an architectural register
+      if (o.is_reg() && !o.is_non_arch_reg()) {
+        out[0] = o.reg_num();
+        arch_reg.dst[0] = o.arch_reg_num();
+      } else if (o.is_vector()) {
+        is_vectorin = 1;
+        unsigned num_elem = o.get_vect_nelem();
+        if (num_elem >= 1) out[0] = o.reg1_num();
+        if (num_elem >= 2) out[1] = o.reg2_num();
+        if (num_elem >= 3) out[2] = o.reg3_num();
+        if (num_elem >= 4) out[3] = o.reg4_num();
+        if (num_elem >= 5) out[4] = o.reg5_num();
+        if (num_elem >= 6) out[5] = o.reg6_num();
+        if (num_elem >= 7) out[6] = o.reg7_num();
+        if (num_elem >= 8) out[7] = o.reg8_num();
+        for (int i = 0; i < num_elem; i++) arch_reg.dst[i] = o.arch_reg_num(i);
       }
-      set_operand_value(dst,reg, type, this, pI);
-   } else {
-      ptx_reg_t data1, data2, data3, data4;
-      memcpy(&data1, data+offset, bytes);
-      offset += bytes;
-      memcpy(&data2, data+offset, bytes);
-      offset += bytes;
-      if (vector_spec != V2_TYPE) { //either V3 or V4
-         memcpy(&data3, data+offset, bytes);
-         offset += bytes;
-         if (vector_spec != V3_TYPE) { //v4
-            memcpy(&data4, data+offset, bytes);
-            offset += bytes;
-            set_vector_operand_values(dst,data1,data2,data3,data4);
-         } else { //v3
-            set_vector_operand_values(dst,data1,data2,data3,data3);
-         }
-      } else { //v2
-         set_vector_operand_values(dst,data1,data2,data2,data2);
+    } else {
+      if (o.is_reg() && !o.is_non_arch_reg()) {
+        int reg_num = o.reg_num();
+        arch_reg.src[m] = o.arch_reg_num();
+        switch (m) {
+          case 0:
+            in[0] = reg_num;
+            break;
+          case 1:
+            in[1] = reg_num;
+            break;
+          case 2:
+            in[2] = reg_num;
+            break;
+          default:
+            break;
+        }
+        m++;
+      } else if (o.is_vector()) {
+        // assert(m == 0); //only support 1 vector operand (for textures) right
+        // now
+        is_vectorout = 1;
+        unsigned num_elem = o.get_vect_nelem();
+        if (num_elem >= 1) in[m + 0] = o.reg1_num();
+        if (num_elem >= 2) in[m + 1] = o.reg2_num();
+        if (num_elem >= 3) in[m + 2] = o.reg3_num();
+        if (num_elem >= 4) in[m + 3] = o.reg4_num();
+        if (num_elem >= 5) in[m + 4] = o.reg5_num();
+        if (num_elem >= 6) in[m + 5] = o.reg6_num();
+        if (num_elem >= 7) in[m + 6] = o.reg7_num();
+        if (num_elem >= 8) in[m + 7] = o.reg8_num();
+        for (int i = 0; i < num_elem; i++)
+          arch_reg.src[m + i] = o.arch_reg_num(i);
+        m += num_elem;
+      }
+    }
+  }
+
+  // Setting number of input and output operands which is required for
+  // scoreboard check
+  for (int i = 0; i < MAX_OUTPUT_VALUES; i++)
+    if (out[i] > 0) outcount++;
+
+  for (int i = 0; i < MAX_INPUT_VALUES; i++)
+    if (in[i] > 0) incount++;
+
+  // Get predicate
+  if (has_pred()) {
+    const operand_info &p = get_pred();
+    pred = p.reg_num();
+  }
+
+  // Get address registers inside memory operands.
+  // Assuming only one memory operand per instruction,
+  //  and maximum of two address registers for one memory operand.
+  if (has_memory_read() || has_memory_write()) {
+    ptx_instruction::const_iterator op = op_iter_begin();
+    for (; op != op_iter_end(); op++, n++) {  // process operands
+      const operand_info &o = *op;
+
+      if (o.is_memory_operand()) {
+        // We do not support the null register as a memory operand
+        assert(!o.is_non_arch_reg());
+
+        // Check PTXPlus-type operand
+        // memory operand with addressing (ex. s[0x4] or g[$r1])
+        if (o.is_memory_operand2()) {
+          // memory operand with one address register (ex. g[$r1+0x4] or
+          // s[$r2+=0x4])
+          if (o.get_double_operand_type() == 0 ||
+              o.get_double_operand_type() == 3) {
+            ar1 = o.reg_num();
+            arch_reg.src[4] = o.arch_reg_num();
+            // TODO: address register in $r2+=0x4 should be an output register
+            // as well
+          }
+          // memory operand with two address register (ex. s[$r1+$r1] or
+          // g[$r1+=$r2])
+          else if (o.get_double_operand_type() == 1 ||
+                   o.get_double_operand_type() == 2) {
+            ar1 = o.reg1_num();
+            arch_reg.src[4] = o.arch_reg_num();
+            ar2 = o.reg2_num();
+            arch_reg.src[5] = o.arch_reg_num();
+            // TODO: first address register in $r1+=$r2 should be an output
+            // register as well
+          }
+        } else if (o.is_immediate_address()) {
+        }
+        // Regular PTX operand
+        else if (o.get_symbol()
+                     ->type()
+                     ->get_key()
+                     .is_reg()) {  // Memory operand contains a register
+          ar1 = o.reg_num();
+          arch_reg.src[4] = o.arch_reg_num();
+        }
+      }
+    }
+  }
+
+  // get reconvergence pc
+  reconvergence_pc = gpgpu_ctx->func_sim->get_converge_point(pc);
+
+  m_decoded = true;
+}
+
+void function_info::add_param_name_type_size(unsigned index, std::string name,
+                                             int type, size_t size, bool ptr,
+                                             memory_space_t space) {
+  unsigned parsed_index;
+  char buffer[2048];
+  snprintf(buffer, 2048, "%s_param_%%u", m_name.c_str());
+  int ntokens = sscanf(name.c_str(), buffer, &parsed_index);
+  if (ntokens == 1) {
+    assert(m_ptx_kernel_param_info.find(parsed_index) ==
+           m_ptx_kernel_param_info.end());
+    m_ptx_kernel_param_info[parsed_index] =
+        param_info(name, type, size, ptr, space);
+  } else {
+    assert(m_ptx_kernel_param_info.find(index) ==
+           m_ptx_kernel_param_info.end());
+    m_ptx_kernel_param_info[index] = param_info(name, type, size, ptr, space);
+  }
+}
+
+void function_info::add_param_data(unsigned argn,
+                                   struct gpgpu_ptx_sim_arg *args) {
+  const void *data = args->m_start;
+
+  bool scratchpad_memory_param =
+      false;  // Is this parameter in CUDA shared memory or OpenCL local memory
+
+  std::map<unsigned, param_info>::iterator i =
+      m_ptx_kernel_param_info.find(argn);
+  if (i != m_ptx_kernel_param_info.end()) {
+    if (i->second.is_ptr_shared()) {
+      assert(
+          args->m_start == NULL &&
+          "OpenCL parameter pointer to local memory must have NULL as value");
+      scratchpad_memory_param = true;
+    } else {
+      param_t tmp;
+      tmp.pdata = args->m_start;
+      tmp.size = args->m_nbytes;
+      tmp.offset = args->m_offset;
+      tmp.type = 0;
+      i->second.add_data(tmp);
+      i->second.add_offset((unsigned)args->m_offset);
+    }
+  } else {
+    scratchpad_memory_param = true;
+  }
+
+  if (scratchpad_memory_param) {
+    // This should only happen for OpenCL:
+    //
+    // The LLVM PTX compiler in NVIDIA's driver (version 190.29)
+    // does not generate an argument in the function declaration
+    // for __constant arguments.
+    //
+    // The associated constant memory space can be allocated in two
+    // ways. It can be explicitly initialized in the .ptx file where
+    // it is declared.  Or, it can be allocated using the clCreateBuffer
+    // on the host. In this later case, the .ptx file will contain
+    // a global declaration of the parameter, but it will have an unknown
+    // array size.  Thus, the symbol's address will not be set and we need
+    // to set it here before executing the PTX.
+
+    char buffer[2048];
+    snprintf(buffer, 2048, "%s_param_%u", m_name.c_str(), argn);
+
+    symbol *p = m_symtab->lookup(buffer);
+    if (p == NULL) {
+      printf(
+          "GPGPU-Sim PTX: ERROR ** could not locate symbol for \'%s\' : cannot "
+          "bind buffer\n",
+          buffer);
+      abort();
+    }
+    if (data)
+      p->set_address((addr_t) * (size_t *)data);
+    else {
+      // clSetKernelArg was passed NULL pointer for data...
+      // this is used for dynamically sized shared memory on NVIDIA platforms
+      bool is_ptr_shared = false;
+      if (i != m_ptx_kernel_param_info.end()) {
+        is_ptr_shared = i->second.is_ptr_shared();
       }
-   }
-}
 
-#ifndef LIBCUDA
-int tensorcore_op(int inst_opcode){
-
-       if((inst_opcode==MMA_OP)||(inst_opcode==MMA_LD_OP)||(inst_opcode==MMA_ST_OP))
-		return 1;
-       else
-		return 0;
+      if (!is_ptr_shared and !p->is_shared()) {
+        printf(
+            "GPGPU-Sim PTX: ERROR ** clSetKernelArg passed NULL but arg not "
+            "shared memory\n");
+        abort();
+      }
+      unsigned num_bits = 8 * args->m_nbytes;
+      printf(
+          "GPGPU-Sim PTX: deferred allocation of shared region for \"%s\" from "
+          "0x%x to 0x%x (shared memory space)\n",
+          p->name().c_str(), m_symtab->get_shared_next(),
+          m_symtab->get_shared_next() + num_bits / 8);
+      fflush(stdout);
+      assert((num_bits % 8) == 0);
+      addr_t addr = m_symtab->get_shared_next();
+      addr_t addr_pad =
+          num_bits
+              ? (((num_bits / 8) - (addr % (num_bits / 8))) % (num_bits / 8))
+              : 0;
+      p->set_address(addr + addr_pad);
+      m_symtab->alloc_shared(num_bits / 8 + addr_pad);
+    }
+  }
 }
-void ptx_thread_info::ptx_exec_inst( warp_inst_t &inst, unsigned lane_id)
-{
-
-   bool skip = false;
-   int op_classification = 0;
-   addr_t pc = next_instr();
-   assert( pc == inst.pc ); // make sure timing model and functional model are in sync
-   const ptx_instruction *pI = m_func_info->get_instruction(pc);
 
-   set_npc( pc + pI->inst_size() );
-
-   // TODO add for vectorLength used by gpu/gpgpu-sim/cude_core.cc
-   unsigned vector_spec = pI->get_vector();
-   if (vector_spec) {
-      if (vector_spec == V2_TYPE) {
-         inst.vectorLength = 2;
+unsigned function_info::get_args_aligned_size() {
+  if (m_args_aligned_size >= 0) return m_args_aligned_size;
+
+  unsigned param_address = 0;
+  unsigned int total_size = 0;
+  for (std::map<unsigned, param_info>::iterator i =
+           m_ptx_kernel_param_info.begin();
+       i != m_ptx_kernel_param_info.end(); i++) {
+    param_info &p = i->second;
+    std::string name = p.get_name();
+    symbol *param = m_symtab->lookup(name.c_str());
+
+    size_t arg_size = p.get_size() / 8;  // size of param in bytes
+    total_size = (total_size + arg_size - 1) / arg_size * arg_size;  // aligned
+    p.add_offset(total_size);
+    param->set_address(param_address + total_size);
+    total_size += arg_size;
+  }
+
+  m_args_aligned_size = (total_size + 3) / 4 * 4;  // final size aligned to word
+
+  return m_args_aligned_size;
+}
+
+void function_info::finalize(memory_space *param_mem) {
+  unsigned param_address = 0;
+  for (std::map<unsigned, param_info>::iterator i =
+           m_ptx_kernel_param_info.begin();
+       i != m_ptx_kernel_param_info.end(); i++) {
+    param_info &p = i->second;
+    if (p.is_ptr_shared())
+      continue;  // Pointer to local memory: Should we pass the allocated shared
+                 // memory address to the param memory space?
+    std::string name = p.get_name();
+    int type = p.get_type();
+    param_t param_value = p.get_value();
+    param_value.type = type;
+    symbol *param = m_symtab->lookup(name.c_str());
+    unsigned xtype = param->type()->get_key().scalar_type();
+    assert(xtype == (unsigned)type);
+    size_t size;
+    size = param_value.size;  // size of param in bytes
+    // assert(param_value.offset == param_address);
+    if (size != p.get_size() / 8) {
+      printf(
+          "GPGPU-Sim PTX: WARNING actual kernel paramter size = %zu bytes vs. "
+          "formal size = %zu (using smaller of two)\n",
+          size, p.get_size() / 8);
+      size = (size < (p.get_size() / 8)) ? size : (p.get_size() / 8);
+    }
+    // copy the parameter over word-by-word so that parameter that crosses a
+    // memory page can be copied over
+    // Jin: copy parameter using aligned rules
+    const type_info *paramtype = param->type();
+    int align_amount = paramtype->get_key().get_alignment_spec();
+    align_amount = (align_amount == -1) ? size : align_amount;
+    param_address = (param_address + align_amount - 1) / align_amount *
+                    align_amount;  // aligned
+
+    const size_t word_size = 4;
+    // param_address = (param_address + size - 1) / size * size; //aligned with
+    // size
+    for (size_t idx = 0; idx < size; idx += word_size) {
+      const char *pdata = reinterpret_cast<const char *>(param_value.pdata) +
+                          idx;  // cast to char * for ptr arithmetic
+      param_mem->write(param_address + idx, word_size, pdata, NULL, NULL);
+    }
+    unsigned offset = p.get_offset();
+    assert(offset == param_address);
+    param->set_address(param_address);
+    param_address += size;
+  }
+}
+
+void function_info::param_to_shared(memory_space *shared_mem,
+                                    symbol_table *symtab) {
+  // TODO: call this only for PTXPlus with GT200 models
+  // extern gpgpu_sim* g_the_gpu;
+  if (not gpgpu_ctx->the_gpgpusim->g_the_gpu->get_config().convert_to_ptxplus())
+    return;
+
+  // copies parameters into simulated shared memory
+  for (std::map<unsigned, param_info>::iterator i =
+           m_ptx_kernel_param_info.begin();
+       i != m_ptx_kernel_param_info.end(); i++) {
+    param_info &p = i->second;
+    if (p.is_ptr_shared())
+      continue;  // Pointer to local memory: Should we pass the allocated shared
+                 // memory address to the param memory space?
+    std::string name = p.get_name();
+    int type = p.get_type();
+    param_t value = p.get_value();
+    value.type = type;
+    symbol *param = symtab->lookup(name.c_str());
+    unsigned xtype = param->type()->get_key().scalar_type();
+    assert(xtype == (unsigned)type);
+
+    int tmp;
+    size_t size;
+    unsigned offset = p.get_offset();
+    type_info_key::type_decode(xtype, size, tmp);
+
+    // Write to shared memory - offset + 0x10
+    shared_mem->write(offset + 0x10, size / 8, value.pdata, NULL, NULL);
+  }
+}
+
+void function_info::list_param(FILE *fout) const {
+  for (std::map<unsigned, param_info>::const_iterator i =
+           m_ptx_kernel_param_info.begin();
+       i != m_ptx_kernel_param_info.end(); i++) {
+    const param_info &p = i->second;
+    std::string name = p.get_name();
+    symbol *param = m_symtab->lookup(name.c_str());
+    addr_t param_addr = param->get_address();
+    fprintf(fout, "%s: %#08x\n", name.c_str(), param_addr);
+  }
+  fflush(fout);
+}
+
+void function_info::ptx_jit_config(
+    std::map<unsigned long long, size_t> mallocPtr_Size,
+    memory_space *param_mem, gpgpu_t *gpu, dim3 gridDim, dim3 blockDim) {
+  static unsigned long long counter = 0;
+  std::vector<std::pair<size_t, unsigned char *> > param_data;
+  std::vector<unsigned> offsets;
+  std::vector<bool> paramIsPointer;
+
+  char *gpgpusim_path = getenv("GPGPUSIM_ROOT");
+  assert(gpgpusim_path != NULL);
+  char *wys_exec_path = getenv("WYS_EXEC_PATH");
+  assert(wys_exec_path != NULL);
+  std::string command =
+      std::string("mkdir ") + gpgpusim_path + "/debug_tools/WatchYourStep/data";
+  std::string filename(std::string(gpgpusim_path) +
+                       "/debug_tools/WatchYourStep/data/params.config" +
+                       std::to_string(counter));
+
+  // initialize paramList
+  char buff[1024];
+  std::string filename_c(filename + "_c");
+  snprintf(buff, 1024, "c++filt %s > %s", get_name().c_str(),
+           filename_c.c_str());
+  assert(system(buff) != NULL);
+  FILE *fp = fopen(filename_c.c_str(), "r");
+  fgets(buff, 1024, fp);
+  fclose(fp);
+  std::string fn(buff);
+  size_t pos1, pos2;
+  pos1 = fn.find_last_of("(");
+  pos2 = fn.find(")", pos1);
+  assert(pos2 > pos1 && pos1 > 0);
+  strcpy(buff, fn.substr(pos1 + 1, pos2 - pos1 - 1).c_str());
+  char *tok;
+  tok = strtok(buff, ",");
+  std::string tmp;
+  while (tok != NULL) {
+    std::string param(tok);
+    if (param.find("<") != std::string::npos) {
+      assert(param.find(">") == std::string::npos);
+      assert(param.find("*") == std::string::npos);
+      tmp = param;
+    } else {
+      if (tmp.length() > 0) {
+        tmp = "";
+        assert(param.find(">") != std::string::npos);
+        assert(param.find("<") == std::string::npos);
+        assert(param.find("*") == std::string::npos);
       }
-      else if (vector_spec == V3_TYPE) {
-         inst.vectorLength = 3;
+      printf("%s\n", param.c_str());
+      if (param.find("*") != std::string::npos) {
+        paramIsPointer.push_back(true);
+      } else {
+        paramIsPointer.push_back(false);
       }
-      else {
-         assert(vector_spec == V4_TYPE);
-         inst.vectorLength = 4;
+    }
+    tok = strtok(NULL, ",");
+  }
+
+  for (std::map<unsigned, param_info>::iterator i =
+           m_ptx_kernel_param_info.begin();
+       i != m_ptx_kernel_param_info.end(); i++) {
+    param_info &p = i->second;
+    std::string name = p.get_name();
+    symbol *param = m_symtab->lookup(name.c_str());
+    addr_t param_addr = param->get_address();
+    param_t param_value = p.get_value();
+    offsets.push_back((unsigned)p.get_offset());
+
+    if (paramIsPointer[i->first] &&
+        (*(unsigned long long *)param_value.pdata != 0)) {
+      // is pointer
+      assert(param_value.size == sizeof(void *) &&
+             "MisID'd this param as pointer");
+      size_t array_size = 0;
+      unsigned long long param_pointer =
+          *(unsigned long long *)param_value.pdata;
+      if (mallocPtr_Size.find(param_pointer) != mallocPtr_Size.end()) {
+        array_size = mallocPtr_Size[param_pointer];
+      } else {
+        for (std::map<unsigned long long, size_t>::iterator j =
+                 mallocPtr_Size.begin();
+             j != mallocPtr_Size.end(); j++) {
+          if (param_pointer > j->first &&
+              param_pointer < j->first + j->second) {
+            array_size = j->first + j->second - param_pointer;
+            break;
+          }
+        }
+        assert(array_size > 0 && "pointer was not previously malloc'd");
       }
-   } else {
-      inst.vectorLength = 1;
-   }
-
 
+      unsigned char *val = (unsigned char *)malloc(param_value.size);
+      param_mem->read(param_addr, param_value.size, (void *)val);
+      unsigned char *array_val = (unsigned char *)malloc(array_size);
+      gpu->get_global_memory()->read(*(unsigned *)((void *)val), array_size,
+                                     (void *)array_val);
+      param_data.push_back(
+          std::pair<size_t, unsigned char *>(array_size, array_val));
+      paramIsPointer.push_back(true);
+    } else {
+      unsigned char *val = (unsigned char *)malloc(param_value.size);
+      param_mem->read(param_addr, param_value.size, (void *)val);
+      param_data.push_back(
+          std::pair<size_t, unsigned char *>(param_value.size, val));
+      paramIsPointer.push_back(false);
+    }
+  }
+
+  FILE *fout = fopen(filename.c_str(), "w");
+  printf("Writing data to %s ...\n", filename.c_str());
+  fprintf(fout, "%s\n", get_name().c_str());
+  fprintf(fout, "%u,%u,%u %u,%u,%u\n", gridDim.x, gridDim.y, gridDim.z,
+          blockDim.x, blockDim.y, blockDim.z);
+  size_t index = 0;
+  for (std::vector<std::pair<size_t, unsigned char *> >::const_iterator i =
+           param_data.begin();
+       i != param_data.end(); i++) {
+    if (paramIsPointer[index]) {
+      fprintf(fout, "*");
+    }
+    fprintf(fout, "%lu :", i->first);
+    for (size_t j = 0; j < i->first; j++) {
+      fprintf(fout, " %u", i->second[j]);
+    }
+    fprintf(fout, " : %u", offsets[index]);
+    free(i->second);
+    fprintf(fout, "\n");
+    index++;
+  }
+  fflush(fout);
+  fclose(fout);
+
+  // ptx config
+  std::string ptx_config_fn(std::string(gpgpusim_path) +
+                            "/debug_tools/WatchYourStep/data/ptx.config" +
+                            std::to_string(counter));
+  snprintf(buff, 1024,
+           "grep -rn \".entry %s\" %s/*.ptx | cut -d \":\" -f 1-2 > %s",
+           get_name().c_str(), wys_exec_path, ptx_config_fn.c_str());
+  if (system(buff) != 0) {
+    printf("WARNING: Failed to execute grep to find ptx source \n");
+    printf("Problematic call: %s", buff);
+    abort();
+  }
+  FILE *fin = fopen(ptx_config_fn.c_str(), "r");
+  char ptx_source[256];
+  unsigned line_number;
+  int numscanned = fscanf(fin, "%[^:]:%u", ptx_source, &line_number);
+  assert(numscanned == 2);
+  fclose(fin);
+  snprintf(buff, 1024,
+           "grep -rn \".version\" %s | cut -d \":\" -f 1 | xargs -I \"{}\" awk "
+           "\"NR>={}&&NR<={}+2\" %s > %s",
+           ptx_source, ptx_source, ptx_config_fn.c_str());
+  if (system(buff) != 0) {
+    printf("WARNING: Failed to execute grep to find ptx header \n");
+    printf("Problematic call: %s", buff);
+    abort();
+  }
+  fin = fopen(ptx_source, "r");
+  assert(fin != NULL);
+  printf("Writing data to %s ...\n", ptx_config_fn.c_str());
+  fout = fopen(ptx_config_fn.c_str(), "a");
+  assert(fout != NULL);
+  for (unsigned i = 0; i < line_number; i++) {
+    assert(fgets(buff, 1024, fin) != NULL);
+    assert(!feof(fin));
+  }
+  fprintf(fout, "\n\n");
+  do {
+    fprintf(fout, "%s", buff);
+    assert(fgets(buff, 1024, fin) != NULL);
+    if (feof(fin)) {
+      break;
+    }
+  } while (strstr(buff, "entry") == NULL);
 
-   try {
+  fclose(fin);
+  fflush(fout);
+  fclose(fout);
+  counter++;
+}
 
-   clearRPC();
-   m_last_set_operand_value.u64 = 0;
+template <int activate_level>
+bool cuda_sim::ptx_debug_exec_dump_cond(int thd_uid, addr_t pc) {
+  if (g_debug_execution >= activate_level) {
+    // check each type of debug dump constraint to filter out dumps
+    if ((g_debug_thread_uid != 0) &&
+        (thd_uid != (unsigned)g_debug_thread_uid)) {
+      return false;
+    }
+    if ((g_debug_pc != 0xBEEF1518) && (pc != g_debug_pc)) {
+      return false;
+    }
 
-   if(is_done())
-   {
-      printf("attempted to execute instruction on a thread that is already done.\n");
+    return true;
+  }
+
+  return false;
+}
+
+void cuda_sim::init_inst_classification_stat() {
+  static std::set<unsigned> init;
+  if (init.find(g_ptx_kernel_count) != init.end()) return;
+  init.insert(g_ptx_kernel_count);
+
+#define MAX_CLASS_KER 1024
+  char kernelname[MAX_CLASS_KER] = "";
+  if (!g_inst_classification_stat)
+    g_inst_classification_stat = (void **)calloc(MAX_CLASS_KER, sizeof(void *));
+  snprintf(kernelname, MAX_CLASS_KER, "Kernel %d Classification\n",
+           g_ptx_kernel_count);
+  assert(g_ptx_kernel_count <
+         MAX_CLASS_KER);  // a static limit on number of kernels increase it if
+                          // it fails!
+  g_inst_classification_stat[g_ptx_kernel_count] =
+      StatCreate(kernelname, 1, 20);
+  if (!g_inst_op_classification_stat)
+    g_inst_op_classification_stat =
+        (void **)calloc(MAX_CLASS_KER, sizeof(void *));
+  snprintf(kernelname, MAX_CLASS_KER, "Kernel %d OP Classification\n",
+           g_ptx_kernel_count);
+  g_inst_op_classification_stat[g_ptx_kernel_count] =
+      StatCreate(kernelname, 1, 100);
+}
+
+static unsigned get_tex_datasize(const ptx_instruction *pI,
+                                 ptx_thread_info *thread) {
+  const operand_info &src1 = pI->src1();  // the name of the texture
+  std::string texname = src1.name();
+
+  /*
+    For programs with many streams, textures can be bound and unbound
+    asynchronously.  This means we need to use the kernel's "snapshot" of
+    the state of the texture mappings when it was launched (so that we
+    don't try to access the incorrect texture mapping if it's been updated,
+    or that we don't access a mapping that has been unbound).
+   */
+  kernel_info_t &k = thread->get_kernel();
+  const struct textureInfo *texInfo = k.get_texinfo(texname);
+
+  unsigned data_size = texInfo->texel_size;
+  return data_size;
+}
+
+int tensorcore_op(int inst_opcode) {
+  if ((inst_opcode == MMA_OP) || (inst_opcode == MMA_LD_OP) ||
+      (inst_opcode == MMA_ST_OP))
+    return 1;
+  else
+    return 0;
+}
+void ptx_thread_info::ptx_exec_inst(warp_inst_t &inst, unsigned lane_id) {
+  bool skip = false;
+  int op_classification = 0;
+  addr_t pc = next_instr();
+  assert(pc ==
+         inst.pc);  // make sure timing model and functional model are in sync
+  const ptx_instruction *pI = m_func_info->get_instruction(pc);
+
+  set_npc(pc + pI->inst_size());
+
+  try {
+    clearRPC();
+    m_last_set_operand_value.u64 = 0;
+
+    if (is_done()) {
+      printf(
+          "attempted to execute instruction on a thread that is already "
+          "done.\n");
       assert(0);
-   }
-
-   if ( g_debug_execution >= 6 || m_gpu->get_config().get_ptx_inst_debug_to_file()) {
-      if ( (g_debug_thread_uid==0) || (get_uid() == (unsigned)g_debug_thread_uid) ) {
+    }
 
-          clear_modifiedregs();
-         enable_debug_trace();
+    if (g_debug_execution >= 6 ||
+        m_gpu->get_config().get_ptx_inst_debug_to_file()) {
+      if ((m_gpu->gpgpu_ctx->func_sim->g_debug_thread_uid == 0) ||
+          (get_uid() ==
+           (unsigned)(m_gpu->gpgpu_ctx->func_sim->g_debug_thread_uid))) {
+        clear_modifiedregs();
+        enable_debug_trace();
       }
-   }
-
+    }
 
-   if( pI->has_pred() ) {
+    if (pI->has_pred()) {
       const operand_info &pred = pI->get_pred();
       ptx_reg_t pred_value = get_operand_value(pred, pred, PRED_TYPE, this, 0);
-      if(pI->get_pred_mod() == -1) {
-            skip = (pred_value.pred & 0x0001) ^ pI->get_pred_neg(); //ptxplus inverts the zero flag
+      if (pI->get_pred_mod() == -1) {
+        skip = (pred_value.pred & 0x0001) ^
+               pI->get_pred_neg();  // ptxplus inverts the zero flag
       } else {
-            skip = !pred_lookup(pI->get_pred_mod(), pred_value.pred & 0x000F);
+        skip = !pred_lookup(pI->get_pred_mod(), pred_value.pred & 0x000F);
       }
-   }
-   int inst_opcode=pI->get_opcode();
+    }
+    int inst_opcode = pI->get_opcode();
 
-   if( skip ) {
+    if (skip) {
       inst.set_not_active(lane_id);
-   } else {
+    } else {
       const ptx_instruction *pI_saved = pI;
       ptx_instruction *pJ = NULL;
-      if( pI->get_opcode() == VOTE_OP ) {
-         pJ = new ptx_instruction(*pI);
-         *((warp_inst_t*)pJ) = inst; // copy active mask information
-         pI = pJ;
+      if (pI->get_opcode() == VOTE_OP || pI->get_opcode() == ACTIVEMASK_OP) {
+        pJ = new ptx_instruction(*pI);
+        *((warp_inst_t *)pJ) = inst;  // copy active mask information
+        pI = pJ;
       }
 
-      if(((inst_opcode==MMA_OP||inst_opcode==MMA_LD_OP||inst_opcode==MMA_ST_OP))){
-      		if(inst.active_count()!=MAX_WARP_SIZE)
-		{
-			printf("Tensor Core operation are warp synchronous operation. All the threads needs to be active.");
-			assert(0);
-		}
+      if (((inst_opcode == MMA_OP || inst_opcode == MMA_LD_OP ||
+            inst_opcode == MMA_ST_OP))) {
+        if (inst.active_count() != MAX_WARP_SIZE) {
+          printf(
+              "Tensor Core operation are warp synchronous operation. All the "
+              "threads needs to be active.");
+          assert(0);
+        }
       }
 
-      //Tensorcore is warp synchronous operation. So these instructions needs to be executed only once. To make the simulation faster removing the redundant tensorcore operation
-      if(!tensorcore_op(inst_opcode)||((tensorcore_op(inst_opcode))&&(lane_id==0))){
-	      switch ( inst_opcode ) {
-	#define OP_DEF(OP,FUNC,STR,DST,CLASSIFICATION) case OP: FUNC(pI,this); op_classification = CLASSIFICATION; break;
-	#define OP_W_DEF(OP,FUNC,STR,DST,CLASSIFICATION) case OP: FUNC(pI,get_core(),inst); op_classification = CLASSIFICATION; break;
-	#include "opcodes.def"
-	#undef OP_DEF
-	#undef OP_W_DEF
-	      default: printf( "Execution error: Invalid opcode (0x%x)\n", pI->get_opcode() ); break;
-	      }
+      // Tensorcore is warp synchronous operation. So these instructions needs
+      // to be executed only once. To make the simulation faster removing the
+      // redundant tensorcore operation
+      if (!tensorcore_op(inst_opcode) ||
+          ((tensorcore_op(inst_opcode)) && (lane_id == 0))) {
+        switch (inst_opcode) {
+#define OP_DEF(OP, FUNC, STR, DST, CLASSIFICATION) \
+  case OP:                                         \
+    FUNC(pI, this);                                \
+    op_classification = CLASSIFICATION;            \
+    break;
+#define OP_W_DEF(OP, FUNC, STR, DST, CLASSIFICATION) \
+  case OP:                                           \
+    FUNC(pI, get_core(), inst);                      \
+    op_classification = CLASSIFICATION;              \
+    break;
+#include "opcodes.def"
+#undef OP_DEF
+#undef OP_W_DEF
+          default:
+            printf("Execution error: Invalid opcode (0x%x)\n",
+                   pI->get_opcode());
+            break;
+        }
       }
       delete pJ;
       pI = pI_saved;
 
-      // TODO schi
       m_gpu->gem5CudaGPU->getCudaCore(m_hw_sid)->record_inst(op_classification);
       // Run exit instruction if exit option included
-      if(pI->is_exit())
-         exit_impl(pI,this);
-   }
-
-
-
-   const gpgpu_functional_sim_config &config = m_gpu->get_config();
+      if (pI->is_exit()) exit_impl(pI, this);
+    }
 
-   // Output instruction information to file and stdout
-   if( config.get_ptx_inst_debug_to_file() != 0 &&
-        (config.get_ptx_inst_debug_thread_uid() == 0 || config.get_ptx_inst_debug_thread_uid() == get_uid()) ) {
-      fprintf(m_gpu->get_ptx_inst_debug_file(),
-             "[thd=%u] : (%s:%u - %s)\n",
-             get_uid(),
-             pI->source_file(), pI->source_line(), pI->get_source() );
-      //fprintf(ptx_inst_debug_file, "has memory read=%d, has memory write=%d\n", pI->has_memory_read(), pI->has_memory_write());
+    const gpgpu_functional_sim_config &config = m_gpu->get_config();
+
+    // Output instruction information to file and stdout
+    if (config.get_ptx_inst_debug_to_file() != 0 &&
+        (config.get_ptx_inst_debug_thread_uid() == 0 ||
+         config.get_ptx_inst_debug_thread_uid() == get_uid())) {
+      fprintf(m_gpu->get_ptx_inst_debug_file(), "[thd=%u] : (%s:%u - %s)\n",
+              get_uid(), pI->source_file(), pI->source_line(),
+              pI->get_source());
+      // fprintf(ptx_inst_debug_file, "has memory read=%d, has memory
+      // write=%d\n", pI->has_memory_read(), pI->has_memory_write());
       fflush(m_gpu->get_ptx_inst_debug_file());
-   }
+    }
 
-   if ( ptx_debug_exec_dump_cond<5>(get_uid(), pc) ) {
+    if (m_gpu->gpgpu_ctx->func_sim->ptx_debug_exec_dump_cond<5>(get_uid(),
+                                                                pc)) {
       dim3 ctaid = get_ctaid();
       dim3 tid = get_tid();
-      printf("%u [thd=%u][i=%u] : ctaid=(%u,%u,%u) tid=(%u,%u,%u) icount=%u [pc=%u] (%s:%u - %s)  [0x%llx]\n",
-             g_ptx_sim_num_insn,
-             get_uid(),
-             pI->uid(), ctaid.x,ctaid.y,ctaid.z,tid.x,tid.y,tid.z,
-             get_icount(),
-             pc, pI->source_file(), pI->source_line(), pI->get_source(),
-             m_last_set_operand_value.u64 );
+      printf(
+          "%u [thd=%u][i=%u] : ctaid=(%u,%u,%u) tid=(%u,%u,%u) icount=%u "
+          "[pc=%u] (%s:%u - %s)  [0x%llx]\n",
+          m_gpu->gpgpu_ctx->func_sim->g_ptx_sim_num_insn, get_uid(), pI->uid(),
+          ctaid.x, ctaid.y, ctaid.z, tid.x, tid.y, tid.z, get_icount(), pc,
+          pI->source_file(), pI->source_line(), pI->get_source(),
+          m_last_set_operand_value.u64);
       fflush(stdout);
-   }
+    }
 
-   addr_t insn_memaddr = 0xFEEBDAED;
-   memory_space_t insn_space = undefined_space;
-   _memory_op_t insn_memory_op = no_memory_op;
-   unsigned insn_data_size = 0;
-   if ( (pI->has_memory_read()  || pI->has_memory_write()) ) {
-      if(!((inst_opcode==MMA_LD_OP||inst_opcode==MMA_ST_OP)))
-      {
+    addr_t insn_memaddr = 0xFEEBDAED;
+    memory_space_t insn_space = undefined_space;
+    _memory_op_t insn_memory_op = no_memory_op;
+    unsigned insn_data_size = 0;
+    if ((pI->has_memory_read() || pI->has_memory_write())) {
+      if (!((inst_opcode == MMA_LD_OP || inst_opcode == MMA_ST_OP))) {
         insn_memaddr = last_eaddr();
         insn_space = last_space();
         unsigned to_type = pI->get_type();
         insn_data_size = datatype2size(to_type);
         insn_memory_op = pI->has_memory_read() ? memory_load : memory_store;
       }
-   }
+    }
 
-   if ( pI->get_opcode() == BAR_OP && pI->barrier_op() == RED_OPTION) {
-	   inst.add_callback( lane_id, last_callback().function, last_callback().instruction, this,false /*not atomic*/);
-   }
+    if (pI->get_opcode() == BAR_OP && pI->barrier_op() == RED_OPTION) {
+      inst.add_callback(lane_id, last_callback().function,
+                        last_callback().instruction, this,
+                        false /*not atomic*/);
+    }
 
-   if ( pI->get_opcode() == ATOM_OP ) {
+    if (pI->get_opcode() == ATOM_OP) {
       insn_memaddr = last_eaddr();
       insn_space = last_space();
-      inst.add_callback( lane_id, last_callback().function, last_callback().instruction, this,true /*atomic*/);
+      inst.add_callback(lane_id, last_callback().function,
+                        last_callback().instruction, this, true /*atomic*/);
       unsigned to_type = pI->get_type();
       insn_data_size = datatype2size(to_type);
-   }
+    }
 
-   if (pI->get_opcode() == TEX_OP) {
-      inst.set_addr(lane_id, last_eaddr() );
-      assert( inst.space == last_space() );
-      insn_data_size = get_tex_datasize(pI, this); // texture obtain its data granularity from the texture info
-   }
+    if (pI->get_opcode() == TEX_OP) {
+      inst.set_addr(lane_id, last_eaddr());
+      assert(inst.space == last_space());
+      insn_data_size = get_tex_datasize(
+          pI,
+          this);  // texture obtain its data granularity from the texture info
+    }
 
-   // Output register information to file and stdout
-   if( config.get_ptx_inst_debug_to_file()!=0 &&
-       (config.get_ptx_inst_debug_thread_uid()==0||config.get_ptx_inst_debug_thread_uid()==get_uid()) ) {
+    // Output register information to file and stdout
+    if (config.get_ptx_inst_debug_to_file() != 0 &&
+        (config.get_ptx_inst_debug_thread_uid() == 0 ||
+         config.get_ptx_inst_debug_thread_uid() == get_uid())) {
       dump_modifiedregs(m_gpu->get_ptx_inst_debug_file());
       dump_regs(m_gpu->get_ptx_inst_debug_file());
-   }
+    }
 
-   if ( g_debug_execution >= 6 ) {
-      if ( ptx_debug_exec_dump_cond<6>(get_uid(), pc) )
-         dump_modifiedregs(stdout);
-   }
-   if ( g_debug_execution >= 10 ) {
-      if ( ptx_debug_exec_dump_cond<10>(get_uid(), pc) )
-         dump_regs(stdout);
-   }
-   update_pc();
-   g_ptx_sim_num_insn++;
-
-   //not using it with functional simulation mode
-   if(!(this->m_functionalSimulationMode))
-       ptx_file_line_stats_add_exec_count(pI);
-
-   if ( gpgpu_ptx_instruction_classification ) {
-      init_inst_classification_stat();
-      unsigned space_type=0;
-      switch ( pI->get_space().get_type() ) {
-      case global_space: space_type = 10; break;
-      case local_space:  space_type = 11; break;
-      case tex_space:    space_type = 12; break;
-      case surf_space:   space_type = 13; break;
-      case param_space_kernel:
-      case param_space_local:
-                         space_type = 14; break;
-      case shared_space: space_type = 15; break;
-      case const_space:  space_type = 16; break;
-      default:
-         space_type = 0 ;
-         break;
+    if (g_debug_execution >= 6) {
+      if (m_gpu->gpgpu_ctx->func_sim->ptx_debug_exec_dump_cond<6>(get_uid(),
+                                                                  pc))
+        dump_modifiedregs(stdout);
+    }
+    if (g_debug_execution >= 10) {
+      if (m_gpu->gpgpu_ctx->func_sim->ptx_debug_exec_dump_cond<10>(get_uid(),
+                                                                   pc))
+        dump_regs(stdout);
+    }
+    update_pc();
+    m_gpu->gpgpu_ctx->func_sim->g_ptx_sim_num_insn++;
+
+    // not using it with functional simulation mode
+    if (!(this->m_functionalSimulationMode))
+      ptx_file_line_stats_add_exec_count(pI);
+
+    if (m_gpu->gpgpu_ctx->func_sim->gpgpu_ptx_instruction_classification) {
+      m_gpu->gpgpu_ctx->func_sim->init_inst_classification_stat();
+      unsigned space_type = 0;
+      switch (pI->get_space().get_type()) {
+        case global_space:
+          space_type = 10;
+          break;
+        case local_space:
+          space_type = 11;
+          break;
+        case tex_space:
+          space_type = 12;
+          break;
+        case surf_space:
+          space_type = 13;
+          break;
+        case param_space_kernel:
+        case param_space_local:
+          space_type = 14;
+          break;
+        case shared_space:
+          space_type = 15;
+          break;
+        case const_space:
+          space_type = 16;
+          break;
+        default:
+          space_type = 0;
+          break;
       }
-      StatAddSample( g_inst_classification_stat[g_ptx_kernel_count],  op_classification);
-      if (space_type) StatAddSample( g_inst_classification_stat[g_ptx_kernel_count], ( int )space_type);
-      StatAddSample( g_inst_op_classification_stat[g_ptx_kernel_count], (int)  pI->get_opcode() );
-   }
-   if ( (g_ptx_sim_num_insn % 100000) == 0 ) {
+      StatAddSample(m_gpu->gpgpu_ctx->func_sim->g_inst_classification_stat
+                        [m_gpu->gpgpu_ctx->func_sim->g_ptx_kernel_count],
+                    op_classification);
+      if (space_type)
+        StatAddSample(m_gpu->gpgpu_ctx->func_sim->g_inst_classification_stat
+                          [m_gpu->gpgpu_ctx->func_sim->g_ptx_kernel_count],
+                      (int)space_type);
+      StatAddSample(m_gpu->gpgpu_ctx->func_sim->g_inst_op_classification_stat
+                        [m_gpu->gpgpu_ctx->func_sim->g_ptx_kernel_count],
+                    (int)pI->get_opcode());
+    }
+    if ((m_gpu->gpgpu_ctx->func_sim->g_ptx_sim_num_insn % 100000) == 0) {
       dim3 ctaid = get_ctaid();
       dim3 tid = get_tid();
-      GPGPUSIM_DPRINTF(LIVENESS, "GPGPU-Sim PTX: %u instructions simulated : ctaid=(%u,%u,%u) tid=(%u,%u,%u)\n",
-             g_ptx_sim_num_insn, ctaid.x,ctaid.y,ctaid.z,tid.x,tid.y,tid.z );
+      GPGPUSIM_DPRINTF(LIVENESS,
+              "GPGPU-Sim PTX: %u instructions simulated : ctaid=(%u,%u,%u) "
+              "tid=(%u,%u,%u)\n",
+              m_gpu->gpgpu_ctx->func_sim->g_ptx_sim_num_insn, ctaid.x, ctaid.y,
+              ctaid.z, tid.x, tid.y, tid.z);
       fflush(stdout);
-   }
+    }
 
-   // "Return values"
-   if(!skip) {
-      if(!((inst_opcode==MMA_LD_OP||inst_opcode==MMA_ST_OP)))
-      {
-   	      inst.space = insn_space;
-          inst.set_addr(lane_id, insn_memaddr);
-          inst.data_size = insn_data_size; // simpleAtomicIntrinsics
-          assert( inst.memory_op == insn_memory_op );
-          if (insn_memory_op == memory_store && (insn_space == global_space || insn_space == const_space || insn_space == local_space)) {
-              // Need to save data to be written for stores
-              uint8_t data[MAX_DATA_BYTES_PER_INSN_PER_THREAD];
-              if ( pI->get_opcode() == ATOM_OP ) {
-                  unsigned data_src_reg = 2; // Use the second operand as data source
-                  readRegister(inst, lane_id, (char*)&data[0], data_src_reg);
-                  if (pI->get_atomic() == ATOMIC_CAS) {
-                      data_src_reg = 3; // Third operand is second data source
-                      // There can be at most 2 atomic inst operands of at most
-                      // 64b or 8B each. Store second operand at byte offet 8
-                      readRegister(inst, lane_id, (char*)&data[8], data_src_reg);
-                  }
-              } else {
-                  readRegister(inst, lane_id, (char*)&data[0]);
-              }
-              inst.set_data(lane_id, data);
-         }
+    // "Return values"
+    if (!skip) {
+      if (!((inst_opcode == MMA_LD_OP || inst_opcode == MMA_ST_OP))) {
+        inst.space = insn_space;
+        inst.set_addr(lane_id, insn_memaddr);
+        inst.data_size = insn_data_size;  // simpleAtomicIntrinsics
+        assert(inst.memory_op == insn_memory_op);
       }
-   }
-
-   } catch ( int x  ) {
-      printf("GPGPU-Sim PTX: ERROR (%d) executing intruction (%s:%u)\n", x, pI->source_file(), pI->source_line() );
-      printf("GPGPU-Sim PTX:       '%s'\n", pI->get_source() );
-      abort();
-   }
+    }
 
+  } catch (int x) {
+    printf("GPGPU-Sim PTX: ERROR (%d) executing intruction (%s:%u)\n", x,
+           pI->source_file(), pI->source_line());
+    printf("GPGPU-Sim PTX:       '%s'\n", pI->get_source());
+    abort();
+  }
 }
 
-void set_param_gpgpu_num_shaders(int num_shaders)
-{
-   gpgpu_param_num_shaders = num_shaders;
+void cuda_sim::set_param_gpgpu_num_shaders(int num_shaders) {
+  gpgpu_param_num_shaders = num_shaders;
 }
 
-const struct gpgpu_ptx_sim_info* ptx_sim_kernel_info(const function_info *kernel)
-{
-   return kernel->get_kernel_info();
+const struct gpgpu_ptx_sim_info *ptx_sim_kernel_info(
+    const function_info *kernel) {
+  return kernel->get_kernel_info();
 }
 
-const warp_inst_t *ptx_fetch_inst( address_type pc )
-{
-    return function_info::pc_to_instruction(pc);
+const warp_inst_t *gpgpu_context::ptx_fetch_inst(address_type pc) {
+  return pc_to_instruction(pc);
 }
 
-unsigned ptx_sim_init_thread( kernel_info_t &kernel,
-                              ptx_thread_info** thread_info,
-                              int sid,
-                              unsigned tid,
-                              unsigned threads_left,
-                              unsigned num_threads,
-                              core_t *core,
-                              unsigned hw_cta_id,
-                              unsigned hw_warp_id,
-                              gpgpu_t *gpu,
-                              bool isInFunctionalSimulationMode)
-{
-   std::list<ptx_thread_info *> &active_threads = kernel.active_threads();
-
-   static std::map<unsigned,memory_space*> shared_memory_lookup;
-   static std::map<unsigned,memory_space*> sstarr_memory_lookup;
-   static std::map<unsigned,ptx_cta_info*> ptx_cta_lookup;
-   static std::map<unsigned,ptx_warp_info*> ptx_warp_lookup;
-   static std::map<unsigned,std::map<unsigned,memory_space*> > local_memory_lookup;
-
-   if ( *thread_info != NULL ) {
-      ptx_thread_info *thd = *thread_info;
-      assert( thd->is_done() );
-      if ( g_debug_execution==-1 ) {
-         dim3 ctaid = thd->get_ctaid();
-         dim3 t = thd->get_tid();
-         printf("GPGPU-Sim PTX simulator:  thread exiting ctaid=(%u,%u,%u) tid=(%u,%u,%u) uid=%u\n",
-                ctaid.x,ctaid.y,ctaid.z,t.x,t.y,t.z, thd->get_uid() );
-         fflush(stdout);
-      }
-      thd->m_cta_info->register_deleted_thread(thd);
-      delete thd;
-      *thread_info = NULL;
-   }
-
-   if ( !active_threads.empty() ) {
-      assert( active_threads.size() <= threads_left );
-      ptx_thread_info *thd = active_threads.front();
-      active_threads.pop_front();
-      *thread_info = thd;
-      thd->init(gpu, core, sid, hw_cta_id, hw_warp_id, tid, isInFunctionalSimulationMode );
-      return 1;
-   }
-
-   if ( kernel.no_more_ctas_to_run() ) {
-      return 0; //finished!
-   }
-
-   if ( threads_left < kernel.threads_per_cta() ) {
-      return 0;
-   }
-
-   if ( g_debug_execution==-1 ) {
-      printf("GPGPU-Sim PTX simulator:  STARTING THREAD ALLOCATION --> \n");
-      fflush(stdout);
-   }
-
-   //initializing new CTA
-   ptx_cta_info *cta_info = NULL;
-   memory_space *shared_mem = NULL;
-   memory_space *sstarr_mem = NULL;
-
-   unsigned cta_size = kernel.threads_per_cta();
-   unsigned max_cta_per_sm = num_threads/cta_size; // e.g., 256 / 48 = 5
-   assert( max_cta_per_sm > 0 );
-
-   //unsigned sm_idx = (tid/cta_size)*gpgpu_param_num_shaders + sid;
-   unsigned sm_idx = hw_cta_id*gpgpu_param_num_shaders + sid;
-
-   if ( shared_memory_lookup.find(sm_idx) == shared_memory_lookup.end() ) {
-      if ( g_debug_execution >= 1 ) {
-         printf("  <CTA alloc> : sm_idx=%u sid=%u max_cta_per_sm=%u\n",
-                sm_idx, sid, max_cta_per_sm );
-      }
-      char buf[512];
-      snprintf(buf,512,"shared_%u", sid);
-      shared_mem = new memory_space_impl<16*1024>(buf,4);
-      shared_memory_lookup[sm_idx] = shared_mem;
-      snprintf(buf,512,"sstarr_%u", sid);
-      sstarr_mem = new memory_space_impl<16*1024>(buf,4);
-      sstarr_memory_lookup[sm_idx] = sstarr_mem;
-      cta_info = new ptx_cta_info(sm_idx);
-      ptx_cta_lookup[sm_idx] = cta_info;
-   } else {
-      if ( g_debug_execution >= 1 ) {
-         printf("  <CTA realloc> : sm_idx=%u sid=%u max_cta_per_sm=%u\n",
-                sm_idx, sid, max_cta_per_sm );
-      }
-      shared_mem = shared_memory_lookup[sm_idx];
-      sstarr_mem = sstarr_memory_lookup[sm_idx];
-      cta_info = ptx_cta_lookup[sm_idx];
-      cta_info->check_cta_thread_status_and_reset();
-   }
+unsigned ptx_sim_init_thread(kernel_info_t &kernel,
+                             ptx_thread_info **thread_info, int sid,
+                             unsigned tid, unsigned threads_left,
+                             unsigned num_threads, core_t *core,
+                             unsigned hw_cta_id, unsigned hw_warp_id,
+                             gpgpu_t *gpu, bool isInFunctionalSimulationMode) {
+  std::list<ptx_thread_info *> &active_threads = kernel.active_threads();
 
-   std::map<unsigned,memory_space*> &local_mem_lookup = local_memory_lookup[sid];
-   while( kernel.more_threads_in_cta() ) {
-      dim3 ctaid3d = kernel.get_next_cta_id();
-      unsigned new_tid = kernel.get_next_thread_id();
-      dim3 tid3d = kernel.get_next_thread_id_3d();
-      kernel.increment_thread_id();
-      new_tid += tid;
-      ptx_thread_info *thd = new ptx_thread_info(kernel);
-      ptx_warp_info *warp_info = NULL;
-      if ( ptx_warp_lookup.find(hw_warp_id) == ptx_warp_lookup.end() ) {
-    	  warp_info = new ptx_warp_info();
-    	  ptx_warp_lookup[hw_warp_id] = warp_info;
-      } else {
-    	  warp_info = ptx_warp_lookup[hw_warp_id];
-      }
-      thd->m_warp_info = warp_info;
+  static std::map<unsigned, memory_space *> shared_memory_lookup;
+  static std::map<unsigned, memory_space *> sstarr_memory_lookup;
+  static std::map<unsigned, ptx_cta_info *> ptx_cta_lookup;
+  static std::map<unsigned, ptx_warp_info *> ptx_warp_lookup;
+  static std::map<unsigned, std::map<unsigned, memory_space *> >
+      local_memory_lookup;
 
-      memory_space *local_mem = NULL;
-      std::map<unsigned,memory_space*>::iterator l = local_mem_lookup.find(new_tid);
-      if ( l != local_mem_lookup.end() ) {
-         local_mem = l->second;
-      } else {
-         char buf[512];
-         snprintf(buf,512,"local_%u_%u", sid, new_tid);
-         local_mem = new memory_space_impl<32>(buf,32);
-         local_mem_lookup[new_tid] = local_mem;
-      }
-      thd->set_info(kernel.entry());
-      thd->set_nctaid(kernel.get_grid_dim());
-      thd->set_ntid(kernel.get_cta_dim());
-      thd->set_ctaid(ctaid3d);
-      thd->set_tid(tid3d);
-      if( kernel.entry()->get_ptx_version().extensions() )
-         thd->cpy_tid_to_reg(tid3d);
-      thd->set_valid();
-      thd->m_shared_mem = shared_mem;
-      thd->m_sstarr_mem = sstarr_mem;
-      function_info *finfo = thd->func_info();
-      symbol_table *st = finfo->get_symtab();
-      thd->func_info()->param_to_shared(thd->m_shared_mem,st);
-      thd->func_info()->param_to_shared(thd->m_sstarr_mem,st);
-      thd->m_cta_info = cta_info;
-      cta_info->add_thread(thd);
-      thd->m_local_mem = local_mem;
-      if ( g_debug_execution==-1 ) {
-         printf("GPGPU-Sim PTX simulator:  allocating thread ctaid=(%u,%u,%u) tid=(%u,%u,%u) @ 0x%Lx\n",
-                ctaid3d.x,ctaid3d.y,ctaid3d.z,tid3d.x,tid3d.y,tid3d.z, (unsigned long long)thd );
-         fflush(stdout);
-      }
-      active_threads.push_back(thd);
-   }
-   if ( g_debug_execution==-1 ) {
-      printf("GPGPU-Sim PTX simulator:  <-- FINISHING THREAD ALLOCATION\n");
+  if (*thread_info != NULL) {
+    ptx_thread_info *thd = *thread_info;
+    assert(thd->is_done());
+    if (g_debug_execution == -1) {
+      dim3 ctaid = thd->get_ctaid();
+      dim3 t = thd->get_tid();
+      printf(
+          "GPGPU-Sim PTX simulator:  thread exiting ctaid=(%u,%u,%u) "
+          "tid=(%u,%u,%u) uid=%u\n",
+          ctaid.x, ctaid.y, ctaid.z, t.x, t.y, t.z, thd->get_uid());
       fflush(stdout);
-   }
-
-   kernel.increment_cta_id();
-
-   assert( active_threads.size() <= threads_left );
-   *thread_info = active_threads.front();
-   (*thread_info)->init(gpu, core, sid, hw_cta_id, hw_warp_id, tid,isInFunctionalSimulationMode );
-   active_threads.pop_front();
-   return 1;
-}
-
-
-#endif //#LIBCUDA
-
-size_t get_kernel_code_size( class function_info *entry )
-{
-   return entry->get_function_size();
-}
-
-/* FIXME schi disable opencl temperaly
-kernel_info_t *gpgpu_opencl_ptx_sim_init_grid(class function_info *entry,
-                                             gpgpu_ptx_sim_arg_list_t args,
-                                             struct dim3 gridDim,
-                                             struct dim3 blockDim,
-                                             gpgpu_t *gpu )
-{
-   kernel_info_t *result = new kernel_info_t(gridDim,blockDim,entry); // ,gpu->getNameArrayMapping(),gpu->getNameInfoMapping());
-   unsigned argcount=args.size();
-   unsigned argn=1;
-   for( gpgpu_ptx_sim_arg_list_t::iterator a = args.begin(); a != args.end(); a++ ) {
-      entry->add_param_data(argcount-argn,&(*a));
-      argn++;
-   }
-   entry->finalize(result->get_param_memory());
-   g_ptx_kernel_count++;
-   fflush(stdout);
-
-   return result;
-}
-*/
-
-#include "../version"
-#include "../detailed_version"
-
-void print_splash()
-{
-   static int splash_printed=0;
-   if ( !splash_printed ) {
-      unsigned build=0;
-      sscanf(g_gpgpusim_build_string, "$Change"": %u $", &build);
-      fprintf(stdout, "\n\n        *** %s [build %s] ***\n\n\n", g_gpgpusim_version_string, g_gpgpusim_build_string );
-      splash_printed=1;
-   }
-}
-
-std::map<const void*,std::string>   g_const_name_lookup; // indexed by hostVar
-std::map<const void*,std::string>   g_global_name_lookup; // indexed by hostVar
-std::set<std::string>   g_globals;
-std::set<std::string>   g_constants;
-
-void gpgpu_ptx_sim_register_const_variable(void *hostVar, const char *deviceName, size_t size )
-{
-   printf("GPGPU-Sim PTX registering constant %s (%zu bytes) to name mapping\n", deviceName, size );
-   g_const_name_lookup[hostVar] = deviceName;
-}
-
-void gpgpu_ptx_sim_register_global_variable(void *hostVar, const char *deviceName, size_t size )
-{
-   printf("GPGPU-Sim PTX registering global %s hostVar to name mapping\n", deviceName );
-   g_global_name_lookup[hostVar] = deviceName;
-}
-
-std::string gpgpu_ptx_sim_hostvar_to_sym_name(const char *hostVar) {
-    // *Note: This code is largely copied from gpgpu_ptx_sim_memcpy_symbol
-    // to be used with gem5 memory system
-    printf("GPGPU-Sim PTX: starting gpgpu_ptx_sim_memcpy_symbol with hostVar 0x%p\n", hostVar);
-    bool found_sym = false;
-    std::string sym_name;
-
-    std::map<const void*,std::string>::iterator c=g_const_name_lookup.find(hostVar);
-    if ( c!=g_const_name_lookup.end() ) {
-       found_sym = true;
-       sym_name = c->second;
     }
-    std::map<const void*,std::string>::iterator g=g_global_name_lookup.find(hostVar);
-    if ( g!=g_global_name_lookup.end() ) {
-       if ( found_sym ) {
-          printf("Execution error: PTX symbol \"%s\" w/ hostVar=0x%Lx is declared both const and global?\n",
-                 sym_name.c_str(), (unsigned long long)hostVar );
-          abort();
-       }
-       found_sym = true;
-       sym_name = g->second;
+    thd->m_cta_info->register_deleted_thread(thd);
+    delete thd;
+    *thread_info = NULL;
+  }
+
+  if (!active_threads.empty()) {
+    assert(active_threads.size() <= threads_left);
+    ptx_thread_info *thd = active_threads.front();
+    active_threads.pop_front();
+    *thread_info = thd;
+    thd->init(gpu, core, sid, hw_cta_id, hw_warp_id, tid,
+              isInFunctionalSimulationMode);
+    return 1;
+  }
+
+  if (kernel.no_more_ctas_to_run()) {
+    return 0;  // finished!
+  }
+
+  if (threads_left < kernel.threads_per_cta()) {
+    return 0;
+  }
+
+  if (g_debug_execution == -1) {
+    printf("GPGPU-Sim PTX simulator:  STARTING THREAD ALLOCATION --> \n");
+    fflush(stdout);
+  }
+
+  // initializing new CTA
+  ptx_cta_info *cta_info = NULL;
+  memory_space *shared_mem = NULL;
+  memory_space *sstarr_mem = NULL;
+
+  unsigned cta_size = kernel.threads_per_cta();
+  unsigned max_cta_per_sm = num_threads / cta_size;  // e.g., 256 / 48 = 5
+  assert(max_cta_per_sm > 0);
+
+  // unsigned sm_idx = (tid/cta_size)*gpgpu_param_num_shaders + sid;
+  unsigned sm_idx =
+      hw_cta_id * gpu->gpgpu_ctx->func_sim->gpgpu_param_num_shaders + sid;
+
+  if (shared_memory_lookup.find(sm_idx) == shared_memory_lookup.end()) {
+    if (g_debug_execution >= 1) {
+      printf("  <CTA alloc> : sm_idx=%u sid=%u max_cta_per_sm=%u\n", sm_idx,
+             sid, max_cta_per_sm);
     }
-    if ( g_globals.find(hostVar) != g_globals.end() ) {
-       found_sym = true;
-       sym_name = hostVar;
+    char buf[512];
+    snprintf(buf, 512, "shared_%u", sid);
+    shared_mem = new memory_space_impl<16 * 1024>(buf, 4);
+    shared_memory_lookup[sm_idx] = shared_mem;
+    snprintf(buf, 512, "sstarr_%u", sid);
+    sstarr_mem = new memory_space_impl<16 * 1024>(buf, 4);
+    sstarr_memory_lookup[sm_idx] = sstarr_mem;
+    cta_info = new ptx_cta_info(sm_idx, gpu->gpgpu_ctx);
+    ptx_cta_lookup[sm_idx] = cta_info;
+  } else {
+    if (g_debug_execution >= 1) {
+      printf("  <CTA realloc> : sm_idx=%u sid=%u max_cta_per_sm=%u\n", sm_idx,
+             sid, max_cta_per_sm);
     }
-    if ( g_constants.find(hostVar) != g_constants.end() ) {
-       found_sym = true;
-       sym_name = hostVar;
+    shared_mem = shared_memory_lookup[sm_idx];
+    sstarr_mem = sstarr_memory_lookup[sm_idx];
+    cta_info = ptx_cta_lookup[sm_idx];
+    cta_info->check_cta_thread_status_and_reset();
+  }
+
+  std::map<unsigned, memory_space *> &local_mem_lookup =
+      local_memory_lookup[sid];
+  while (kernel.more_threads_in_cta()) {
+    dim3 ctaid3d = kernel.get_next_cta_id();
+    unsigned new_tid = kernel.get_next_thread_id();
+    dim3 tid3d = kernel.get_next_thread_id_3d();
+    kernel.increment_thread_id();
+    new_tid += tid;
+    ptx_thread_info *thd = new ptx_thread_info(kernel);
+    ptx_warp_info *warp_info = NULL;
+    if (ptx_warp_lookup.find(hw_warp_id) == ptx_warp_lookup.end()) {
+      warp_info = new ptx_warp_info();
+      ptx_warp_lookup[hw_warp_id] = warp_info;
+    } else {
+      warp_info = ptx_warp_lookup[hw_warp_id];
     }
+    thd->m_warp_info = warp_info;
 
-    if ( !found_sym ) {
-       printf("Execution error: No information for PTX symbol w/ hostVar=0x%Lx\n", (unsigned long long)hostVar );
-       abort();
-    } else printf("GPGPU-Sim PTX: gpgpu_ptx_sim_memcpy_symbol: Found PTX symbol w/ hostVar=0x%Lx\n", (unsigned long long)hostVar );
-    return sym_name;
-}
-
-void gpgpu_ptx_sim_memcpy_symbol(const char *hostVar, const void *src, size_t count, size_t offset, int to, gpgpu_t *gpu )
-{
-   printf("GPGPU-Sim PTX: starting gpgpu_ptx_sim_memcpy_symbol with hostVar 0x%p\n", hostVar);
-   bool found_sym = false;
-   memory_space_t mem_region = undefined_space;
-   std::string sym_name;
-
-   std::map<const void*,std::string>::iterator c=g_const_name_lookup.find(hostVar);
-   if ( c!=g_const_name_lookup.end() ) {
-      found_sym = true;
-      sym_name = c->second;
-      mem_region = const_space;
-   }
-   std::map<const void*,std::string>::iterator g=g_global_name_lookup.find(hostVar);
-   if ( g!=g_global_name_lookup.end() ) {
-      if ( found_sym ) {
-         printf("Execution error: PTX symbol \"%s\" w/ hostVar=0x%Lx is declared both const and global?\n",
-                sym_name.c_str(), (unsigned long long)hostVar );
-         abort();
-      }
-      found_sym = true;
-      sym_name = g->second;
-      mem_region = global_space;
-   }
-   if( g_globals.find(hostVar) != g_globals.end() ) {
-      found_sym = true;
-      sym_name = hostVar;
-      mem_region = global_space;
-   }
-   if( g_constants.find(hostVar) != g_constants.end() ) {
-      found_sym = true;
-      sym_name = hostVar;
-      mem_region = const_space;
-   }
-
-   if ( !found_sym ) {
-      printf("Execution error: No information for PTX symbol w/ hostVar=0x%Lx\n", (unsigned long long)hostVar );
+    memory_space *local_mem = NULL;
+    std::map<unsigned, memory_space *>::iterator l =
+        local_mem_lookup.find(new_tid);
+    if (l != local_mem_lookup.end()) {
+      local_mem = l->second;
+    } else {
+      char buf[512];
+      snprintf(buf, 512, "local_%u_%u", sid, new_tid);
+      local_mem = new memory_space_impl<32>(buf, 32);
+      local_mem_lookup[new_tid] = local_mem;
+    }
+    thd->set_info(kernel.entry());
+    thd->set_nctaid(kernel.get_grid_dim());
+    thd->set_ntid(kernel.get_cta_dim());
+    thd->set_ctaid(ctaid3d);
+    thd->set_tid(tid3d);
+    if (kernel.entry()->get_ptx_version().extensions())
+      thd->cpy_tid_to_reg(tid3d);
+    thd->set_valid();
+    thd->m_shared_mem = shared_mem;
+    thd->m_sstarr_mem = sstarr_mem;
+    function_info *finfo = thd->func_info();
+    symbol_table *st = finfo->get_symtab();
+    thd->func_info()->param_to_shared(thd->m_shared_mem, st);
+    thd->func_info()->param_to_shared(thd->m_sstarr_mem, st);
+    thd->m_cta_info = cta_info;
+    cta_info->add_thread(thd);
+    thd->m_local_mem = local_mem;
+    if (g_debug_execution == -1) {
+      printf(
+          "GPGPU-Sim PTX simulator:  allocating thread ctaid=(%u,%u,%u) "
+          "tid=(%u,%u,%u) @ 0x%Lx\n",
+          ctaid3d.x, ctaid3d.y, ctaid3d.z, tid3d.x, tid3d.y, tid3d.z,
+          (unsigned long long)thd);
+      fflush(stdout);
+    }
+    active_threads.push_back(thd);
+  }
+  if (g_debug_execution == -1) {
+    printf("GPGPU-Sim PTX simulator:  <-- FINISHING THREAD ALLOCATION\n");
+    fflush(stdout);
+  }
+
+  kernel.increment_cta_id();
+
+  assert(active_threads.size() <= threads_left);
+  *thread_info = active_threads.front();
+  (*thread_info)
+      ->init(gpu, core, sid, hw_cta_id, hw_warp_id, tid,
+             isInFunctionalSimulationMode);
+  active_threads.pop_front();
+  return 1;
+}
+
+size_t get_kernel_code_size(class function_info *entry) {
+  return entry->get_function_size();
+}
+
+kernel_info_t *cuda_sim::gpgpu_opencl_ptx_sim_init_grid(
+    class function_info *entry, gpgpu_ptx_sim_arg_list_t args,
+    struct dim3 gridDim, struct dim3 blockDim, gpgpu_t *gpu) {
+  kernel_info_t *result =
+      new kernel_info_t(gridDim, blockDim, entry, gpu->getNameArrayMapping(),
+                        gpu->getNameInfoMapping());
+  unsigned argcount = args.size();
+  unsigned argn = 1;
+  for (gpgpu_ptx_sim_arg_list_t::iterator a = args.begin(); a != args.end();
+       a++) {
+    entry->add_param_data(argcount - argn, &(*a));
+    argn++;
+  }
+  entry->finalize(result->get_param_memory());
+  g_ptx_kernel_count++;
+  fflush(stdout);
+
+  return result;
+}
+
+#include "../../version"
+#include "detailed_version"
+
+void print_splash() {
+  static int splash_printed = 0;
+  if (!splash_printed) {
+    fprintf(stdout, "\n\n        *** %s [build %s] ***\n\n\n",
+            g_gpgpusim_version_string, g_gpgpusim_build_string);
+    splash_printed = 1;
+  }
+}
+
+void cuda_sim::gpgpu_ptx_sim_register_const_variable(void *hostVar,
+                                                     const char *deviceName,
+                                                     size_t size) {
+  printf("GPGPU-Sim PTX registering constant %s (%zu bytes) to name mapping\n",
+         deviceName, size);
+  g_const_name_lookup[hostVar] = deviceName;
+}
+
+void cuda_sim::gpgpu_ptx_sim_register_global_variable(void *hostVar,
+                                                      const char *deviceName,
+                                                      size_t size) {
+  printf("GPGPU-Sim PTX registering global %s hostVar to name mapping\n",
+         deviceName);
+  g_global_name_lookup[hostVar] = deviceName;
+}
+
+unsigned cuda_sim::gpgpu_ptx_hostvar_to_sym_address(const char *hostVar, gpgpu_t *gpu) {
+  printf(
+      "GPGPU-Sim PTX: starting gpgpu_ptx_sim_memcpy_symbol with hostVar 0x%p\n",
+      hostVar);
+  bool found_sym = false;
+  memory_space_t mem_region = undefined_space;
+  std::string sym_name;
+
+  std::map<const void *, std::string>::iterator c =
+      gpu->gpgpu_ctx->func_sim->g_const_name_lookup.find(hostVar);
+  if (c != gpu->gpgpu_ctx->func_sim->g_const_name_lookup.end()) {
+    found_sym = true;
+    sym_name = c->second;
+    mem_region = const_space;
+  }
+  std::map<const void *, std::string>::iterator g =
+      gpu->gpgpu_ctx->func_sim->g_global_name_lookup.find(hostVar);
+  if (g != gpu->gpgpu_ctx->func_sim->g_global_name_lookup.end()) {
+    if (found_sym) {
+      printf(
+          "Execution error: PTX symbol \"%s\" w/ hostVar=0x%Lx is declared "
+          "both const and global?\n",
+          sym_name.c_str(), (unsigned long long)hostVar);
+      abort();
+    }
+    found_sym = true;
+    sym_name = g->second;
+    mem_region = global_space;
+  }
+  if (g_globals.find(hostVar) != g_globals.end()) {
+    found_sym = true;
+    sym_name = hostVar;
+    mem_region = global_space;
+  }
+  if (g_constants.find(hostVar) != g_constants.end()) {
+    found_sym = true;
+    sym_name = hostVar;
+    mem_region = const_space;
+  }
+
+  if (!found_sym) {
+    printf("Execution error: No information for PTX symbol w/ hostVar=0x%Lx\n",
+           (unsigned long long)hostVar);
+    abort();
+  } else
+    printf(
+        "GPGPU-Sim PTX: gpgpu_ptx_sim_memcpy_symbol: Found PTX symbol w/ "
+        "hostVar=0x%Lx\n",
+        (unsigned long long)hostVar);
+  const char *mem_name = NULL;
+  memory_space *mem = NULL;
+
+  std::map<std::string, symbol_table *>::iterator st =
+      gpgpu_ctx->ptx_parser->g_sym_name_to_symbol_table.find(sym_name.c_str());
+  assert(st != gpgpu_ctx->ptx_parser->g_sym_name_to_symbol_table.end());
+  symbol_table *symtab = st->second;
+
+  symbol *sym = symtab->lookup(sym_name.c_str());
+  assert(sym);
+  unsigned dst = sym->get_address();
+  return dst;
+}
+
+void cuda_sim::gpgpu_ptx_sim_memcpy_symbol(const char *hostVar, const void *src,
+                                           size_t count, size_t offset, int to,
+                                           gpgpu_t *gpu) {
+  printf(
+      "GPGPU-Sim PTX: starting gpgpu_ptx_sim_memcpy_symbol with hostVar 0x%p\n",
+      hostVar);
+  bool found_sym = false;
+  memory_space_t mem_region = undefined_space;
+  std::string sym_name;
+
+  std::map<const void *, std::string>::iterator c =
+      gpu->gpgpu_ctx->func_sim->g_const_name_lookup.find(hostVar);
+  if (c != gpu->gpgpu_ctx->func_sim->g_const_name_lookup.end()) {
+    found_sym = true;
+    sym_name = c->second;
+    mem_region = const_space;
+  }
+  std::map<const void *, std::string>::iterator g =
+      gpu->gpgpu_ctx->func_sim->g_global_name_lookup.find(hostVar);
+  if (g != gpu->gpgpu_ctx->func_sim->g_global_name_lookup.end()) {
+    if (found_sym) {
+      printf(
+          "Execution error: PTX symbol \"%s\" w/ hostVar=0x%Lx is declared "
+          "both const and global?\n",
+          sym_name.c_str(), (unsigned long long)hostVar);
       abort();
-   } else printf("GPGPU-Sim PTX: gpgpu_ptx_sim_memcpy_symbol: Found PTX symbol w/ hostVar=0x%Lx\n", (unsigned long long)hostVar );
-   const char *mem_name = NULL;
-   memory_space *mem = NULL;
-
-   std::map<std::string,symbol_table*>::iterator st = g_sym_name_to_symbol_table.find(sym_name.c_str());
-   assert( st != g_sym_name_to_symbol_table.end() );
-   symbol_table *symtab = st->second;
-
-   symbol *sym = symtab->lookup(sym_name.c_str());
-   assert(sym);
-   unsigned dst = sym->get_address() + offset;
-   switch (mem_region.get_type()) {
-   case const_space:
+    }
+    found_sym = true;
+    sym_name = g->second;
+    mem_region = global_space;
+  }
+  if (g_globals.find(hostVar) != g_globals.end()) {
+    found_sym = true;
+    sym_name = hostVar;
+    mem_region = global_space;
+  }
+  if (g_constants.find(hostVar) != g_constants.end()) {
+    found_sym = true;
+    sym_name = hostVar;
+    mem_region = const_space;
+  }
+
+  if (!found_sym) {
+    printf("Execution error: No information for PTX symbol w/ hostVar=0x%Lx\n",
+           (unsigned long long)hostVar);
+    abort();
+  } else
+    printf(
+        "GPGPU-Sim PTX: gpgpu_ptx_sim_memcpy_symbol: Found PTX symbol w/ "
+        "hostVar=0x%Lx\n",
+        (unsigned long long)hostVar);
+  const char *mem_name = NULL;
+  memory_space *mem = NULL;
+
+  std::map<std::string, symbol_table *>::iterator st =
+      gpgpu_ctx->ptx_parser->g_sym_name_to_symbol_table.find(sym_name.c_str());
+  assert(st != gpgpu_ctx->ptx_parser->g_sym_name_to_symbol_table.end());
+  symbol_table *symtab = st->second;
+
+  symbol *sym = symtab->lookup(sym_name.c_str());
+  assert(sym);
+  unsigned dst = sym->get_address() + offset;
+  switch (mem_region.get_type()) {
+    case const_space:
       mem = gpu->get_global_memory();
       mem_name = "const";
       break;
-   case global_space:
+    case global_space:
       mem = gpu->get_global_memory();
       mem_name = "global";
       break;
-   default:
+    default:
       abort();
-   }
-   printf("GPGPU-Sim PTX: gpgpu_ptx_sim_memcpy_symbol: copying %s memory %zu bytes %s symbol %s+%zu @0x%x ...\n",
-          mem_name, count, (to?" to ":"from"), sym_name.c_str(), offset, dst );
-   for ( unsigned n=0; n < count; n++ ) {
-      if( to ) mem->write(dst+n,1,((char*)src)+n,NULL,NULL);
-      else mem->read(dst+n,1,((char*)src)+n);
-   }
-   fflush(stdout);
+  }
+  printf(
+      "GPGPU-Sim PTX: gpgpu_ptx_sim_memcpy_symbol: copying %s memory %zu bytes "
+      "%s symbol %s+%zu @0x%x ...\n",
+      mem_name, count, (to ? " to " : "from"), sym_name.c_str(), offset, dst);
+  for (unsigned n = 0; n < count; n++) {
+    if (to)
+      mem->write(dst + n, 1, ((char *)src) + n, NULL, NULL);
+    else
+      mem->read(dst + n, 1, ((char *)src) + n);
+  }
+  fflush(stdout);
 }
 
-int g_ptx_sim_mode; // if non-zero run functional simulation only (i.e., no notion of a clock cycle)
-
 extern int ptx_debug;
 
-bool g_cuda_launch_blocking = false;
-
-void read_sim_environment_variables()
-{
-   ptx_debug = 0;
-   g_debug_execution = 3;
-   g_interactive_debugger_enabled = false;
-
-   char *mode = getenv("PTX_SIM_MODE_FUNC");
-   if ( mode )
-      sscanf(mode,"%u", &g_ptx_sim_mode);
-   printf("GPGPU-Sim PTX: simulation mode %d (can change with PTX_SIM_MODE_FUNC environment variable:\n", g_ptx_sim_mode);
-   printf("               1=functional simulation only, 0=detailed performance simulator)\n");
-   char *dbg_inter = getenv("GPGPUSIM_DEBUG");
-   if ( dbg_inter && strlen(dbg_inter) ) {
-      printf("GPGPU-Sim PTX: enabling interactive debugger\n");
-      fflush(stdout);
-      g_interactive_debugger_enabled = true;
-   }
-   char *dbg_level = getenv("PTX_SIM_DEBUG");
-   if ( dbg_level && strlen(dbg_level) ) {
-      printf("GPGPU-Sim PTX: setting debug level to %s\n", dbg_level );
-      fflush(stdout);
-      sscanf(dbg_level,"%d", &g_debug_execution);
-   }
-   char *dbg_thread = getenv("PTX_SIM_DEBUG_THREAD_UID");
-   if ( dbg_thread && strlen(dbg_thread) ) {
-      printf("GPGPU-Sim PTX: printing debug information for thread uid %s\n", dbg_thread );
-      fflush(stdout);
-      sscanf(dbg_thread,"%d", &g_debug_thread_uid);
-   }
-   char *dbg_pc = getenv("PTX_SIM_DEBUG_PC");
-   if ( dbg_pc && strlen(dbg_pc) ) {
-      printf("GPGPU-Sim PTX: printing debug information for instruction with PC = %s\n", dbg_pc );
-      fflush(stdout);
-      sscanf(dbg_pc,"%d", &g_debug_pc);
-   }
+void cuda_sim::read_sim_environment_variables() {
+  ptx_debug = 0;
+  g_debug_execution = 0;
+  g_interactive_debugger_enabled = false;
+
+  char *mode = getenv("PTX_SIM_MODE_FUNC");
+  if (mode) sscanf(mode, "%u", &g_ptx_sim_mode);
+  printf(
+      "GPGPU-Sim PTX: simulation mode %d (can change with PTX_SIM_MODE_FUNC "
+      "environment variable:\n",
+      g_ptx_sim_mode);
+  printf(
+      "               1=functional simulation only, 0=detailed performance "
+      "simulator)\n");
+  char *dbg_inter = getenv("GPGPUSIM_DEBUG");
+  if (dbg_inter && strlen(dbg_inter)) {
+    printf("GPGPU-Sim PTX: enabling interactive debugger\n");
+    fflush(stdout);
+    g_interactive_debugger_enabled = true;
+  }
+  char *dbg_level = getenv("PTX_SIM_DEBUG");
+  if (dbg_level && strlen(dbg_level)) {
+    printf("GPGPU-Sim PTX: setting debug level to %s\n", dbg_level);
+    fflush(stdout);
+    sscanf(dbg_level, "%d", &g_debug_execution);
+  }
+  char *dbg_thread = getenv("PTX_SIM_DEBUG_THREAD_UID");
+  if (dbg_thread && strlen(dbg_thread)) {
+    printf("GPGPU-Sim PTX: printing debug information for thread uid %s\n",
+           dbg_thread);
+    fflush(stdout);
+    sscanf(dbg_thread, "%d", &g_debug_thread_uid);
+  }
+  char *dbg_pc = getenv("PTX_SIM_DEBUG_PC");
+  if (dbg_pc && strlen(dbg_pc)) {
+    printf(
+        "GPGPU-Sim PTX: printing debug information for instruction with PC = "
+        "%s\n",
+        dbg_pc);
+    fflush(stdout);
+    sscanf(dbg_pc, "%d", &g_debug_pc);
+  }
 
 #if CUDART_VERSION > 1010
-    g_override_embedded_ptx = false;
-    char *usefile = getenv("PTX_SIM_USE_PTX_FILE");
-    if (usefile && strlen(usefile)) {
-        printf("GPGPU-Sim PTX: overriding embedded ptx with ptx file (PTX_SIM_USE_PTX_FILE is set)\n");
-        fflush(stdout);
-        g_override_embedded_ptx = true;
-    }
-    char *blocking = getenv("CUDA_LAUNCH_BLOCKING");
-    if( blocking && !strcmp(blocking,"1") ) {
-        g_cuda_launch_blocking = true;
-    }
+  g_override_embedded_ptx = false;
+  char *usefile = getenv("PTX_SIM_USE_PTX_FILE");
+  if (usefile && strlen(usefile)) {
+    printf(
+        "GPGPU-Sim PTX: overriding embedded ptx with ptx file "
+        "(PTX_SIM_USE_PTX_FILE is set)\n");
+    fflush(stdout);
+    g_override_embedded_ptx = true;
+  }
+  char *blocking = getenv("CUDA_LAUNCH_BLOCKING");
+  if (blocking && !strcmp(blocking, "1")) {
+    g_cuda_launch_blocking = true;
+  }
 #else
-   g_cuda_launch_blocking = true;
-   g_override_embedded_ptx = true;
+  g_cuda_launch_blocking = true;
+  g_override_embedded_ptx = true;
 #endif
 
-   if ( g_debug_execution >= 40 ) {
-      ptx_debug = 1;
-   }
-}
-
-ptx_cta_info *g_func_cta_info = NULL;
-
-#define MAX(a,b) (((a)>(b))?(a):(b))
-
-unsigned max_cta (const struct gpgpu_ptx_sim_info *kernel_info, unsigned threads_per_cta, unsigned int warp_size, unsigned int n_thread_per_shader, unsigned int gpgpu_shmem_size, unsigned int gpgpu_shader_registers, unsigned int max_cta_per_core)
-{
-
-    unsigned int padded_cta_size = threads_per_cta;
-    if (padded_cta_size%warp_size)
-        padded_cta_size = ((padded_cta_size/warp_size)+1)*(warp_size);
-     unsigned int result_thread = n_thread_per_shader / padded_cta_size;
-
-     unsigned int result_shmem = (unsigned)-1;
-     if (kernel_info->smem > 0)
-        result_shmem = gpgpu_shmem_size / kernel_info->smem;
-     unsigned int result_regs = (unsigned)-1;
-     if (kernel_info->regs > 0)
-        result_regs = gpgpu_shader_registers / (padded_cta_size * ((kernel_info->regs+3)&~3));
-     printf("padded cta size is %d and %d and %d",padded_cta_size, kernel_info->regs, ((kernel_info->regs+3)&~3) );
-     //Limit by CTA
-     unsigned int result_cta = max_cta_per_core;
-
-     unsigned result = result_thread;
-     result = gs_min2(result, result_shmem);
-     result = gs_min2(result, result_regs);
-     result = gs_min2(result, result_cta);
-
-     printf ("GPGPU-Sim uArch: CTA/core = %u, limited by:", result);
-     if (result == result_thread) printf (" threads");
-     if (result == result_shmem) printf (" shmem");
-     if (result == result_regs) printf (" regs");
-     if (result == result_cta) printf (" cta_limit");
-     printf ("\n");
-
-     return result;
+  if (g_debug_execution >= 40) {
+    ptx_debug = 1;
+  }
+}
+
+#define MAX(a, b) (((a) > (b)) ? (a) : (b))
+
+unsigned max_cta(const struct gpgpu_ptx_sim_info *kernel_info,
+                 unsigned threads_per_cta, unsigned int warp_size,
+                 unsigned int n_thread_per_shader,
+                 unsigned int gpgpu_shmem_size,
+                 unsigned int gpgpu_shader_registers,
+                 unsigned int max_cta_per_core) {
+  unsigned int padded_cta_size = threads_per_cta;
+  if (padded_cta_size % warp_size)
+    padded_cta_size = ((padded_cta_size / warp_size) + 1) * (warp_size);
+  unsigned int result_thread = n_thread_per_shader / padded_cta_size;
+
+  unsigned int result_shmem = (unsigned)-1;
+  if (kernel_info->smem > 0)
+    result_shmem = gpgpu_shmem_size / kernel_info->smem;
+  unsigned int result_regs = (unsigned)-1;
+  if (kernel_info->regs > 0)
+    result_regs = gpgpu_shader_registers /
+                  (padded_cta_size * ((kernel_info->regs + 3) & ~3));
+  printf("padded cta size is %d and %d and %d", padded_cta_size,
+         kernel_info->regs, ((kernel_info->regs + 3) & ~3));
+  // Limit by CTA
+  unsigned int result_cta = max_cta_per_core;
+
+  unsigned result = result_thread;
+  result = gs_min2(result, result_shmem);
+  result = gs_min2(result, result_regs);
+  result = gs_min2(result, result_cta);
+
+  printf("GPGPU-Sim uArch: CTA/core = %u, limited by:", result);
+  if (result == result_thread) printf(" threads");
+  if (result == result_shmem) printf(" shmem");
+  if (result == result_regs) printf(" regs");
+  if (result == result_cta) printf(" cta_limit");
+  printf("\n");
+
+  return result;
 }
-
-#ifndef LIBCUDA
 /*!
-This function simulates the CUDA code functionally, it takes a kernel_info_t parameter
-which holds the data for the CUDA kernel to be executed
+This function simulates the CUDA code functionally, it takes a kernel_info_t
+parameter which holds the data for the CUDA kernel to be executed
 !*/
-void gpgpu_cuda_ptx_sim_main_func( kernel_info_t &kernel, bool openCL )
-{
-     printf("GPGPU-Sim: Performing Functional Simulation, executing kernel %s...\n",kernel.name().c_str());
-
-     //using a shader core object for book keeping, it is not needed but as most function built for performance simulation need it we use it here
-    extern gpgpu_sim *g_the_gpu;
-    //before we execute, we should do PDOM analysis for functional simulation scenario.
-    function_info *kernel_func_info = kernel.entry();
-    const struct gpgpu_ptx_sim_info *kernel_info = ptx_sim_kernel_info(kernel_func_info);
-    checkpoint *g_checkpoint;
-    g_checkpoint = new checkpoint();
-
-    if (kernel_func_info->is_pdom_set()) {
-    	printf("GPGPU-Sim PTX: PDOM analysis already done for %s \n", kernel.name().c_str() );
+void cuda_sim::gpgpu_cuda_ptx_sim_main_func(kernel_info_t &kernel,
+                                            bool openCL) {
+  printf(
+      "GPGPU-Sim: Performing Functional Simulation, executing kernel %s...\n",
+      kernel.name().c_str());
+
+  // using a shader core object for book keeping, it is not needed but as most
+  // function built for performance simulation need it we use it here
+  // extern gpgpu_sim *g_the_gpu;
+  // before we execute, we should do PDOM analysis for functional simulation
+  // scenario.
+  function_info *kernel_func_info = kernel.entry();
+  const struct gpgpu_ptx_sim_info *kernel_info =
+      ptx_sim_kernel_info(kernel_func_info);
+  checkpoint *g_checkpoint;
+  g_checkpoint = new checkpoint();
+
+  if (kernel_func_info->is_pdom_set()) {
+    printf("GPGPU-Sim PTX: PDOM analysis already done for %s \n",
+           kernel.name().c_str());
+  } else {
+    printf("GPGPU-Sim PTX: finding reconvergence points for \'%s\'...\n",
+           kernel.name().c_str());
+    kernel_func_info->do_pdom();
+    kernel_func_info->set_pdom();
+  }
+
+  unsigned max_cta_tot = max_cta(
+      kernel_info, kernel.threads_per_cta(),
+      gpgpu_ctx->the_gpgpusim->g_the_gpu->getShaderCoreConfig()->warp_size,
+      gpgpu_ctx->the_gpgpusim->g_the_gpu->getShaderCoreConfig()
+          ->n_thread_per_shader,
+      gpgpu_ctx->the_gpgpusim->g_the_gpu->getShaderCoreConfig()
+          ->gpgpu_shmem_size,
+      gpgpu_ctx->the_gpgpusim->g_the_gpu->getShaderCoreConfig()
+          ->gpgpu_shader_registers,
+      gpgpu_ctx->the_gpgpusim->g_the_gpu->getShaderCoreConfig()
+          ->max_cta_per_core);
+  printf("Max CTA : %d\n", max_cta_tot);
+
+  int cp_op = gpgpu_ctx->the_gpgpusim->g_the_gpu->checkpoint_option;
+  int cp_kernel = gpgpu_ctx->the_gpgpusim->g_the_gpu->checkpoint_kernel;
+  cp_count = gpgpu_ctx->the_gpgpusim->g_the_gpu->checkpoint_insn_Y;
+  cp_cta_resume = gpgpu_ctx->the_gpgpusim->g_the_gpu->checkpoint_CTA_t;
+  int cta_launched = 0;
+
+  // we excute the kernel one CTA (Block) at the time, as synchronization
+  // functions work block wise
+  while (!kernel.no_more_ctas_to_run()) {
+    unsigned temp = kernel.get_next_cta_id_single();
+
+    if (cp_op == 0 ||
+        (cp_op == 1 && cta_launched < cp_cta_resume &&
+         kernel.get_uid() == cp_kernel) ||
+        kernel.get_uid() < cp_kernel)  // just fro testing
+    {
+      functionalCoreSim cta(
+          &kernel, gpgpu_ctx->the_gpgpusim->g_the_gpu,
+          gpgpu_ctx->the_gpgpusim->g_the_gpu->getShaderCoreConfig()->warp_size);
+      cta.execute(cp_count, temp);
+
+#if (CUDART_VERSION >= 5000)
+      gpgpu_ctx->device_runtime->launch_all_device_kernels();
+#endif
     } else {
-	 printf("GPGPU-Sim PTX: finding reconvergence points for \'%s\'...\n", kernel.name().c_str() );
-	 kernel_func_info->do_pdom();
-	 kernel_func_info->set_pdom();
+      kernel.increment_cta_id();
     }
-
-    unsigned max_cta_tot = max_cta(kernel_info,kernel.threads_per_cta(), g_the_gpu->getShaderCoreConfig()->warp_size, g_the_gpu->getShaderCoreConfig()->n_thread_per_shader, g_the_gpu->getShaderCoreConfig()->gpgpu_shmem_size, g_the_gpu->getShaderCoreConfig()->gpgpu_shader_registers, g_the_gpu->getShaderCoreConfig()->max_cta_per_core);
-    printf("Max CTA : %d\n",max_cta_tot);
-
-
-
-
-
-    int inst_count=50;
-    int cp_op= g_the_gpu->checkpoint_option;
-    int cp_CTA = g_the_gpu->checkpoint_CTA;
-    int cp_kernel= g_the_gpu->checkpoint_kernel;
-    cp_count= g_the_gpu->checkpoint_insn_Y;
-    cp_cta_resume= g_the_gpu->checkpoint_CTA_t;
-    int cta_launched =0;
-
-    //we excute the kernel one CTA (Block) at the time, as synchronization functions work block wise
-    while(!kernel.no_more_ctas_to_run()){
-        unsigned temp=kernel.get_next_cta_id_single();
-
-
-        if(cp_op==0 || (cp_op==1 && cta_launched<cp_cta_resume && kernel.get_uid()==cp_kernel) || kernel.get_uid()< cp_kernel) // just fro testing
-        {
-           functionalCoreSim cta(
-               &kernel,
-               g_the_gpu,
-               g_the_gpu->getShaderCoreConfig()->warp_size
-           );
-           cta.execute(cp_count,temp);
-
-            #if (CUDART_VERSION >= 5000)
-            	launch_all_device_kernels();
-            #endif
-         }
-         else
-         {
-            kernel.increment_cta_id();
-         }
     cta_launched++;
+  }
+
+  if (cp_op == 1) {
+    char f1name[2048];
+    snprintf(f1name, 2048, "checkpoint_files/global_mem_%d.txt",
+             kernel.get_uid());
+    g_checkpoint->store_global_mem(
+        gpgpu_ctx->the_gpgpusim->g_the_gpu->get_global_memory(), f1name,
+        (char *)"%08x");
+  }
+
+  // registering this kernel as done
+
+  // openCL kernel simulation calls don't register the kernel so we don't
+  // register its exit
+  if (!openCL) {
+    // extern stream_manager *g_stream_manager;
+    gpgpu_ctx->the_gpgpusim->g_stream_manager->register_finished_kernel(
+        kernel.get_uid());
+  }
+
+  //******PRINTING*******
+  printf("GPGPU-Sim: Done functional simulation (%u instructions simulated).\n",
+         g_ptx_sim_num_insn);
+  if (gpgpu_ptx_instruction_classification) {
+    StatDisp(g_inst_classification_stat[g_ptx_kernel_count]);
+    StatDisp(g_inst_op_classification_stat[g_ptx_kernel_count]);
+  }
+
+  // time_t variables used to calculate the total simulation time
+  // the start time of simulation is hold by the global variable
+  // g_simulation_starttime g_simulation_starttime is initilized by
+  // gpgpu_ptx_sim_init_perf() in gpgpusim_entrypoint.cc upon starting gpgpu-sim
+  time_t end_time, elapsed_time, days, hrs, minutes, sec;
+  end_time = time((time_t *)NULL);
+  elapsed_time =
+      MAX(end_time - gpgpu_ctx->the_gpgpusim->g_simulation_starttime, 1);
+
+  // calculating and printing simulation time in terms of days, hours, minutes
+  // and seconds
+  days = elapsed_time / (3600 * 24);
+  hrs = elapsed_time / 3600 - 24 * days;
+  minutes = elapsed_time / 60 - 60 * (hrs + 24 * days);
+  sec = elapsed_time - 60 * (minutes + 60 * (hrs + 24 * days));
+
+  fflush(stderr);
+  printf(
+      "\n\ngpgpu_simulation_time = %u days, %u hrs, %u min, %u sec (%u sec)\n",
+      (unsigned)days, (unsigned)hrs, (unsigned)minutes, (unsigned)sec,
+      (unsigned)elapsed_time);
+  printf("gpgpu_simulation_rate = %u (inst/sec)\n",
+         (unsigned)(g_ptx_sim_num_insn / elapsed_time));
+  fflush(stdout);
+}
+
+void functionalCoreSim::initializeCTA(unsigned ctaid_cp) {
+  int ctaLiveThreads = 0;
+  symbol_table *symtab = m_kernel->entry()->get_symtab();
+
+  for (int i = 0; i < m_warp_count; i++) {
+    m_warpAtBarrier[i] = false;
+    m_liveThreadCount[i] = 0;
+  }
+  for (int i = 0; i < m_warp_count * m_warp_size; i++) m_thread[i] = NULL;
+
+  // get threads for a cta
+  for (unsigned i = 0; i < m_kernel->threads_per_cta(); i++) {
+    ptx_sim_init_thread(*m_kernel, &m_thread[i], 0, i,
+                        m_kernel->threads_per_cta() - i,
+                        m_kernel->threads_per_cta(), this, 0, i / m_warp_size,
+                        (gpgpu_t *)m_gpu, true);
+    assert(m_thread[i] != NULL && !m_thread[i]->is_done());
+    char fname[2048];
+    snprintf(fname, 2048, "checkpoint_files/thread_%d_0_reg.txt", i);
+    if (m_gpu->gpgpu_ctx->func_sim->cp_cta_resume == 1)
+      m_thread[i]->resume_reg_thread(fname, symtab);
+    ctaLiveThreads++;
+  }
+
+  for (int k = 0; k < m_warp_count; k++) createWarp(k);
+}
+
+void functionalCoreSim::createWarp(unsigned warpId) {
+  simt_mask_t initialMask;
+  unsigned liveThreadsCount = 0;
+  initialMask.set();
+  for (int i = warpId * m_warp_size; i < warpId * m_warp_size + m_warp_size;
+       i++) {
+    if (m_thread[i] == NULL)
+      initialMask.reset(i - warpId * m_warp_size);
+    else
+      liveThreadsCount++;
+  }
+
+  assert(m_thread[warpId * m_warp_size] != NULL);
+  m_simt_stack[warpId]->launch(m_thread[warpId * m_warp_size]->get_pc(),
+                               initialMask);
+  char fname[2048];
+  snprintf(fname, 2048, "checkpoint_files/warp_%d_0_simt.txt", warpId);
+
+  if (m_gpu->gpgpu_ctx->func_sim->cp_cta_resume == 1) {
+    unsigned pc, rpc;
+    m_simt_stack[warpId]->resume(fname);
+    m_simt_stack[warpId]->get_pdom_stack_top_info(&pc, &rpc);
+    for (int i = warpId * m_warp_size; i < warpId * m_warp_size + m_warp_size;
+         i++) {
+      m_thread[i]->set_npc(pc);
+      m_thread[i]->update_pc();
     }
-
-
-
-     if(cp_op==1)
-	{
-      char f1name[2048];
-      snprintf(f1name,2048,"checkpoint_files/global_mem_%d.txt", kernel.get_uid() );
-      g_checkpoint->store_global_mem(g_the_gpu->get_global_memory(), f1name , "%08x");
-	}
-
-
-
-
-   //registering this kernel as done
-
-   //openCL kernel simulation calls don't register the kernel so we don't register its exit
-   if(!openCL) {
-      extern stream_manager *g_stream_manager;
-      g_stream_manager->register_finished_kernel(kernel.get_uid());
-   }
-
-   //******PRINTING*******
-   printf( "GPGPU-Sim: Done functional simulation (%u instructions simulated).\n", g_ptx_sim_num_insn );
-   if ( gpgpu_ptx_instruction_classification ) {
-      StatDisp( g_inst_classification_stat[g_ptx_kernel_count]);
-      StatDisp ( g_inst_op_classification_stat[g_ptx_kernel_count]);
-   }
-
-   //time_t variables used to calculate the total simulation time
-   //the start time of simulation is hold by the global variable g_simulation_starttime
-   //g_simulation_starttime is initilized by gpgpu_ptx_sim_init_perf() in gpgpusim_entrypoint.cc upon starting gpgpu-sim
-   time_t end_time, elapsed_time, days, hrs, minutes, sec;
-   end_time = time((time_t *)NULL);
-   elapsed_time = MAX(end_time - g_simulation_starttime, 1);
-
-
-   //calculating and printing simulation time in terms of days, hours, minutes and seconds
-   days    = elapsed_time/(3600*24);
-   hrs     = elapsed_time/3600 - 24*days;
-   minutes = elapsed_time/60 - 60*(hrs + 24*days);
-   sec = elapsed_time - 60*(minutes + 60*(hrs + 24*days));
-
-   fflush(stderr);
-   printf("\n\ngpgpu_simulation_time = %u days, %u hrs, %u min, %u sec (%u sec)\n",
-          (unsigned)days, (unsigned)hrs, (unsigned)minutes, (unsigned)sec, (unsigned)elapsed_time );
-   printf("gpgpu_simulation_rate = %u (inst/sec)\n", (unsigned)(g_ptx_sim_num_insn / elapsed_time) );
-   fflush(stdout);
-}
-
-void functionalCoreSim::initializeCTA(unsigned ctaid_cp)
-{
-    int ctaLiveThreads=0;
-    symbol_table * symtab= m_kernel->entry()->get_symtab();
-
-    for(int i=0; i< m_warp_count; i++){
-        m_warpAtBarrier[i]=false;
-        m_liveThreadCount[i]=0;
-    }
-    for(int i=0; i< m_warp_count*m_warp_size;i++)
-        m_thread[i]=NULL;
-
-    //get threads for a cta
-    for(unsigned i=0; i<m_kernel->threads_per_cta();i++) {
-        ptx_sim_init_thread(*m_kernel,&m_thread[i],0,i,m_kernel->threads_per_cta()-i,m_kernel->threads_per_cta(),this,0,i/m_warp_size,(gpgpu_t*)m_gpu, true);
-        assert(m_thread[i]!=NULL && !m_thread[i]->is_done());
-        char fname[2048];
-        snprintf(fname,2048,"checkpoint_files/thread_%d_0_reg.txt",i );
-        if(cp_cta_resume==1)
-            m_thread[i]->resume_reg_thread(fname,symtab);
-        ctaLiveThreads++;
+  }
+  m_liveThreadCount[warpId] = liveThreadsCount;
+}
+
+void functionalCoreSim::execute(int inst_count, unsigned ctaid_cp) {
+  m_gpu->gpgpu_ctx->func_sim->cp_count = m_gpu->checkpoint_insn_Y;
+  m_gpu->gpgpu_ctx->func_sim->cp_cta_resume = m_gpu->checkpoint_CTA_t;
+  initializeCTA(ctaid_cp);
+
+  int count = 0;
+  while (true) {
+    bool someOneLive = false;
+    bool allAtBarrier = true;
+    for (unsigned i = 0; i < m_warp_count; i++) {
+      executeWarp(i, allAtBarrier, someOneLive);
+      count++;
     }
 
-    for(int k=0;k<m_warp_count;k++)
-        createWarp(k);
-}
-
-void  functionalCoreSim::createWarp(unsigned warpId)
-{
-   simt_mask_t initialMask;
-   unsigned liveThreadsCount=0;
-   initialMask.set();
-    for(int i=warpId*m_warp_size; i<warpId*m_warp_size+m_warp_size;i++){
-        if(m_thread[i]==NULL) initialMask.reset(i-warpId*m_warp_size);
-        else liveThreadsCount++;
+    if (inst_count > 0 && count > inst_count &&
+        (m_kernel->get_uid() == m_gpu->checkpoint_kernel) &&
+        (ctaid_cp >= m_gpu->checkpoint_CTA) &&
+        (ctaid_cp < m_gpu->checkpoint_CTA_t) && m_gpu->checkpoint_option == 1) {
+      someOneLive = false;
+      break;
     }
-
-   assert(m_thread[warpId*m_warp_size]!=NULL);
-   m_simt_stack[warpId]->launch(m_thread[warpId*m_warp_size]->get_pc(),initialMask);
-   char fname[2048];
-   snprintf(fname,2048,"checkpoint_files/warp_%d_0_simt.txt",warpId );
-
-   if(cp_cta_resume==1)
-   {
-      unsigned pc,rpc;
-      m_simt_stack[warpId]->resume(fname);
-      m_simt_stack[warpId]->get_pdom_stack_top_info(&pc,&rpc);
-      for(int i=warpId*m_warp_size; i<warpId*m_warp_size+m_warp_size;i++){
-        m_thread[i]->set_npc(pc);
-        m_thread[i]->update_pc();
+    if (!someOneLive) break;
+    if (allAtBarrier) {
+      for (unsigned i = 0; i < m_warp_count; i++) m_warpAtBarrier[i] = false;
     }
-
-   }
-   m_liveThreadCount[warpId]= liveThreadsCount;
-}
-
-void functionalCoreSim::execute(int inst_count, unsigned ctaid_cp)
- {
-   cp_count= m_gpu->checkpoint_insn_Y;
-    cp_cta_resume= m_gpu->checkpoint_CTA_t;
-    initializeCTA(ctaid_cp);
-
-    int count=0;
-    while(true){
-        bool someOneLive= false;
-        bool allAtBarrier = true;
-        for(unsigned i=0;i<m_warp_count;i++){
-            executeWarp(i,allAtBarrier,someOneLive);
-            count++;
-        }
-
-        if(inst_count>0 && count>inst_count && (m_kernel->get_uid()==m_gpu->checkpoint_kernel) && (ctaid_cp>=m_gpu->checkpoint_CTA) && (ctaid_cp<m_gpu->checkpoint_CTA_t) && m_gpu->checkpoint_option==1)
-         {
-            someOneLive=false;
-            break;
-         }
-        if(!someOneLive) break;
-        if(allAtBarrier){
-             for(unsigned i=0;i<m_warp_count;i++)
-                 m_warpAtBarrier[i]=false;
-        }
+  }
+
+  checkpoint *g_checkpoint;
+  g_checkpoint = new checkpoint();
+
+  ptx_reg_t regval;
+  regval.u64 = 123;
+
+  unsigned ctaid = m_kernel->get_next_cta_id_single();
+  if (m_gpu->checkpoint_option == 1 &&
+      (m_kernel->get_uid() == m_gpu->checkpoint_kernel) &&
+      (ctaid_cp >= m_gpu->checkpoint_CTA) &&
+      (ctaid_cp < m_gpu->checkpoint_CTA_t)) {
+    char fname[2048];
+    snprintf(fname, 2048, "checkpoint_files/shared_mem_%d.txt", ctaid - 1);
+    g_checkpoint->store_global_mem(m_thread[0]->m_shared_mem, fname,
+                                   (char *)"%08x");
+    for (int i = 0; i < 32 * m_warp_count; i++) {
+      char fname[2048];
+      snprintf(fname, 2048, "checkpoint_files/thread_%d_%d_reg.txt", i,
+               ctaid - 1);
+      m_thread[i]->print_reg_thread(fname);
+      char f1name[2048];
+      snprintf(f1name, 2048, "checkpoint_files/local_mem_thread_%d_%d_reg.txt",
+               i, ctaid - 1);
+      g_checkpoint->store_global_mem(m_thread[i]->m_local_mem, f1name,
+                                     (char *)"%08x");
+      m_thread[i]->set_done();
+      m_thread[i]->exitCore();
+      m_thread[i]->registerExit();
     }
 
-    checkpoint *g_checkpoint;
-    g_checkpoint = new checkpoint();
-
-    symbol * sym;
-    ptx_reg_t regval;
-    regval.u64= 123;
-    symbol_table * symtab= m_kernel->entry()->get_symtab();
-
-
-    unsigned ctaid =m_kernel->get_next_cta_id_single();
-    if(m_gpu->checkpoint_option==1 && (m_kernel->get_uid()==m_gpu->checkpoint_kernel) && (ctaid_cp>=m_gpu->checkpoint_CTA) && (ctaid_cp<m_gpu->checkpoint_CTA_t))
-   {
-       char fname[2048];
-       snprintf(fname,2048,"checkpoint_files/shared_mem_%d.txt",ctaid-1 );
-       g_checkpoint->store_global_mem(m_thread[0]->m_shared_mem, fname , "%08x");
-      for(int i=0; i<32*m_warp_count;i++)
-      {
-         char fname[2048];
-         snprintf(fname,2048,"checkpoint_files/thread_%d_%d_reg.txt",i,ctaid-1 );
-          m_thread[i]->print_reg_thread(fname);
-          char f1name[2048];
-         snprintf(f1name,2048,"checkpoint_files/local_mem_thread_%d_%d_reg.txt",i,ctaid-1 );
-         g_checkpoint->store_global_mem(m_thread[i]->m_local_mem, f1name , "%08x");
-         m_thread[i]->set_done();
-         m_thread[i]->exitCore();
-         m_thread[i]->registerExit();
-      }
-
-      for(int i=0;i<m_warp_count;i++)
-      {
-
-         char fname[2048];
-         snprintf(fname,2048,"checkpoint_files/warp_%d_%d_simt.txt",i,ctaid-1 );
-         FILE * fp = fopen(fname,"w");
-         assert(fp!=NULL);
-         m_simt_stack[i]->print_checkpoint(fp);
-         fclose(fp);
-      }
-   }
-
+    for (int i = 0; i < m_warp_count; i++) {
+      char fname[2048];
+      snprintf(fname, 2048, "checkpoint_files/warp_%d_%d_simt.txt", i,
+               ctaid - 1);
+      FILE *fp = fopen(fname, "w");
+      assert(fp != NULL);
+      m_simt_stack[i]->print_checkpoint(fp);
+      fclose(fp);
+    }
+  }
 }
 
-void functionalCoreSim::executeWarp(unsigned i, bool &allAtBarrier, bool & someOneLive)
-{
-    if(!m_warpAtBarrier[i] && m_liveThreadCount[i]!=0){
-        warp_inst_t inst =getExecuteWarp(i);
-        execute_warp_inst_t(inst,i);
-        if(inst.isatomic()) inst.do_atomic(true);
-        if(inst.op==BARRIER_OP || inst.op==MEMORY_BARRIER_OP ) m_warpAtBarrier[i]=true;
-        updateSIMTStack( i, &inst );
-    }
-    if(m_liveThreadCount[i]>0) someOneLive=true;
-    if(!m_warpAtBarrier[i]&& m_liveThreadCount[i]>0) allAtBarrier = false;
+void functionalCoreSim::executeWarp(unsigned i, bool &allAtBarrier,
+                                    bool &someOneLive) {
+  if (!m_warpAtBarrier[i] && m_liveThreadCount[i] != 0) {
+    warp_inst_t inst = getExecuteWarp(i);
+    execute_warp_inst_t(inst, i);
+    if (inst.isatomic()) inst.do_atomic(true);
+    if (inst.op == BARRIER_OP || inst.op == MEMORY_BARRIER_OP)
+      m_warpAtBarrier[i] = true;
+    updateSIMTStack(i, &inst);
+  }
+  if (m_liveThreadCount[i] > 0) someOneLive = true;
+  if (!m_warpAtBarrier[i] && m_liveThreadCount[i] > 0) allAtBarrier = false;
 }
 
-unsigned translate_pc_to_ptxlineno(unsigned pc)
-{
-   // this function assumes that the kernel fits inside a single PTX file
-   // function_info *pFunc = g_func_info; // assume that the current kernel is the one in query
-   const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
-   unsigned ptx_line_number = pInsn->source_line();
+unsigned gpgpu_context::translate_pc_to_ptxlineno(unsigned pc) {
+  // this function assumes that the kernel fits inside a single PTX file
+  // function_info *pFunc = g_func_info; // assume that the current kernel is
+  // the one in query
+  const ptx_instruction *pInsn = pc_to_instruction(pc);
+  unsigned ptx_line_number = pInsn->source_line();
 
-   return ptx_line_number;
+  return ptx_line_number;
 }
 
-#endif // LIBCUDA
-
 // ptxinfo parser
 
-extern std::map<unsigned,const char*> get_duplicate();
-
-int g_ptxinfo_error_detected;
+extern std::map<unsigned, const char *> get_duplicate();
 
 static char *g_ptxinfo_kname = NULL;
 static struct gpgpu_ptx_sim_info g_ptxinfo;
-static std::map<unsigned,const char*> g_duplicate;
+static std::map<unsigned, const char *> g_duplicate;
 static const char *g_last_dup_type;
 
-const char *get_ptxinfo_kname()
-{
-    return g_ptxinfo_kname;
-}
+const char *get_ptxinfo_kname() { return g_ptxinfo_kname; }
 
-void print_ptxinfo()
-{
-    if(! get_ptxinfo_kname()){
-    	printf ("GPGPU-Sim PTX: Binary info : gmem=%u, cmem=%u\n",
-    			g_ptxinfo.gmem,
-    			g_ptxinfo.cmem);
-    }
-    if(get_ptxinfo_kname()){
-    	printf ("GPGPU-Sim PTX: Kernel \'%s\' : regs=%u, lmem=%u, smem=%u, cmem=%u\n",
-    			get_ptxinfo_kname(),
-    			g_ptxinfo.regs,
-    			g_ptxinfo.lmem,
-    			g_ptxinfo.smem,
-    			g_ptxinfo.cmem );
-    }
+void print_ptxinfo() {
+  if (!get_ptxinfo_kname()) {
+    printf("GPGPU-Sim PTX: Binary info : gmem=%u, cmem=%u\n", g_ptxinfo.gmem,
+           g_ptxinfo.cmem);
+  }
+  if (get_ptxinfo_kname()) {
+    printf(
+        "GPGPU-Sim PTX: Kernel \'%s\' : regs=%u, lmem=%u, smem=%u, cmem=%u\n",
+        get_ptxinfo_kname(), g_ptxinfo.regs, g_ptxinfo.lmem, g_ptxinfo.smem,
+        g_ptxinfo.cmem);
+  }
 }
 
-
-struct gpgpu_ptx_sim_info get_ptxinfo()
-{
-    return g_ptxinfo;
+struct gpgpu_ptx_sim_info get_ptxinfo() {
+  return g_ptxinfo;
 }
 
-std::map<unsigned,const char*> get_duplicate()
-{
-	return g_duplicate;
-}
+std::map<unsigned, const char *> get_duplicate() { return g_duplicate; }
 
-extern "C" void ptxinfo_linenum( unsigned linenum )
-{
-	g_duplicate[linenum] = g_last_dup_type;
+void ptxinfo_linenum(unsigned linenum) {
+  g_duplicate[linenum] = g_last_dup_type;
 }
 
-extern "C" void ptxinfo_dup_type( const char *dup_type )
-{
-	g_last_dup_type = dup_type;
-}
+void ptxinfo_dup_type(const char *dup_type) { g_last_dup_type = dup_type; }
 
-extern "C" void ptxinfo_function(const char *fname )
-{
-    clear_ptxinfo();
-    g_ptxinfo_kname = strdup(fname);
+void ptxinfo_function(const char *fname) {
+  clear_ptxinfo();
+  g_ptxinfo_kname = strdup(fname);
 }
 
-extern "C" void ptxinfo_regs( unsigned nregs )
-{
-    g_ptxinfo.regs=nregs;
-}
+void ptxinfo_regs(unsigned nregs) { g_ptxinfo.regs = nregs; }
 
-extern "C" void ptxinfo_lmem( unsigned declared, unsigned system )
-{
-    g_ptxinfo.lmem=declared+system;
+void ptxinfo_lmem(unsigned declared, unsigned system) {
+  g_ptxinfo.lmem = declared + system;
 }
 
-extern "C" void ptxinfo_gmem( unsigned declared, unsigned system )
-{
-    g_ptxinfo.gmem=declared+system;
+void ptxinfo_gmem(unsigned declared, unsigned system) {
+  g_ptxinfo.gmem = declared + system;
 }
 
-extern "C" void ptxinfo_smem( unsigned declared, unsigned system )
-{
-    g_ptxinfo.smem=declared+system;
+void ptxinfo_smem(unsigned declared, unsigned system) {
+  g_ptxinfo.smem = declared + system;
 }
 
-extern "C" void ptxinfo_cmem( unsigned nbytes, unsigned bank )
-{
-    g_ptxinfo.cmem+=nbytes;
-}
+void ptxinfo_cmem(unsigned nbytes, unsigned bank) { g_ptxinfo.cmem += nbytes; }
 
-void clear_ptxinfo()
-{
-    free(g_ptxinfo_kname);
-    g_ptxinfo_kname=NULL;
-    g_ptxinfo.regs=0;
-    g_ptxinfo.lmem=0;
-    g_ptxinfo.smem=0;
-    g_ptxinfo.cmem=0;
-    g_ptxinfo.gmem=0;
-    g_ptxinfo.ptx_version=0;
-    g_ptxinfo.sm_target=0;
+void clear_ptxinfo() {
+  free(g_ptxinfo_kname);
+  g_ptxinfo_kname = NULL;
+  g_ptxinfo.regs = 0;
+  g_ptxinfo.lmem = 0;
+  g_ptxinfo.smem = 0;
+  g_ptxinfo.cmem = 0;
+  g_ptxinfo.gmem = 0;
+  g_ptxinfo.ptx_version = 0;
+  g_ptxinfo.sm_target = 0;
 }
 
+void ptxinfo_opencl_addinfo(std::map<std::string, function_info *> &kernels) {
+  if (!g_ptxinfo_kname) {
+    printf("GPGPU-Sim PTX: Binary info : gmem=%u, cmem=%u\n", g_ptxinfo.gmem,
+           g_ptxinfo.cmem);
+    clear_ptxinfo();
+    return;
+  }
 
-void ptxinfo_opencl_addinfo( std::map<std::string,function_info*> &kernels )
-{
-
-   if(! g_ptxinfo_kname) {
-	  printf ("GPGPU-Sim PTX: Binary info : gmem=%u, cmem=%u\n",
-			  g_ptxinfo.gmem,
-			  g_ptxinfo.cmem);
-	  clear_ptxinfo();
-	  return;
-   }
-
-   if( !strcmp("__cuda_dummy_entry__",g_ptxinfo_kname) ) {
-      // this string produced by ptxas for empty ptx files (e.g., bandwidth test)
-      clear_ptxinfo();
-      return;
-   }
-   std::map<std::string,function_info*>::iterator k=kernels.find(g_ptxinfo_kname);
-   if( k==kernels.end() ) {
-      printf ("GPGPU-Sim PTX: ERROR ** implementation for '%s' not found.\n", g_ptxinfo_kname );
-      abort();
-   } else {
-      printf ("GPGPU-Sim PTX: Kernel \'%s\' : regs=%u, lmem=%u, smem=%u, cmem=%u\n",
-              g_ptxinfo_kname,
-              g_ptxinfo.regs,
-              g_ptxinfo.lmem,
-              g_ptxinfo.smem,
-              g_ptxinfo.cmem );
-      function_info *finfo = k->second;
-      assert(finfo!=NULL);
-      finfo->set_kernel_info( g_ptxinfo );
-   }
-   clear_ptxinfo();
-}
-
-struct rec_pts {
-   gpgpu_recon_t *s_kernel_recon_points;
-   int s_num_recon;
-};
-
-class std::map<function_info*,rec_pts> g_rpts;
-
-struct rec_pts find_reconvergence_points( function_info *finfo )
-{
-   rec_pts tmp;
-   std::map<function_info*,rec_pts>::iterator r=g_rpts.find(finfo);
-
-   if( r==g_rpts.end() ) {
-      int num_recon = finfo->get_num_reconvergence_pairs();
-
-      gpgpu_recon_t *kernel_recon_points = (struct gpgpu_recon_t*) calloc(num_recon, sizeof(struct gpgpu_recon_t));
-      finfo->get_reconvergence_pairs(kernel_recon_points);
-      printf("GPGPU-Sim PTX: reconvergence points for %s...\n", finfo->get_name().c_str() );
-      for (int i=0;i<num_recon;i++) {
-         printf("GPGPU-Sim PTX: %2u (potential) branch divergence @ ", i+1 );
-         kernel_recon_points[i].source_inst->print_insn();
-         printf("\n");
-         printf("GPGPU-Sim PTX:    immediate post dominator      @ " );
-         if( kernel_recon_points[i].target_inst )
-            kernel_recon_points[i].target_inst->print_insn();
-         printf("\n");
-      }
-      printf("GPGPU-Sim PTX: ... end of reconvergence points for %s\n", finfo->get_name().c_str() );
-
-      tmp.s_kernel_recon_points = kernel_recon_points;
-      tmp.s_num_recon = num_recon;
-      g_rpts[finfo] = tmp;
-   } else {
-      tmp = r->second;
-   }
-   return tmp;
-}
-
-address_type get_return_pc( void *thd )
-{
-    // function call return
-    ptx_thread_info *the_thread = (ptx_thread_info*)thd;
-    assert( the_thread != NULL );
-    return the_thread->get_return_PC();
-}
-
-address_type get_converge_point( address_type pc )
-{
-   // the branch could encode the reconvergence point and/or a bit that indicates the
-   // reconvergence point is the return PC on the call stack in the case the branch has
-   // no immediate postdominator in the function (i.e., due to multiple return points).
-
-   std::map<unsigned,function_info*>::iterator f=g_pc_to_finfo.find(pc);
-   assert( f != g_pc_to_finfo.end() );
-   function_info *finfo = f->second;
-   rec_pts tmp = find_reconvergence_points(finfo);
-
-   int i=0;
-   for (; i < tmp.s_num_recon; ++i) {
-      if (tmp.s_kernel_recon_points[i].source_pc == pc) {
-          if( tmp.s_kernel_recon_points[i].target_pc == (unsigned) -2 ) {
-              return RECONVERGE_RETURN_PC;
-          } else {
-              return tmp.s_kernel_recon_points[i].target_pc;
-          }
+  if (!strcmp("__cuda_dummy_entry__", g_ptxinfo_kname)) {
+    // this string produced by ptxas for empty ptx files (e.g., bandwidth test)
+    clear_ptxinfo();
+    return;
+  }
+  std::map<std::string, function_info *>::iterator k =
+      kernels.find(g_ptxinfo_kname);
+  if (k == kernels.end()) {
+    printf("GPGPU-Sim PTX: ERROR ** implementation for '%s' not found.\n",
+           g_ptxinfo_kname);
+    abort();
+  } else {
+    printf(
+        "GPGPU-Sim PTX: Kernel \'%s\' : regs=%u, lmem=%u, smem=%u, cmem=%u\n",
+        g_ptxinfo_kname, g_ptxinfo.regs, g_ptxinfo.lmem, g_ptxinfo.smem,
+        g_ptxinfo.cmem);
+    function_info *finfo = k->second;
+    assert(finfo != NULL);
+    finfo->set_kernel_info(g_ptxinfo);
+  }
+  clear_ptxinfo();
+}
+
+struct rec_pts cuda_sim::find_reconvergence_points(function_info *finfo) {
+  rec_pts tmp;
+  std::map<function_info *, rec_pts>::iterator r = g_rpts.find(finfo);
+
+  if (r == g_rpts.end()) {
+    int num_recon = finfo->get_num_reconvergence_pairs();
+
+    gpgpu_recon_t *kernel_recon_points =
+        (struct gpgpu_recon_t *)calloc(num_recon, sizeof(struct gpgpu_recon_t));
+    finfo->get_reconvergence_pairs(kernel_recon_points);
+    printf("GPGPU-Sim PTX: reconvergence points for %s...\n",
+           finfo->get_name().c_str());
+    for (int i = 0; i < num_recon; i++) {
+      printf("GPGPU-Sim PTX: %2u (potential) branch divergence @ ", i + 1);
+      kernel_recon_points[i].source_inst->print_insn();
+      printf("\n");
+      printf("GPGPU-Sim PTX:    immediate post dominator      @ ");
+      if (kernel_recon_points[i].target_inst)
+        kernel_recon_points[i].target_inst->print_insn();
+      printf("\n");
+    }
+    printf("GPGPU-Sim PTX: ... end of reconvergence points for %s\n",
+           finfo->get_name().c_str());
+
+    tmp.s_kernel_recon_points = kernel_recon_points;
+    tmp.s_num_recon = num_recon;
+    g_rpts[finfo] = tmp;
+  } else {
+    tmp = r->second;
+  }
+  return tmp;
+}
+
+address_type get_return_pc(void *thd) {
+  // function call return
+  ptx_thread_info *the_thread = (ptx_thread_info *)thd;
+  assert(the_thread != NULL);
+  return the_thread->get_return_PC();
+}
+
+address_type cuda_sim::get_converge_point(address_type pc) {
+  // the branch could encode the reconvergence point and/or a bit that indicates
+  // the reconvergence point is the return PC on the call stack in the case the
+  // branch has no immediate postdominator in the function (i.e., due to
+  // multiple return points).
+
+  std::map<unsigned, function_info *>::iterator f = g_pc_to_finfo.find(pc);
+  assert(f != g_pc_to_finfo.end());
+  function_info *finfo = f->second;
+  rec_pts tmp = find_reconvergence_points(finfo);
+
+  int i = 0;
+  for (; i < tmp.s_num_recon; ++i) {
+    if (tmp.s_kernel_recon_points[i].source_pc == pc) {
+      if (tmp.s_kernel_recon_points[i].target_pc == (unsigned)-2) {
+        return RECONVERGE_RETURN_PC;
+      } else {
+        return tmp.s_kernel_recon_points[i].target_pc;
       }
-   }
-   return NO_BRANCH_DIVERGENCE;
+    }
+  }
+  return NO_BRANCH_DIVERGENCE;
 }
 
-void functionalCoreSim::warp_exit( unsigned warp_id )
-{
-    for(int i=0;i<m_warp_count*m_warp_size;i++){
-        if(m_thread[i]!=NULL){
-             m_thread[i]->m_cta_info->register_deleted_thread(m_thread[i]);
-             delete m_thread[i];
-        }
+void functionalCoreSim::warp_exit(unsigned warp_id) {
+  for (int i = 0; i < m_warp_count * m_warp_size; i++) {
+    if (m_thread[i] != NULL) {
+      m_thread[i]->m_cta_info->register_deleted_thread(m_thread[i]);
+      delete m_thread[i];
     }
+  }
 }
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.h
index a3230af066..8deefd95d5 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda-sim.h
@@ -28,120 +28,181 @@
 #ifndef CUDASIM_H_INCLUDED
 #define CUDASIM_H_INCLUDED
 
-#ifndef LIBCUDA
-#include "../abstract_hardware_model.h"
-#include"../gpgpu-sim/shader.h"
-#else
-#include "../libcuda/abstract_hardware_model.h"
-#include"../option_parser.h"
-#endif
-
 #include <stdlib.h>
 #include <map>
-#include <vector>
 #include <string>
+#include <vector>
+#include "../abstract_hardware_model.h"
+#include "../gpgpu-sim/shader.h"
 #include"ptx_sim.h"
 
+class gpgpu_context;
 class memory_space;
 class function_info;
 class symbol_table;
 
 extern const char *g_gpgpusim_version_string;
-extern int g_ptx_sim_mode;
 extern int g_debug_execution;
-extern int g_debug_thread_uid;
-extern void ** g_inst_classification_stat;
-extern void ** g_inst_op_classification_stat;
-extern int g_ptx_kernel_count; // used for classification stat collection purposes 
-extern char *opcode_latency_int, *opcode_latency_fp, *opcode_latency_dp,*opcode_latency_sfu,*opcode_latency_tensor;
-
-
-void ptx_opcocde_latency_options (option_parser_t opp);
-extern class kernel_info_t *gpgpu_opencl_ptx_sim_init_grid(class function_info *entry,
-                                            gpgpu_ptx_sim_arg_list_t args, 
-                                            struct dim3 gridDim, 
-                                            struct dim3 blockDim, 
-                                                          class gpgpu_t *gpu );
-extern void gpgpu_cuda_ptx_sim_main_func( kernel_info_t &kernel, bool openCL = false );
-extern void   print_splash();
-extern void   gpgpu_ptx_sim_register_const_variable(void*, const char *deviceName, size_t size );
-extern void   gpgpu_ptx_sim_register_global_variable(void *hostVar, const char *deviceName, size_t size );
-
-// TODO schi 
-extern std::string gpgpu_ptx_sim_hostvar_to_sym_name(const char *hostVar);
-
-extern void   gpgpu_ptx_sim_memcpy_symbol(const char *hostVar, const void *src, size_t count, size_t offset, int to, gpgpu_t *gpu );
 
-extern void read_sim_environment_variables();
-extern void ptxinfo_opencl_addinfo( std::map<std::string,function_info*> &kernels );
-unsigned ptx_sim_init_thread( kernel_info_t &kernel,
-                              class ptx_thread_info** thread_info,
-                              int sid,
-                              unsigned tid,
-                              unsigned threads_left,
-                              unsigned num_threads, 
-                              class core_t *core, 
-                              unsigned hw_cta_id, 
-                              unsigned hw_warp_id,
-                              gpgpu_t *gpu,
-                              bool functionalSimulationMode = false);
-const warp_inst_t *ptx_fetch_inst( address_type pc );
-const struct gpgpu_ptx_sim_info* ptx_sim_kernel_info(const class function_info *kernel);
-void ptx_print_insn( address_type pc, FILE *fp );
-std::string ptx_get_insn_str( address_type pc );
-void set_param_gpgpu_num_shaders(int num_shaders);
+extern void print_splash();
 
+extern void ptxinfo_opencl_addinfo(
+    std::map<std::string, function_info *> &kernels);
+unsigned ptx_sim_init_thread(kernel_info_t &kernel,
+                             class ptx_thread_info **thread_info, int sid,
+                             unsigned tid, unsigned threads_left,
+                             unsigned num_threads, class core_t *core,
+                             unsigned hw_cta_id, unsigned hw_warp_id,
+                             gpgpu_t *gpu,
+                             bool functionalSimulationMode = false);
+const struct gpgpu_ptx_sim_info *ptx_sim_kernel_info(
+    const class function_info *kernel);
 
 /*!
- * This class functionally executes a kernel. It uses the basic data structures and procedures in core_t 
+ * This class functionally executes a kernel. It uses the basic data structures
+ * and procedures in core_t
  */
-class functionalCoreSim: public core_t
-{    
-public:
-    functionalCoreSim(kernel_info_t * kernel, gpgpu_sim *g, unsigned warp_size)
-        : core_t( g, kernel, warp_size, kernel->threads_per_cta() )
-    {
-        m_warpAtBarrier =  new bool [m_warp_count];
-        m_liveThreadCount = new unsigned [m_warp_count];
-    }
-    virtual ~functionalCoreSim(){
-        warp_exit(0);
-        delete[] m_liveThreadCount;
-        delete[] m_warpAtBarrier;
-    }
-    //! executes all warps till completion 
-    void execute(int inst_count, unsigned ctaid_cp);
-    virtual void warp_exit( unsigned warp_id );
-    virtual bool warp_waiting_at_barrier( unsigned warp_id ) const  
-    {
-        return (m_warpAtBarrier[warp_id] || !(m_liveThreadCount[warp_id]>0));
-    }
-    
-private:
-    void executeWarp(unsigned, bool &, bool &);
-    //initializes threads in the CTA block which we are executing
-    void initializeCTA(unsigned ctaid_cp);
-    virtual void checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t, unsigned tid)
-    {
-    if(m_thread[tid]==NULL || m_thread[tid]->is_done()){
-        m_liveThreadCount[tid/m_warp_size]--;
-        }
+class functionalCoreSim : public core_t {
+ public:
+  functionalCoreSim(kernel_info_t *kernel, gpgpu_sim *g, unsigned warp_size)
+      : core_t(g, kernel, warp_size, kernel->threads_per_cta()) {
+    m_warpAtBarrier = new bool[m_warp_count];
+    m_liveThreadCount = new unsigned[m_warp_count];
+  }
+  virtual ~functionalCoreSim() {
+    warp_exit(0);
+    delete[] m_liveThreadCount;
+    delete[] m_warpAtBarrier;
+  }
+  //! executes all warps till completion
+  void execute(int inst_count, unsigned ctaid_cp);
+  virtual void warp_exit(unsigned warp_id);
+  virtual bool warp_waiting_at_barrier(unsigned warp_id) const {
+    return (m_warpAtBarrier[warp_id] || !(m_liveThreadCount[warp_id] > 0));
+  }
+
+ private:
+  void executeWarp(unsigned, bool &, bool &);
+  // initializes threads in the CTA block which we are executing
+  void initializeCTA(unsigned ctaid_cp);
+  virtual void checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t,
+                                             unsigned tid) {
+    if (m_thread[tid] == NULL || m_thread[tid]->is_done()) {
+      m_liveThreadCount[tid / m_warp_size]--;
     }
-    
-    // lunches the stack and set the threads count
-    void  createWarp(unsigned warpId);
-    
-    //each warp live thread count and barrier indicator
-    unsigned * m_liveThreadCount;
-    bool* m_warpAtBarrier;
+  }
+
+  // lunches the stack and set the threads count
+  void createWarp(unsigned warpId);
+
+  // each warp live thread count and barrier indicator
+  unsigned *m_liveThreadCount;
+  bool *m_warpAtBarrier;
 };
 
 #define RECONVERGE_RETURN_PC ((address_type)-2)
 #define NO_BRANCH_DIVERGENCE ((address_type)-1)
-address_type get_return_pc( void *thd );
+address_type get_return_pc(void *thd );
 const char *get_ptxinfo_kname();
 void print_ptxinfo();
 void clear_ptxinfo();
 struct gpgpu_ptx_sim_info get_ptxinfo();
 
+class gpgpu_recon_t;
+struct rec_pts {
+  gpgpu_recon_t *s_kernel_recon_points;
+  int s_num_recon;
+};
+
+class cuda_sim {
+ public:
+  cuda_sim(gpgpu_context *ctx) {
+    g_ptx_sim_num_insn = 0;
+    g_ptx_kernel_count =
+        -1;  // used for classification stat collection purposes
+    gpgpu_param_num_shaders = 0;
+    g_cuda_launch_blocking = false;
+    g_inst_classification_stat = NULL;
+    g_inst_op_classification_stat = NULL;
+    g_assemble_code_next_pc = 0;
+    g_debug_thread_uid = 0;
+    g_override_embedded_ptx = false;
+    ptx_tex_regs = NULL;
+    g_ptx_thread_info_delete_count = 0;
+    g_ptx_thread_info_uid_next = 1;
+    g_debug_pc = 0xBEEF1518;
+    gpgpu_ctx = ctx;
+  }
+  // global variables
+  char *opcode_latency_int;
+  char *opcode_latency_fp;
+  char *opcode_latency_dp;
+  char *opcode_latency_sfu;
+  char *opcode_latency_tensor;
+  char *opcode_initiation_int;
+  char *opcode_initiation_fp;
+  char *opcode_initiation_dp;
+  char *opcode_initiation_sfu;
+  char *opcode_initiation_tensor;
+  int cp_count;
+  int cp_cta_resume;
+  int g_ptxinfo_error_detected;
+  unsigned g_ptx_sim_num_insn;
+  char *cdp_latency_str;
+  int g_ptx_kernel_count;  // used for classification stat collection purposes
+  std::map<const void *, std::string>
+      g_global_name_lookup;  // indexed by hostVar
+  std::map<const void *, std::string>
+      g_const_name_lookup;  // indexed by hostVar
+  int g_ptx_sim_mode;  // if non-zero run functional simulation only (i.e., no
+                       // notion of a clock cycle)
+  unsigned gpgpu_param_num_shaders;
+  class std::map<function_info *, rec_pts> g_rpts;
+  bool g_cuda_launch_blocking;
+  void **g_inst_classification_stat;
+  void **g_inst_op_classification_stat;
+  std::set<std::string> g_globals;
+  std::set<std::string> g_constants;
+  std::map<unsigned, function_info *> g_pc_to_finfo;
+  int gpgpu_ptx_instruction_classification;
+  unsigned cdp_latency[5];
+  unsigned g_assemble_code_next_pc;
+  int g_debug_thread_uid;
+  bool g_override_embedded_ptx;
+  std::set<unsigned long long> g_ptx_cta_info_sm_idx_used;
+  ptx_reg_t *ptx_tex_regs;
+  unsigned g_ptx_thread_info_delete_count;
+  unsigned g_ptx_thread_info_uid_next;
+  addr_t g_debug_pc;
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+  // global functions
+  void ptx_opcocde_latency_options(option_parser_t opp);
+  void gpgpu_cuda_ptx_sim_main_func(kernel_info_t &kernel, bool openCL = false);
+  int gpgpu_opencl_ptx_sim_main_func(kernel_info_t *grid);
+  void init_inst_classification_stat();
+  kernel_info_t *gpgpu_opencl_ptx_sim_init_grid(class function_info *entry,
+                                                gpgpu_ptx_sim_arg_list_t args,
+                                                struct dim3 gridDim,
+                                                struct dim3 blockDim,
+                                                gpgpu_t *gpu);
+  void gpgpu_ptx_sim_register_global_variable(void *hostVar,
+                                              const char *deviceName,
+                                              size_t size);
+  void gpgpu_ptx_sim_register_const_variable(void *, const char *deviceName,
+                                             size_t size);
+  void read_sim_environment_variables();
+  void set_param_gpgpu_num_shaders(int num_shaders);
+  struct rec_pts find_reconvergence_points(function_info *finfo);
+  address_type get_converge_point(address_type pc);
+  unsigned gpgpu_ptx_hostvar_to_sym_address(const char *hostVar, gpgpu_t *gpu);
+  void gpgpu_ptx_sim_memcpy_symbol(const char *hostVar, const void *src,
+                                   size_t count, size_t offset, int to,
+                                   gpgpu_t *gpu);
+  void ptx_print_insn(address_type pc, FILE *fp);
+  std::string ptx_get_insn_str(address_type pc);
+  template <int activate_level>
+  bool ptx_debug_exec_dump_cond(int thd_uid, addr_t pc);
+};
+
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.cc
index 9ac07270b2..1da6b85871 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.cc
@@ -7,108 +7,112 @@
 //
 // Redistributions of source code must retain the above copyright notice, this
 // list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// Redistributions in binary form must reproduce the above copyright notice,
+// this list of conditions and the following disclaimer in the documentation
+// and/or other materials provided with the distribution. Neither the name of
+// The University of British Columbia nor the names of its contributors may be
+// used to endorse or promote products derived from this software without
+// specific prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
 
 #include "cuda_device_printf.h"
 #include "ptx_ir.h"
 
-void decode_space( memory_space_t &space, ptx_thread_info *thread, const operand_info &op, memory_space *&mem, addr_t &addr);
+void decode_space(memory_space_t &space, ptx_thread_info *thread,
+                  const operand_info &op, memory_space *&mem, addr_t &addr);
 
-void my_cuda_printf(const char *fmtstr,const char *arg_list)
-{
-   FILE *fp = stdout;
-   unsigned i=0,j=0;
-   unsigned arg_offset=0;
-   char buf[64];
-   bool in_fmt=false;
-   while( fmtstr[i] ) {
-      char c = fmtstr[i++];
-      if( !in_fmt ) {
-         if( c != '%' ) {
-            fprintf(fp,"%c",c);
-         } else {
-            in_fmt=true;
-            buf[0] = c;
-            j=1;
-         }
+void my_cuda_printf(const char *fmtstr, const char *arg_list) {
+  FILE *fp = stdout;
+  unsigned i = 0, j = 0;
+  unsigned arg_offset = 0;
+  char buf[64];
+  bool in_fmt = false;
+  while (fmtstr[i]) {
+    char c = fmtstr[i++];
+    if (!in_fmt) {
+      if (c != '%') {
+        fprintf(fp, "%c", c);
       } else {
-         if(!( c == 'u' || c == 'f' || c == 'd' )) {
-            printf("GPGPU-Sim PTX: ERROR ** printf parsing support is limited to %%u, %%f, %%d at present");
-            abort();
-         }
-         buf[j] = c;
-         buf[j+1] = 0;
-         void* ptr = (void*)&arg_list[arg_offset];
-         //unsigned long long value = ((unsigned long long*)arg_list)[arg_offset];
-         if( c == 'u' || c == 'd' ) {
-            fprintf(fp,buf,*((unsigned long long*)ptr));
-         } else if( c == 'f' ) {
-            double tmp = *((double*)ptr);
-            fprintf(fp,buf,tmp);
-         }
-         arg_offset++;
-         in_fmt=false;
+        in_fmt = true;
+        buf[0] = c;
+        j = 1;
       }
-   }
+    } else {
+      if (!(c == 'u' || c == 'f' || c == 'd')) {
+        printf(
+            "GPGPU-Sim PTX: ERROR ** printf parsing support is limited to %%u, "
+            "%%f, %%d at present");
+        abort();
+      }
+      buf[j] = c;
+      buf[j + 1] = 0;
+      void *ptr = (void *)&arg_list[arg_offset];
+      // unsigned long long value = ((unsigned long long*)arg_list)[arg_offset];
+      if (c == 'u' || c == 'd') {
+        fprintf(fp, buf, *((unsigned long long *)ptr));
+      } else if (c == 'f') {
+        double tmp = *((double *)ptr);
+        fprintf(fp, buf, tmp);
+      }
+      arg_offset++;
+      in_fmt = false;
+    }
+  }
 }
 
-void gpgpusim_cuda_vprintf(const ptx_instruction * pI, ptx_thread_info * thread, const function_info * target_func ) 
-{
-      char *fmtstr = NULL;
-      char *arg_list = NULL;
-      unsigned n_return = target_func->has_return();
-      unsigned n_args = target_func->num_args();
-      assert( n_args == 2 );
-      for( unsigned arg=0; arg < n_args; arg ++ ) {
-         const operand_info &actual_param_op = pI->operand_lookup(n_return+1+arg);
-         const symbol *formal_param = target_func->get_arg(arg);
-         unsigned size=formal_param->get_size_in_bytes();
-         assert( formal_param->is_param_local() );
-         assert( actual_param_op.is_param_local() );
-         addr_t from_addr = actual_param_op.get_symbol()->get_address();
-         unsigned long long buffer[1024];
-         assert(size<1024*sizeof(unsigned long long));
-         thread->m_local_mem->read(from_addr,size,buffer);
-         addr_t addr = (addr_t)buffer[0]; // should be pointer to generic memory location
-         memory_space *mem=NULL;
-         memory_space_t space = generic_space;
-         decode_space(space,thread,actual_param_op,mem,addr); // figure out which space
-         if( arg == 0 ) {
-            unsigned len = 0;
-            char b = 0;
-            do { // figure out length
-               mem->read(addr+len,1,&b);
-               len++;
-            } while(b);
-            fmtstr = (char*)malloc(len+64);
-            for( int i=0; i < len; i++ ) 
-               mem->read(addr+i,1,fmtstr+i);
-            //mem->read(addr,len,fmtstr);
-         } else {
-            unsigned len = thread->get_finfo()->local_mem_framesize();
-            arg_list = (char*)malloc(len+64);
-            for( int i=0; i < len; i++ ) 
-               mem->read(addr+i,1,arg_list+i);
-            //mem->read(addr,len,arg_list);
-         }
-      }
-      my_cuda_printf(fmtstr,arg_list);
-      free(fmtstr);
-      free(arg_list);
+void gpgpusim_cuda_vprintf(const ptx_instruction *pI, ptx_thread_info *thread,
+                           const function_info *target_func) {
+  char *fmtstr = NULL;
+  char *arg_list = NULL;
+  unsigned n_return = target_func->has_return();
+  unsigned n_args = target_func->num_args();
+  assert(n_args == 2);
+  for (unsigned arg = 0; arg < n_args; arg++) {
+    const operand_info &actual_param_op =
+        pI->operand_lookup(n_return + 1 + arg);
+    const symbol *formal_param = target_func->get_arg(arg);
+    unsigned size = formal_param->get_size_in_bytes();
+    assert(formal_param->is_param_local());
+    assert(actual_param_op.is_param_local());
+    addr_t from_addr = actual_param_op.get_symbol()->get_address();
+    unsigned long long buffer[1024];
+    assert(size < 1024 * sizeof(unsigned long long));
+    thread->m_local_mem->read(from_addr, size, buffer);
+    addr_t addr =
+        (addr_t)buffer[0];  // should be pointer to generic memory location
+    memory_space *mem = NULL;
+    memory_space_t space = generic_space;
+    decode_space(space, thread, actual_param_op, mem,
+                 addr);  // figure out which space
+    if (arg == 0) {
+      unsigned len = 0;
+      char b = 0;
+      do {  // figure out length
+        mem->read(addr + len, 1, &b);
+        len++;
+      } while (b);
+      fmtstr = (char *)malloc(len + 64);
+      for (int i = 0; i < len; i++) mem->read(addr + i, 1, fmtstr + i);
+      // mem->read(addr,len,fmtstr);
+    } else {
+      unsigned len = thread->get_finfo()->local_mem_framesize();
+      arg_list = (char *)malloc(len + 64);
+      for (int i = 0; i < len; i++) mem->read(addr + i, 1, arg_list + i);
+      // mem->read(addr,len,arg_list);
+    }
+  }
+  my_cuda_printf(fmtstr, arg_list);
+  free(fmtstr);
+  free(arg_list);
 }
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.h
index 4e9baaae0b..c4ee75dbe2 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_printf.h
@@ -7,27 +7,30 @@
 //
 // Redistributions of source code must retain the above copyright notice, this
 // list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// Redistributions in binary form must reproduce the above copyright notice,
+// this list of conditions and the following disclaimer in the documentation
+// and/or other materials provided with the distribution. Neither the name of
+// The University of British Columbia nor the names of its contributors may be
+// used to endorse or promote products derived from this software without
+// specific prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
 
 #ifndef CUDA_DEVICE_PRINTF_INCLUDED
 #define CUDA_DEVICE_PRINTF_INCLUDED
 
-void gpgpusim_cuda_vprintf(const class ptx_instruction * pI, class ptx_thread_info * thread, const class function_info * target_func );
+void gpgpusim_cuda_vprintf(const class ptx_instruction* pI,
+                           class ptx_thread_info* thread,
+                           const class function_info* target_func);
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.cc
index 917e7a85d7..9912e703d1 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.cc
@@ -1,335 +1,327 @@
-//Jin: cuda_device_runtime.cc
-//Defines CUDA device runtime APIs for CDP support
-
+// Jin: cuda_device_runtime.cc
+// Defines CUDA device runtime APIs for CDP support
 
 #include <iostream>
 #include <map>
 
-unsigned long long g_total_param_size = 0;
-unsigned long long g_max_total_param_size = 0;
-
-
 #if (CUDART_VERSION >= 5000)
 #define __CUDA_RUNTIME_API_H__
 
 #include <builtin_types.h>
 #include <driver_types.h>
+#include "../../libcuda_sim/gpgpu_context.h"
 #include "../gpgpu-sim/gpu-sim.h"
-#include "cuda-sim.h"
-#include "ptx_ir.h"
+#include "../gpgpusim_entrypoint.h"
 #include "../stream_manager.h"
+#include "cuda-sim.h"
 #include "cuda_device_runtime.h"
+#include "ptx_ir.h"
 
-#define DEV_RUNTIME_REPORT(a) \
-   if( g_debug_execution ) { \
-      std::cout << __FILE__ << ", " << __LINE__ << ": " << a << "\n"; \
-      std::cout.flush(); \
-   }
-
-class device_launch_config_t {
-
-public:
-    device_launch_config_t() {}
-
-    device_launch_config_t(dim3 _grid_dim,
-        dim3 _block_dim,
-        unsigned int _shared_mem,
-        function_info * _entry):
-            grid_dim(_grid_dim),
-            block_dim(_block_dim),
-            shared_mem(_shared_mem),
-            entry(_entry) {}
-    
-    dim3 grid_dim;
-    dim3 block_dim;
-    unsigned int shared_mem;
-    function_info * entry;
-
-};
-
-class device_launch_operation_t {
-
-public:
-    device_launch_operation_t() {}
-    device_launch_operation_t(kernel_info_t *_grid,
-        CUstream_st * _stream) :
-            grid(_grid), stream(_stream) {}
-
-    kernel_info_t * grid; //a new child grid
-
-    CUstream_st * stream; 
-
-};
-
-
-std::map<void *, device_launch_config_t> g_cuda_device_launch_param_map;
-std::list<device_launch_operation_t> g_cuda_device_launch_op;
-extern stream_manager *g_stream_manager;
-
-//Handling device runtime api:
-//void * cudaGetParameterBufferV2(void *func, dim3 gridDimension, dim3 blockDimension, unsigned int sharedMemSize)
-void gpgpusim_cuda_getParameterBufferV2(const ptx_instruction * pI, ptx_thread_info * thread, const function_info * target_func)
-{
-    DEV_RUNTIME_REPORT("Calling cudaGetParameterBufferV2");
-      
-    unsigned n_return = target_func->has_return();
-    assert(n_return);
-    unsigned n_args = target_func->num_args();
-    assert( n_args == 4 );
-
-    function_info * child_kernel_entry;
-    struct dim3 grid_dim, block_dim;
-    unsigned int shared_mem;
-
-    for( unsigned arg=0; arg < n_args; arg ++ ) {
-        const operand_info &actual_param_op = pI->operand_lookup(n_return+1+arg); //param#
-        const symbol *formal_param = target_func->get_arg(arg); //cudaGetParameterBufferV2_param_#
-        unsigned size=formal_param->get_size_in_bytes();
-        assert( formal_param->is_param_local() );
-        assert( actual_param_op.is_param_local() );
-        addr_t from_addr = actual_param_op.get_symbol()->get_address();
-
-        if(arg == 0) {//function_info* for the child kernel
-            unsigned long long buf;
-            assert(size == sizeof(function_info *));
-            thread->m_local_mem->read(from_addr, size, &buf);
-            child_kernel_entry = (function_info *)buf;
-            assert(child_kernel_entry);
-            DEV_RUNTIME_REPORT("child kernel name " << child_kernel_entry->get_name());
-        }
-        else if(arg == 1) { //dim3 grid_dim for the child kernel
-            assert(size == sizeof(struct dim3));
-            thread->m_local_mem->read(from_addr, size, & grid_dim);
-            DEV_RUNTIME_REPORT("grid (" << grid_dim.x << ", " << grid_dim.y << ", " << grid_dim.z << ")");
-        }
-        else if(arg == 2) { //dim3 block_dim for the child kernel
-            assert(size == sizeof(struct dim3));
-            thread->m_local_mem->read(from_addr, size, & block_dim);
-            DEV_RUNTIME_REPORT("block (" << block_dim.x << ", " << block_dim.y << ", " << block_dim.z << ")");
-        }
-        else if(arg == 3) { //unsigned int shared_mem
-            assert(size == sizeof(unsigned int));
-            thread->m_local_mem->read(from_addr, size, & shared_mem);
-            DEV_RUNTIME_REPORT("shared memory " << shared_mem);
-        }
+#define DEV_RUNTIME_REPORT(a)                                       \
+  if (g_debug_execution) {                                          \
+    std::cout << __FILE__ << ", " << __LINE__ << ": " << a << "\n"; \
+    std::cout.flush();                                              \
+  }
+
+// Handling device runtime api:
+// void * cudaGetParameterBufferV2(void *func, dim3 gridDimension, dim3
+// blockDimension, unsigned int sharedMemSize)
+void cuda_device_runtime::gpgpusim_cuda_getParameterBufferV2(
+    const ptx_instruction *pI, ptx_thread_info *thread,
+    const function_info *target_func) {
+  DEV_RUNTIME_REPORT("Calling cudaGetParameterBufferV2");
+
+  unsigned n_return = target_func->has_return();
+  assert(n_return);
+  unsigned n_args = target_func->num_args();
+  assert(n_args == 4);
+
+  function_info *child_kernel_entry;
+  struct dim3 grid_dim, block_dim;
+  unsigned int shared_mem;
+
+  for (unsigned arg = 0; arg < n_args; arg++) {
+    const operand_info &actual_param_op =
+        pI->operand_lookup(n_return + 1 + arg);  // param#
+    const symbol *formal_param =
+        target_func->get_arg(arg);  // cudaGetParameterBufferV2_param_#
+    unsigned size = formal_param->get_size_in_bytes();
+    assert(formal_param->is_param_local());
+    assert(actual_param_op.is_param_local());
+    addr_t from_addr = actual_param_op.get_symbol()->get_address();
+
+    if (arg == 0) {  // function_info* for the child kernel
+      unsigned long long buf;
+      assert(size == sizeof(function_info *));
+      thread->m_local_mem->read(from_addr, size, &buf);
+      child_kernel_entry = (function_info *)buf;
+      assert(child_kernel_entry);
+      DEV_RUNTIME_REPORT("child kernel name "
+                         << child_kernel_entry->get_name());
+    } else if (arg == 1) {  // dim3 grid_dim for the child kernel
+      assert(size == sizeof(struct dim3));
+      thread->m_local_mem->read(from_addr, size, &grid_dim);
+      DEV_RUNTIME_REPORT("grid (" << grid_dim.x << ", " << grid_dim.y << ", "
+                                  << grid_dim.z << ")");
+    } else if (arg == 2) {  // dim3 block_dim for the child kernel
+      assert(size == sizeof(struct dim3));
+      thread->m_local_mem->read(from_addr, size, &block_dim);
+      DEV_RUNTIME_REPORT("block (" << block_dim.x << ", " << block_dim.y << ", "
+                                   << block_dim.z << ")");
+    } else if (arg == 3) {  // unsigned int shared_mem
+      assert(size == sizeof(unsigned int));
+      thread->m_local_mem->read(from_addr, size, &shared_mem);
+      DEV_RUNTIME_REPORT("shared memory " << shared_mem);
     }
-
-    //get total child kernel argument size and malloc buffer in global memory
-    unsigned child_kernel_arg_size = child_kernel_entry->get_args_aligned_size();
-    void * param_buffer = thread->get_gpu()->gpu_malloc(child_kernel_arg_size);
-    g_total_param_size += ((child_kernel_arg_size + 255) / 256 * 256);
-    DEV_RUNTIME_REPORT("child kernel arg size total " << child_kernel_arg_size << ", parameter buffer allocated at " << param_buffer);
-    if(g_total_param_size > g_max_total_param_size)
-        g_max_total_param_size = g_total_param_size;
-
-    //store param buffer address and launch config
-    device_launch_config_t device_launch_config(grid_dim, block_dim, shared_mem, child_kernel_entry);
-    assert(g_cuda_device_launch_param_map.find(param_buffer) == g_cuda_device_launch_param_map.end());
-    g_cuda_device_launch_param_map[param_buffer] = device_launch_config;
-
-    //copy the buffer address to retval0
-    const operand_info &actual_return_op = pI->operand_lookup(0); //retval0
-    const symbol *formal_return = target_func->get_return_var(); //void *
-    unsigned int return_size = formal_return->get_size_in_bytes();
-    DEV_RUNTIME_REPORT("cudaGetParameterBufferV2 return value has size of " << return_size);
-    assert(actual_return_op.is_param_local());
-    assert(actual_return_op.get_symbol()->get_size_in_bytes() == return_size && return_size == sizeof(void *));
-    addr_t ret_param_addr = actual_return_op.get_symbol()->get_address();
-    thread->m_local_mem->write(ret_param_addr, return_size, &param_buffer, NULL, NULL);
-
+  }
+
+  // get total child kernel argument size and malloc buffer in global memory
+  unsigned child_kernel_arg_size = child_kernel_entry->get_args_aligned_size();
+  void *param_buffer = thread->get_gpu()->gpu_malloc(child_kernel_arg_size);
+  g_total_param_size += ((child_kernel_arg_size + 255) / 256 * 256);
+  DEV_RUNTIME_REPORT("child kernel arg size total "
+                     << child_kernel_arg_size
+                     << ", parameter buffer allocated at " << param_buffer);
+  if (g_total_param_size > g_max_total_param_size)
+    g_max_total_param_size = g_total_param_size;
+
+  // store param buffer address and launch config
+  device_launch_config_t device_launch_config(grid_dim, block_dim, shared_mem,
+                                              child_kernel_entry);
+  assert(g_cuda_device_launch_param_map.find(param_buffer) ==
+         g_cuda_device_launch_param_map.end());
+  g_cuda_device_launch_param_map[param_buffer] = device_launch_config;
+
+  // copy the buffer address to retval0
+  const operand_info &actual_return_op = pI->operand_lookup(0);  // retval0
+  const symbol *formal_return = target_func->get_return_var();   // void *
+  unsigned int return_size = formal_return->get_size_in_bytes();
+  DEV_RUNTIME_REPORT("cudaGetParameterBufferV2 return value has size of "
+                     << return_size);
+  assert(actual_return_op.is_param_local());
+  assert(actual_return_op.get_symbol()->get_size_in_bytes() == return_size &&
+         return_size == sizeof(void *));
+  addr_t ret_param_addr = actual_return_op.get_symbol()->get_address();
+  thread->m_local_mem->write(ret_param_addr, return_size, &param_buffer, NULL,
+                             NULL);
 }
 
-//Handling device runtime api:
-//cudaError_t cudaLaunchDeviceV2(void *parameterBuffer, cudaStream_t stream)
-void gpgpusim_cuda_launchDeviceV2(const ptx_instruction * pI, ptx_thread_info * thread, const function_info * target_func) {
-    DEV_RUNTIME_REPORT("Calling cudaLaunchDeviceV2");
-
-    unsigned n_return = target_func->has_return();
-    assert(n_return);
-    unsigned n_args = target_func->num_args();
-    assert( n_args == 2 );
-
-    kernel_info_t * device_grid = NULL;
-    function_info * device_kernel_entry = NULL;
-    void * parameter_buffer;
-    struct CUstream_st * child_stream;
-    device_launch_config_t config;
-    device_launch_operation_t device_launch_op;
-
-    for( unsigned arg=0; arg < n_args; arg ++ ) {
-        const operand_info &actual_param_op = pI->operand_lookup(n_return+1+arg); //param#
-        const symbol *formal_param = target_func->get_arg(arg); //cudaLaunchDeviceV2_param_#
-        unsigned size=formal_param->get_size_in_bytes();
-        assert( formal_param->is_param_local() );
-        assert( actual_param_op.is_param_local() );
-        addr_t from_addr = actual_param_op.get_symbol()->get_address();
-
-        if(arg == 0) {//paramter buffer for child kernel (in global memory)
-            //get parameter_buffer from the cudaLaunchDeviceV2_param0
-            assert(size == sizeof(void *));
-            thread->m_local_mem->read(from_addr, size, &parameter_buffer);
-            assert((size_t)parameter_buffer >= GLOBAL_HEAP_START);
-            DEV_RUNTIME_REPORT("Parameter buffer locating at global memory " << parameter_buffer);
-
-            //get child grid info through parameter_buffer address
-            assert(g_cuda_device_launch_param_map.find(parameter_buffer) != g_cuda_device_launch_param_map.end());
-            config = g_cuda_device_launch_param_map[parameter_buffer];
-            //device_grid = op.grid;
-            device_kernel_entry = config.entry;
-            DEV_RUNTIME_REPORT("find device kernel " << device_kernel_entry->get_name());
-	    
-	    //PDOM analysis is done for Parent kernel but not for child kernel.
-	    if (device_kernel_entry->is_pdom_set()) {
-		    printf("GPGPU-Sim PTX: PDOM analysis already done for %s \n", device_kernel_entry->get_name().c_str() );
-	    } else {
-		    printf("GPGPU-Sim PTX: finding reconvergence points for \'%s\'...\n", device_kernel_entry->get_name().c_str() );
-		    /*
-		     * Some of the instructions like printf() gives the gpgpusim the wrong impression that it is a function call.
-		     * As printf() doesnt have a body like functions do, doing pdom analysis for printf() causes a crash.
-		     */
-		    if (device_kernel_entry->get_function_size() >0)
-			    device_kernel_entry->do_pdom();
-		    device_kernel_entry->set_pdom();
-	    }
-
-            //copy data in parameter_buffer to device kernel param memory
-            unsigned device_kernel_arg_size = device_kernel_entry->get_args_aligned_size();
-            DEV_RUNTIME_REPORT("device_kernel_arg_size " << device_kernel_arg_size);
-            memory_space *device_kernel_param_mem;
-
-            //create child kernel_info_t and index it with parameter_buffer address
-	    gpgpu_t* gpu=thread->get_gpu();
-            device_grid = new kernel_info_t(config.grid_dim, config.block_dim, device_kernel_entry, gpu->getNameArrayMapping(), gpu->getNameInfoMapping());
-            device_grid->launch_cycle = gpu_sim_cycle + gpu_tot_sim_cycle;
-            kernel_info_t & parent_grid = thread->get_kernel();
-            DEV_RUNTIME_REPORT("child kernel launched by " << parent_grid.name() << ", cta (" <<
-                thread->get_ctaid().x << ", " << thread->get_ctaid().y << ", " << thread->get_ctaid().z <<
-                "), thread (" << thread->get_tid().x << ", " << thread->get_tid().y << ", " << thread->get_tid().z <<
-                ")");
-            device_grid->set_parent(&parent_grid, thread->get_ctaid(), thread->get_tid());  
-            device_launch_op = device_launch_operation_t(device_grid, NULL);
-            device_kernel_param_mem = device_grid->get_param_memory(); //kernel param
-            size_t param_start_address = 0;
-            //copy in word
-            for(unsigned n = 0; n < device_kernel_arg_size; n += 4) {
-                unsigned int oneword;
-                thread->get_gpu()->get_global_memory()->read((size_t)parameter_buffer + n, 4, &oneword);
-                device_kernel_param_mem->write(param_start_address + n, 4, &oneword, NULL, NULL); 
-            }
-        }
-        else if(arg == 1) { //cudaStream for the child kernel
-
-            assert(size == sizeof(cudaStream_t));
-            thread->m_local_mem->read(from_addr, size, &child_stream);
-     
-            kernel_info_t & parent_kernel = thread->get_kernel();
-            if(child_stream == 0) { //default stream on device for current CTA
-                child_stream = parent_kernel.get_default_stream_cta(thread->get_ctaid()); 
-                DEV_RUNTIME_REPORT("launching child kernel " << device_grid->get_uid() << 
-                    " to default stream of the cta " << child_stream->get_uid() << ": " << child_stream);
-            }
-            else {
-               assert(parent_kernel.cta_has_stream(thread->get_ctaid(), child_stream)); 
-                DEV_RUNTIME_REPORT("launching child kernel " << device_grid->get_uid() << 
-                " to stream " << child_stream->get_uid() << ": " << child_stream);
-            }
-    
-            device_launch_op.stream = child_stream;
-        }
-        
+// Handling device runtime api:
+// cudaError_t cudaLaunchDeviceV2(void *parameterBuffer, cudaStream_t stream)
+void cuda_device_runtime::gpgpusim_cuda_launchDeviceV2(
+    const ptx_instruction *pI, ptx_thread_info *thread,
+    const function_info *target_func) {
+  DEV_RUNTIME_REPORT("Calling cudaLaunchDeviceV2");
+
+  unsigned n_return = target_func->has_return();
+  assert(n_return);
+  unsigned n_args = target_func->num_args();
+  assert(n_args == 2);
+
+  kernel_info_t *device_grid = NULL;
+  function_info *device_kernel_entry = NULL;
+  void *parameter_buffer;
+  struct CUstream_st *child_stream;
+  device_launch_config_t config;
+  device_launch_operation_t device_launch_op;
+
+  for (unsigned arg = 0; arg < n_args; arg++) {
+    const operand_info &actual_param_op =
+        pI->operand_lookup(n_return + 1 + arg);  // param#
+    const symbol *formal_param =
+        target_func->get_arg(arg);  // cudaLaunchDeviceV2_param_#
+    unsigned size = formal_param->get_size_in_bytes();
+    assert(formal_param->is_param_local());
+    assert(actual_param_op.is_param_local());
+    addr_t from_addr = actual_param_op.get_symbol()->get_address();
+
+    if (arg == 0) {  // paramter buffer for child kernel (in global memory)
+      // get parameter_buffer from the cudaLaunchDeviceV2_param0
+      assert(size == sizeof(void *));
+      thread->m_local_mem->read(from_addr, size, &parameter_buffer);
+      assert((size_t)parameter_buffer >= GLOBAL_HEAP_START);
+      DEV_RUNTIME_REPORT("Parameter buffer locating at global memory "
+                         << parameter_buffer);
+
+      // get child grid info through parameter_buffer address
+      assert(g_cuda_device_launch_param_map.find(parameter_buffer) !=
+             g_cuda_device_launch_param_map.end());
+      config = g_cuda_device_launch_param_map[parameter_buffer];
+      // device_grid = op.grid;
+      device_kernel_entry = config.entry;
+      DEV_RUNTIME_REPORT("find device kernel "
+                         << device_kernel_entry->get_name());
+
+      // PDOM analysis is done for Parent kernel but not for child kernel.
+      if (device_kernel_entry->is_pdom_set()) {
+        printf("GPGPU-Sim PTX: PDOM analysis already done for %s \n",
+               device_kernel_entry->get_name().c_str());
+      } else {
+        printf("GPGPU-Sim PTX: finding reconvergence points for \'%s\'...\n",
+               device_kernel_entry->get_name().c_str());
+        /*
+         * Some of the instructions like printf() gives the gpgpusim the wrong
+         * impression that it is a function call. As printf() doesnt have a body
+         * like functions do, doing pdom analysis for printf() causes a crash.
+         */
+        if (device_kernel_entry->get_function_size() > 0)
+          device_kernel_entry->do_pdom();
+        device_kernel_entry->set_pdom();
+      }
+
+      // copy data in parameter_buffer to device kernel param memory
+      unsigned device_kernel_arg_size =
+          device_kernel_entry->get_args_aligned_size();
+      DEV_RUNTIME_REPORT("device_kernel_arg_size " << device_kernel_arg_size);
+      memory_space *device_kernel_param_mem;
+
+      // create child kernel_info_t and index it with parameter_buffer address
+      gpgpu_t *gpu = thread->get_gpu();
+      device_grid = new kernel_info_t(
+          config.grid_dim, config.block_dim, device_kernel_entry,
+          gpu->getNameArrayMapping(), gpu->getNameInfoMapping());
+      device_grid->launch_cycle = gpu->gpu_sim_cycle + gpu->gpu_tot_sim_cycle;
+      kernel_info_t &parent_grid = thread->get_kernel();
+      DEV_RUNTIME_REPORT(
+          "child kernel launched by "
+          << parent_grid.name() << ", cta (" << thread->get_ctaid().x << ", "
+          << thread->get_ctaid().y << ", " << thread->get_ctaid().z
+          << "), thread (" << thread->get_tid().x << ", " << thread->get_tid().y
+          << ", " << thread->get_tid().z << ")");
+      device_grid->set_parent(&parent_grid, thread->get_ctaid(),
+                              thread->get_tid());
+      device_launch_op = device_launch_operation_t(device_grid, NULL);
+      device_kernel_param_mem = device_grid->get_param_memory();  // kernel
+                                                                  // param
+      size_t param_start_address = 0;
+      // copy in word
+      for (unsigned n = 0; n < device_kernel_arg_size; n += 4) {
+        unsigned int oneword;
+        thread->get_gpu()->get_global_memory()->read(
+            (size_t)parameter_buffer + n, 4, &oneword);
+        device_kernel_param_mem->write(param_start_address + n, 4, &oneword,
+                                       NULL, NULL);
+      }
+    } else if (arg == 1) {  // cudaStream for the child kernel
+
+      assert(size == sizeof(cudaStream_t));
+      thread->m_local_mem->read(from_addr, size, &child_stream);
+
+      kernel_info_t &parent_kernel = thread->get_kernel();
+      if (child_stream == 0) {  // default stream on device for current CTA
+        child_stream =
+            parent_kernel.get_default_stream_cta(thread->get_ctaid());
+        DEV_RUNTIME_REPORT("launching child kernel "
+                           << device_grid->get_uid()
+                           << " to default stream of the cta "
+                           << child_stream->get_uid() << ": " << child_stream);
+      } else {
+        assert(parent_kernel.cta_has_stream(thread->get_ctaid(), child_stream));
+        DEV_RUNTIME_REPORT("launching child kernel "
+                           << device_grid->get_uid() << " to stream "
+                           << child_stream->get_uid() << ": " << child_stream);
+      }
+
+      device_launch_op.stream = child_stream;
     }
-
-  
-    //launch child kernel
-    g_cuda_device_launch_op.push_back(device_launch_op);
-    g_cuda_device_launch_param_map.erase(parameter_buffer);
-
-    //set retval0
-    const operand_info &actual_return_op = pI->operand_lookup(0); //retval0
-    const symbol *formal_return = target_func->get_return_var(); //cudaError_t
-    unsigned int return_size = formal_return->get_size_in_bytes();
-    DEV_RUNTIME_REPORT("cudaLaunchDeviceV2 return value has size of " << return_size);
-    assert(actual_return_op.is_param_local());
-    assert(actual_return_op.get_symbol()->get_size_in_bytes() == return_size 
-        && return_size == sizeof(cudaError_t));
-    cudaError_t error = cudaSuccess;
-    addr_t ret_param_addr = actual_return_op.get_symbol()->get_address();
-    thread->m_local_mem->write(ret_param_addr, return_size, &error, NULL, NULL);
-
+  }
+
+  // launch child kernel
+  g_cuda_device_launch_op.push_back(device_launch_op);
+  g_cuda_device_launch_param_map.erase(parameter_buffer);
+
+  // set retval0
+  const operand_info &actual_return_op = pI->operand_lookup(0);  // retval0
+  const symbol *formal_return = target_func->get_return_var();   // cudaError_t
+  unsigned int return_size = formal_return->get_size_in_bytes();
+  DEV_RUNTIME_REPORT("cudaLaunchDeviceV2 return value has size of "
+                     << return_size);
+  assert(actual_return_op.is_param_local());
+  assert(actual_return_op.get_symbol()->get_size_in_bytes() == return_size &&
+         return_size == sizeof(cudaError_t));
+  cudaError_t error = cudaSuccess;
+  addr_t ret_param_addr = actual_return_op.get_symbol()->get_address();
+  thread->m_local_mem->write(ret_param_addr, return_size, &error, NULL, NULL);
 }
 
-
-//Handling device runtime api:
-//cudaError_t cudaStreamCreateWithFlags ( cudaStream_t* pStream, unsigned int  flags)
-//flags can only be cudaStreamNonBlocking
-void gpgpusim_cuda_streamCreateWithFlags(const ptx_instruction * pI, ptx_thread_info * thread, const function_info * target_func) {
-    DEV_RUNTIME_REPORT("Calling cudaStreamCreateWithFlags");
-
-    unsigned n_return = target_func->has_return();
-    assert(n_return);
-    unsigned n_args = target_func->num_args();
-    assert( n_args == 2 );
-
-    size_t generic_pStream_addr;
-    addr_t pStream_addr;
-    unsigned int flags;
-    for( unsigned arg=0; arg < n_args; arg ++ ) {
-        const operand_info &actual_param_op = pI->operand_lookup(n_return+1+arg); //param#
-        const symbol *formal_param = target_func->get_arg(arg); //cudaStreamCreateWithFlags_param_#
-        unsigned size=formal_param->get_size_in_bytes();
-        assert( formal_param->is_param_local() );
-        assert( actual_param_op.is_param_local() );
-        addr_t from_addr = actual_param_op.get_symbol()->get_address();
-
-        if(arg == 0) {//cudaStream_t * pStream, address of cudaStream_t
-            assert(size == sizeof(cudaStream_t *));
-            thread->m_local_mem->read(from_addr, size, &generic_pStream_addr);
-            
-            //pStream should be non-zero address in local memory
-            pStream_addr = generic_to_local(thread->get_hw_sid(), thread->get_hw_tid(), generic_pStream_addr);
-
-            DEV_RUNTIME_REPORT("pStream locating at local memory " << pStream_addr);
-        }
-        else if(arg == 1) { //unsigned int flags, should be cudaStreamNonBlocking
-            assert(size == sizeof(unsigned int));
-            thread->m_local_mem->read(from_addr, size, &flags);
-            assert(flags == cudaStreamNonBlocking);
-        }
+// Handling device runtime api:
+// cudaError_t cudaStreamCreateWithFlags ( cudaStream_t* pStream, unsigned int
+// flags) flags can only be cudaStreamNonBlocking
+void cuda_device_runtime::gpgpusim_cuda_streamCreateWithFlags(
+    const ptx_instruction *pI, ptx_thread_info *thread,
+    const function_info *target_func) {
+  DEV_RUNTIME_REPORT("Calling cudaStreamCreateWithFlags");
+
+  unsigned n_return = target_func->has_return();
+  assert(n_return);
+  unsigned n_args = target_func->num_args();
+  assert(n_args == 2);
+
+  size_t generic_pStream_addr;
+  addr_t pStream_addr;
+  unsigned int flags;
+  for (unsigned arg = 0; arg < n_args; arg++) {
+    const operand_info &actual_param_op =
+        pI->operand_lookup(n_return + 1 + arg);  // param#
+    const symbol *formal_param =
+        target_func->get_arg(arg);  // cudaStreamCreateWithFlags_param_#
+    unsigned size = formal_param->get_size_in_bytes();
+    assert(formal_param->is_param_local());
+    assert(actual_param_op.is_param_local());
+    addr_t from_addr = actual_param_op.get_symbol()->get_address();
+
+    if (arg == 0) {  // cudaStream_t * pStream, address of cudaStream_t
+      assert(size == sizeof(cudaStream_t *));
+      thread->m_local_mem->read(from_addr, size, &generic_pStream_addr);
+
+      // pStream should be non-zero address in local memory
+      pStream_addr = generic_to_local(
+          thread->get_hw_sid(), thread->get_hw_tid(), generic_pStream_addr);
+
+      DEV_RUNTIME_REPORT("pStream locating at local memory " << pStream_addr);
+    } else if (arg ==
+               1) {  // unsigned int flags, should be cudaStreamNonBlocking
+      assert(size == sizeof(unsigned int));
+      thread->m_local_mem->read(from_addr, size, &flags);
+      assert(flags == cudaStreamNonBlocking);
     }
-
-    //create stream and write back to param0
-    CUstream_st * stream = thread->get_kernel().create_stream_cta(thread->get_ctaid());
-    DEV_RUNTIME_REPORT("Create stream " << stream->get_uid() << ": " << stream);
-    thread->m_local_mem->write(pStream_addr, sizeof(cudaStream_t), &stream, NULL, NULL);
- 
-    //set retval0
-    const operand_info &actual_return_op = pI->operand_lookup(0); //retval0
-    const symbol *formal_return = target_func->get_return_var(); //cudaError_t
-    unsigned int return_size = formal_return->get_size_in_bytes();
-    DEV_RUNTIME_REPORT("cudaStreamCreateWithFlags return value has size of " << return_size);
-    assert(actual_return_op.is_param_local());
-    assert(actual_return_op.get_symbol()->get_size_in_bytes() == return_size 
-        && return_size == sizeof(cudaError_t));
-    cudaError_t error = cudaSuccess;
-    addr_t ret_param_addr = actual_return_op.get_symbol()->get_address();
-    thread->m_local_mem->write(ret_param_addr, return_size, &error, NULL, NULL);
-
+  }
+
+  // create stream and write back to param0
+  CUstream_st *stream =
+      thread->get_kernel().create_stream_cta(thread->get_ctaid());
+  DEV_RUNTIME_REPORT("Create stream " << stream->get_uid() << ": " << stream);
+  thread->m_local_mem->write(pStream_addr, sizeof(cudaStream_t), &stream, NULL,
+                             NULL);
+
+  // set retval0
+  const operand_info &actual_return_op = pI->operand_lookup(0);  // retval0
+  const symbol *formal_return = target_func->get_return_var();   // cudaError_t
+  unsigned int return_size = formal_return->get_size_in_bytes();
+  DEV_RUNTIME_REPORT("cudaStreamCreateWithFlags return value has size of "
+                     << return_size);
+  assert(actual_return_op.is_param_local());
+  assert(actual_return_op.get_symbol()->get_size_in_bytes() == return_size &&
+         return_size == sizeof(cudaError_t));
+  cudaError_t error = cudaSuccess;
+  addr_t ret_param_addr = actual_return_op.get_symbol()->get_address();
+  thread->m_local_mem->write(ret_param_addr, return_size, &error, NULL, NULL);
 }
 
+void cuda_device_runtime::launch_one_device_kernel() {
+  if (!g_cuda_device_launch_op.empty()) {
+    device_launch_operation_t &op = g_cuda_device_launch_op.front();
 
-void launch_one_device_kernel() {
-    if(!g_cuda_device_launch_op.empty()) {
-        device_launch_operation_t &op = g_cuda_device_launch_op.front();
-
-        stream_operation stream_op = stream_operation(op.grid, g_ptx_sim_mode, op.stream);
-        g_stream_manager->push(stream_op);
-        g_cuda_device_launch_op.pop_front();
-    }
+    stream_operation stream_op = stream_operation(
+        op.grid, gpgpu_ctx->func_sim->g_ptx_sim_mode, op.stream);
+    gpgpu_ctx->the_gpgpusim->g_stream_manager->push(stream_op);
+    g_cuda_device_launch_op.pop_front();
+  }
 }
 
-void launch_all_device_kernels() {
-    while(!g_cuda_device_launch_op.empty()) {
-        launch_one_device_kernel();
-    }
+void cuda_device_runtime::launch_all_device_kernels() {
+  while (!g_cuda_device_launch_op.empty()) {
+    launch_one_device_kernel();
+  }
 }
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.h
index 6dbcd716d7..b06dd24747 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/cuda_device_runtime.h
@@ -1,11 +1,68 @@
-//Jin: cuda_device_runtime.h
-//Defines CUDA device runtime APIs for CDP support
+#ifndef __cuda_device_runtime_h__
+#define __cuda_device_runtime_h__
+// Jin: cuda_device_runtime.h
+// Defines CUDA device runtime APIs for CDP support
+class device_launch_config_t {
+ public:
+  device_launch_config_t() {}
+
+  device_launch_config_t(dim3 _grid_dim, dim3 _block_dim,
+                         unsigned int _shared_mem, function_info* _entry)
+      : grid_dim(_grid_dim),
+        block_dim(_block_dim),
+        shared_mem(_shared_mem),
+        entry(_entry) {}
+
+  dim3 grid_dim;
+  dim3 block_dim;
+  unsigned int shared_mem;
+  function_info* entry;
+};
+
+class device_launch_operation_t {
+ public:
+  device_launch_operation_t() {}
+  device_launch_operation_t(kernel_info_t* _grid, CUstream_st* _stream)
+      : grid(_grid), stream(_stream) {}
+
+  kernel_info_t* grid;  // a new child grid
+
+  CUstream_st* stream;
+};
+
+class gpgpu_context;
+
+class cuda_device_runtime {
+ public:
+  cuda_device_runtime(gpgpu_context* ctx) {
+    g_total_param_size = 0;
+    g_max_total_param_size = 0;
+    gpgpu_ctx = ctx;
+  }
+  unsigned long long g_total_param_size;
+  std::map<void*, device_launch_config_t> g_cuda_device_launch_param_map;
+  std::list<device_launch_operation_t> g_cuda_device_launch_op;
+  unsigned g_kernel_launch_latency;
+  unsigned g_TB_launch_latency;
+  unsigned long long g_max_total_param_size;
+  bool g_cdp_enabled;
+
+  // backward pointer
+  class gpgpu_context* gpgpu_ctx;
 #if (CUDART_VERSION >= 5000)
 #pragma once
-
-void gpgpusim_cuda_getParameterBufferV2(const ptx_instruction * pI, ptx_thread_info * thread, const function_info * target_func);
-void gpgpusim_cuda_launchDeviceV2(const ptx_instruction * pI, ptx_thread_info * thread, const function_info * target_func);
-void gpgpusim_cuda_streamCreateWithFlags(const ptx_instruction * pI, ptx_thread_info * thread, const function_info * target_func);
-void launch_all_device_kernels();
-void launch_one_device_kernel();
+  void gpgpusim_cuda_launchDeviceV2(const ptx_instruction* pI,
+                                    ptx_thread_info* thread,
+                                    const function_info* target_func);
+  void gpgpusim_cuda_streamCreateWithFlags(const ptx_instruction* pI,
+                                           ptx_thread_info* thread,
+                                           const function_info* target_func);
+  void gpgpusim_cuda_getParameterBufferV2(const ptx_instruction* pI,
+                                          ptx_thread_info* thread,
+                                          const function_info* target_func);
+  void launch_all_device_kernels();
+  void launch_one_device_kernel();
 #endif
+};
+
+#endif /* __cuda_device_runtime_h__  */
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.cc
index cd28f33462..5b34dd6acd 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.cc
@@ -25,3037 +25,3368 @@
 // CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#include "instructions.h"
 #include "half.h"
 #include "half.hpp"
-#include "instructions.h"
-#include "instructions_extra.h"
-#include "ptx_ir.h"
 #include "opcodes.h"
+#include "ptx_ir.h"
 #include "ptx_sim.h"
-#include "ptx.tab.h"
-#include <stdlib.h>
+typedef void *yyscan_t;
+class ptx_recognizer;
+#include <assert.h>
+#include <fenv.h>
 #include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
 #include <cmath>
-#include <fenv.h>
-
-#include "cuda-math.h"
-// #include "cuda_runtime_api.h"
+#include <map>
+#include <sstream>
+#include <string>
 #include "../abstract_hardware_model.h"
-#include "ptx_loader.h"
-#include "cuda_device_printf.h"
-
-#ifndef LIBCUDA
 #include "gpu/gpgpu-sim/cuda_gpu.hh"
-
 #include "../gpgpu-sim/gpu-sim.h"
 #include "../gpgpu-sim/shader.h"
-#endif
-
-#include <assert.h>
-#include <string.h>
-#include <sstream>
-#include <stdio.h>
-#include <string>
-#include <map>
-#include <stdlib.h>
+#include "cuda-math.h"
+#include "cuda_device_printf.h"
+#include "ptx.tab.h"
+#include "ptx_loader.h"
 
 //Jin: include device runtime for CDP
 #include "cuda_device_runtime.h"
 
 #include <stdarg.h>
-using half_float::half;
-
-unsigned ptx_instruction::g_num_ptx_inst_uid=0;
-bool debug_tensorcore = 0;
+#include "../../libcuda_sim/gpgpu_context.h"
 
+using half_float::half;
 
 const char *g_opcode_string[NUM_OPCODES] = {
-#define OP_DEF(OP,FUNC,STR,DST,CLASSIFICATION) STR,
-#define OP_W_DEF(OP,FUNC,STR,DST,CLASSIFICATION) STR,
+#define OP_DEF(OP, FUNC, STR, DST, CLASSIFICATION) STR,
+#define OP_W_DEF(OP, FUNC, STR, DST, CLASSIFICATION) STR,
 #include "opcodes.def"
 #undef OP_DEF
 #undef OP_W_DEF
 };
-//Using profiled information::check the TensorCoreMatrixArrangement.xls for details
-unsigned thread_group_offset(int thread,unsigned  wmma_type,unsigned wmma_layout,unsigned type,int stride){
-
-	unsigned offset;
-	unsigned load_a_row[8]={0,128,0,128,64,192,64,192};
-	unsigned load_a_col[8]={0,8,0,8,4,12,4,12};
-	unsigned load_b_row[8]={0,8,0,8,4,12,4,12};
-	unsigned load_b_col[8]={0,128,0,128,64,192,64,192};
-	unsigned load_c_float_row[8]={0,128,8,136,64,192,72,200};	
-	unsigned load_c_float_col[8]={0,8,128,136,4,12,132,140};	
-	unsigned load_c_half_row[8]={0,128,8,136,64,192,72,200};	
-	unsigned load_c_half_col[8]={0,8,128,136,4,12,132,140};	
-	unsigned thread_group =	thread/4;
-	unsigned in_tg_index  =	thread%4;
-
-	switch(wmma_type){
-		case LOAD_A:
-				if(wmma_layout==ROW)
-					offset=load_a_row[thread_group]+16*in_tg_index;
-				else
-					offset=load_a_col[thread_group]+16*in_tg_index;
-			break;
-
-
-		case LOAD_B:
-				if(wmma_layout==ROW)
-					offset=load_b_row[thread_group]+16*in_tg_index;
-				else	
-					offset=load_b_col[thread_group]+16*in_tg_index;
-			break;	
-
-		case LOAD_C:
-		case STORE_D:
-			if(type==F16_TYPE){
-				if(wmma_layout==ROW)
-					offset=load_c_half_row[thread_group]+16*in_tg_index;
-				else
-					offset=load_c_half_col[thread_group]+in_tg_index;
-			}
-			else{
-				if(wmma_layout==ROW)
-					offset=load_c_float_row[thread_group];
-				else
-					offset=load_c_float_col[thread_group];
-
-				switch(in_tg_index){
-					case 0:
-						break;
-					case 1:
-						if(wmma_layout==ROW)
-							offset+=16;
-						else
-							offset+=1;
-						break;
-					case 2:
-						if(wmma_layout==ROW)
-							offset+=2;
-						else
-							offset+=32;
-						break;
-					case 3:
-						if(wmma_layout==ROW)
-							offset+=18;
-						else
-							offset+=33;
-						break;
-					default:
-						abort();
-				}
-			}
-			break;	
-
-         	default:
-         	   abort();
-		
-	}
-	offset = (offset/16)*stride+offset%16;
-	return offset;
-}
-
-int acc_float_offset(int index,int wmma_layout,int stride){
-
-	int c_row_offset[]={0,1,32,33,4,5,36,37};
-	int c_col_offset[]={0,16,2,18,64,80,66,82};
-	int offset;
-	
-	
-	if(wmma_layout==ROW)
-		offset=c_row_offset[index];
-	else if(wmma_layout==COL)
-		offset=c_col_offset[index];
-	else{
-		printf("wrong layout");
-		abort();
-	}
-	offset = (offset/16)*stride+offset%16;
-	return offset;
-}
+// Using profiled information::check the TensorCoreMatrixArrangement.xls for
+// details
+unsigned thread_group_offset(int thread, unsigned wmma_type,
+                             unsigned wmma_layout, unsigned type, int stride) {
+  unsigned offset;
+  unsigned load_a_row[8] = {0, 128, 0, 128, 64, 192, 64, 192};
+  unsigned load_a_col[8] = {0, 8, 0, 8, 4, 12, 4, 12};
+  unsigned load_b_row[8] = {0, 8, 0, 8, 4, 12, 4, 12};
+  unsigned load_b_col[8] = {0, 128, 0, 128, 64, 192, 64, 192};
+  unsigned load_c_float_row[8] = {0, 128, 8, 136, 64, 192, 72, 200};
+  unsigned load_c_float_col[8] = {0, 8, 128, 136, 4, 12, 132, 140};
+  unsigned load_c_half_row[8] = {0, 128, 8, 136, 64, 192, 72, 200};
+  unsigned load_c_half_col[8] = {0, 8, 128, 136, 4, 12, 132, 140};
+  unsigned thread_group = thread / 4;
+  unsigned in_tg_index = thread % 4;
+
+  switch (wmma_type) {
+    case LOAD_A:
+      if (wmma_layout == ROW)
+        offset = load_a_row[thread_group] + 16 * in_tg_index;
+      else
+        offset = load_a_col[thread_group] + 16 * in_tg_index;
+      break;
+
+    case LOAD_B:
+      if (wmma_layout == ROW)
+        offset = load_b_row[thread_group] + 16 * in_tg_index;
+      else
+        offset = load_b_col[thread_group] + 16 * in_tg_index;
+      break;
+
+    case LOAD_C:
+    case STORE_D:
+      if (type == F16_TYPE) {
+        if (wmma_layout == ROW)
+          offset = load_c_half_row[thread_group] + 16 * in_tg_index;
+        else
+          offset = load_c_half_col[thread_group] + in_tg_index;
+      } else {
+        if (wmma_layout == ROW)
+          offset = load_c_float_row[thread_group];
+        else
+          offset = load_c_float_col[thread_group];
 
-void inst_not_implemented( const ptx_instruction * pI ) ;
-ptx_reg_t srcOperandModifiers(ptx_reg_t opData, operand_info opInfo, operand_info dstInfo, unsigned type, ptx_thread_info *thread);
+        switch (in_tg_index) {
+          case 0:
+            break;
+          case 1:
+            if (wmma_layout == ROW)
+              offset += 16;
+            else
+              offset += 1;
+            break;
+          case 2:
+            if (wmma_layout == ROW)
+              offset += 2;
+            else
+              offset += 32;
+            break;
+          case 3:
+            if (wmma_layout == ROW)
+              offset += 18;
+            else
+              offset += 33;
+            break;
+          default:
+            abort();
+        }
+      }
+      break;
 
-void sign_extend( ptx_reg_t &data, unsigned src_size, const operand_info &dst );
+    default:
+      abort();
+  }
+  offset = (offset / 16) * stride + offset % 16;
+  return offset;
+}
 
-void ptx_thread_info::set_reg( const symbol *reg, const ptx_reg_t &value ) 
-{
-   assert( reg != NULL );
-   if( reg->name() == "_" ) return;
-   assert( !m_regs.empty() );
-   assert( reg->uid() > 0 );
-   m_regs.back()[ reg ] = value;
-   if (m_enable_debug_trace ) 
-      m_debug_trace_regs_modified.back()[ reg ] = value;
-   m_last_set_operand_value = value;
+int acc_float_offset(int index, int wmma_layout, int stride) {
+  int c_row_offset[] = {0, 1, 32, 33, 4, 5, 36, 37};
+  int c_col_offset[] = {0, 16, 2, 18, 64, 80, 66, 82};
+  int offset;
+
+  if (wmma_layout == ROW)
+    offset = c_row_offset[index];
+  else if (wmma_layout == COL)
+    offset = c_col_offset[index];
+  else {
+    printf("wrong layout");
+    abort();
+  }
+  offset = (offset / 16) * stride + offset % 16;
+  return offset;
 }
 
-void ptx_thread_info::print_reg_thread(char * fname)
-{
+void inst_not_implemented(const ptx_instruction *pI);
+ptx_reg_t srcOperandModifiers(ptx_reg_t opData, operand_info opInfo,
+                              operand_info dstInfo, unsigned type,
+                              ptx_thread_info *thread);
+
+void video_mem_instruction(const ptx_instruction *pI, ptx_thread_info *thread,
+                           int op_code);
+
+void sign_extend(ptx_reg_t &data, unsigned src_size, const operand_info &dst);
+
+void ptx_thread_info::set_reg(const symbol *reg, const ptx_reg_t &value) {
+  assert(reg != NULL);
+  if (reg->name() == "_") return;
+  assert(!m_regs.empty());
+  assert(reg->uid() > 0);
+  m_regs.back()[reg] = value;
+  if (m_enable_debug_trace) m_debug_trace_regs_modified.back()[reg] = value;
+  m_last_set_operand_value = value;
+}
 
-  FILE *fp= fopen(fname,"w");
-  assert(fp!=NULL);
+void ptx_thread_info::print_reg_thread(char *fname) {
+  FILE *fp = fopen(fname, "w");
+  assert(fp != NULL);
 
   int size = m_regs.size();
-  
-  if(size>0)
-  {  
-  reg_map_t reg = m_regs.back();
-  
-      reg_map_t::const_iterator it;
-      for (it = reg.begin(); it != reg.end(); ++it) 
-        {
-          const std::string &name = it->first->name();
-          const std::string &dec= it->first->decl_location();
-          unsigned size = it->first->get_size_in_bytes();
-          fprintf(fp,"%s %llu %s %d\n",name.c_str(), it->second, dec.c_str(),size );
-          
-        }
-   //m_regs.pop_back();     
+
+  if (size > 0) {
+    reg_map_t reg = m_regs.back();
+
+    reg_map_t::const_iterator it;
+    for (it = reg.begin(); it != reg.end(); ++it) {
+      const std::string &name = it->first->name();
+      const std::string &dec = it->first->decl_location();
+      unsigned size = it->first->get_size_in_bytes();
+      fprintf(fp, "%s %llu %s %d\n", name.c_str(), it->second, dec.c_str(),
+              size);
+    }
+    // m_regs.pop_back();
   }
   fclose(fp);
+}
 
+void ptx_thread_info::resume_reg_thread(char *fname, symbol_table *symtab) {
+  FILE *fp2 = fopen(fname, "r");
+  assert(fp2 != NULL);
+  // m_regs.push_back( reg_map_t() );
+  char line[200];
+  while (fgets(line, sizeof line, fp2) != NULL) {
+    symbol *reg;
+    char *pch;
+    pch = strtok(line, " ");
+    char *name = pch;
+    reg = symtab->lookup(name);
+    ptx_reg_t data;
+    pch = strtok(NULL, " ");
+    data = atoi(pch);
+    pch = strtok(NULL, " ");
+    pch = strtok(NULL, " ");
+    m_regs.back()[reg] = data;
   }
+  fclose(fp2);
+}
 
-void ptx_thread_info::resume_reg_thread(char * fname, symbol_table * symtab)
-{
-
-
-      FILE * fp2 = fopen(fname, "r");
-      assert(fp2!=NULL);
-      //m_regs.push_back( reg_map_t() );
-      char line [ 200 ];
-      while ( fgets ( line, sizeof line, fp2 ) != NULL )
-      {
-          symbol *reg;
-          char * pch;
-          unsigned size;
-          pch = strtok (line," ");
-          char * name =pch;
-          reg= symtab->lookup(name);
-          ptx_reg_t data;
-          pch = strtok (NULL," "); 
-          data = atoi(pch);
-          pch = strtok (NULL," ");
-          char * decl= pch;
-          pch = strtok (NULL," ");
-          size = atoi(pch);
-
+ptx_reg_t ptx_thread_info::get_reg(const symbol *reg) {
+  static bool unfound_register_warned = false;
+  assert(reg != NULL);
+  assert(!m_regs.empty());
+  reg_map_t::iterator regs_iter = m_regs.back().find(reg);
+  if (regs_iter == m_regs.back().end()) {
+    assert(reg->type()->get_key().is_reg());
+    const std::string &name = reg->name();
+    unsigned call_uid = m_callstack.back().m_call_uid;
+    ptx_reg_t uninit_reg;
+    uninit_reg.u32 = 0x0;
+    set_reg(reg, uninit_reg);  // give it a value since we are going to warn the
+                               // user anyway
+    std::string file_loc = get_location();
+    if (!unfound_register_warned) {
+      printf(
+          "GPGPU-Sim PTX: WARNING (%s) ** reading undefined register \'%s\' "
+          "(cuid:%u). Setting to 0X00000000. This is okay if you are "
+          "simulating the native ISA"
+          "\n",
+          file_loc.c_str(), name.c_str(), call_uid);
+      unfound_register_warned = true;
+    }
+    regs_iter = m_regs.back().find(reg);
+  }
+  if (m_enable_debug_trace)
+    m_debug_trace_regs_read.back()[reg] = regs_iter->second;
+  return regs_iter->second;
+}
 
-          m_regs.back()[reg] = data;
-      }
-      fclose ( fp2 );
-}
-    
-
-ptx_reg_t ptx_thread_info::get_reg( const symbol *reg )
-{
-   static bool unfound_register_warned = false;
-   assert( reg != NULL );
-   assert( !m_regs.empty() );
-   reg_map_t::iterator regs_iter = m_regs.back().find(reg);
-   if (regs_iter == m_regs.back().end()) {
-      assert( reg->type()->get_key().is_reg() );
-      const std::string &name = reg->name();
-      unsigned call_uid = m_callstack.back().m_call_uid;
-      ptx_reg_t uninit_reg;
-      uninit_reg.u32 = 0x0;
-      set_reg(reg, uninit_reg); // give it a value since we are going to warn the user anyway
-      std::string file_loc = get_location();
-      if( !unfound_register_warned ) {
-          printf("GPGPU-Sim PTX: WARNING (%s) ** reading undefined register \'%s\' (cuid:%u). Setting to 0X00000000. This is okay if you are simulating the native ISA"
-        		  "\n",
-                 file_loc.c_str(), name.c_str(), call_uid );
-          unfound_register_warned = true;
-      }
-      regs_iter = m_regs.back().find(reg);
-   }
-   if (m_enable_debug_trace ) 
-      m_debug_trace_regs_read.back()[ reg ] = regs_iter->second;
-   return regs_iter->second;
-}
-
-ptx_reg_t ptx_thread_info::get_operand_value( const operand_info &op, operand_info dstInfo, unsigned opType, ptx_thread_info *thread, int derefFlag )
-{
-   ptx_reg_t result, tmp;
-
-
-   if(op.get_double_operand_type() == 0) {
-      if(((opType != BB128_TYPE) && (opType != BB64_TYPE) && (opType != FF64_TYPE)) || (op.get_addr_space() != undefined_space)) {
-         if ( op.is_reg() ) {
-            result = get_reg( op.get_symbol() );
-         } else if ( op.is_builtin()) {
-            result.u32 = get_builtin( op.get_int(), op.get_addr_offset() );
-         } else  if(op.is_immediate_address()){
-    		 result.u64 = op.get_addr_offset();
-    	 } else if ( op.is_memory_operand() ) {
-            // a few options here...
-            const symbol *sym = op.get_symbol();
-            const type_info *type = sym->type();
-            const type_info_key &info = type->get_key();
-
-            if ( info.is_reg() ) {
-               const symbol *name = op.get_symbol();
-               result.u64 = get_reg(name).u64 + op.get_addr_offset(); 
-            } else if ( info.is_param_kernel() ) {
-               result.u64 = sym->get_address() + op.get_addr_offset();
-            } else if ( info.is_param_local() ) {
-               result.u64 = sym->get_address() + op.get_addr_offset();
-            } else if ( info.is_global() ) {
-               assert( op.get_addr_offset() == 0 );
-               result.u64 = sym->get_address();
-            } else if ( info.is_local() ) {
-               result.u64 = sym->get_address() + op.get_addr_offset();
-            } else if ( info.is_const() ) {
-               result.u64 = sym->get_address() + op.get_addr_offset();
-            } else if ( op.is_shared() ) {
-               result.u64 = op.get_symbol()->get_address() + op.get_addr_offset();
-            } else if ( op.is_sstarr() ) {
-               result.u64 = op.get_symbol()->get_address() + op.get_addr_offset();
-            } else {
-                 const char *name = op.name().c_str();
-	    	printf("GPGPU-Sim PTX: ERROR ** get_operand_value : unknown memory operand type for %s\n", name );
-            	abort();
-            }
+ptx_reg_t ptx_thread_info::get_operand_value(const operand_info &op,
+                                             operand_info dstInfo,
+                                             unsigned opType,
+                                             ptx_thread_info *thread,
+                                             int derefFlag) {
+  ptx_reg_t result, tmp;
+
+  if (op.get_double_operand_type() == 0) {
+    if (((opType != BB128_TYPE) && (opType != BB64_TYPE) &&
+         (opType != FF64_TYPE)) ||
+        (op.get_addr_space() != undefined_space)) {
+      if (op.is_reg()) {
+        result = get_reg(op.get_symbol());
+      } else if (op.is_builtin()) {
+        result.u32 = get_builtin(op.get_int(), op.get_addr_offset());
+      } else if (op.is_immediate_address()) {
+        result.u64 = op.get_addr_offset();
+      } else if (op.is_memory_operand()) {
+        // a few options here...
+        const symbol *sym = op.get_symbol();
+        const type_info *type = sym->type();
+        const type_info_key &info = type->get_key();
+
+        if (info.is_reg()) {
+          const symbol *name = op.get_symbol();
+          result.u64 = get_reg(name).u64 + op.get_addr_offset();
+        } else if (info.is_param_kernel()) {
+          result.u64 = sym->get_address() + op.get_addr_offset();
+        } else if (info.is_param_local()) {
+          result.u64 = sym->get_address() + op.get_addr_offset();
+        } else if (info.is_global()) {
+          assert(op.get_addr_offset() == 0);
+          result.u64 = sym->get_address();
+        } else if (info.is_local()) {
+          result.u64 = sym->get_address() + op.get_addr_offset();
+        } else if (info.is_const()) {
+          result.u64 = sym->get_address() + op.get_addr_offset();
+        } else if (op.is_shared()) {
+          result.u64 = op.get_symbol()->get_address() + op.get_addr_offset();
+        } else if (op.is_sstarr()) {
+          result.u64 = op.get_symbol()->get_address() + op.get_addr_offset();
+        } else {
+          const char *name = op.name().c_str();
+          printf(
+              "GPGPU-Sim PTX: ERROR ** get_operand_value : unknown memory "
+              "operand type for %s\n",
+              name);
+          abort();
+        }
 
-         } else if ( op.is_literal() ) {
-            result = op.get_literal_value();
-         } else if ( op.is_label() ) {
-            result.u64 = op.get_symbol()->get_address();
-         } else if ( op.is_shared() ) {
-            result.u64 = op.get_symbol()->get_address();
-         } else if ( op.is_sstarr() ) {
-            result.u64 = op.get_symbol()->get_address();
-         } else if ( op.is_const() ) {
-            result.u64 = op.get_symbol()->get_address();
-         } else if ( op.is_global() ) {
-            result.u64 = op.get_symbol()->get_address();
-         } else if ( op.is_local() ) {
-            result.u64 = op.get_symbol()->get_address();
-         } else if ( op.is_function_address() ) {
-            result.u64 = (size_t)op.get_symbol()->get_pc();
-	 } else if ( op.is_param_kernel()) {
-            result.u64 = op.get_symbol()->get_address();
-         }else {
-            const char *name = op.name().c_str();
-            const symbol *sym2 = op.get_symbol();
-            const type_info *type2 = sym2->type();
-            const type_info_key &info2 = type2->get_key();
-            if ( info2.is_param_kernel() ) {
-               result.u64 = sym2->get_address()+ op.get_addr_offset();
-            }
-	    else{ 
-             printf("GPGPU-Sim PTX: ERROR ** get_operand_value : unknown operand type for %s\n", name );
-             assert(0);
-	    }
-         }
-
-         if(op.get_operand_lohi() == 1) 
-              result.u64 = result.u64 & 0xFFFF;
-         else if(op.get_operand_lohi() == 2) 
-              result.u64 = (result.u64>>16) & 0xFFFF;
-      } else if (opType == BB128_TYPE) {
-          // b128
-          result.u128.lowest = get_reg( op.vec_symbol(0) ).u32;
-          result.u128.low = get_reg( op.vec_symbol(1) ).u32;
-          result.u128.high = get_reg( op.vec_symbol(2) ).u32;
-          result.u128.highest = get_reg( op.vec_symbol(3) ).u32;
+      } else if (op.is_literal()) {
+        result = op.get_literal_value();
+      } else if (op.is_label()) {
+        result.u64 = op.get_symbol()->get_address();
+      } else if (op.is_shared()) {
+        result.u64 = op.get_symbol()->get_address();
+      } else if (op.is_sstarr()) {
+        result.u64 = op.get_symbol()->get_address();
+      } else if (op.is_const()) {
+        result.u64 = op.get_symbol()->get_address();
+      } else if (op.is_global()) {
+        result.u64 = op.get_symbol()->get_address();
+      } else if (op.is_local()) {
+        result.u64 = op.get_symbol()->get_address();
+      } else if (op.is_function_address()) {
+        result.u64 = (size_t)op.get_symbol()->get_pc();
+      } else if (op.is_param_kernel()) {
+        result.u64 = op.get_symbol()->get_address();
       } else {
-          // bb64 or ff64
-          result.bits.ls = get_reg( op.vec_symbol(0) ).u32;
-          result.bits.ms = get_reg( op.vec_symbol(1) ).u32;
+        const char *name = op.name().c_str();
+        const symbol *sym2 = op.get_symbol();
+        const type_info *type2 = sym2->type();
+        const type_info_key &info2 = type2->get_key();
+        if (info2.is_param_kernel()) {
+          result.u64 = sym2->get_address() + op.get_addr_offset();
+        } else {
+          printf(
+              "GPGPU-Sim PTX: ERROR ** get_operand_value : unknown operand "
+              "type for %s\n",
+              name);
+          assert(0);
+        }
       }
-   } else if (op.get_double_operand_type() == 1) {
-      ptx_reg_t firstHalf, secondHalf;
-      firstHalf.u64 = get_reg( op.vec_symbol(0) ).u64;
-      secondHalf.u64 = get_reg( op.vec_symbol(1) ).u64;
-      if(op.get_operand_lohi() == 1)
-           secondHalf.u64 = secondHalf.u64 & 0xFFFF;
-      else if(op.get_operand_lohi() == 2)
-           secondHalf.u64 = (secondHalf.u64>>16) & 0xFFFF;
-      result.u64 = firstHalf.u64 + secondHalf.u64;
-   } else if (op.get_double_operand_type() == 2) {
-      // s[reg1 += reg2]
-      // reg1 is incremented after value is returned: the value returned is s[reg1]
-      ptx_reg_t firstHalf, secondHalf;
-      firstHalf.u64 = get_reg(op.vec_symbol(0)).u64;
-      secondHalf.u64 = get_reg(op.vec_symbol(1)).u64;
-      if(op.get_operand_lohi() == 1)
-           secondHalf.u64 = secondHalf.u64 & 0xFFFF;
-      else if(op.get_operand_lohi() == 2)
-           secondHalf.u64 = (secondHalf.u64>>16) & 0xFFFF;
-      result.u64 = firstHalf.u64;
-      firstHalf.u64 = firstHalf.u64 + secondHalf.u64;
-      set_reg(op.vec_symbol(0),firstHalf);
-   } else if (op.get_double_operand_type() == 3) {
-      // s[reg += immediate]
-      // reg is incremented after value is returned: the value returned is s[reg]
-      ptx_reg_t firstHalf;
-      firstHalf.u64 = get_reg(op.get_symbol()).u64;
-      result.u64 = firstHalf.u64;
-      firstHalf.u64 = firstHalf.u64 + op.get_addr_offset();
-      set_reg(op.get_symbol(),firstHalf);
-   }
 
-   ptx_reg_t finalResult;
-   memory_space *mem = NULL;
-   size_t size=0;
-   int t=0;
-   finalResult.u64=0;
-
-   //complete other cases for reading from memory, such as reading from other const memory
-   if((op.get_addr_space() == global_space)&&(derefFlag)) {
-       // global memory - g[4], g[$r0]
-       mem = thread->get_global_memory();
-       type_info_key::type_decode(opType,size,t);
-       mem->read(result.u32,size/8,&finalResult.u128);
-       thread->m_last_effective_address = result.u32;
-       thread->m_last_memory_space = global_space;
-
-       if( opType == S16_TYPE || opType == S32_TYPE )
-         sign_extend(finalResult,size,dstInfo);
-   } else if((op.get_addr_space() == shared_space)&&(derefFlag)) {
-      // shared memory - s[4], s[$r0]
-       mem = thread->m_shared_mem;
-       type_info_key::type_decode(opType,size,t);
-       mem->read(result.u32,size/8,&finalResult.u128);
-       thread->m_last_effective_address = result.u32;
-       thread->m_last_memory_space = shared_space;
-
-       if( opType == S16_TYPE || opType == S32_TYPE ) 
-         sign_extend(finalResult,size,dstInfo);
-   } else if((op.get_addr_space() == const_space)&&(derefFlag)) {
-      // const memory - ce0c1[4], ce0c1[$r0]
-       mem = thread->get_global_memory();
-       type_info_key::type_decode(opType,size,t);
-       mem->read((result.u32 + op.get_const_mem_offset()),size/8,&finalResult.u128);
-       thread->m_last_effective_address = result.u32;
-       thread->m_last_memory_space = const_space;
-       if( opType == S16_TYPE || opType == S32_TYPE )
-         sign_extend(finalResult,size,dstInfo);
-   } else if((op.get_addr_space() == local_space)&&(derefFlag)) {
-      // local memory - l0[4], l0[$r0]
-       mem = thread->m_local_mem;
-       type_info_key::type_decode(opType,size,t);
-       mem->read(result.u32,size/8,&finalResult.u128);
-       thread->m_last_effective_address = result.u32;
-       thread->m_last_memory_space = local_space;
-       if( opType == S16_TYPE || opType == S32_TYPE ) 
-         sign_extend(finalResult,size,dstInfo);
-   } else {
-       finalResult = result;
-   }
+      if (op.get_operand_lohi() == 1)
+        result.u64 = result.u64 & 0xFFFF;
+      else if (op.get_operand_lohi() == 2)
+        result.u64 = (result.u64 >> 16) & 0xFFFF;
+    } else if (opType == BB128_TYPE) {
+      // b128
+      result.u128.lowest = get_reg(op.vec_symbol(0)).u32;
+      result.u128.low = get_reg(op.vec_symbol(1)).u32;
+      result.u128.high = get_reg(op.vec_symbol(2)).u32;
+      result.u128.highest = get_reg(op.vec_symbol(3)).u32;
+    } else {
+      // bb64 or ff64
+      result.bits.ls = get_reg(op.vec_symbol(0)).u32;
+      result.bits.ms = get_reg(op.vec_symbol(1)).u32;
+    }
+  } else if (op.get_double_operand_type() == 1) {
+    ptx_reg_t firstHalf, secondHalf;
+    firstHalf.u64 = get_reg(op.vec_symbol(0)).u64;
+    secondHalf.u64 = get_reg(op.vec_symbol(1)).u64;
+    if (op.get_operand_lohi() == 1)
+      secondHalf.u64 = secondHalf.u64 & 0xFFFF;
+    else if (op.get_operand_lohi() == 2)
+      secondHalf.u64 = (secondHalf.u64 >> 16) & 0xFFFF;
+    result.u64 = firstHalf.u64 + secondHalf.u64;
+  } else if (op.get_double_operand_type() == 2) {
+    // s[reg1 += reg2]
+    // reg1 is incremented after value is returned: the value returned is
+    // s[reg1]
+    ptx_reg_t firstHalf, secondHalf;
+    firstHalf.u64 = get_reg(op.vec_symbol(0)).u64;
+    secondHalf.u64 = get_reg(op.vec_symbol(1)).u64;
+    if (op.get_operand_lohi() == 1)
+      secondHalf.u64 = secondHalf.u64 & 0xFFFF;
+    else if (op.get_operand_lohi() == 2)
+      secondHalf.u64 = (secondHalf.u64 >> 16) & 0xFFFF;
+    result.u64 = firstHalf.u64;
+    firstHalf.u64 = firstHalf.u64 + secondHalf.u64;
+    set_reg(op.vec_symbol(0), firstHalf);
+  } else if (op.get_double_operand_type() == 3) {
+    // s[reg += immediate]
+    // reg is incremented after value is returned: the value returned is s[reg]
+    ptx_reg_t firstHalf;
+    firstHalf.u64 = get_reg(op.get_symbol()).u64;
+    result.u64 = firstHalf.u64;
+    firstHalf.u64 = firstHalf.u64 + op.get_addr_offset();
+    set_reg(op.get_symbol(), firstHalf);
+  }
+
+  ptx_reg_t finalResult;
+  memory_space *mem = NULL;
+  size_t size = 0;
+  int t = 0;
+  finalResult.u64 = 0;
+
+  // complete other cases for reading from memory, such as reading from other
+  // const memory
+  if ((op.get_addr_space() == global_space) && (derefFlag)) {
+    // global memory - g[4], g[$r0]
+    mem = thread->get_global_memory();
+    type_info_key::type_decode(opType, size, t);
+    mem->read(result.u32, size / 8, &finalResult.u128);
+    thread->m_last_effective_address = result.u32;
+    thread->m_last_memory_space = global_space;
+
+    if (opType == S16_TYPE || opType == S32_TYPE)
+      sign_extend(finalResult, size, dstInfo);
+  } else if ((op.get_addr_space() == shared_space) && (derefFlag)) {
+    // shared memory - s[4], s[$r0]
+    mem = thread->m_shared_mem;
+    type_info_key::type_decode(opType, size, t);
+    mem->read(result.u32, size / 8, &finalResult.u128);
+    thread->m_last_effective_address = result.u32;
+    thread->m_last_memory_space = shared_space;
+
+    if (opType == S16_TYPE || opType == S32_TYPE)
+      sign_extend(finalResult, size, dstInfo);
+  } else if ((op.get_addr_space() == const_space) && (derefFlag)) {
+    // const memory - ce0c1[4], ce0c1[$r0]
+    mem = thread->get_global_memory();
+    type_info_key::type_decode(opType, size, t);
+    mem->read((result.u32 + op.get_const_mem_offset()), size / 8,
+              &finalResult.u128);
+    thread->m_last_effective_address = result.u32;
+    thread->m_last_memory_space = const_space;
+    if (opType == S16_TYPE || opType == S32_TYPE)
+      sign_extend(finalResult, size, dstInfo);
+  } else if ((op.get_addr_space() == local_space) && (derefFlag)) {
+    // local memory - l0[4], l0[$r0]
+    mem = thread->m_local_mem;
+    type_info_key::type_decode(opType, size, t);
+    mem->read(result.u32, size / 8, &finalResult.u128);
+    thread->m_last_effective_address = result.u32;
+    thread->m_last_memory_space = local_space;
+    if (opType == S16_TYPE || opType == S32_TYPE)
+      sign_extend(finalResult, size, dstInfo);
+  } else {
+    finalResult = result;
+  }
 
-   if((op.get_operand_neg() == true)&&(derefFlag)) {
-      switch( opType ) {
+  if ((op.get_operand_neg() == true) && (derefFlag)) {
+    switch (opType) {
       // Default to f32 for now, need to add support for others
       case S8_TYPE:
       case U8_TYPE:
       case B8_TYPE:
-         finalResult.s8 = -finalResult.s8;
-         break;
+        finalResult.s8 = -finalResult.s8;
+        break;
       case S16_TYPE:
       case U16_TYPE:
       case B16_TYPE:
-         finalResult.s16 = -finalResult.s16;
-         break;
+        finalResult.s16 = -finalResult.s16;
+        break;
       case S32_TYPE:
       case U32_TYPE:
       case B32_TYPE:
-         finalResult.s32 = -finalResult.s32;
-         break;
+        finalResult.s32 = -finalResult.s32;
+        break;
       case S64_TYPE:
       case U64_TYPE:
       case B64_TYPE:
-         finalResult.s64 = -finalResult.s64;
-         break;
+        finalResult.s64 = -finalResult.s64;
+        break;
       case F16_TYPE:
-         finalResult.f16 = -finalResult.f16;
-         break;
+        finalResult.f16 = -finalResult.f16;
+        break;
       case F32_TYPE:
-         finalResult.f32 = -finalResult.f32;
-         break;
+        finalResult.f32 = -finalResult.f32;
+        break;
       case F64_TYPE:
       case FF64_TYPE:
-         finalResult.f64 = -finalResult.f64;
-         break;
+        finalResult.f64 = -finalResult.f64;
+        break;
       default:
-         assert(0);
-      }
+        assert(0);
+    }
+  }
 
-   }
+  return finalResult;
+}
 
-   return finalResult;
-
-}
-
-unsigned get_operand_nbits( const operand_info &op )
-{
-   if ( op.is_reg() ) {
-      const symbol *sym = op.get_symbol();
-      const type_info *typ = sym->type();
-      type_info_key t = typ->get_key();
-      switch( t.scalar_type() ) {
-      case PRED_TYPE: 
-         return 1;
-      case B8_TYPE: case S8_TYPE: case U8_TYPE:
-         return 8;
-      case S16_TYPE: case U16_TYPE: case F16_TYPE: case B16_TYPE:
-         return 16;
-      case S32_TYPE: case U32_TYPE: case F32_TYPE: case B32_TYPE:
-         return 32;
-      case S64_TYPE: case U64_TYPE: case F64_TYPE: case B64_TYPE:
-         return 64;
+unsigned get_operand_nbits(const operand_info &op) {
+  if (op.is_reg()) {
+    const symbol *sym = op.get_symbol();
+    const type_info *typ = sym->type();
+    type_info_key t = typ->get_key();
+    switch (t.scalar_type()) {
+      case PRED_TYPE:
+        return 1;
+      case B8_TYPE:
+      case S8_TYPE:
+      case U8_TYPE:
+        return 8;
+      case S16_TYPE:
+      case U16_TYPE:
+      case F16_TYPE:
+      case B16_TYPE:
+        return 16;
+      case S32_TYPE:
+      case U32_TYPE:
+      case F32_TYPE:
+      case B32_TYPE:
+        return 32;
+      case S64_TYPE:
+      case U64_TYPE:
+      case F64_TYPE:
+      case B64_TYPE:
+        return 64;
       default:
-         printf("ERROR: unknown register type\n");
-         fflush(stdout);
-         abort();
-      }
-   } else {
-      printf("ERROR: Need to implement get_operand_nbits() for currently unsupported operand_info type\n");
-      fflush(stdout);
-      abort();
-   }
+        printf("ERROR: unknown register type\n");
+        fflush(stdout);
+        abort();
+    }
+  } else {
+    printf(
+        "ERROR: Need to implement get_operand_nbits() for currently "
+        "unsupported operand_info type\n");
+    fflush(stdout);
+    abort();
+  }
 
-   return 0;
+  return 0;
 }
 
-void ptx_thread_info::get_vector_operand_values( const operand_info &op, ptx_reg_t* ptx_regs, unsigned num_elements )
-{
-   assert( op.is_vector() );
-   assert( num_elements <= 8 );
-
-   for (int idx = num_elements - 1; idx >= 0; --idx) {
-      const symbol *sym = NULL;
-      sym = op.vec_symbol(idx);
-      if( strcmp(sym->name().c_str(),"_") != 0) {
-         reg_map_t::iterator reg_iter = m_regs.back().find(sym);
-         assert( reg_iter != m_regs.back().end() );
-         ptx_regs[idx] = reg_iter->second;
-      }
-   }
+void ptx_thread_info::get_vector_operand_values(const operand_info &op,
+                                                ptx_reg_t *ptx_regs,
+                                                unsigned num_elements) {
+  assert(op.is_vector());
+  assert(num_elements <= 8);
+
+  for (int idx = num_elements - 1; idx >= 0; --idx) {
+    const symbol *sym = NULL;
+    sym = op.vec_symbol(idx);
+    if (strcmp(sym->name().c_str(), "_") != 0) {
+      reg_map_t::iterator reg_iter = m_regs.back().find(sym);
+      assert(reg_iter != m_regs.back().end());
+      ptx_regs[idx] = reg_iter->second;
+    }
+  }
 }
 
-void sign_extend( ptx_reg_t &data, unsigned src_size, const operand_info &dst )
-{
-   if( !dst.is_reg() )
-      return;
-   unsigned dst_size = get_operand_nbits( dst );
-   if( src_size >= dst_size ) 
-      return;
-   // src_size < dst_size
-   unsigned long long mask = 1;
-   mask <<= (src_size-1);
-   if( (mask & data.u64) == 0 ) {
-      // no need to sign extend
-      return;
-   }
-   // need to sign extend
-   mask = 1;
-   mask <<= dst_size-src_size;
-   mask -= 1;
-   mask <<= src_size;
-   data.u64 |= mask;
+void sign_extend(ptx_reg_t &data, unsigned src_size, const operand_info &dst) {
+  if (!dst.is_reg()) return;
+  unsigned dst_size = get_operand_nbits(dst);
+  if (src_size >= dst_size) return;
+  // src_size < dst_size
+  unsigned long long mask = 1;
+  mask <<= (src_size - 1);
+  if ((mask & data.u64) == 0) {
+    // no need to sign extend
+    return;
+  }
+  // need to sign extend
+  mask = 1;
+  mask <<= dst_size - src_size;
+  mask -= 1;
+  mask <<= src_size;
+  data.u64 |= mask;
 }
 
-void ptx_thread_info::set_operand_value( const operand_info &dst, const ptx_reg_t &data, unsigned type, ptx_thread_info *thread, const ptx_instruction *pI, int overflow, int carry )
-{
-    thread->set_operand_value( dst, data, type, thread, pI );
-
-    if (dst.get_double_operand_type() == -2)
-    {
-        ptx_reg_t predValue;
-        
-        const symbol *sym = dst.vec_symbol(0);
-        predValue.u64 = (m_regs.back()[ sym ].u64) & ~(0x0C);
-        predValue.u64 |= ((overflow & 0x01)<<3);
-        predValue.u64 |= ((carry & 0x01)<<2);
-
-        set_reg(sym,predValue);
-    }
-    else if (dst.get_double_operand_type() == 0)
-    {
-        //intentionally do nothing
-    }
-    else
-    {
-        printf("Unexpected double destination\n");
-        assert(0);
-    }
-
+void ptx_thread_info::set_operand_value(const operand_info &dst,
+                                        const ptx_reg_t &data, unsigned type,
+                                        ptx_thread_info *thread,
+                                        const ptx_instruction *pI, int overflow,
+                                        int carry) {
+  thread->set_operand_value(dst, data, type, thread, pI);
+
+  if (dst.get_double_operand_type() == -2) {
+    ptx_reg_t predValue;
+
+    const symbol *sym = dst.vec_symbol(0);
+    predValue.u64 = (m_regs.back()[sym].u64) & ~(0x0C);
+    predValue.u64 |= ((overflow & 0x01) << 3);
+    predValue.u64 |= ((carry & 0x01) << 2);
+
+    set_reg(sym, predValue);
+  } else if (dst.get_double_operand_type() == 0) {
+    // intentionally do nothing
+  } else {
+    printf("Unexpected double destination\n");
+    assert(0);
+  }
 }
 
-void ptx_thread_info::set_operand_value( const operand_info &dst, const ptx_reg_t &data, unsigned type, ptx_thread_info *thread, const ptx_instruction *pI )
-{
-   ptx_reg_t dstData;
-   memory_space *mem = NULL;
-   size_t size;
-   int t;
-
-   type_info_key::type_decode(type,size,t);
-
-   /*complete this section for other cases*/
-   if(dst.get_addr_space() == undefined_space)
-   {
-      ptx_reg_t setValue;
-      setValue.u64 = data.u64;
-
-      // Double destination in set instruction ($p0|$p1) - second is negation of first
-      if (dst.get_double_operand_type() == -1)
-      {
-          ptx_reg_t setValue2;
-          const symbol *name1 = dst.vec_symbol(0);
-          const symbol *name2 = dst.vec_symbol(1);
-
-          if ( (type==F16_TYPE)||(type==F32_TYPE)||(type==F64_TYPE)||(type==FF64_TYPE) ) {
-             setValue2.f32 = (setValue.u64==0)?1.0f:0.0f;
-          } else {
-             setValue2.u32 = (setValue.u64==0)?0xFFFFFFFF:0;
-          }
-
-          set_reg(name1,setValue);
-          set_reg(name2,setValue2);
+void ptx_thread_info::set_operand_value(const operand_info &dst,
+                                        const ptx_reg_t &data, unsigned type,
+                                        ptx_thread_info *thread,
+                                        const ptx_instruction *pI) {
+  ptx_reg_t dstData;
+  memory_space *mem = NULL;
+  size_t size;
+  int t;
+
+  type_info_key::type_decode(type, size, t);
+
+  /*complete this section for other cases*/
+  if (dst.get_addr_space() == undefined_space) {
+    ptx_reg_t setValue;
+    setValue.u64 = data.u64;
+
+    // Double destination in set instruction ($p0|$p1) - second is negation of
+    // first
+    if (dst.get_double_operand_type() == -1) {
+      ptx_reg_t setValue2;
+      const symbol *name1 = dst.vec_symbol(0);
+      const symbol *name2 = dst.vec_symbol(1);
+
+      if ((type == F16_TYPE) || (type == F32_TYPE) || (type == F64_TYPE) ||
+          (type == FF64_TYPE)) {
+        setValue2.f32 = (setValue.u64 == 0) ? 1.0f : 0.0f;
+      } else {
+        setValue2.u32 = (setValue.u64 == 0) ? 0xFFFFFFFF : 0;
       }
 
-      // Double destination in cvt,shr,mul,etc. instruction ($p0|$r4) - second register operand receives data, first predicate operand
-      // is set as $p0=($r4!=0)
-      // Also for Double destination in set instruction ($p0/$r1)
-      else if ((dst.get_double_operand_type() == -2)||(dst.get_double_operand_type() == -3))
-      {
-          ptx_reg_t predValue;
-          const symbol *predName = dst.vec_symbol(0);
-          const symbol *regName = dst.vec_symbol(1);
-          predValue.u64 = 0;
-
-          switch ( type ) {
-          case S8_TYPE:
-              if((setValue.s8 & 0x7F) == 0)
-                  predValue.u64 |= 1;
-              break;
-          case S16_TYPE:
-              if((setValue.s16 & 0x7FFF) == 0)
-                  predValue.u64 |= 1;
-              break;
-          case S32_TYPE:
-              if((setValue.s32 & 0x7FFFFFFF) == 0)
-                  predValue.u64 |= 1;
-              break;
-          case S64_TYPE:
-              if((setValue.s64 & 0x7FFFFFFFFFFFFFFF) == 0)
-                  predValue.u64 |= 1;
-              break;
-          case U8_TYPE:
-          case B8_TYPE:
-              if(setValue.u8 == 0)
-                  predValue.u64 |= 1;
-              break;
-          case U16_TYPE:
-          case B16_TYPE:
-              if(setValue.u16 == 0)
-                  predValue.u64 |= 1;
-              break;
-          case U32_TYPE:
-          case B32_TYPE:
-              if(setValue.u32 == 0)
-                  predValue.u64 |= 1;
-              break;
-          case U64_TYPE:
-          case B64_TYPE:
-              if(setValue.u64 == 0)
-                  predValue.u64 |= 1;
-              break;
-          case F16_TYPE:
-              if(setValue.f16 == 0)
-                  predValue.u64 |= 1;
-              break;
-          case F32_TYPE:
-              if(setValue.f32 == 0)
-                  predValue.u64 |= 1;
-              break;
-          case F64_TYPE:
-          case FF64_TYPE:
-              if(setValue.f64 == 0)
-                  predValue.u64 |= 1;
-              break;
-          default: assert(0); break;
-          }
-
-
-          if ( (type==S8_TYPE)||(type==S16_TYPE)||(type==S32_TYPE)||(type==S64_TYPE)||
-               (type==U8_TYPE)||(type==U16_TYPE)||(type==U32_TYPE)||(type==U64_TYPE)||
-               (type==B8_TYPE)||(type==B16_TYPE)||(type==B32_TYPE)||(type==B64_TYPE)) {
-              if((setValue.u32 & (1<<(size-1))) != 0)
-                  predValue.u64 |= 1<<1;
-          }
-          if ( type==F32_TYPE ) {
-              if(setValue.f32 < 0)
-                  predValue.u64 |= 1<<1;
-          }
-
-          if(dst.get_operand_lohi() == 1)
-          {
-              setValue.u64 = ((m_regs.back()[ regName ].u64) & (~(0xFFFF))) + (data.u64 & 0xFFFF);
-          }
-          else if(dst.get_operand_lohi() == 2)
-          {
-              setValue.u64 = ((m_regs.back()[ regName ].u64) & (~(0xFFFF0000))) + ((data.u64<<16) & 0xFFFF0000);
-          }
+      set_reg(name1, setValue);
+      set_reg(name2, setValue2);
+    }
 
-          set_reg(predName,predValue);
-          set_reg(regName,setValue);
-      }
-      else if (type == BB128_TYPE)
-      {
-          //b128 stuff here.
-          ptx_reg_t setValue2, setValue3, setValue4;
-          setValue.u64 = 0;
-          setValue2.u64 = 0;
-          setValue3.u64 = 0;
-          setValue4.u64 = 0;
-          setValue.u32 = data.u128.lowest;
-          setValue2.u32 = data.u128.low;
-          setValue3.u32 = data.u128.high;
-          setValue4.u32 = data.u128.highest;
-
-          const symbol *name1, *name2, *name3, *name4 = NULL;
-
-          name1 = dst.vec_symbol(0);
-          name2 = dst.vec_symbol(1);
-          name3 = dst.vec_symbol(2);
-          name4 = dst.vec_symbol(3);
-
-          set_reg(name1,setValue);
-          set_reg(name2,setValue2);
-          set_reg(name3,setValue3);
-          set_reg(name4,setValue4);
+    // Double destination in cvt,shr,mul,etc. instruction ($p0|$r4) - second
+    // register operand receives data, first predicate operand is set as
+    // $p0=($r4!=0) Also for Double destination in set instruction ($p0/$r1)
+    else if ((dst.get_double_operand_type() == -2) ||
+             (dst.get_double_operand_type() == -3)) {
+      ptx_reg_t predValue;
+      const symbol *predName = dst.vec_symbol(0);
+      const symbol *regName = dst.vec_symbol(1);
+      predValue.u64 = 0;
+
+      switch (type) {
+        case S8_TYPE:
+          if ((setValue.s8 & 0x7F) == 0) predValue.u64 |= 1;
+          break;
+        case S16_TYPE:
+          if ((setValue.s16 & 0x7FFF) == 0) predValue.u64 |= 1;
+          break;
+        case S32_TYPE:
+          if ((setValue.s32 & 0x7FFFFFFF) == 0) predValue.u64 |= 1;
+          break;
+        case S64_TYPE:
+          if ((setValue.s64 & 0x7FFFFFFFFFFFFFFF) == 0) predValue.u64 |= 1;
+          break;
+        case U8_TYPE:
+        case B8_TYPE:
+          if (setValue.u8 == 0) predValue.u64 |= 1;
+          break;
+        case U16_TYPE:
+        case B16_TYPE:
+          if (setValue.u16 == 0) predValue.u64 |= 1;
+          break;
+        case U32_TYPE:
+        case B32_TYPE:
+          if (setValue.u32 == 0) predValue.u64 |= 1;
+          break;
+        case U64_TYPE:
+        case B64_TYPE:
+          if (setValue.u64 == 0) predValue.u64 |= 1;
+          break;
+        case F16_TYPE:
+          if (setValue.f16 == 0) predValue.u64 |= 1;
+          break;
+        case F32_TYPE:
+          if (setValue.f32 == 0) predValue.u64 |= 1;
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          if (setValue.f64 == 0) predValue.u64 |= 1;
+          break;
+        default:
+          assert(0);
+          break;
       }
-      else if (type == BB64_TYPE || type == FF64_TYPE)
-      {
-          //ptxplus version of storing 64 bit values to registers stores to two adjacent registers
-          ptx_reg_t setValue2;
-          setValue.u32 = 0;
-          setValue2.u32 = 0;
-
-          setValue.u32 = data.bits.ls;
-          setValue2.u32 = data.bits.ms;
-
-          const symbol *name1, *name2 = NULL;
 
-          name1 = dst.vec_symbol(0);
-          name2 = dst.vec_symbol(1);
-
-          set_reg(name1,setValue);
-          set_reg(name2,setValue2);
+      if ((type == S8_TYPE) || (type == S16_TYPE) || (type == S32_TYPE) ||
+          (type == S64_TYPE) || (type == U8_TYPE) || (type == U16_TYPE) ||
+          (type == U32_TYPE) || (type == U64_TYPE) || (type == B8_TYPE) ||
+          (type == B16_TYPE) || (type == B32_TYPE) || (type == B64_TYPE)) {
+        if ((setValue.u32 & (1 << (size - 1))) != 0) predValue.u64 |= 1 << 1;
       }
-      else
-      {
-          if(dst.get_operand_lohi() == 1)
-          {
-              setValue.u64 = ((m_regs.back()[ dst.get_symbol() ].u64) & (~(0xFFFF))) + (data.u64 & 0xFFFF);
-          }
-          else if(dst.get_operand_lohi() == 2)
-          {
-              setValue.u64 = ((m_regs.back()[ dst.get_symbol() ].u64) & (~(0xFFFF0000))) + ((data.u64<<16) & 0xFFFF0000);
-          }
-          set_reg(dst.get_symbol(),setValue);
+      if (type == F32_TYPE) {
+        if (setValue.f32 < 0) predValue.u64 |= 1 << 1;
       }
-   }
 
-   // global memory - g[4], g[$r0]
-   else if(dst.get_addr_space() == global_space)
-   {
-       dstData = thread->get_operand_value(dst, dst, type, thread, 0);
-       mem = thread->get_global_memory();
-       type_info_key::type_decode(type,size,t);
+      if (dst.get_operand_lohi() == 1) {
+        setValue.u64 =
+            ((m_regs.back()[regName].u64) & (~(0xFFFF))) + (data.u64 & 0xFFFF);
+      } else if (dst.get_operand_lohi() == 2) {
+        setValue.u64 = ((m_regs.back()[regName].u64) & (~(0xFFFF0000))) +
+                       ((data.u64 << 16) & 0xFFFF0000);
+      }
 
-       mem->write(dstData.u32,size/8,&data.u128,thread,pI);
-       thread->m_last_effective_address = dstData.u32;
-       thread->m_last_memory_space = global_space;
-   }
+      set_reg(predName, predValue);
+      set_reg(regName, setValue);
+    } else if (type == BB128_TYPE) {
+      // b128 stuff here.
+      ptx_reg_t setValue2, setValue3, setValue4;
+      setValue.u64 = 0;
+      setValue2.u64 = 0;
+      setValue3.u64 = 0;
+      setValue4.u64 = 0;
+      setValue.u32 = data.u128.lowest;
+      setValue2.u32 = data.u128.low;
+      setValue3.u32 = data.u128.high;
+      setValue4.u32 = data.u128.highest;
+
+      const symbol *name1, *name2, *name3, *name4 = NULL;
+
+      name1 = dst.vec_symbol(0);
+      name2 = dst.vec_symbol(1);
+      name3 = dst.vec_symbol(2);
+      name4 = dst.vec_symbol(3);
+
+      set_reg(name1, setValue);
+      set_reg(name2, setValue2);
+      set_reg(name3, setValue3);
+      set_reg(name4, setValue4);
+    } else if (type == BB64_TYPE || type == FF64_TYPE) {
+      // ptxplus version of storing 64 bit values to registers stores to two
+      // adjacent registers
+      ptx_reg_t setValue2;
+      setValue.u32 = 0;
+      setValue2.u32 = 0;
+
+      setValue.u32 = data.bits.ls;
+      setValue2.u32 = data.bits.ms;
+
+      const symbol *name1, *name2 = NULL;
+
+      name1 = dst.vec_symbol(0);
+      name2 = dst.vec_symbol(1);
+
+      set_reg(name1, setValue);
+      set_reg(name2, setValue2);
+    } else {
+      if (dst.get_operand_lohi() == 1) {
+        setValue.u64 = ((m_regs.back()[dst.get_symbol()].u64) & (~(0xFFFF))) +
+                       (data.u64 & 0xFFFF);
+      } else if (dst.get_operand_lohi() == 2) {
+        setValue.u64 =
+            ((m_regs.back()[dst.get_symbol()].u64) & (~(0xFFFF0000))) +
+            ((data.u64 << 16) & 0xFFFF0000);
+      }
+      set_reg(dst.get_symbol(), setValue);
+    }
+  }
 
-   // shared memory - s[4], s[$r0]
-   else if(dst.get_addr_space() == shared_space)
-   {
-       dstData = thread->get_operand_value(dst, dst, type, thread, 0);
-       mem = thread->m_shared_mem;
-       type_info_key::type_decode(type,size,t);
+  // global memory - g[4], g[$r0]
+  else if (dst.get_addr_space() == global_space) {
+    dstData = thread->get_operand_value(dst, dst, type, thread, 0);
+    mem = thread->get_global_memory();
+    type_info_key::type_decode(type, size, t);
 
-       mem->write(dstData.u32,size/8,&data.u128,thread,pI);
-       thread->m_last_effective_address = dstData.u32;
-       thread->m_last_memory_space = shared_space;
-   }
+    mem->write(dstData.u32, size / 8, &data.u128, thread, pI);
+    thread->m_last_effective_address = dstData.u32;
+    thread->m_last_memory_space = global_space;
+  }
 
-   // local memory - l0[4], l0[$r0]
-   else if(dst.get_addr_space() == local_space)
-   {
-       dstData = thread->get_operand_value(dst, dst, type, thread, 0);
-       mem = thread->m_local_mem;
-       type_info_key::type_decode(type,size,t);
+  // shared memory - s[4], s[$r0]
+  else if (dst.get_addr_space() == shared_space) {
+    dstData = thread->get_operand_value(dst, dst, type, thread, 0);
+    mem = thread->m_shared_mem;
+    type_info_key::type_decode(type, size, t);
 
-       mem->write(dstData.u32,size/8,&data.u128,thread,pI);
-       thread->m_last_effective_address = dstData.u32;
-       thread->m_last_memory_space = local_space;
-   }
+    mem->write(dstData.u32, size / 8, &data.u128, thread, pI);
+    thread->m_last_effective_address = dstData.u32;
+    thread->m_last_memory_space = shared_space;
+  }
 
-   else
-   {
-       printf("Destination stores to unknown location.");
-       assert(0);
-   }
+  // local memory - l0[4], l0[$r0]
+  else if (dst.get_addr_space() == local_space) {
+    dstData = thread->get_operand_value(dst, dst, type, thread, 0);
+    mem = thread->m_local_mem;
+    type_info_key::type_decode(type, size, t);
 
+    mem->write(dstData.u32, size / 8, &data.u128, thread, pI);
+    thread->m_last_effective_address = dstData.u32;
+    thread->m_last_memory_space = local_space;
+  }
 
+  else {
+    printf("Destination stores to unknown location.");
+    assert(0);
+  }
 }
 
-void ptx_thread_info::set_vector_operand_values( const operand_info &dst, 
-                                                 const ptx_reg_t &data1, 
-                                                 const ptx_reg_t &data2, 
-                                                 const ptx_reg_t &data3, 
-                                                 const ptx_reg_t &data4 )
-{
-   unsigned num_elements = dst.get_vect_nelem(); 
-   if (num_elements > 0) {
-       set_reg(dst.vec_symbol(0), data1);
-       if (num_elements > 1) {
-           set_reg(dst.vec_symbol(1), data2);
-           if (num_elements > 2) {
-              set_reg(dst.vec_symbol(2), data3);
-              if (num_elements > 3) {
-                 set_reg(dst.vec_symbol(3), data4);
-              }
-           }
-       }
-   }
-
-   m_last_set_operand_value = data1;
-}
-void ptx_thread_info::set_wmma_vector_operand_values( const operand_info &dst, 
-                                                 const ptx_reg_t &data1, 
-                                                 const ptx_reg_t &data2, 
-                                                 const ptx_reg_t &data3, 
-                                                 const ptx_reg_t &data4, 
-                                                 const ptx_reg_t &data5, 
-                                                 const ptx_reg_t &data6, 
-                                                 const ptx_reg_t &data7, 
-                                                 const ptx_reg_t &data8 )
-{
-   unsigned num_elements = dst.get_vect_nelem(); 
-   if (num_elements == 8) {
-       set_reg(dst.vec_symbol(0), data1);
-       set_reg(dst.vec_symbol(1), data2);
-       set_reg(dst.vec_symbol(2), data3);
-       set_reg(dst.vec_symbol(3), data4);
-       set_reg(dst.vec_symbol(4), data5);
-       set_reg(dst.vec_symbol(5), data6);
-       set_reg(dst.vec_symbol(6), data7);
-       set_reg(dst.vec_symbol(7), data8);
-   }
-   else{
-	printf("error:set_wmma_vector_operands");
-   }
+void ptx_thread_info::set_vector_operand_values(const operand_info &dst,
+                                                const ptx_reg_t &data1,
+                                                const ptx_reg_t &data2,
+                                                const ptx_reg_t &data3,
+                                                const ptx_reg_t &data4) {
+  unsigned num_elements = dst.get_vect_nelem();
+  if (num_elements > 0) {
+    set_reg(dst.vec_symbol(0), data1);
+    if (num_elements > 1) {
+      set_reg(dst.vec_symbol(1), data2);
+      if (num_elements > 2) {
+        set_reg(dst.vec_symbol(2), data3);
+        if (num_elements > 3) {
+          set_reg(dst.vec_symbol(3), data4);
+        }
+      }
+    }
+  }
 
-   m_last_set_operand_value = data8;
+  m_last_set_operand_value = data1;
 }
+void ptx_thread_info::set_wmma_vector_operand_values(
+    const operand_info &dst, const ptx_reg_t &data1, const ptx_reg_t &data2,
+    const ptx_reg_t &data3, const ptx_reg_t &data4, const ptx_reg_t &data5,
+    const ptx_reg_t &data6, const ptx_reg_t &data7, const ptx_reg_t &data8) {
+  unsigned num_elements = dst.get_vect_nelem();
+  if (num_elements == 8) {
+    set_reg(dst.vec_symbol(0), data1);
+    set_reg(dst.vec_symbol(1), data2);
+    set_reg(dst.vec_symbol(2), data3);
+    set_reg(dst.vec_symbol(3), data4);
+    set_reg(dst.vec_symbol(4), data5);
+    set_reg(dst.vec_symbol(5), data6);
+    set_reg(dst.vec_symbol(6), data7);
+    set_reg(dst.vec_symbol(7), data8);
+  } else {
+    printf("error:set_wmma_vector_operands");
+  }
 
-#ifndef LIBCUDA
-
-#define my_abs(a) (((a)<0)?(-a):(a))
+  m_last_set_operand_value = data8;
+}
 
-#define MY_MAX_I(a,b) (a > b) ? a : b
-#define MY_MAX_F(a,b) isNaN(a) ? b : isNaN(b) ? a : (a > b) ? a : b
+#define my_abs(a) (((a) < 0) ? (-a) : (a))
 
-#define MY_MIN_I(a,b) (a < b) ? a : b
-#define MY_MIN_F(a,b) isNaN(a) ? b : isNaN(b) ? a : (a < b) ? a : b
+#define MY_MAX_I(a, b) (a > b) ? a : b
+#define MY_MAX_F(a, b) isNaN(a) ? b : isNaN(b) ? a : (a > b) ? a : b
 
-#define MY_INC_I(a,b) (a >= b) ? 0 : a+1
-#define MY_DEC_I(a,b) ((a == 0) || (a > b)) ? b : a-1
+#define MY_MIN_I(a, b) (a < b) ? a : b
+#define MY_MIN_F(a, b) isNaN(a) ? b : isNaN(b) ? a : (a < b) ? a : b
 
-#define MY_CAS_I(a,b,c) (a == b) ? c : a
+#define MY_INC_I(a, b) (a >= b) ? 0 : a + 1
+#define MY_DEC_I(a, b) ((a == 0) || (a > b)) ? b : a - 1
 
-#define MY_EXCH(a,b) b
+#define MY_CAS_I(a, b, c) (a == b) ? c : a
 
-void abs_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+#define MY_EXCH(a, b) b
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+void abs_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case S16_TYPE: d.s16 = my_abs(a.s16); break;
-   case S32_TYPE: d.s32 = my_abs(a.s32); break;
-   case S64_TYPE: d.s64 = my_abs(a.s64); break;
-   case U16_TYPE: d.s16 = my_abs(a.u16); break;
-   case U32_TYPE: d.s32 = my_abs(a.u32); break;
-   case U64_TYPE: d.s64 = my_abs(a.u64); break;
-   case F32_TYPE: d.f32 = my_abs(a.f32); break;
-   case F64_TYPE: case FF64_TYPE: d.f64 = my_abs(a.f64); break;
-   default:
+  switch (i_type) {
+    case S16_TYPE:
+      d.s16 = my_abs(a.s16);
+      break;
+    case S32_TYPE:
+      d.s32 = my_abs(a.s32);
+      break;
+    case S64_TYPE:
+      d.s64 = my_abs(a.s64);
+      break;
+    case U16_TYPE:
+      d.s16 = my_abs(a.u16);
+      break;
+    case U32_TYPE:
+      d.s32 = my_abs(a.u32);
+      break;
+    case U64_TYPE:
+      d.s64 = my_abs(a.u64);
+      break;
+    case F32_TYPE:
+      d.f32 = my_abs(a.f32);
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      d.f64 = my_abs(a.f64);
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void addp_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-   //PTXPlus add instruction with carry (carry is kept in a predicate) register
-   ptx_reg_t src1_data, src2_data, src3_data, data;
-   int overflow = 0;
-   int carry = 0;
-
-   const operand_info &dst  = pI->dst();  //get operand info of sources and destination
-   const operand_info &src1 = pI->src1(); //use them to determine that they are of type 'register'
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
-
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   src3_data = thread->get_operand_value(src3, dst, i_type, thread, 1);
-
-   unsigned rounding_mode = pI->rounding_mode();
-   int orig_rm = fegetround();
-   switch ( rounding_mode ) {
-   case RN_OPTION: break;
-   case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-   default: assert(0); break;
-   }
+void addp_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  // PTXPlus add instruction with carry (carry is kept in a predicate) register
+  ptx_reg_t src1_data, src2_data, src3_data, data;
+  int overflow = 0;
+  int carry = 0;
+
+  const operand_info &dst =
+      pI->dst();  // get operand info of sources and destination
+  const operand_info &src1 =
+      pI->src1();  // use them to determine that they are of type 'register'
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  src3_data = thread->get_operand_value(src3, dst, i_type, thread, 1);
+
+  unsigned rounding_mode = pI->rounding_mode();
+  int orig_rm = fegetround();
+  switch (rounding_mode) {
+    case RN_OPTION:
+      break;
+    case RZ_OPTION:
+      fesetround(FE_TOWARDZERO);
+      break;
+    default:
+      assert(0);
+      break;
+  }
 
-   //performs addition. Sets carry and overflow if needed.
-   //src3_data.pred&0x4 is the carry flag
-   switch ( i_type ) {
-   case S8_TYPE:
-      data.s64 = (src1_data.s64 & 0x0000000FF) + (src2_data.s64 & 0x0000000FF) + (src3_data.pred & 0x4);
-      if(((src1_data.s64 & 0x80)-(src2_data.s64 & 0x80)) == 0) {overflow=((src1_data.s64 & 0x80)-(data.s64 & 0x80))==0?0:1; }
-      carry = (data.u64 & 0x000000100)>>8;
-      break;
-   case S16_TYPE:
-      data.s64 = (src1_data.s64 & 0x00000FFFF) + (src2_data.s64 & 0x00000FFFF) + (src3_data.pred & 0x4);
-      if(((src1_data.s64 & 0x8000)-(src2_data.s64 & 0x8000)) == 0) {overflow=((src1_data.s64 & 0x8000)-(data.s64 & 0x8000))==0?0:1; }
-      carry = (data.u64 & 0x000010000)>>16;
-      break;
-   case S32_TYPE:
-      data.s64 = (src1_data.s64 & 0x0FFFFFFFF) + (src2_data.s64 & 0x0FFFFFFFF) + (src3_data.pred & 0x4);
-      if(((src1_data.s64 & 0x80000000)-(src2_data.s64 & 0x80000000)) == 0) {overflow=((src1_data.s64 & 0x80000000)-(data.s64 & 0x80000000))==0?0:1; }
-      carry = (data.u64 & 0x100000000)>>32;
-      break;
-   case S64_TYPE:
+  // performs addition. Sets carry and overflow if needed.
+  // src3_data.pred&0x4 is the carry flag
+  switch (i_type) {
+    case S8_TYPE:
+      data.s64 = (src1_data.s64 & 0x0000000FF) + (src2_data.s64 & 0x0000000FF) +
+                 (src3_data.pred & 0x4);
+      if (((src1_data.s64 & 0x80) - (src2_data.s64 & 0x80)) == 0) {
+        overflow = ((src1_data.s64 & 0x80) - (data.s64 & 0x80)) == 0 ? 0 : 1;
+      }
+      carry = (data.u64 & 0x000000100) >> 8;
+      break;
+    case S16_TYPE:
+      data.s64 = (src1_data.s64 & 0x00000FFFF) + (src2_data.s64 & 0x00000FFFF) +
+                 (src3_data.pred & 0x4);
+      if (((src1_data.s64 & 0x8000) - (src2_data.s64 & 0x8000)) == 0) {
+        overflow =
+            ((src1_data.s64 & 0x8000) - (data.s64 & 0x8000)) == 0 ? 0 : 1;
+      }
+      carry = (data.u64 & 0x000010000) >> 16;
+      break;
+    case S32_TYPE:
+      data.s64 = (src1_data.s64 & 0x0FFFFFFFF) + (src2_data.s64 & 0x0FFFFFFFF) +
+                 (src3_data.pred & 0x4);
+      if (((src1_data.s64 & 0x80000000) - (src2_data.s64 & 0x80000000)) == 0) {
+        overflow = ((src1_data.s64 & 0x80000000) - (data.s64 & 0x80000000)) == 0
+                       ? 0
+                       : 1;
+      }
+      carry = (data.u64 & 0x100000000) >> 32;
+      break;
+    case S64_TYPE:
       data.s64 = src1_data.s64 + src2_data.s64 + (src3_data.pred & 0x4);
       break;
-   case U8_TYPE:
-      data.u64 = (src1_data.u64 & 0xFF) + (src2_data.u64 & 0xFF) + (src3_data.pred & 0x4);
-      carry = (data.u64 & 0x100)>>8;
+    case U8_TYPE:
+      data.u64 = (src1_data.u64 & 0xFF) + (src2_data.u64 & 0xFF) +
+                 (src3_data.pred & 0x4);
+      carry = (data.u64 & 0x100) >> 8;
       break;
-   case U16_TYPE:
-      data.u64 = (src1_data.u64 & 0xFFFF) + (src2_data.u64 & 0xFFFF) + (src3_data.pred & 0x4);
-      carry = (data.u64 & 0x10000)>>16;
+    case U16_TYPE:
+      data.u64 = (src1_data.u64 & 0xFFFF) + (src2_data.u64 & 0xFFFF) +
+                 (src3_data.pred & 0x4);
+      carry = (data.u64 & 0x10000) >> 16;
       break;
-   case U32_TYPE:
-      data.u64 = (src1_data.u64 & 0xFFFFFFFF) + (src2_data.u64 & 0xFFFFFFFF) + (src3_data.pred & 0x4);
-      carry = (data.u64 & 0x100000000)>>32;
+    case U32_TYPE:
+      data.u64 = (src1_data.u64 & 0xFFFFFFFF) + (src2_data.u64 & 0xFFFFFFFF) +
+                 (src3_data.pred & 0x4);
+      carry = (data.u64 & 0x100000000) >> 32;
       break;
-   case U64_TYPE:
+    case U64_TYPE:
       data.s64 = src1_data.s64 + src2_data.s64 + (src3_data.pred & 0x4);
       break;
-   case F16_TYPE: data.f16=src1_data.f16+src2_data.f16; break;//assert(0); break;
-   case F32_TYPE: data.f32 = src1_data.f32 + src2_data.f32; break;
-   case F64_TYPE: case FF64_TYPE: data.f64 = src1_data.f64 + src2_data.f64; break;
-   default: assert(0); break;
-   }
-   fesetround( orig_rm );
+    case F16_TYPE:
+      data.f16 = src1_data.f16 + src2_data.f16;
+      break;  // assert(0); break;
+    case F32_TYPE:
+      data.f32 = src1_data.f32 + src2_data.f32;
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      data.f64 = src1_data.f64 + src2_data.f64;
+      break;
+    default:
+      assert(0);
+      break;
+  }
+  fesetround(orig_rm);
 
-   thread->set_operand_value(dst, data, i_type, thread, pI, overflow, carry  );
+  thread->set_operand_value(dst, data, i_type, thread, pI, overflow, carry);
 }
 
-void add_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
-   int overflow = 0;
-   int carry = 0;
-
-   const operand_info &dst  = pI->dst();  //get operand info of sources and destination 
-   const operand_info &src1 = pI->src1(); //use them to determine that they are of type 'register'
-   const operand_info &src2 = pI->src2();
-
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
-
-   unsigned rounding_mode = pI->rounding_mode();
-   int orig_rm = fegetround();
-   switch ( rounding_mode ) {
-   case RN_OPTION: break;
-   case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-   default: assert(0); break;
-   }
+void add_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
+  int overflow = 0;
+  int carry = 0;
+
+  const operand_info &dst =
+      pI->dst();  // get operand info of sources and destination
+  const operand_info &src1 =
+      pI->src1();  // use them to determine that they are of type 'register'
+  const operand_info &src2 = pI->src2();
+
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+
+  unsigned rounding_mode = pI->rounding_mode();
+  int orig_rm = fegetround();
+  switch (rounding_mode) {
+    case RN_OPTION:
+      break;
+    case RZ_OPTION:
+      fesetround(FE_TOWARDZERO);
+      break;
+    default:
+      assert(0);
+      break;
+  }
 
-   //performs addition. Sets carry and overflow if needed.
-   switch ( i_type ) {
-   case S8_TYPE:
+  // performs addition. Sets carry and overflow if needed.
+  switch (i_type) {
+    case S8_TYPE:
       data.s64 = (src1_data.s64 & 0x0000000FF) + (src2_data.s64 & 0x0000000FF);
-      if(((src1_data.s64 & 0x80)-(src2_data.s64 & 0x80)) == 0) {overflow=((src1_data.s64 & 0x80)-(data.s64 & 0x80))==0?0:1; }
-      carry = (data.u64 & 0x000000100)>>8;
+      if (((src1_data.s64 & 0x80) - (src2_data.s64 & 0x80)) == 0) {
+        overflow = ((src1_data.s64 & 0x80) - (data.s64 & 0x80)) == 0 ? 0 : 1;
+      }
+      carry = (data.u64 & 0x000000100) >> 8;
       break;
-   case S16_TYPE:
+    case S16_TYPE:
       data.s64 = (src1_data.s64 & 0x00000FFFF) + (src2_data.s64 & 0x00000FFFF);
-      if(((src1_data.s64 & 0x8000)-(src2_data.s64 & 0x8000)) == 0) {overflow=((src1_data.s64 & 0x8000)-(data.s64 & 0x8000))==0?0:1; }
-      carry = (data.u64 & 0x000010000)>>16;
+      if (((src1_data.s64 & 0x8000) - (src2_data.s64 & 0x8000)) == 0) {
+        overflow =
+            ((src1_data.s64 & 0x8000) - (data.s64 & 0x8000)) == 0 ? 0 : 1;
+      }
+      carry = (data.u64 & 0x000010000) >> 16;
       break;
-   case S32_TYPE:
+    case S32_TYPE:
       data.s64 = (src1_data.s64 & 0x0FFFFFFFF) + (src2_data.s64 & 0x0FFFFFFFF);
-      if(((src1_data.s64 & 0x80000000)-(src2_data.s64 & 0x80000000)) == 0) {overflow=((src1_data.s64 & 0x80000000)-(data.s64 & 0x80000000))==0?0:1; }
-      carry = (data.u64 & 0x100000000)>>32;
+      if (((src1_data.s64 & 0x80000000) - (src2_data.s64 & 0x80000000)) == 0) {
+        overflow = ((src1_data.s64 & 0x80000000) - (data.s64 & 0x80000000)) == 0
+                       ? 0
+                       : 1;
+      }
+      carry = (data.u64 & 0x100000000) >> 32;
       break;
-   case S64_TYPE:
+    case S64_TYPE:
       data.s64 = src1_data.s64 + src2_data.s64;
       break;
-   case U8_TYPE:
+    case U8_TYPE:
       data.u64 = (src1_data.u64 & 0xFF) + (src2_data.u64 & 0xFF);
-      carry = (data.u64 & 0x100)>>8;
+      carry = (data.u64 & 0x100) >> 8;
       break;
-   case U16_TYPE:
+    case U16_TYPE:
       data.u64 = (src1_data.u64 & 0xFFFF) + (src2_data.u64 & 0xFFFF);
-      carry = (data.u64 & 0x10000)>>16;
+      carry = (data.u64 & 0x10000) >> 16;
       break;
-   case U32_TYPE:
+    case U32_TYPE:
       data.u64 = (src1_data.u64 & 0xFFFFFFFF) + (src2_data.u64 & 0xFFFFFFFF);
-      carry = (data.u64 & 0x100000000)>>32;
+      carry = (data.u64 & 0x100000000) >> 32;
       break;
-   case U64_TYPE:
+    case U64_TYPE:
       data.u64 = src1_data.u64 + src2_data.u64;
       break;
-   case F16_TYPE: data.f16=src1_data.f16+src2_data.f16; break;//assert(0); break;
-   case F32_TYPE: data.f32 = src1_data.f32 + src2_data.f32; break;
-   case F64_TYPE: case FF64_TYPE: data.f64 = src1_data.f64 + src2_data.f64; break;
-   default: assert(0); break;
-   }
-   fesetround( orig_rm );
+    case F16_TYPE:
+      data.f16 = src1_data.f16 + src2_data.f16;
+      break;  // assert(0); break;
+    case F32_TYPE:
+      data.f32 = src1_data.f32 + src2_data.f32;
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      data.f64 = src1_data.f64 + src2_data.f64;
+      break;
+    default:
+      assert(0);
+      break;
+  }
+  fesetround(orig_rm);
 
-   thread->set_operand_value(dst, data, i_type, thread, pI, overflow, carry  );
+  thread->set_operand_value(dst, data, i_type, thread, pI, overflow, carry);
 }
 
-void addc_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-
-void and_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
+void addc_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+void and_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   //the way ptxplus handles predicates: 1 = false and 0 = true
-   if(i_type == PRED_TYPE)
-      data.pred = ~(~(src1_data.pred) & ~(src2_data.pred));
-   else
-      data.u64 = src1_data.u64 & src2_data.u64;
+  // the way ptxplus handles predicates: 1 = false and 0 = true
+  if (i_type == PRED_TYPE)
+    data.pred = ~(~(src1_data.pred) & ~(src2_data.pred));
+  else
+    data.u64 = src1_data.u64 & src2_data.u64;
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void andn_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
+void andn_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case B16_TYPE:  src2_data.u16  = ~src2_data.u16; break;
-   case B32_TYPE:  src2_data.u32  = ~src2_data.u32; break;
-   case B64_TYPE:  src2_data.u64  = ~src2_data.u64; break;
-   default:
+  switch (i_type) {
+    case B16_TYPE:
+      src2_data.u16 = ~src2_data.u16;
+      break;
+    case B32_TYPE:
+      src2_data.u32 = ~src2_data.u32;
+      break;
+    case B64_TYPE:
+      src2_data.u64 = ~src2_data.u64;
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   data.u64 = src1_data.u64 & src2_data.u64;
+  data.u64 = src1_data.u64 & src2_data.u64;
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void bar_callback( const inst_t* inst, ptx_thread_info* thread)
-{
-	unsigned ctaid = thread->get_cta_uid();
-	unsigned barid = inst->bar_id;
-	unsigned value = thread->get_reduction_value(ctaid,barid);
-	const ptx_instruction *pI = dynamic_cast<const ptx_instruction*>(inst);
-	const operand_info &dst  = pI->dst();
-	ptx_reg_t data;
-	data.u32 = value;
-	thread->set_operand_value(dst,value, U32_TYPE, thread, pI);
+void bar_callback(const inst_t *inst, ptx_thread_info *thread) {
+  unsigned ctaid = thread->get_cta_uid();
+  unsigned barid = inst->bar_id;
+  unsigned value = thread->get_reduction_value(ctaid, barid);
+  const ptx_instruction *pI = dynamic_cast<const ptx_instruction *>(inst);
+  const operand_info &dst = pI->dst();
+  ptx_reg_t data;
+  data.u32 = value;
+  thread->set_operand_value(dst, value, U32_TYPE, thread, pI);
 }
 
-void atom_callback( const inst_t* inst, ptx_thread_info* thread)
-{
-   const ptx_instruction *pI = dynamic_cast<const ptx_instruction*>(inst);
+void atom_callback(const inst_t *inst, ptx_thread_info *thread) {
+  const ptx_instruction *pI = dynamic_cast<const ptx_instruction *>(inst);
+
+  // "Decode" the output type
+  unsigned to_type = pI->get_type();
+  size_t size;
+  int t;
+  type_info_key::type_decode(to_type, size, t);
+
+  // Set up operand variables
+  ptx_reg_t data;       // d
+  ptx_reg_t src1_data;  // a
+  ptx_reg_t src2_data;  // b
+  ptx_reg_t op_result;  // temp variable to hold operation result
+
+  bool data_ready = false;
+
+  // Get operand info of sources and destination
+  const operand_info &dst = pI->dst();    // d
+  const operand_info &src1 = pI->src1();  // a
+  const operand_info &src2 = pI->src2();  // b
+
+  // Get operand values
+  src1_data = thread->get_operand_value(src1, src1, to_type, thread, 1);  // a
+  if (dst.get_symbol()->type()) {
+    src2_data = thread->get_operand_value(src2, dst, to_type, thread, 1);  // b
+  } else {
+    // This is the case whent he first argument (dest) is '_'
+    src2_data = thread->get_operand_value(src2, src1, to_type, thread, 1);  // b
+  }
 
-   // "Decode" the output type
-   unsigned to_type = pI->get_type();
-   size_t size;
-   int t;
-   type_info_key::type_decode(to_type, size, t);
-
-   // Set up operand variables
-   ptx_reg_t data;        // d
-   ptx_reg_t src1_data;   // a
-   ptx_reg_t src2_data;   // b
-   ptx_reg_t op_result;   // temp variable to hold operation result
+  // Check state space
+  addr_t effective_address = src1_data.u64;
+  memory_space_t space = pI->get_space();
+  if (space == undefined_space) {
+    // generic space - determine space via address
+    if (whichspace(effective_address) == global_space) {
+      effective_address = generic_to_global(effective_address);
+      space = global_space;
+    } else if (whichspace(effective_address) == shared_space) {
+      unsigned smid = thread->get_hw_sid();
+      effective_address = generic_to_shared(smid, effective_address);
+      space = shared_space;
+    } else {
+      abort();
+    }
+  }
+  assert(space == global_space || space == shared_space);
 
-   bool data_ready = false;
+  memory_space *mem = NULL;
+  if (space == global_space)
+       // TODO schi
+       panic("gem5-gpu: Global atomics shouldn't call atom_callback!\n");
+       // mem = thread->get_global_memory();
+  else if (space == shared_space)
+    mem = thread->m_shared_mem;
+  else
+    abort();
+
+  // Copy value pointed to in operand 'a' into register 'd'
+  // (i.e. copy src1_data to dst)
+  mem->read(effective_address, size / 8, &data.s64);
+  if (dst.get_symbol()->type()) {
+    thread->set_operand_value(dst, data, to_type, thread,
+                              pI);  // Write value into register 'd'
+  }
 
-   // Get operand info of sources and destination
-   const operand_info &dst  = pI->dst();     // d
-   const operand_info &src1 = pI->src1();    // a
-   const operand_info &src2 = pI->src2();    // b
+  // Get the atomic operation to be performed
+  unsigned m_atomic_spec = pI->get_atomic();
 
-   // Get operand values
-   src1_data = thread->get_operand_value(src1, src1, to_type, thread, 1);        // a
-   if (dst.get_symbol()->type()){
-      src2_data = thread->get_operand_value(src2, dst, to_type, thread, 1);      // b
-   } else {
-	   //This is the case whent he first argument (dest) is '_'
-      src2_data = thread->get_operand_value(src2, src1, to_type, thread, 1);     // b
-   }
+  switch (m_atomic_spec) {
+    // AND
+    case ATOMIC_AND: {
+      switch (to_type) {
+        case B32_TYPE:
+        case U32_TYPE:
+          op_result.u32 = data.u32 & src2_data.u32;
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = data.s32 & src2_data.s32;
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch (%x) with instruction\natom.AND "
+              "only accepts b32\n",
+              to_type);
+          assert(0);
+          break;
+      }
 
-   // Check state space
-   addr_t effective_address = src1_data.u64;  
-   memory_space_t space = pI->get_space();
-   if (space == undefined_space) {
-      // generic space - determine space via address
-      if( whichspace(effective_address) == global_space ) {
-         effective_address = generic_to_global(effective_address);
-         space = global_space;
-      } else if( whichspace(effective_address) == shared_space ) {
-         unsigned smid = thread->get_hw_sid();
-         effective_address = generic_to_shared(smid,effective_address);
-         space = shared_space;
-      } else {
-         abort();
+      break;
+    }
+      // OR
+    case ATOMIC_OR: {
+      switch (to_type) {
+        case B32_TYPE:
+        case U32_TYPE:
+          op_result.u32 = data.u32 | src2_data.u32;
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = data.s32 | src2_data.s32;
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch (%x) with instruction\natom.OR "
+              "only accepts b32\n",
+              to_type);
+          assert(0);
+          break;
       }
-   } 
-   assert( space == global_space || space == shared_space );
 
-   memory_space *mem = NULL;
-   if(space == global_space)
-       // TODO schi
-       panic("gem5-gpu: Global atomics shouldn't call atom_callback!\n");
-       // mem = thread->get_global_memory();
-   else if(space == shared_space)
-       mem = thread->m_shared_mem;
-   else
-       abort();
-
-   // Copy value pointed to in operand 'a' into register 'd'
-   // (i.e. copy src1_data to dst)
-   mem->read(effective_address,size/8,&data.s64);
-   if (dst.get_symbol()->type()){
-	   thread->set_operand_value(dst, data, to_type, thread, pI);                         // Write value into register 'd'
-   }
+      break;
+    }
+      // XOR
+    case ATOMIC_XOR: {
+      switch (to_type) {
+        case B32_TYPE:
+        case U32_TYPE:
+          op_result.u32 = data.u32 ^ src2_data.u32;
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = data.s32 ^ src2_data.s32;
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch (%x) with instruction\natom.XOR "
+              "only accepts b32\n",
+              to_type);
+          assert(0);
+          break;
+      }
 
-   // Get the atomic operation to be performed
-   unsigned m_atomic_spec = pI->get_atomic();
+      break;
+    }
+      // CAS
+    case ATOMIC_CAS: {
+      ptx_reg_t src3_data;
+      const operand_info &src3 = pI->src3();
+      src3_data = thread->get_operand_value(src3, dst, to_type, thread, 1);
 
-   switch ( m_atomic_spec ) {
-   // AND
-   case ATOMIC_AND:
-      {
+      switch (to_type) {
+        case B32_TYPE:
+        case U32_TYPE:
+          op_result.u32 = MY_CAS_I(data.u32, src2_data.u32, src3_data.u32);
+          data_ready = true;
+          break;
+        case B64_TYPE:
+        case U64_TYPE:
+          op_result.u64 = MY_CAS_I(data.u64, src2_data.u64, src3_data.u64);
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = MY_CAS_I(data.s32, src2_data.s32, src3_data.s32);
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch (%x) with instruction\natom.CAS "
+              "only accepts b32 and b64\n",
+              to_type);
+          assert(0);
+          break;
+      }
 
-         switch ( to_type ) {
-         case B32_TYPE:
-         case U32_TYPE:
-            op_result.u32 = data.u32 & src2_data.u32;
-            data_ready = true;
-            break;
-         case S32_TYPE:
-            op_result.s32 = data.s32 & src2_data.s32;
-            data_ready = true;
-            break;
-         default:
-            printf("Execution error: type mismatch (%x) with instruction\natom.AND only accepts b32\n", to_type);
-            assert(0);
-            break;
-         }
+      break;
+    }
+      // EXCH
+    case ATOMIC_EXCH: {
+      switch (to_type) {
+        case B32_TYPE:
+        case U32_TYPE:
+          op_result.u32 = MY_EXCH(data.u32, src2_data.u32);
+          data_ready = true;
+          break;
+        case B64_TYPE:
+        case U64_TYPE:
+          op_result.u64 = MY_EXCH(data.u64, src2_data.u64);
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = MY_EXCH(data.s32, src2_data.s32);
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch (%x) with instruction\natom.EXCH "
+              "only accepts b32\n",
+              to_type);
+          assert(0);
+          break;
+      }
 
-         break;
+      break;
+    }
+      // ADD
+    case ATOMIC_ADD: {
+      switch (to_type) {
+        case U32_TYPE:
+          op_result.u32 = data.u32 + src2_data.u32;
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = data.s32 + src2_data.s32;
+          data_ready = true;
+          break;
+        case U64_TYPE:
+          op_result.u64 = data.u64 + src2_data.u64;
+          data_ready = true;
+          break;
+        case F32_TYPE:
+          op_result.f32 = data.f32 + src2_data.f32;
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch with instruction\natom.ADD only "
+              "accepts u32, s32, u64, and f32\n");
+          assert(0);
+          break;
       }
-      // OR
-   case ATOMIC_OR:
-      {
-
-         switch ( to_type ) {
-         case B32_TYPE:
-         case U32_TYPE:
-            op_result.u32 = data.u32 | src2_data.u32;
-            data_ready = true;
-            break;
-         case S32_TYPE:
-            op_result.s32 = data.s32 | src2_data.s32;
-            data_ready = true;
-            break;
-         default:
-            printf("Execution error: type mismatch (%x) with instruction\natom.OR only accepts b32\n", to_type);
-            assert(0);
-            break;
-         }
 
-         break;
+      break;
+    }
+      // INC
+    case ATOMIC_INC: {
+      switch (to_type) {
+        case U32_TYPE:
+          op_result.u32 = MY_INC_I(data.u32, src2_data.u32);
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch with instruction\natom.INC only "
+              "accepts u32 and s32\n");
+          assert(0);
+          break;
       }
-      // XOR
-   case ATOMIC_XOR:
-      {
-
-         switch ( to_type ) {
-         case B32_TYPE:
-         case U32_TYPE:
-            op_result.u32 = data.u32 ^ src2_data.u32;
-            data_ready = true;
-            break;
-         case S32_TYPE:
-            op_result.s32 = data.s32 ^ src2_data.s32;
-            data_ready = true;
-            break;
-         default:
-            printf("Execution error: type mismatch (%x) with instruction\natom.XOR only accepts b32\n", to_type);
-            assert(0);
-            break;
-         }
 
-         break;
+      break;
+    }
+      // DEC
+    case ATOMIC_DEC: {
+      switch (to_type) {
+        case U32_TYPE:
+          op_result.u32 = MY_DEC_I(data.u32, src2_data.u32);
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch with instruction\natom.DEC only "
+              "accepts u32 and s32\n");
+          assert(0);
+          break;
       }
-      // CAS
-   case ATOMIC_CAS:
-      {
-
-         ptx_reg_t src3_data;
-         const operand_info &src3 = pI->src3();
-         src3_data = thread->get_operand_value(src3, dst, to_type, thread, 1);
-
-         switch ( to_type ) {
-         case B32_TYPE:
-         case U32_TYPE:
-            op_result.u32 = MY_CAS_I(data.u32, src2_data.u32, src3_data.u32);
-            data_ready = true;
-            break;
-         case B64_TYPE:
-         case U64_TYPE:
-            op_result.u64 = MY_CAS_I(data.u64, src2_data.u64, src3_data.u64);
-            data_ready = true;
-            break;
-         case S32_TYPE:
-            op_result.s32 = MY_CAS_I(data.s32, src2_data.s32, src3_data.s32);
-            data_ready = true;
-            break;
-         default:
-            printf("Execution error: type mismatch (%x) with instruction\natom.CAS only accepts b32 and b64\n", to_type);
-            assert(0);
-            break;
-         }
 
-         break;
+      break;
+    }
+      // MIN
+    case ATOMIC_MIN: {
+      switch (to_type) {
+        case U32_TYPE:
+          op_result.u32 = MY_MIN_I(data.u32, src2_data.u32);
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = MY_MIN_I(data.s32, src2_data.s32);
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch with instruction\natom.MIN only "
+              "accepts u32 and s32\n");
+          assert(0);
+          break;
       }
-      // EXCH
-   case ATOMIC_EXCH:
-      {
-         switch ( to_type ) {
-         case B32_TYPE:
-         case U32_TYPE:
-            op_result.u32 = MY_EXCH(data.u32, src2_data.u32);
-            data_ready = true;
-            break;
-         case B64_TYPE:
-         case U64_TYPE:
-            op_result.u64 = MY_EXCH(data.u64, src2_data.u64);
-            data_ready = true;
-            break;
-         case S32_TYPE:
-            op_result.s32 = MY_EXCH(data.s32, src2_data.s32);
-            data_ready = true;
-            break;
-         default:
-            printf("Execution error: type mismatch (%x) with instruction\natom.EXCH only accepts b32\n", to_type);
-            assert(0);
-            break;
-         }
 
-         break;
+      break;
+    }
+      // MAX
+    case ATOMIC_MAX: {
+      switch (to_type) {
+        case U32_TYPE:
+          op_result.u32 = MY_MAX_I(data.u32, src2_data.u32);
+          data_ready = true;
+          break;
+        case S32_TYPE:
+          op_result.s32 = MY_MAX_I(data.s32, src2_data.s32);
+          data_ready = true;
+          break;
+        default:
+          printf(
+              "Execution error: type mismatch with instruction\natom.MAX only "
+              "accepts u32 and s32\n");
+          assert(0);
+          break;
       }
-      // ADD
-   case ATOMIC_ADD:
-      {
 
-         switch ( to_type ) {
-         case U32_TYPE:
-            op_result.u32 = data.u32 + src2_data.u32;
-            data_ready = true;
-            break;
-         case S32_TYPE:
-            op_result.s32 = data.s32 + src2_data.s32;
-            data_ready = true;
-            break;
-         case U64_TYPE: 
-            op_result.u64 = data.u64 + src2_data.u64; 
-            data_ready = true; 
-            break; 
-         case F32_TYPE: 
-            op_result.f32 = data.f32 + src2_data.f32; 
-            data_ready = true; 
-            break; 
-         default:
-            printf("Execution error: type mismatch with instruction\natom.ADD only accepts u32, s32, u64, and f32\n");
-            assert(0);
-            break;
-         }
+      break;
+    }
+      // DEFAULT
+    default: {
+      assert(0);
+      break;
+    }
+  }
 
-         break;
-      }
-      // INC
-   case ATOMIC_INC:
-      {
-         switch ( to_type ) {
-         case U32_TYPE: 
-            op_result.u32 = MY_INC_I(data.u32, src2_data.u32);
-            data_ready = true;
-            break;
-         default:
-            printf("Execution error: type mismatch with instruction\natom.INC only accepts u32 and s32\n");
-            assert(0);
-            break;
-         }
+  // Write operation result into  memory
+  // (i.e. copy src1_data to dst)
+  if (data_ready) {
+    mem->write(effective_address, size / 8, &op_result.s64, thread, pI);
+  } else {
+    printf("Execution error: data_ready not set\n");
+    assert(0);
+  }
+}
+
+// atom_impl will now result in a callback being called in mem_ctrl_pop
+// (gpu-sim.c)
+void atom_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  // SYNTAX
+  // atom.space.operation.type d, a, b[, c]; (now read in callback)
+
+  // obtain memory space of the operation
+  memory_space_t space = pI->get_space();
+
+  // get the memory address
+  const operand_info &src1 = pI->src1();
+  // const operand_info &dst  = pI->dst();  // not needed for effective address
+  // calculation
+  unsigned i_type = pI->get_type();
+  ptx_reg_t src1_data;
+  src1_data = thread->get_operand_value(src1, src1, i_type, thread, 1);
+  addr_t effective_address = src1_data.u64;
+
+  addr_t effective_address_final;
+
+  // handle generic memory space by converting it to global
+  if (space == undefined_space) {
+    if (whichspace(effective_address) == global_space) {
+      effective_address_final = generic_to_global(effective_address);
+      space = global_space;
+    } else if (whichspace(effective_address) == shared_space) {
+      unsigned smid = thread->get_hw_sid();
+      effective_address_final = generic_to_shared(smid, effective_address);
+      space = shared_space;
+    } else {
+      abort();
+    }
+  } else {
+    assert(space == global_space || space == shared_space);
+    effective_address_final = effective_address;
+  }
 
-         break;
+  // Check state space
+  assert(space == global_space || space == shared_space);
+
+  thread->m_last_effective_address = effective_address_final;
+  thread->m_last_memory_space = space;
+  thread->m_last_dram_callback.function = atom_callback;
+  thread->m_last_dram_callback.instruction = pI;
+}
+
+void bar_impl(const ptx_instruction *pIin, ptx_thread_info *thread) {
+  ptx_instruction *pI = const_cast<ptx_instruction *>(pIin);
+  unsigned bar_op = pI->barrier_op();
+  unsigned red_op = pI->get_atomic();
+  unsigned ctaid = thread->get_cta_uid();
+
+  switch (bar_op) {
+    case SYNC_OPTION: {
+      if (pI->get_num_operands() > 1) {
+        const operand_info &op0 = pI->dst();
+        const operand_info &op1 = pI->src1();
+        ptx_reg_t op0_data;
+        ptx_reg_t op1_data;
+        op0_data = thread->get_operand_value(op0, op0, U32_TYPE, thread, 1);
+        op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
+        pI->set_bar_id(op0_data.u32);
+        pI->set_bar_count(op1_data.u32);
+      } else {
+        const operand_info &op0 = pI->dst();
+        ptx_reg_t op0_data;
+        op0_data = thread->get_operand_value(op0, op0, U32_TYPE, thread, 1);
+        pI->set_bar_id(op0_data.u32);
       }
-      // DEC
-   case ATOMIC_DEC:
-      {
-         switch ( to_type ) {
-         case U32_TYPE: 
-            op_result.u32 = MY_DEC_I(data.u32, src2_data.u32);
-            data_ready = true;
+      break;
+    }
+    case ARRIVE_OPTION: {
+      const operand_info &op0 = pI->dst();
+      const operand_info &op1 = pI->src1();
+      ptx_reg_t op0_data;
+      ptx_reg_t op1_data;
+      op0_data = thread->get_operand_value(op0, op0, U32_TYPE, thread, 1);
+      op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
+      pI->set_bar_id(op0_data.u32);
+      pI->set_bar_count(op1_data.u32);
+      break;
+    }
+    case RED_OPTION: {
+      if (pI->get_num_operands() > 3) {
+        const operand_info &op1 = pI->src1();
+        const operand_info &op2 = pI->src2();
+        const operand_info &op3 = pI->src3();
+        ptx_reg_t op1_data;
+        ptx_reg_t op2_data;
+        ptx_reg_t op3_data;
+        op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
+        op2_data = thread->get_operand_value(op2, op2, U32_TYPE, thread, 1);
+        op3_data = thread->get_operand_value(op3, op3, PRED_TYPE, thread, 1);
+        op3_data.u32 = !(op3_data.pred & 0x0001);
+        pI->set_bar_id(op1_data.u32);
+        pI->set_bar_count(op2_data.u32);
+        switch (red_op) {
+          case ATOMIC_POPC:
+            thread->popc_reduction(ctaid, op1_data.u32, op3_data.u32);
             break;
-         default:
-            printf("Execution error: type mismatch with instruction\natom.DEC only accepts u32 and s32\n");
-            assert(0);
+          case ATOMIC_AND:
+            thread->and_reduction(ctaid, op1_data.u32, op3_data.u32);
             break;
-         }
-
-         break;
-      }
-      // MIN
-   case ATOMIC_MIN:
-      {
-         switch ( to_type ) {
-         case U32_TYPE: 
-            op_result.u32 = MY_MIN_I(data.u32, src2_data.u32);
-            data_ready = true;
+          case ATOMIC_OR:
+            thread->or_reduction(ctaid, op1_data.u32, op3_data.u32);
             break;
-         case S32_TYPE:
-            op_result.s32 = MY_MIN_I(data.s32, src2_data.s32);
-            data_ready = true;
+          default:
+            abort();
             break;
-         default:
-            printf("Execution error: type mismatch with instruction\natom.MIN only accepts u32 and s32\n");
-            assert(0);
+        }
+      } else {
+        const operand_info &op1 = pI->src1();
+        const operand_info &op2 = pI->src2();
+        ptx_reg_t op1_data;
+        ptx_reg_t op2_data;
+        op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
+        op2_data = thread->get_operand_value(op2, op2, PRED_TYPE, thread, 1);
+        op2_data.u32 = !(op2_data.pred & 0x0001);
+        pI->set_bar_id(op1_data.u32);
+        pI->set_bar_count(thread->get_ntid().x * thread->get_ntid().y *
+                          thread->get_ntid().z);
+        switch (red_op) {
+          case ATOMIC_POPC:
+            thread->popc_reduction(ctaid, op1_data.u32, op2_data.u32);
             break;
-         }
-
-         break;
-      }
-      // MAX
-   case ATOMIC_MAX:
-      {
-         switch ( to_type ) {
-         case U32_TYPE:
-            op_result.u32 = MY_MAX_I(data.u32, src2_data.u32);
-            data_ready = true;
+          case ATOMIC_AND:
+            thread->and_reduction(ctaid, op1_data.u32, op2_data.u32);
             break;
-         case S32_TYPE:
-            op_result.s32 = MY_MAX_I(data.s32, src2_data.s32);
-            data_ready = true;
+          case ATOMIC_OR:
+            thread->or_reduction(ctaid, op1_data.u32, op2_data.u32);
             break;
-         default:
-            printf("Execution error: type mismatch with instruction\natom.MAX only accepts u32 and s32\n");
-            assert(0);
+          default:
+            abort();
             break;
-         }
-
-         break;
-      }
-      // DEFAULT
-   default:
-      {
-         assert(0);
-         break;
+        }
       }
-   }
+      break;
+    }
+    default:
+      abort();
+      break;
+  }
 
-   // Write operation result into  memory
-   // (i.e. copy src1_data to dst)
-   if ( data_ready ) {
-      mem->write(effective_address,size/8,&op_result.s64,thread,pI);
-   } else {
-      printf("Execution error: data_ready not set\n");
-      assert(0);
-   }
+  thread->m_last_dram_callback.function = bar_callback;
+  thread->m_last_dram_callback.instruction = pIin;
 }
 
-// atom_impl will now result in a callback being called in mem_ctrl_pop (gpu-sim.c)
-void atom_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{   
-   // SYNTAX
-   // atom.space.operation.type d, a, b[, c]; (now read in callback)
-
-   // obtain memory space of the operation 
-   memory_space_t space = pI->get_space(); 
-
-   // get the memory address
-   const operand_info &src1 = pI->src1();
-   // const operand_info &dst  = pI->dst();  // not needed for effective address calculation 
-   unsigned i_type = pI->get_type();
-   ptx_reg_t src1_data;
-   src1_data = thread->get_operand_value(src1, src1, i_type, thread, 1);
-   addr_t effective_address = src1_data.u64; 
-
-   addr_t effective_address_final; 
-
-   // handle generic memory space by converting it to global 
-   if ( space == undefined_space ) {
-      if( whichspace(effective_address) == global_space ) {
-         effective_address_final = generic_to_global(effective_address);
-         space = global_space;
-      } else if( whichspace(effective_address) == shared_space ) {
-         unsigned smid = thread->get_hw_sid();
-         effective_address_final = generic_to_shared(smid,effective_address);
-         space = shared_space;
+void bfe_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  unsigned i_type = pI->get_type();
+  unsigned msb = (i_type == U32_TYPE || i_type == S32_TYPE) ? 31 : 63;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+  ptx_reg_t src = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  ptx_reg_t c = thread->get_operand_value(src3, dst, i_type, thread, 1);
+  ptx_reg_t data;
+  unsigned pos = b.u32 & 0xFF;
+  unsigned len = c.u32 & 0xFF;
+  switch (i_type) {
+    case U32_TYPE: {
+      unsigned mask;
+      data.u32 = src.u32 >> pos;
+      mask = 0xFFFFFFFF >> (32 - len);
+      data.u32 &= mask;
+      break;
+    }
+    case U64_TYPE: {
+      unsigned long mask;
+      data.u64 = src.u64 >> pos;
+      mask = 0xFFFFFFFFFFFFFFFF >> (64 - len);
+      data.u64 &= mask;
+      break;
+    }
+    case S32_TYPE: {
+      unsigned mask;
+      unsigned min = MY_MIN_I(pos + len - 1, msb);
+      unsigned sbit = len == 0 ? 0 : (src.s32 >> min) & 0x1;
+      data.s32 = src.s32 >> pos;
+      if (sbit > 0) {
+        mask = 0xFFFFFFFF << len;
+        data.s32 |= mask;
       } else {
-         abort();
+        mask = 0xFFFFFFFF >> (32 - len);
+        data.s32 &= mask;
       }
-   } else {
-      assert( space == global_space || space == shared_space );
-      effective_address_final = effective_address; 
-   }
-
-   // Check state space
-   assert( space == global_space || space == shared_space );
-
-   thread->m_last_effective_address = effective_address_final;
-   thread->m_last_memory_space = space;
-   thread->m_last_dram_callback.function = atom_callback;
-   thread->m_last_dram_callback.instruction = pI; 
-}
-
-void bar_impl( const ptx_instruction *pIin, ptx_thread_info *thread )
-{ 
-   ptx_instruction * pI = const_cast<ptx_instruction *>(pIin);
-   unsigned bar_op = pI->barrier_op();
-   unsigned red_op = pI->get_atomic();
-   unsigned ctaid = thread->get_cta_uid();
-
-   switch(bar_op){
-   	   case SYNC_OPTION:
-   	   {
-   		   if(pI->get_num_operands()>1){
-   			   const operand_info &op0 = pI->dst();
-   			   const operand_info &op1 = pI->src1();
-   			   ptx_reg_t op0_data;
-   			   ptx_reg_t op1_data;
-   			   op0_data = thread->get_operand_value(op0, op0, U32_TYPE, thread, 1);
-   			   op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
-   			   pI->set_bar_id(op0_data.u32);
-   			   pI->set_bar_count(op1_data.u32);
-   		   }else{
-   			   const operand_info &op0 = pI->dst();
-   			   ptx_reg_t op0_data;
-   			   op0_data = thread->get_operand_value(op0, op0, U32_TYPE, thread, 1);
-   			   pI->set_bar_id(op0_data.u32);
-   		   }
-   		   break;
-   	   }
-   	   case ARRIVE_OPTION:
-   	   {
-			   const operand_info &op0 = pI->dst();
-			   const operand_info &op1 = pI->src1();
-			   ptx_reg_t op0_data;
-			   ptx_reg_t op1_data;
-			   op0_data = thread->get_operand_value(op0, op0, U32_TYPE, thread, 1);
-			   op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
-   			   pI->set_bar_id(op0_data.u32);
-   			   pI->set_bar_count(op1_data.u32);
-   		   break;
-   	   }
-   	   case RED_OPTION:
-   	   {
-   		   if(pI->get_num_operands()>3){
-   			   const operand_info &op1 = pI->src1();
-   			   const operand_info &op2 = pI->src2();
-   			   const operand_info &op3 = pI->src3();
-   			   ptx_reg_t op1_data;
-   			   ptx_reg_t op2_data;
-   			   ptx_reg_t op3_data;
-   			   op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
-   			   op2_data = thread->get_operand_value(op2, op2, U32_TYPE, thread, 1);
-   			   op3_data = thread->get_operand_value(op3, op3, PRED_TYPE, thread, 1);
-   			   op3_data.u32=!(op3_data.pred & 0x0001);
-   			   pI->set_bar_id(op1_data.u32);
-   			   pI->set_bar_count(op2_data.u32);
-   			   switch(red_op){
-   			   	   case ATOMIC_POPC:
-   			   		   thread->popc_reduction(ctaid,op1_data.u32,op3_data.u32);
-   			   		   break;
-   			   	   case ATOMIC_AND:
-   			   		   thread->and_reduction(ctaid,op1_data.u32,op3_data.u32);
-   			   		   break;
-   			   	   case ATOMIC_OR:
-   			   		   thread->or_reduction(ctaid,op1_data.u32,op3_data.u32);
-   			   		   break;
-   			   	   default:
-   			   		   abort();
-   			   		   break;
-   			   }
-   		   }else{
-   			   const operand_info &op1 = pI->src1();
-   			   const operand_info &op2 = pI->src2();
-   			   ptx_reg_t op1_data;
-   			   ptx_reg_t op2_data;
-   			   op1_data = thread->get_operand_value(op1, op1, U32_TYPE, thread, 1);
-   			   op2_data = thread->get_operand_value(op2, op2, PRED_TYPE, thread, 1);
-               op2_data.u32=!(op2_data.pred & 0x0001);
-   			   pI->set_bar_id(op1_data.u32);
-   			   switch(red_op){
-   			   	   case ATOMIC_POPC:
-   			   		   thread->popc_reduction(ctaid,op1_data.u32,op2_data.u32);
-   			   		   break;
-   			   	   case ATOMIC_AND:
-   			   		   thread->and_reduction(ctaid,op1_data.u32,op2_data.u32);
-   			   		   break;
-   			   	   case ATOMIC_OR:
-   			   		   thread->or_reduction(ctaid,op1_data.u32,op2_data.u32);
-   			   		   break;
-   			   	   default:
-   			   		   abort();
-   			   		   break;
-   			   }
-   		   }
-   		   break;
-   	   }
-   	   default:
-   		   abort();
-   		   break;
-   }
-
-   thread->m_last_dram_callback.function = bar_callback;
-   thread->m_last_dram_callback.instruction = pIin;
-}
-
-void bfe_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-	unsigned i_type = pI->get_type();
-	unsigned msb = (i_type == U32_TYPE || i_type == S32_TYPE) ? 31 : 63;
-    const operand_info &dst  = pI->dst();
-    const operand_info &src1 = pI->src1();
-    const operand_info &src2 = pI->src2();
-    const operand_info &src3 = pI->src3();
-    ptx_reg_t src = thread->get_operand_value(src1, dst, i_type, thread, 1);
-    ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-    ptx_reg_t c = thread->get_operand_value(src3, dst, i_type, thread, 1);
-    ptx_reg_t data;
-	unsigned pos = b.u32 & 0xFF;
-	unsigned len = c.u32 & 0xFF;
-	switch (i_type)
-	{
-		case U32_TYPE:
-		{
-			unsigned mask;
-			data.u32 = src.u32 >> pos;
-			mask = 0xFFFFFFFF >> (32 - len);
-			data.u32 &= mask;
-			break;
-		}
-		case U64_TYPE:
-		{
-			unsigned long mask;
-			data.u64 = src.u64 >> pos;	
-			mask = 0xFFFFFFFFFFFFFFFF >> (64 - len);
-			data.u64 &= mask;
-			break;
-		}
-		case S32_TYPE:
-		{
-			unsigned mask;
-			unsigned min = MY_MIN_I(pos + len - 1, msb);
-			unsigned sbit = len == 0 ? 0 : (src.s32 >> min) & 0x1;
-			data.s32 = src.s32 >> pos;
-			if (sbit > 0)
-			{
-				mask = 0xFFFFFFFF << len;
-				data.s32 |= mask;
-			}
-			else
-			{
-				mask = 0xFFFFFFFF >> (32 - len);
-				data.s32 &= mask;
-			}
-			break;
-		}
-		case S64_TYPE:
-		{
-			unsigned long mask;
-			unsigned min = MY_MIN_I(pos + len - 1, msb);
-			unsigned sbit = len == 0 ? 0 : (src.s64 >> min) & 0x1;
-			data.s64 = src.s64 >> pos;
-			if (sbit > 0)
-			{
-				mask = 0xFFFFFFFFFFFFFFFF << len;
-				data.s64 |= mask;
-			}
-			else
-			{
-				mask = 0xFFFFFFFFFFFFFFFF >> (64 - len);
-				data.s64 &= mask;
-			}
-			break;
-		}
-		default:
-		printf("Operand type not supported for BFE instruction.\n");
-		abort();
-		return;
-	}
-    thread->set_operand_value(dst, data, i_type, thread, pI);
+      break;
+    }
+    case S64_TYPE: {
+      unsigned long mask;
+      unsigned min = MY_MIN_I(pos + len - 1, msb);
+      unsigned sbit = len == 0 ? 0 : (src.s64 >> min) & 0x1;
+      data.s64 = src.s64 >> pos;
+      if (sbit > 0) {
+        mask = 0xFFFFFFFFFFFFFFFF << len;
+        data.s64 |= mask;
+      } else {
+        mask = 0xFFFFFFFFFFFFFFFF >> (64 - len);
+        data.s64 &= mask;
+      }
+      break;
+    }
+    default:
+      printf("Operand type not supported for BFE instruction.\n");
+      abort();
+      return;
+  }
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void bfi_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { 
-   int i,max;
-   ptx_reg_t src1_data, src2_data;
-   ptx_reg_t src3_data, src4_data, data;
-
-   const operand_info &dst  = pI->dst();  //get operand info of sources and destination 
-   const operand_info &src1 = pI->src1(); //use them to determine that they are of type 'register'
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
-   const operand_info &src4 = pI->src4();
-
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   src3_data = thread->get_operand_value(src3, dst, i_type, thread, 1);
-   src4_data = thread->get_operand_value(src4, dst, i_type, thread, 1);
-
-   switch ( i_type ) {
-   case B32_TYPE:
+void bfi_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  int i, max;
+  ptx_reg_t src1_data, src2_data;
+  ptx_reg_t src3_data, src4_data, data;
+
+  const operand_info &dst =
+      pI->dst();  // get operand info of sources and destination
+  const operand_info &src1 =
+      pI->src1();  // use them to determine that they are of type 'register'
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+  const operand_info &src4 = pI->src4();
+
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  src3_data = thread->get_operand_value(src3, dst, i_type, thread, 1);
+  src4_data = thread->get_operand_value(src4, dst, i_type, thread, 1);
+
+  switch (i_type) {
+    case B32_TYPE:
       max = 32;
       break;
-   case B64_TYPE:
+    case B64_TYPE:
       max = 64;
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
-   data=src2_data;
-   unsigned pos = src3_data.u32 & 0xFF;
-   unsigned len = src4_data.u32 & 0xFF;
-   for(i=0;i<len && pos+i<max;i++){
-	data.u32=(~((0x00000001)<<(pos+i)))&data.u32;
-	data.u32=data.u32|((src1_data.u32&((0x00000001)<<(i)))<<(pos));
-   }
-   thread->set_operand_value(dst, data, i_type, thread, pI);
+  }
+  data = src2_data;
+  unsigned pos = src3_data.u32 & 0xFF;
+  unsigned len = src4_data.u32 & 0xFF;
+  for (i = 0; i < len && pos + i < max; i++) {
+    data.u32 = (~((0x00000001) << (pos + i))) & data.u32;
+    data.u32 = data.u32 | ((src1_data.u32 & ((0x00000001) << (i))) << (pos));
+  }
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
-void bfind_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
+void bfind_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const unsigned i_type = pI->get_type();
+
+  const ptx_reg_t src1_data =
+      thread->get_operand_value(src1, dst, i_type, thread, 1);
+  const int msb = (i_type == U32_TYPE || i_type == S32_TYPE) ? 31 : 63;
+
+  unsigned long a = 0;
+  switch (i_type) {
+    case S32_TYPE:
+      a = src1_data.s32;
+      break;
+    case U32_TYPE:
+      a = src1_data.u32;
+      break;
+    case S64_TYPE:
+      a = src1_data.s64;
+      break;
+    case U64_TYPE:
+      a = src1_data.u64;
+      break;
+    default:
+      assert(false);
+      abort();
+  }
+
+  // negate negative signed inputs
+  if ((i_type == S32_TYPE || i_type == S64_TYPE) && (a & (1 << msb))) {
+    a = ~a;
+  }
+  uint32_t d_data = 0xffffffff;
+  for (uint32_t i = msb; i >= 0; i--) {
+    if (a & (1 << i)) {
+      d_data = i;
+      break;
+    }
+  }
 
-void bra_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   const operand_info &target  = pI->dst();
-   ptx_reg_t target_pc = thread->get_operand_value(target, target, U32_TYPE, thread, 1);
+  // if (.shiftamt && d != 0xffffffff)  { d = msb - d; }
 
-   thread->m_branch_taken = true;
-   thread->set_npc(target_pc);
+  // store d
+  thread->set_operand_value(dst, d_data, U32_TYPE, thread, pI);
 }
 
-void brx_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   const operand_info &target  = pI->dst();
-   ptx_reg_t target_pc = thread->get_operand_value(target, target, U32_TYPE, thread, 1);
+void bra_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &target = pI->dst();
+  ptx_reg_t target_pc =
+      thread->get_operand_value(target, target, U32_TYPE, thread, 1);
 
-   thread->m_branch_taken = true;
-   thread->set_npc(target_pc);
+  thread->m_branch_taken = true;
+  thread->set_npc(target_pc);
 }
 
-void break_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   const operand_info &target  = thread->pop_breakaddr();
-   ptx_reg_t target_pc = thread->get_operand_value(target, target, U32_TYPE, thread, 1);
+void brx_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &target = pI->dst();
+  ptx_reg_t target_pc =
+      thread->get_operand_value(target, target, U32_TYPE, thread, 1);
 
-   thread->m_branch_taken = true;
-   thread->set_npc(target_pc);
+  thread->m_branch_taken = true;
+  thread->set_npc(target_pc);
 }
 
-void breakaddr_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   const operand_info &target  = pI->dst();
-   thread->push_breakaddr(target);
-   assert(pI->has_pred() == false); // pdom analysis cannot handle if this instruction is predicated 
+void break_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &target = thread->pop_breakaddr();
+  ptx_reg_t target_pc =
+      thread->get_operand_value(target, target, U32_TYPE, thread, 1);
+
+  thread->m_branch_taken = true;
+  thread->set_npc(target_pc);
 }
 
-void brev_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-    ptx_reg_t src1_data, data;
-    const operand_info &dst  = pI->dst();
-    const operand_info &src1 = pI->src1();
-	unsigned i_type = pI->get_type();
-    src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+void breakaddr_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &target = pI->dst();
+  thread->push_breakaddr(target);
+  assert(
+      pI->has_pred() ==
+      false);  // pdom analysis cannot handle if this instruction is predicated
+}
 
-    unsigned msb;
-    switch(i_type){
-        case B32_TYPE:
-            msb = 31;
-            for (unsigned i=0; i<=msb; i++) {
-                if((src1_data.u32 & (1 << i)))
-                    data.u32 |= 1 << (msb - i);
-            }
-            break;
-        case B64_TYPE:
-            msb = 63;
-            for (unsigned i=0; i<=msb; i++) {
-                if((src1_data.u64 & (1 << i)))
-                    data.u64 |= 1 << (msb - i);
-            }
-            break;
-        default: assert(0);
-    }
-    thread->set_operand_value(dst,data, i_type, thread, pI);
+void brev_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, data;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+
+  unsigned msb;
+  switch (i_type) {
+    case B32_TYPE:
+      msb = 31;
+      for (unsigned i = 0; i <= msb; i++) {
+        if ((src1_data.u32 & (1 << i))) data.u32 |= 1 << (msb - i);
+      }
+      break;
+    case B64_TYPE:
+      msb = 63;
+      for (unsigned i = 0; i <= msb; i++) {
+        if ((src1_data.u64 & (1 << i))) data.u64 |= 1 << (msb - i);
+      }
+      break;
+    default:
+      assert(0);
+  }
+  thread->set_operand_value(dst, data, i_type, thread, pI);
+}
+void brkpt_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
 }
-void brkpt_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
 
 unsigned trunc(unsigned num, unsigned precision) {
-	int mask = 1, latest_one = -1;
-	unsigned data = num;
-	for (unsigned j = 0; j < sizeof(unsigned)*8; j++) {
-		int bit = data & mask;
-		if (bit == 1) latest_one = j;
-		data >>= 1;
-	}
-	if (latest_one >= precision) {
-		// round_up is 1 if the most significant truncated digit is a 1, otherwise it is 0
-		//int round_up = (num & (1 << (latest_one-precision))) >> (latest_one-precision);
-		//unsigned shifted_output = num >> (latest_one-precision+1);
-		// if shifted_output is a number like 1111, don't round up
-		//if (shifted_output == (pow(2,precision)-1)) round_up = 0;
-		//num = shifted_output + round_up;
-		num >>= (latest_one-precision+1);
-	}
-	return num;
-}
-void mapping(int thread,int wmma_type,int wmma_layout,int type,int index,int stride,int &row,int &col,int &assg_offset){
-	int offset;
-	int c_row_offset[]={0,8,0,8,4,12,4,12};
-	int c_col_offset[]={0,0,8,8,0,0,8,8};
-	int c_tg_inside_row_offset[]={0,1,0,1};	
-	int c_tg_inside_col_offset[]={0,0,2,2};	
-	int c_inside_row_offset[]={0,0,2,2,0,0,2,2};
-	int c_inside_col_offset[]={0,1,0,1,4,5,4,5};
-
-	offset=thread_group_offset(thread,wmma_type,wmma_layout,type,stride);
-
-	if(wmma_type==LOAD_A){
-		if(wmma_layout==ROW){
-			offset+=index+8*((thread%16)/8);
-		}
-		else{
-			offset+=64*(index/4)+index%4+128*((thread%16)/8);	
-		}
-		offset=(offset/16)*stride+offset%16;
-		assg_offset=index+8*((thread%16)/8);
-	}
-	else if(wmma_type==LOAD_B){
-		if(wmma_layout==ROW){
-			offset+=64*(index/4)+index%4+128*((thread%16)/8);	
-		}
-		else{
-			offset+=index+8*((thread%16)/8);
-		}
-		offset=(offset/16)*stride+offset%16;
-		assg_offset=index+8*((thread%16)/8);
-	}
-	else if( wmma_type==LOAD_C){
-		if(type==F16_TYPE){
-			row=c_row_offset[thread/4]+thread%4;	
-			col=c_col_offset[thread/4]+index;
-		}
-		else{
-			row=c_row_offset[thread/4]+c_tg_inside_row_offset[thread%4]+c_inside_row_offset[index];
-			col=c_col_offset[thread/4]+c_tg_inside_col_offset[thread%4]+c_inside_col_offset[index];
-		}
-		assg_offset=index;
-	}
-
-	if(wmma_type==LOAD_A||wmma_type==LOAD_B){	
-		if(wmma_layout==ROW){
-			row=offset/16;
-			col=offset%16;
-		}
-		else{
-			col=offset/16;
-			row=offset%16;
-		}
-	}
-}
-
-void mma_impl( const ptx_instruction *pI, core_t *core, warp_inst_t inst )
-{
-	int i,j,k,thrd;
-	int row,col,offset;
-   	ptx_reg_t matrix_a[16][16];
-   	ptx_reg_t matrix_b[16][16];
-   	ptx_reg_t matrix_c[16][16];
-   	ptx_reg_t matrix_d[16][16];
-   	ptx_reg_t src_data;
-	ptx_thread_info *thread;
-	int stride;
-
-	unsigned wmma_type = pI->get_wmma_type();
-	unsigned a_layout = pI->get_wmma_layout(0);
-	unsigned b_layout = pI->get_wmma_layout(1);
-	unsigned type = pI->get_type();
-	unsigned type2 = pI->get_type2();
-	int tid ;
-	const operand_info &dst = pI->operand_lookup(0);
-	
-        if(core->get_gpu()->is_functional_sim())
-         	tid= inst.warp_id_func()*core->get_warp_size();
-        else
-         	tid= inst.warp_id()*core->get_warp_size();
-	unsigned thread_group_index;
-	float temp;
-	half temp2; 	
-	
-	for (thrd=0; thrd < core->get_warp_size(); thrd++){
-		thread = core->get_thread_info()[tid+thrd];
-		if(debug_tensorcore)
-			printf("THREAD=%d\n:",thrd);
-		for(int operand_num=1;operand_num<=3;operand_num++){
-			const operand_info &src_a=  pI->operand_lookup(operand_num);
-         		unsigned nelem = src_a.get_vect_nelem();
-         		ptx_reg_t v[8];
-         		thread->get_vector_operand_values( src_a, v, nelem );
-			if(debug_tensorcore){
-				printf("Thread%d_Iteration=%d\n:",thrd,operand_num);
-				for(k=0;k<nelem;k++){
-					printf("%x ",v[k].u64);
-				}
-				printf("\n");
-			}
-			ptx_reg_t nw_v[16];
-			int hex_val;
-
-			if(!((operand_num==3)&&(type2==F32_TYPE))){
-					for(k=0;k<2*nelem;k++){
-					if(k%2==1)
-						hex_val=(v[k/2].s64&0xffff);
-					else
-						hex_val=((v[k/2].s64&0xffff0000)>>16);
-					nw_v[k].f16 =*((half *)&hex_val); 
-				}
-			}
-			if(!((operand_num==3)&&(type2==F32_TYPE))){
-				for(k=0;k<2*nelem;k++){
-					temp=nw_v[k].f16;
-					if(debug_tensorcore)
-						printf("%.2f ",temp);
-				}
-				if(debug_tensorcore)
-					printf("\n");
-			}
-			else{
-				if(debug_tensorcore){
-					for(k=0;k<8;k++){
-						printf("%.2f ",v[k].f32);
-					}
-					printf("\n");
-				}
-			}
-			switch(operand_num) {
-			      case 1 ://operand 1
-				for(k=0;k<8;k++){
-					mapping(thrd,LOAD_A,a_layout,F16_TYPE,k,16,row,col,offset);
-					if(debug_tensorcore)
-						printf("A:thread=%d,row=%d,col=%d,offset=%d\n",thrd,row,col,offset);
-				 	matrix_a[row][col]=nw_v[offset];
-			        }
-			      break;
-			      case 2 ://operand 2
-				for(k=0;k<8;k++){
-					mapping(thrd,LOAD_B,b_layout,F16_TYPE,k,16,row,col,offset);
-					if(debug_tensorcore)
-						printf("B:thread=%d,row=%d,col=%d,offset=%d\n",thrd,row,col,offset);
-				 	matrix_b[row][col]=nw_v[offset];
-				}	
-			      break;
-			      case 3 ://operand 3
-				for(k=0;k<8;k++){
-					mapping(thrd,LOAD_C,ROW,type2,k,16,row,col,offset);
-					if(debug_tensorcore)
-						printf("C:thread=%d,row=%d,col=%d,offset=%d\n",thrd,row,col,offset);
-					if(type2!=F16_TYPE){
-					 	matrix_c[row][col]=v[offset];
-					}
-					else {
-				 		matrix_c[row][col]=nw_v[offset];
-					}
-				}
-			        break;
-			      default :
-			         printf("Invalid Operand Index\n" );
-			}
-		}
-		if(debug_tensorcore)
-			printf("\n");
-	}
-	if(debug_tensorcore){
-		printf("MATRIX_A\n");
-		for (i=0;i<16;i++){
-			for(j=0;j<16;j++){
-				temp=matrix_a[i][j].f16;
-				printf("%.2f ",temp);
-			}
-			printf("\n");
-		}
-		printf("MATRIX_B\n");
-		for (i=0;i<16;i++){
-			for(j=0;j<16;j++){
-				temp=matrix_b[i][j].f16;
-				printf("%.2f ",temp);
-			}
-			printf("\n");
-		}	
-		printf("MATRIX_C\n");
-		for (i=0;i<16;i++){
-			for(j=0;j<16;j++){
-				if(type2==F16_TYPE){
-					temp=matrix_c[i][j].f16;
-					printf("%.2f ",temp);
-				}
-				else
-					printf("%.2f ",matrix_c[i][j].f32);
-			}
-			printf("\n");
-		}	
-	}
-	for (i=0;i<16;i++){
-		for(j=0;j<16;j++){
-				matrix_d[i][j].f16=0;
-		}
-	}
-	
-	for (i=0;i<16;i++){
-		for(j=0;j<16;j++){
-			for(k=0;k<16;k++){
-				matrix_d[i][j].f16=matrix_d[i][j].f16+matrix_a[i][k].f16*matrix_b[k][j].f16;
-			}
-			if((type==F16_TYPE)&&(type2==F16_TYPE))
-				matrix_d[i][j].f16+=matrix_c[i][j].f16;
-			else if((type==F32_TYPE)&&(type2==F16_TYPE)){
-				temp2=matrix_d[i][j].f16+matrix_c[i][j].f16;
-				temp=temp2;
-				matrix_d[i][j].f32=temp;
-			}
-			else if((type==F16_TYPE)&&(type2==F32_TYPE)){
-				temp=matrix_d[i][j].f16;
-				temp+=matrix_c[i][j].f32;
-				matrix_d[i][j].f16=half(temp);
-			}
-			else{
-				temp=matrix_d[i][j].f16;
-				temp+=matrix_c[i][j].f32;
-				matrix_d[i][j].f32=temp;
-			}
-		}
-	}
-	if(debug_tensorcore){
-		printf("MATRIX_D\n");
-		for (i=0;i<16;i++){
-			for(j=0;j<16;j++){
-				if(type==F16_TYPE){
-					temp=matrix_d[i][j].f16;
-					printf("%.2f ",temp);
-				}
-				else
-					printf("%.2f ",matrix_d[i][j].f32);
-			}
-			printf("\n");
-		}	
-	}
-	for (thrd=0; thrd < core->get_warp_size(); thrd++){
-		int row_t[8];
-		int col_t[8];	
-		for(k=0;k<8;k++){
-			mapping(thrd,LOAD_C,ROW,type,k,16,row_t[k],col_t[k],offset);
-			if(debug_tensorcore)
-				printf("mma:store:row:%d,col%d\n",row_t[k],col_t[k]);
-		}
-		thread = core->get_thread_info()[tid+thrd];
-		
-	
-		if(type==F32_TYPE){
-			thread->set_wmma_vector_operand_values(dst,matrix_d[row_t[0]][col_t[0]],matrix_d[row_t[1]][col_t[1]],matrix_d[row_t[2]][col_t[2]],matrix_d[row_t[3]][col_t[3]],matrix_d[row_t[4]][col_t[4]],matrix_d[row_t[5]][col_t[5]],matrix_d[row_t[6]][col_t[6]],matrix_d[row_t[7]][col_t[7]]);
-		
-			if(debug_tensorcore)
-			{
-				printf("thread%d:",thrd);
-				for(k=0;k<8;k++){
-					printf("%.2f ",matrix_d[row_t[k]][col_t[k]].f32);
-				}
-				printf("\n");
-			}
-		}
-		else if(type==F16_TYPE){
-			if(debug_tensorcore){	
-				printf("thread%d:",thrd);
-				for(k=0;k<8;k++){
-					temp=matrix_d[row_t[k]][col_t[k]].f16;
-					printf("%.2f ",temp);
-				}
-				printf("\n");
-
-				printf("thread%d:",thrd);
-				for(k=0;k<8;k++){
-					printf("%x ",matrix_d[row_t[k]][col_t[k]].f16);
-				}
-				printf("\n");
-			}
-			ptx_reg_t nw_data1, nw_data2, nw_data3, nw_data4;
-			nw_data1.s64=((matrix_d[row_t[0]][col_t[0]].s64   & 0xffff))|((matrix_d[row_t[1]][col_t[1]].s64&0xffff)<<16);
-			nw_data2.s64=((matrix_d[row_t[2]][col_t[2]].s64   & 0xffff))|((matrix_d[row_t[3]][col_t[3]].s64&0xffff)<<16);
-			nw_data3.s64=((matrix_d[row_t[4]][col_t[4]].s64   & 0xffff))|((matrix_d[row_t[5]][col_t[5]].s64&0xffff)<<16);
-			nw_data4.s64=((matrix_d[row_t[6]][col_t[6]].s64   & 0xffff))|((matrix_d[row_t[7]][col_t[7]].s64&0xffff)<<16);
-   			thread->set_vector_operand_values(dst,nw_data1,nw_data2,nw_data3,nw_data4);
-			if(debug_tensorcore)
-		 		printf("thread%d=%x,%x,%x,%x",thrd,nw_data1.s64,nw_data2.s64,nw_data3.s64,nw_data4.s64);
-		
-		}
-		else{
-			printf("wmma:mma:wrong type\n");
-			abort();
-		}
-	}
-}
-
-void call_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   static unsigned call_uid_next = 1;
-    
-   const operand_info &target  = pI->func_addr();
-   assert( target.is_function_address() );
-   const symbol *func_addr = target.get_symbol();
-   function_info *target_func = func_addr->get_pc();
-   if (target_func->is_pdom_set()) {
-      printf("GPGPU-Sim PTX: PDOM analysis already done for %s \n", target_func->get_name().c_str() );
-   } else {
-      printf("GPGPU-Sim PTX: finding reconvergence points for \'%s\'...\n", target_func->get_name().c_str() );
-      /*
-       * Some of the instructions like printf() gives the gpgpusim the wrong impression that it is a function call.
-       * As printf() doesnt have a body like functions do, doing pdom analysis for printf() causes a crash.
-       */
-      if (target_func->get_function_size() >0)
-          target_func->do_pdom();
-      target_func->set_pdom();
-   }
+  int mask = 1, latest_one = -1;
+  unsigned data = num;
+  for (unsigned j = 0; j < sizeof(unsigned) * 8; j++) {
+    int bit = data & mask;
+    if (bit == 1) latest_one = j;
+    data >>= 1;
+  }
+  if (latest_one >= precision) {
+    // round_up is 1 if the most significant truncated digit is a 1, otherwise
+    // it is 0
+    // int round_up = (num & (1 << (latest_one-precision))) >>
+    // (latest_one-precision); unsigned shifted_output = num >>
+    // (latest_one-precision+1);
+    // if shifted_output is a number like 1111, don't round up
+    // if (shifted_output == (pow(2,precision)-1)) round_up = 0;
+    // num = shifted_output + round_up;
+    num >>= (latest_one - precision + 1);
+  }
+  return num;
+}
+void mapping(int thread, int wmma_type, int wmma_layout, int type, int index,
+             int stride, int &row, int &col, int &assg_offset) {
+  int offset;
+  int c_row_offset[] = {0, 8, 0, 8, 4, 12, 4, 12};
+  int c_col_offset[] = {0, 0, 8, 8, 0, 0, 8, 8};
+  int c_tg_inside_row_offset[] = {0, 1, 0, 1};
+  int c_tg_inside_col_offset[] = {0, 0, 2, 2};
+  int c_inside_row_offset[] = {0, 0, 2, 2, 0, 0, 2, 2};
+  int c_inside_col_offset[] = {0, 1, 0, 1, 4, 5, 4, 5};
+
+  offset = thread_group_offset(thread, wmma_type, wmma_layout, type, stride);
+
+  if (wmma_type == LOAD_A) {
+    if (wmma_layout == ROW) {
+      offset += index + 8 * ((thread % 16) / 8);
+    } else {
+      offset += 64 * (index / 4) + index % 4 + 128 * ((thread % 16) / 8);
+    }
+    offset = (offset / 16) * stride + offset % 16;
+    assg_offset = index + 8 * ((thread % 16) / 8);
+  } else if (wmma_type == LOAD_B) {
+    if (wmma_layout == ROW) {
+      offset += 64 * (index / 4) + index % 4 + 128 * ((thread % 16) / 8);
+    } else {
+      offset += index + 8 * ((thread % 16) / 8);
+    }
+    offset = (offset / 16) * stride + offset % 16;
+    assg_offset = index + 8 * ((thread % 16) / 8);
+  } else if (wmma_type == LOAD_C) {
+    if (type == F16_TYPE) {
+      row = c_row_offset[thread / 4] + thread % 4;
+      col = c_col_offset[thread / 4] + index;
+    } else {
+      row = c_row_offset[thread / 4] + c_tg_inside_row_offset[thread % 4] +
+            c_inside_row_offset[index];
+      col = c_col_offset[thread / 4] + c_tg_inside_col_offset[thread % 4] +
+            c_inside_col_offset[index];
+    }
+    assg_offset = index;
+  }
 
-   // check that number of args and return match function requirements
-   if( pI->has_return() ^ target_func->has_return() ) {
-      printf("GPGPU-Sim PTX: Execution error - mismatch in number of return values between\n"
-             "               call instruction and function declaration\n");
-      abort(); 
-   }
-   unsigned n_return = target_func->has_return();
-   unsigned n_args = target_func->num_args();
-   unsigned n_operands = pI->get_num_operands();
-
-   if( n_operands != (n_return+1+n_args) ) {
-      printf("GPGPU-Sim PTX: Execution error - mismatch in number of arguements between\n"
-             "               call instruction and function declaration\n");
-      abort(); 
-   }
+  if (wmma_type == LOAD_A || wmma_type == LOAD_B) {
+    if (wmma_layout == ROW) {
+      row = offset / 16;
+      col = offset % 16;
+    } else {
+      col = offset / 16;
+      row = offset % 16;
+    }
+  }
+}
 
-   // handle intrinsic functions
-   std::string fname = target_func->get_name();
-   if( fname == "vprintf" ) {
-      gpgpusim_cuda_vprintf(pI, thread, target_func);
-      return;
-   }
+void mma_impl(const ptx_instruction *pI, core_t *core, warp_inst_t inst) {
+  int i, j, k, thrd;
+  int row, col, offset;
+  ptx_reg_t matrix_a[16][16];
+  ptx_reg_t matrix_b[16][16];
+  ptx_reg_t matrix_c[16][16];
+  ptx_reg_t matrix_d[16][16];
+  ptx_reg_t src_data;
+  ptx_thread_info *thread;
+
+  unsigned a_layout = pI->get_wmma_layout(0);
+  unsigned b_layout = pI->get_wmma_layout(1);
+  unsigned type = pI->get_type();
+  unsigned type2 = pI->get_type2();
+  int tid;
+  const operand_info &dst = pI->operand_lookup(0);
+
+  if (core->get_gpu()->is_functional_sim())
+    tid = inst.warp_id_func() * core->get_warp_size();
+  else
+    tid = inst.warp_id() * core->get_warp_size();
+  float temp;
+  half temp2;
+
+  for (thrd = 0; thrd < core->get_warp_size(); thrd++) {
+    thread = core->get_thread_info()[tid + thrd];
+    if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+      printf("THREAD=%d\n:", thrd);
+    for (int operand_num = 1; operand_num <= 3; operand_num++) {
+      const operand_info &src_a = pI->operand_lookup(operand_num);
+      unsigned nelem = src_a.get_vect_nelem();
+      ptx_reg_t v[8];
+      thread->get_vector_operand_values(src_a, v, nelem);
+      if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+        printf("Thread%d_Iteration=%d\n:", thrd, operand_num);
+        for (k = 0; k < nelem; k++) {
+          printf("%llx ", v[k].u64);
+        }
+        printf("\n");
+      }
+      ptx_reg_t nw_v[16];
+      int hex_val;
+
+      if (!((operand_num == 3) && (type2 == F32_TYPE))) {
+        for (k = 0; k < 2 * nelem; k++) {
+          if (k % 2 == 1)
+            hex_val = (v[k / 2].s64 & 0xffff);
+          else
+            hex_val = ((v[k / 2].s64 & 0xffff0000) >> 16);
+          nw_v[k].f16 = *((half *)&hex_val);
+        }
+      }
+      if (!((operand_num == 3) && (type2 == F32_TYPE))) {
+        for (k = 0; k < 2 * nelem; k++) {
+          temp = nw_v[k].f16;
+          if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+            printf("%.2f ", temp);
+        }
+        if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) printf("\n");
+      } else {
+        if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+          for (k = 0; k < 8; k++) {
+            printf("%.2f ", v[k].f32);
+          }
+          printf("\n");
+        }
+      }
+      switch (operand_num) {
+        case 1:  // operand 1
+          for (k = 0; k < 8; k++) {
+            mapping(thrd, LOAD_A, a_layout, F16_TYPE, k, 16, row, col, offset);
+            if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+              printf("A:thread=%d,row=%d,col=%d,offset=%d\n", thrd, row, col,
+                     offset);
+            matrix_a[row][col] = nw_v[offset];
+          }
+          break;
+        case 2:  // operand 2
+          for (k = 0; k < 8; k++) {
+            mapping(thrd, LOAD_B, b_layout, F16_TYPE, k, 16, row, col, offset);
+            if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+              printf("B:thread=%d,row=%d,col=%d,offset=%d\n", thrd, row, col,
+                     offset);
+            matrix_b[row][col] = nw_v[offset];
+          }
+          break;
+        case 3:  // operand 3
+          for (k = 0; k < 8; k++) {
+            mapping(thrd, LOAD_C, ROW, type2, k, 16, row, col, offset);
+            if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+              printf("C:thread=%d,row=%d,col=%d,offset=%d\n", thrd, row, col,
+                     offset);
+            if (type2 != F16_TYPE) {
+              matrix_c[row][col] = v[offset];
+            } else {
+              matrix_c[row][col] = nw_v[offset];
+            }
+          }
+          break;
+        default:
+          printf("Invalid Operand Index\n");
+      }
+    }
+    if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) printf("\n");
+  }
+  if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+    printf("MATRIX_A\n");
+    for (i = 0; i < 16; i++) {
+      for (j = 0; j < 16; j++) {
+        temp = matrix_a[i][j].f16;
+        printf("%.2f ", temp);
+      }
+      printf("\n");
+    }
+    printf("MATRIX_B\n");
+    for (i = 0; i < 16; i++) {
+      for (j = 0; j < 16; j++) {
+        temp = matrix_b[i][j].f16;
+        printf("%.2f ", temp);
+      }
+      printf("\n");
+    }
+    printf("MATRIX_C\n");
+    for (i = 0; i < 16; i++) {
+      for (j = 0; j < 16; j++) {
+        if (type2 == F16_TYPE) {
+          temp = matrix_c[i][j].f16;
+          printf("%.2f ", temp);
+        } else
+          printf("%.2f ", matrix_c[i][j].f32);
+      }
+      printf("\n");
+    }
+  }
+  for (i = 0; i < 16; i++) {
+    for (j = 0; j < 16; j++) {
+      matrix_d[i][j].f16 = 0;
+    }
+  }
+
+  for (i = 0; i < 16; i++) {
+    for (j = 0; j < 16; j++) {
+      for (k = 0; k < 16; k++) {
+        matrix_d[i][j].f16 =
+            matrix_d[i][j].f16 + matrix_a[i][k].f16 * matrix_b[k][j].f16;
+      }
+      if ((type == F16_TYPE) && (type2 == F16_TYPE))
+        matrix_d[i][j].f16 += matrix_c[i][j].f16;
+      else if ((type == F32_TYPE) && (type2 == F16_TYPE)) {
+        temp2 = matrix_d[i][j].f16 + matrix_c[i][j].f16;
+        temp = temp2;
+        matrix_d[i][j].f32 = temp;
+      } else if ((type == F16_TYPE) && (type2 == F32_TYPE)) {
+        temp = matrix_d[i][j].f16;
+        temp += matrix_c[i][j].f32;
+        matrix_d[i][j].f16 = half(temp);
+      } else {
+        temp = matrix_d[i][j].f16;
+        temp += matrix_c[i][j].f32;
+        matrix_d[i][j].f32 = temp;
+      }
+    }
+  }
+  if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+    printf("MATRIX_D\n");
+    for (i = 0; i < 16; i++) {
+      for (j = 0; j < 16; j++) {
+        if (type == F16_TYPE) {
+          temp = matrix_d[i][j].f16;
+          printf("%.2f ", temp);
+        } else
+          printf("%.2f ", matrix_d[i][j].f32);
+      }
+      printf("\n");
+    }
+  }
+  for (thrd = 0; thrd < core->get_warp_size(); thrd++) {
+    int row_t[8];
+    int col_t[8];
+    for (k = 0; k < 8; k++) {
+      mapping(thrd, LOAD_C, ROW, type, k, 16, row_t[k], col_t[k], offset);
+      if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+        printf("mma:store:row:%d,col%d\n", row_t[k], col_t[k]);
+    }
+    thread = core->get_thread_info()[tid + thrd];
+
+    if (type == F32_TYPE) {
+      thread->set_wmma_vector_operand_values(
+          dst, matrix_d[row_t[0]][col_t[0]], matrix_d[row_t[1]][col_t[1]],
+          matrix_d[row_t[2]][col_t[2]], matrix_d[row_t[3]][col_t[3]],
+          matrix_d[row_t[4]][col_t[4]], matrix_d[row_t[5]][col_t[5]],
+          matrix_d[row_t[6]][col_t[6]], matrix_d[row_t[7]][col_t[7]]);
+
+      if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+        printf("thread%d:", thrd);
+        for (k = 0; k < 8; k++) {
+          printf("%.2f ", matrix_d[row_t[k]][col_t[k]].f32);
+        }
+        printf("\n");
+      }
+    } else if (type == F16_TYPE) {
+      if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+        printf("thread%d:", thrd);
+        for (k = 0; k < 8; k++) {
+          temp = matrix_d[row_t[k]][col_t[k]].f16;
+          printf("%.2f ", temp);
+        }
+        printf("\n");
+
+        printf("thread%d:", thrd);
+        for (k = 0; k < 8; k++) {
+          printf("%x ", (unsigned int)matrix_d[row_t[k]][col_t[k]].f16);
+        }
+        printf("\n");
+      }
+      ptx_reg_t nw_data1, nw_data2, nw_data3, nw_data4;
+      nw_data1.s64 = ((matrix_d[row_t[0]][col_t[0]].s64 & 0xffff)) |
+                     ((matrix_d[row_t[1]][col_t[1]].s64 & 0xffff) << 16);
+      nw_data2.s64 = ((matrix_d[row_t[2]][col_t[2]].s64 & 0xffff)) |
+                     ((matrix_d[row_t[3]][col_t[3]].s64 & 0xffff) << 16);
+      nw_data3.s64 = ((matrix_d[row_t[4]][col_t[4]].s64 & 0xffff)) |
+                     ((matrix_d[row_t[5]][col_t[5]].s64 & 0xffff) << 16);
+      nw_data4.s64 = ((matrix_d[row_t[6]][col_t[6]].s64 & 0xffff)) |
+                     ((matrix_d[row_t[7]][col_t[7]].s64 & 0xffff) << 16);
+      thread->set_vector_operand_values(dst, nw_data1, nw_data2, nw_data3,
+                                        nw_data4);
+      if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+        printf("thread%d=%llx,%llx,%llx,%llx", thrd, nw_data1.s64, nw_data2.s64,
+               nw_data3.s64, nw_data4.s64);
+
+    } else {
+      printf("wmma:mma:wrong type\n");
+      abort();
+    }
+  }
+}
+
+void call_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  static unsigned call_uid_next = 1;
+
+  const operand_info &target = pI->func_addr();
+  assert(target.is_function_address());
+  const symbol *func_addr = target.get_symbol();
+  function_info *target_func = func_addr->get_pc();
+  if (target_func->is_pdom_set()) {
+    printf("GPGPU-Sim PTX: PDOM analysis already done for %s \n",
+           target_func->get_name().c_str());
+  } else {
+    printf("GPGPU-Sim PTX: finding reconvergence points for \'%s\'...\n",
+           target_func->get_name().c_str());
+    /*
+     * Some of the instructions like printf() gives the gpgpusim the wrong
+     * impression that it is a function call. As printf() doesnt have a body
+     * like functions do, doing pdom analysis for printf() causes a crash.
+     */
+    if (target_func->get_function_size() > 0) target_func->do_pdom();
+    target_func->set_pdom();
+  }
+
+  // check that number of args and return match function requirements
+  if (pI->has_return() ^ target_func->has_return()) {
+    printf(
+        "GPGPU-Sim PTX: Execution error - mismatch in number of return values "
+        "between\n"
+        "               call instruction and function declaration\n");
+    abort();
+  }
+  unsigned n_return = target_func->has_return();
+  unsigned n_args = target_func->num_args();
+  unsigned n_operands = pI->get_num_operands();
+
+  if (n_operands != (n_return + 1 + n_args)) {
+    printf(
+        "GPGPU-Sim PTX: Execution error - mismatch in number of arguements "
+        "between\n"
+        "               call instruction and function declaration\n");
+    abort();
+  }
 
+  // handle intrinsic functions
+  std::string fname = target_func->get_name();
+  if (fname == "vprintf") {
+    gpgpusim_cuda_vprintf(pI, thread, target_func);
+    return;
+  }
 #if (CUDART_VERSION >= 5000)
-   //Jin: handle device runtime apis for CDP
-   else if(fname == "cudaGetParameterBufferV2") {
-      gpgpusim_cuda_getParameterBufferV2(pI, thread, target_func);
-	  return;
-   }
-   else if(fname == "cudaLaunchDeviceV2") {
-      gpgpusim_cuda_launchDeviceV2(pI, thread, target_func);
-	  return;
-   }
-   else if(fname == "cudaStreamCreateWithFlags") {
-      gpgpusim_cuda_streamCreateWithFlags(pI, thread, target_func);
-	  return;
-   }
+  // Jin: handle device runtime apis for CDP
+  else if (fname == "cudaGetParameterBufferV2") {
+    target_func->gpgpu_ctx->device_runtime->gpgpusim_cuda_getParameterBufferV2(
+        pI, thread, target_func);
+    return;
+  } else if (fname == "cudaLaunchDeviceV2") {
+    target_func->gpgpu_ctx->device_runtime->gpgpusim_cuda_launchDeviceV2(
+        pI, thread, target_func);
+    return;
+  } else if (fname == "cudaStreamCreateWithFlags") {
+    target_func->gpgpu_ctx->device_runtime->gpgpusim_cuda_streamCreateWithFlags(
+        pI, thread, target_func);
+    return;
+  }
 #endif
 
-   // read source arguements into register specified in declaration of function
-   arg_buffer_list_t arg_values;
-   copy_args_into_buffer_list(pI, thread, target_func, arg_values);
+  // read source arguements into register specified in declaration of function
+  arg_buffer_list_t arg_values;
+  copy_args_into_buffer_list(pI, thread, target_func, arg_values);
 
-   // record local for return value (we only support a single return value)
-   const symbol *return_var_src = NULL;
-   const symbol *return_var_dst = NULL;
-   if( target_func->has_return() ) {
-      return_var_dst = pI->dst().get_symbol();
-      return_var_src = target_func->get_return_var();
-   }
+  // record local for return value (we only support a single return value)
+  const symbol *return_var_src = NULL;
+  const symbol *return_var_dst = NULL;
+  if (target_func->has_return()) {
+    return_var_dst = pI->dst().get_symbol();
+    return_var_src = target_func->get_return_var();
+  }
 
-   gpgpu_sim *gpu = thread->get_gpu();
-   unsigned callee_pc=0, callee_rpc=0;
-   if( gpu->simd_model() == POST_DOMINATOR ) {
-      thread->get_core()->get_pdom_stack_top_info(thread->get_hw_wid(),&callee_pc,&callee_rpc);
-      assert( callee_pc == thread->get_pc() );
-   }
+  gpgpu_sim *gpu = thread->get_gpu();
+  unsigned callee_pc = 0, callee_rpc = 0;
+  if (gpu->simd_model() == POST_DOMINATOR) {
+    thread->get_core()->get_pdom_stack_top_info(thread->get_hw_wid(),
+                                                &callee_pc, &callee_rpc);
+    assert(callee_pc == thread->get_pc());
+  }
 
-   thread->callstack_push(callee_pc + pI->inst_size(), callee_rpc, return_var_src, return_var_dst, call_uid_next++);
+  thread->callstack_push(callee_pc + pI->inst_size(), callee_rpc,
+                         return_var_src, return_var_dst, call_uid_next++);
 
-   copy_buffer_list_into_frame(thread, arg_values);
+  copy_buffer_list_into_frame(thread, arg_values);
 
-   thread->set_npc(target_func);
+  thread->set_npc(target_func);
 }
 
-//Ptxplus version of call instruction. Jumps to a label not a different Kernel.
-void callp_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-   
-   static unsigned call_uid_next = 1;
+// Ptxplus version of call instruction. Jumps to a label not a different Kernel.
+void callp_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  static unsigned call_uid_next = 1;
 
-   const operand_info &target  = pI->dst();
-   ptx_reg_t target_pc = thread->get_operand_value(target, target, U32_TYPE, thread, 1);
+  const operand_info &target = pI->dst();
+  ptx_reg_t target_pc =
+      thread->get_operand_value(target, target, U32_TYPE, thread, 1);
 
-   const symbol *return_var_src = NULL;
-   const symbol *return_var_dst = NULL;
+  const symbol *return_var_src = NULL;
+  const symbol *return_var_dst = NULL;
 
-   gpgpu_sim *gpu = thread->get_gpu();
-   unsigned callee_pc=0, callee_rpc=0;
-   if( gpu->simd_model() == POST_DOMINATOR ) {
-      thread->get_core()->get_pdom_stack_top_info(thread->get_hw_wid(),&callee_pc,&callee_rpc);
-      assert( callee_pc == thread->get_pc() );
-   } 
+  gpgpu_sim *gpu = thread->get_gpu();
+  unsigned callee_pc = 0, callee_rpc = 0;
+  if (gpu->simd_model() == POST_DOMINATOR) {
+    thread->get_core()->get_pdom_stack_top_info(thread->get_hw_wid(),
+                                                &callee_pc, &callee_rpc);
+    assert(callee_pc == thread->get_pc());
+  }
 
-   thread->callstack_push_plus(callee_pc + pI->inst_size(), callee_rpc, return_var_src, return_var_dst, call_uid_next++);
-   thread->set_npc(target_pc);
+  thread->callstack_push_plus(callee_pc + pI->inst_size(), callee_rpc,
+                              return_var_src, return_var_dst, call_uid_next++);
+  thread->set_npc(target_pc);
 }
 
-void clz_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-   ptx_reg_t a, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+void clz_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   int max;
-   unsigned long long mask;
-   d.u64 = 0;
+  int max;
+  unsigned long long mask;
+  d.u64 = 0;
 
-   switch ( i_type ) {
-   case B32_TYPE:
+  switch (i_type) {
+    case B32_TYPE:
       max = 32;
       mask = 0x80000000;
       break;
-   case B64_TYPE:
+    case B64_TYPE:
       max = 64;
       mask = 0x8000000000000000;
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   while ((d.u32 < max) && ((a.u64&mask) == 0) ) {
-      d.u32++;
-      a.u64 = a.u64 << 1;
-   }
+  while ((d.u32 < max) && ((a.u64 & mask) == 0)) {
+    d.u32++;
+    a.u64 = a.u64 << 1;
+  }
 
-   thread->set_operand_value(dst,d, B32_TYPE, thread, pI);
+  thread->set_operand_value(dst, d, B32_TYPE, thread, pI);
 }
 
-void cnot_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, b, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+void cnot_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case PRED_TYPE: d.pred = ((a.pred & 0x0001) == 0)?1:0; break;
-   case B16_TYPE:  d.u16  = (a.u16  == 0)?1:0; break;
-   case B32_TYPE:  d.u32  = (a.u32  == 0)?1:0; break;
-   case B64_TYPE:  d.u64  = (a.u64  == 0)?1:0; break;
-   default:
+  switch (i_type) {
+    case PRED_TYPE:
+      d.pred = ((a.pred & 0x0001) == 0) ? 1 : 0;
+      break;
+    case B16_TYPE:
+      d.u16 = (a.u16 == 0) ? 1 : 0;
+      break;
+    case B32_TYPE:
+      d.u32 = (a.u32 == 0) ? 1 : 0;
+      break;
+    case B64_TYPE:
+      d.u64 = (a.u64 == 0) ? 1 : 0;
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void cos_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t a, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+void cos_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-
-   switch ( i_type ) {
-   case F32_TYPE: 
+  switch (i_type) {
+    case F32_TYPE:
       d.f32 = cos(a.f32);
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-ptx_reg_t chop( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   switch ( to_width ) {
-   case 8:  x.mask_and(0,0xFF);  break;
-   case 16: x.mask_and(0,0xFFFF);      break;
-   case 32: x.mask_and(0,0xFFFFFFFF);  break;
-   case 64: break;
-   default: assert(0);
-   }
-   return x;
-}
-
-ptx_reg_t sext( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   x=chop(x,0,from_width,0,rounding_mode,saturation_mode);
-   switch ( from_width ) {
-   case 8: if ( x.get_bit(7) ) x.mask_or(0xFFFFFFFF,0xFFFFFF00);break;
-   case 16:if ( x.get_bit(15) ) x.mask_or(0xFFFFFFFF,0xFFFF0000);break;
-   case 32: if ( x.get_bit(31) ) x.mask_or(0xFFFFFFFF,0x00000000);break;
-   case 64: break;
-   default: assert(0);
-   }
-   return x;
-}
-
-// sign extend depending on the destination register size - hack to get SobelFilter working in CUDA 4.2
-ptx_reg_t sexd( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   x=chop(x,0,from_width,0,rounding_mode,saturation_mode);
-   switch ( to_width ) {
-   case 8: if ( x.get_bit(7) ) x.mask_or(0xFFFFFFFF,0xFFFFFF00);break;
-   case 16:if ( x.get_bit(15) ) x.mask_or(0xFFFFFFFF,0xFFFF0000);break;
-   case 32: if ( x.get_bit(31) ) x.mask_or(0xFFFFFFFF,0x00000000);break;
-   case 64: break;
-   default: assert(0);
-   }
-   return x;
+ptx_reg_t chop(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+               int rounding_mode, int saturation_mode) {
+  switch (to_width) {
+    case 8:
+      x.mask_and(0, 0xFF);
+      break;
+    case 16:
+      x.mask_and(0, 0xFFFF);
+      break;
+    case 32:
+      x.mask_and(0, 0xFFFFFFFF);
+      break;
+    case 64:
+      break;
+    default:
+      assert(0);
+  }
+  return x;
 }
 
-ptx_reg_t zext( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   return chop(x,0,from_width,0,rounding_mode,saturation_mode);
+ptx_reg_t sext(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+               int rounding_mode, int saturation_mode) {
+  x = chop(x, 0, from_width, 0, rounding_mode, saturation_mode);
+  switch (from_width) {
+    case 8:
+      if (x.get_bit(7)) x.mask_or(0xFFFFFFFF, 0xFFFFFF00);
+      break;
+    case 16:
+      if (x.get_bit(15)) x.mask_or(0xFFFFFFFF, 0xFFFF0000);
+      break;
+    case 32:
+      if (x.get_bit(31)) x.mask_or(0xFFFFFFFF, 0x00000000);
+      break;
+    case 64:
+      break;
+    default:
+      assert(0);
+  }
+  return x;
 }
 
-int saturatei(int a, int max, int min) 
-{
-   if (a > max) a = max;
-   else if (a < min) a = min;
-   return a;
+// sign extend depending on the destination register size - hack to get
+// SobelFilter working in CUDA 4.2
+ptx_reg_t sexd(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+               int rounding_mode, int saturation_mode) {
+  x = chop(x, 0, from_width, 0, rounding_mode, saturation_mode);
+  switch (to_width) {
+    case 8:
+      if (x.get_bit(7)) x.mask_or(0xFFFFFFFF, 0xFFFFFF00);
+      break;
+    case 16:
+      if (x.get_bit(15)) x.mask_or(0xFFFFFFFF, 0xFFFF0000);
+      break;
+    case 32:
+      if (x.get_bit(31)) x.mask_or(0xFFFFFFFF, 0x00000000);
+      break;
+    case 64:
+      break;
+    default:
+      assert(0);
+  }
+  return x;
 }
 
-unsigned int saturatei(unsigned int a, unsigned int max) 
-{
-   if (a > max) a = max;
-   return a;
+ptx_reg_t zext(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+               int rounding_mode, int saturation_mode) {
+  return chop(x, 0, from_width, 0, rounding_mode, saturation_mode);
 }
 
-ptx_reg_t f2x( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
- half mytemp;
- float myfloat;
+int saturatei(int a, int max, int min) {
+  if (a > max)
+    a = max;
+  else if (a < min)
+    a = min;
+  return a;
+}
+
+unsigned int saturatei(unsigned int a, unsigned int max) {
+  if (a > max) a = max;
+  return a;
+}
+
+ptx_reg_t f2x(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+              int rounding_mode, int saturation_mode) {
+  half mytemp;
   half_float::half tmp_h;
-   //assert( from_width == 32); 
-
-   enum cuda_math::cudaRoundMode mode = cuda_math::cudaRoundZero;
-   switch (rounding_mode) {
-   case RZI_OPTION: mode = cuda_math::cudaRoundZero;    break;
-   case RNI_OPTION: mode = cuda_math::cudaRoundNearest; break;
-   case RMI_OPTION: mode = cuda_math::cudaRoundMinInf;  break;
-   case RPI_OPTION: mode = cuda_math::cudaRoundPosInf;  break;
-   default: break; 
-   }
+  // assert( from_width == 32);
 
-   ptx_reg_t y;
-   if ( to_sign == 1 ) { // convert to 64-bit number first?
-      int tmp = cuda_math::float2int(x.f32, mode);
-      if ((x.u32 & 0x7f800000) == 0)
-         tmp = 0; // round denorm. FP to 0
-      if (saturation_mode && to_width < 32) {
-         tmp = saturatei(tmp, (1<<to_width) - 1, -(1<<to_width));
-      }
-      switch ( to_width ) {
-      case 8:  y.s8  = (char)tmp; break;
-      case 16: y.s16 = (short)tmp; break;
-      case 32: y.s32 = (int)tmp; break;
-      case 64: y.s64 = (long long)tmp; break;
-      default: assert(0); break;
-      }
-   } else if ( to_sign == 0 ) {
-      unsigned int tmp = cuda_math::float2uint(x.f32, mode);
-      if ((x.u32 & 0x7f800000) == 0)
-         tmp = 0; // round denorm. FP to 0
-      if (saturation_mode && to_width < 32) {
-         tmp = saturatei(tmp, (1<<to_width) - 1);
-      }
-      switch ( to_width ) {
-      case 8:  y.u8  = (unsigned char)tmp; break;
-      case 16: y.u16 = (unsigned short)tmp; break;
-      case 32: y.u32 = (unsigned int)tmp; break;
-      case 64: y.u64 = (unsigned long long)tmp; break;
-      default: assert(0); break;
-      }
-   } else {
-      switch ( to_width ) {
-      case 16: 
-	 y.f16 =half_float::half_cast<half,std::numeric_limits<float>::round_style>(x.f32);//mytemp;
- 	 break;
-      case 32: 
-	 y.f32=float(x.f16);
-	 break; // handled by f2f
-      case 64: 
-         y.f64 = x.f32; 
-         break;
-      default: assert(0); break;
-      }
-   }
-   return y;
+  enum cuda_math::cudaRoundMode mode = cuda_math::cudaRoundZero;
+  switch (rounding_mode) {
+    case RZI_OPTION:
+      mode = cuda_math::cudaRoundZero;
+      break;
+    case RNI_OPTION:
+      mode = cuda_math::cudaRoundNearest;
+      break;
+    case RMI_OPTION:
+      mode = cuda_math::cudaRoundMinInf;
+      break;
+    case RPI_OPTION:
+      mode = cuda_math::cudaRoundPosInf;
+      break;
+    default:
+      break;
+  }
+
+  ptx_reg_t y;
+  if (to_sign == 1) {  // convert to 64-bit number first?
+    int tmp = cuda_math::float2int(x.f32, mode);
+    if ((x.u32 & 0x7f800000) == 0) tmp = 0;  // round denorm. FP to 0
+    if (saturation_mode && to_width < 32) {
+      tmp = saturatei(tmp, (1 << to_width) - 1, -(1 << to_width));
+    }
+    switch (to_width) {
+      case 8:
+        y.s8 = (char)tmp;
+        break;
+      case 16:
+        y.s16 = (short)tmp;
+        break;
+      case 32:
+        y.s32 = (int)tmp;
+        break;
+      case 64:
+        y.s64 = (long long)tmp;
+        break;
+      default:
+        assert(0);
+        break;
+    }
+  } else if (to_sign == 0) {
+    unsigned int tmp = cuda_math::float2uint(x.f32, mode);
+    if ((x.u32 & 0x7f800000) == 0) tmp = 0;  // round denorm. FP to 0
+    if (saturation_mode && to_width < 32) {
+      tmp = saturatei(tmp, (1 << to_width) - 1);
+    }
+    switch (to_width) {
+      case 8:
+        y.u8 = (unsigned char)tmp;
+        break;
+      case 16:
+        y.u16 = (unsigned short)tmp;
+        break;
+      case 32:
+        y.u32 = (unsigned int)tmp;
+        break;
+      case 64:
+        y.u64 = (unsigned long long)tmp;
+        break;
+      default:
+        assert(0);
+        break;
+    }
+  } else {
+    switch (to_width) {
+      case 16:
+        y.f16 = half_float::half_cast<half,
+                                      std::numeric_limits<float>::round_style>(
+            x.f32);  // mytemp;
+        break;
+      case 32:
+        y.f32 = float(x.f16);
+        break;  // handled by f2f
+      case 64:
+        y.f64 = x.f32;
+        break;
+      default:
+        assert(0);
+        break;
+    }
+  }
+  return y;
+}
+
+double saturated2i(double a, double max, double min) {
+  if (a > max)
+    a = max;
+  else if (a < min)
+    a = min;
+  return a;
+}
+
+ptx_reg_t d2x(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+              int rounding_mode, int saturation_mode) {
+  assert(from_width == 64);
+
+  double tmp;
+  switch (rounding_mode) {
+    case RZI_OPTION:
+      tmp = trunc(x.f64);
+      break;
+    case RNI_OPTION:
+      tmp = nearbyint(x.f64);
+      break;
+    case RMI_OPTION:
+      tmp = floor(x.f64);
+      break;
+    case RPI_OPTION:
+      tmp = ceil(x.f64);
+      break;
+    default:
+      tmp = x.f64;
+      break;
+  }
+
+  ptx_reg_t y;
+  if (to_sign == 1) {
+    tmp = saturated2i(tmp, ((1 << (to_width - 1)) - 1), (1 << (to_width - 1)));
+    switch (to_width) {
+      case 8:
+        y.s8 = (char)tmp;
+        break;
+      case 16:
+        y.s16 = (short)tmp;
+        break;
+      case 32:
+        y.s32 = (int)tmp;
+        break;
+      case 64:
+        y.s64 = (long long)tmp;
+        break;
+      default:
+        assert(0);
+        break;
+    }
+  } else if (to_sign == 0) {
+    tmp = saturated2i(tmp, ((1 << (to_width - 1)) - 1), 0);
+    switch (to_width) {
+      case 8:
+        y.u8 = (unsigned char)tmp;
+        break;
+      case 16:
+        y.u16 = (unsigned short)tmp;
+        break;
+      case 32:
+        y.u32 = (unsigned int)tmp;
+        break;
+      case 64:
+        y.u64 = (unsigned long long)tmp;
+        break;
+      default:
+        assert(0);
+        break;
+    }
+  } else {
+    switch (to_width) {
+      case 16:
+        assert(0);
+        break;
+      case 32:
+        y.f32 = x.f64;
+        break;
+      case 64:
+        y.f64 = x.f64;  // should be handled by d2d
+        break;
+      default:
+        assert(0);
+        break;
+    }
+  }
+  return y;
 }
 
-double saturated2i (double a, double max, double min) {
-   if (a > max) a = max;
-   else if (a < min) a = min;
-   return a;
+ptx_reg_t s2f(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+              int rounding_mode, int saturation_mode) {
+  ptx_reg_t y;
+
+  if (from_width < 64) {  // 32-bit conversion
+    y = sext(x, from_width, 32, 0, rounding_mode, saturation_mode);
+
+    switch (to_width) {
+      case 16:
+        assert(0);
+        break;
+      case 32:
+        switch (rounding_mode) {
+          case RZ_OPTION:
+            y.f32 = cuda_math::__int2float_rz(y.s32);
+            break;
+          case RN_OPTION:
+            y.f32 = cuda_math::__int2float_rn(y.s32);
+            break;
+          case RM_OPTION:
+            y.f32 = cuda_math::__int2float_rd(y.s32);
+            break;
+          case RP_OPTION:
+            y.f32 = cuda_math::__int2float_ru(y.s32);
+            break;
+          default:
+            break;
+        }
+        break;
+      case 64:
+        y.f64 = y.s32;
+        break;  // no rounding needed
+      default:
+        assert(0);
+        break;
+    }
+  } else {
+    switch (to_width) {
+      case 16:
+        assert(0);
+        break;
+      case 32:
+        switch (rounding_mode) {
+          case RZ_OPTION:
+            y.f32 = cuda_math::__ll2float_rz(y.s64);
+            break;
+          case RN_OPTION:
+            y.f32 = cuda_math::__ll2float_rn(y.s64);
+            break;
+          case RM_OPTION:
+            y.f32 = cuda_math::__ll2float_rd(y.s64);
+            break;
+          case RP_OPTION:
+            y.f32 = cuda_math::__ll2float_ru(y.s64);
+            break;
+          default:
+            break;
+        }
+        break;
+      case 64:
+        y.f64 = y.s64;
+        break;  // no internal implementation found
+      default:
+        assert(0);
+        break;
+    }
+  }
+
+  // saturating an integer to 1 or 0?
+  return y;
 }
 
-ptx_reg_t d2x( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   assert( from_width == 64); 
+ptx_reg_t u2f(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+              int rounding_mode, int saturation_mode) {
+  ptx_reg_t y;
 
-   double tmp;
-   switch (rounding_mode) {
-   case RZI_OPTION: tmp = trunc(x.f64);     break;
-   case RNI_OPTION: tmp = nearbyint(x.f64); break;
-   case RMI_OPTION: tmp = floor(x.f64);     break;
-   case RPI_OPTION: tmp = ceil(x.f64);      break;
-   default: tmp = x.f64; break; 
-   }
+  if (from_width < 64) {  // 32-bit conversion
+    y = zext(x, from_width, 32, 0, rounding_mode, saturation_mode);
 
-   ptx_reg_t y;
-   if ( to_sign == 1 ) {
-      tmp = saturated2i(tmp, ((1<<(to_width - 1)) - 1), (1<<(to_width - 1)) );
-      switch ( to_width ) {
-      case 8:  y.s8  = (char)tmp; break;
-      case 16: y.s16 = (short)tmp; break;
-      case 32: y.s32 = (int)tmp; break;
-      case 64: y.s64 = (long long)tmp; break;
-      default: assert(0); break;
-      }
-   } else if ( to_sign == 0 ) {
-      tmp = saturated2i(tmp, ((1<<(to_width - 1)) - 1), 0);
-      switch ( to_width ) {
-      case 8:  y.u8  = (unsigned char)tmp; break;
-      case 16: y.u16 = (unsigned short)tmp; break;
-      case 32: y.u32 = (unsigned int)tmp; break;
-      case 64: y.u64 = (unsigned long long)tmp; break;
-      default: assert(0); break;
-      }
-   } else {
-      switch ( to_width ) {
-      case 16: assert(0); break;
+    switch (to_width) {
+      case 16:
+        assert(0);
+        break;
       case 32:
-         y.f32 = x.f64;  
-         break;
-      case 64: 
-         y.f64 = x.f64; // should be handled by d2d
-         break;
-      default: assert(0); break;
-      }
-   }
-   return y;
-}
-
-ptx_reg_t s2f( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   ptx_reg_t y;
-
-   if (from_width < 64) { // 32-bit conversion
-      y = sext(x,from_width,32,0,rounding_mode,saturation_mode);
-
-      switch ( to_width ) {
-      case 16: assert(0); break;
-      case 32: 
-         switch (rounding_mode) {
-         case RZ_OPTION: y.f32 = cuda_math::__int2float_rz(y.s32); break;
-         case RN_OPTION: y.f32 = cuda_math::__int2float_rn(y.s32); break;
-         case RM_OPTION: y.f32 = cuda_math::__int2float_rd(y.s32); break;
-         case RP_OPTION: y.f32 = cuda_math::__int2float_ru(y.s32); break;
-         default: break; 
-         }
-         break;
-      case 64: y.f64 = y.s32; break; // no rounding needed
-      default: assert(0); break;
-      }
-   } else {
-      switch ( to_width ) {
-      case 16: assert(0); break;
-      case 32: 
-         switch (rounding_mode) {
-         case RZ_OPTION: y.f32 = cuda_math::__ll2float_rz(y.s64); break; 
-         case RN_OPTION: y.f32 = cuda_math::__ll2float_rn(y.s64); break;
-         case RM_OPTION: y.f32 = cuda_math::__ll2float_rd(y.s64); break; 
-         case RP_OPTION: y.f32 = cuda_math::__ll2float_ru(y.s64); break;
-         default: break; 
-         }
-         break;
-      case 64: y.f64 = y.s64; break; // no internal implementation found
-      default: assert(0); break;
-      }
-   }
+        switch (rounding_mode) {
+          case RZ_OPTION:
+            y.f32 = cuda_math::__uint2float_rz(y.u32);
+            break;
+          case RN_OPTION:
+            y.f32 = cuda_math::__uint2float_rn(y.u32);
+            break;
+          case RM_OPTION:
+            y.f32 = cuda_math::__uint2float_rd(y.u32);
+            break;
+          case RP_OPTION:
+            y.f32 = cuda_math::__uint2float_ru(y.u32);
+            break;
+          default:
+            break;
+        }
+        break;
+      case 64:
+        y.f64 = y.u32;
+        break;  // no rounding needed
+      default:
+        assert(0);
+        break;
+    }
+  } else {
+    switch (to_width) {
+      case 16:
+        assert(0);
+        break;
+      case 32:
+        switch (rounding_mode) {
+          case RZ_OPTION:
+            y.f32 = cuda_math::__ull2float_rn(y.u64);
+            break;
+          case RN_OPTION:
+            y.f32 = cuda_math::__ull2float_rn(y.u64);
+            break;
+          case RM_OPTION:
+            y.f32 = cuda_math::__ull2float_rn(y.u64);
+            break;
+          case RP_OPTION:
+            y.f32 = cuda_math::__ull2float_rn(y.u64);
+            break;
+          default:
+            break;
+        }
+        break;
+      case 64:
+        y.f64 = y.u64;
+        break;  // no internal implementation found
+      default:
+        assert(0);
+        break;
+    }
+  }
 
-   // saturating an integer to 1 or 0?
-   return y;
-}
-
-ptx_reg_t u2f( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   ptx_reg_t y;
-
-   if (from_width < 64) { // 32-bit conversion
-      y = zext(x,from_width,32,0,rounding_mode,saturation_mode);
-
-      switch ( to_width ) {
-      case 16: assert(0); break;
-      case 32: 
-         switch (rounding_mode) {
-         case RZ_OPTION: y.f32 = cuda_math::__uint2float_rz(y.u32); break;
-         case RN_OPTION: y.f32 = cuda_math::__uint2float_rn(y.u32); break;
-         case RM_OPTION: y.f32 = cuda_math::__uint2float_rd(y.u32); break;
-         case RP_OPTION: y.f32 = cuda_math::__uint2float_ru(y.u32); break;
-         default: break; 
-         }
-         break;
-      case 64: y.f64 = y.u32; break; // no rounding needed
-      default: assert(0); break;
-      }
-   } else {
-      switch ( to_width ) {
-      case 16: assert(0); break;
-      case 32: 
-         switch (rounding_mode) {
-         case RZ_OPTION: y.f32 = cuda_math::__ull2float_rn(y.u64); break; 
-         case RN_OPTION: y.f32 = cuda_math::__ull2float_rn(y.u64); break;
-         case RM_OPTION: y.f32 = cuda_math::__ull2float_rn(y.u64); break; 
-         case RP_OPTION: y.f32 = cuda_math::__ull2float_rn(y.u64); break; 
-         default: break; 
-         }
-         break;
-      case 64: y.f64 = y.u64; break; // no internal implementation found
-      default: assert(0); break;
-      }
-   }
+  // saturating an integer to 1 or 0?
+  return y;
+}
 
-   // saturating an integer to 1 or 0?
-   return y;
-}
-
-ptx_reg_t f2f( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   ptx_reg_t y;
-   if (from_width == 16){
-       half_float::detail::uint16 val = x.u16;
-       y.f32 = half_float::detail::half2float<float>(val);
-   }else{
-       switch ( rounding_mode ) {
-       case RZI_OPTION: 
-          y.f32 = truncf(x.f32); 
-          break;          
-       case RNI_OPTION: 
-    #if CUDART_VERSION >= 3000
-          y.f32 = nearbyintf(x.f32); 
-    #else
-          y.f32 = cuda_math::__internal_nearbyintf(x.f32); 
-    #endif
-          break;          
-       case RMI_OPTION: 
-          if ((x.u32 & 0x7f800000) == 0) {
-             y.u32 = x.u32 & 0x80000000; // round denorm. FP to 0, keeping sign
-          } else {
-             y.f32 = floorf(x.f32); 
-          }
-          break;          
-       case RPI_OPTION: 
-          if ((x.u32 & 0x7f800000) == 0) {
-             y.u32 = x.u32 & 0x80000000; // round denorm. FP to 0, keeping sign
-          } else {
-             y.f32 = ceilf(x.f32); 
-          }
-          break;          
-       default: 
-          if ((x.u32 & 0x7f800000) == 0) {
-             y.u32 = x.u32 & 0x80000000; // round denorm. FP to 0, keeping sign
-          } else {
-             y.f32 = x.f32;
-          }
-          break; 
-       }
-    #if CUDART_VERSION >= 3000
-       if (isnanf(y.f32)) 
-    #else
-       if (cuda_math::__cuda___isnanf(y.f32)) 
-    #endif
-       {
-          y.u32 = 0x7fffffff;
-       } else if (saturation_mode) {
-          y.f32 = cuda_math::__saturatef(y.f32);
-       }
-   }
+ptx_reg_t f2f(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+              int rounding_mode, int saturation_mode) {
+  ptx_reg_t y;
+  if (from_width == 16) {
+    half_float::detail::uint16 val = x.u16;
+    y.f32 = half_float::detail::half2float<float>(val);
+  } else {
+    switch (rounding_mode) {
+      case RZI_OPTION:
+        y.f32 = truncf(x.f32);
+        break;
+      case RNI_OPTION:
+#if CUDART_VERSION >= 3000
+        y.f32 = nearbyintf(x.f32);
+#else
+        y.f32 = cuda_math::__internal_nearbyintf(x.f32);
+#endif
+        break;
+      case RMI_OPTION:
+        if ((x.u32 & 0x7f800000) == 0) {
+          y.u32 = x.u32 & 0x80000000;  // round denorm. FP to 0, keeping sign
+        } else {
+          y.f32 = floorf(x.f32);
+        }
+        break;
+      case RPI_OPTION:
+        if ((x.u32 & 0x7f800000) == 0) {
+          y.u32 = x.u32 & 0x80000000;  // round denorm. FP to 0, keeping sign
+        } else {
+          y.f32 = ceilf(x.f32);
+        }
+        break;
+      default:
+        if ((x.u32 & 0x7f800000) == 0) {
+          y.u32 = x.u32 & 0x80000000;  // round denorm. FP to 0, keeping sign
+        } else {
+          y.f32 = x.f32;
+        }
+        break;
+    }
+#if CUDART_VERSION >= 3000
+    if (isnanf(y.f32))
+#else
+    if (cuda_math::__cuda___isnanf(y.f32))
+#endif
+    {
+      y.u32 = 0x7fffffff;
+    } else if (saturation_mode) {
+      y.f32 = cuda_math::__saturatef(y.f32);
+    }
+  }
 
-   return y;
+  return y;
 }
 
-ptx_reg_t d2d( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, int rounding_mode, int saturation_mode )
-{
-   ptx_reg_t y;
-   switch ( rounding_mode ) {
-   case RZI_OPTION: 
-      y.f64 = trunc(x.f64); 
-      break;          
-   case RNI_OPTION: 
+ptx_reg_t d2d(ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign,
+              int rounding_mode, int saturation_mode) {
+  ptx_reg_t y;
+  switch (rounding_mode) {
+    case RZI_OPTION:
+      y.f64 = trunc(x.f64);
+      break;
+    case RNI_OPTION:
 #if CUDART_VERSION >= 3000
-      y.f64 = nearbyint(x.f64); 
+      y.f64 = nearbyint(x.f64);
 #else
-      y.f64 = cuda_math::__internal_nearbyintf(x.f64); 
+      y.f64 = cuda_math::__internal_nearbyintf(x.f64);
 #endif
-      break;          
-   case RMI_OPTION: 
-      y.f64 = floor(x.f64); 
-      break;          
-   case RPI_OPTION: 
-      y.f64 = ceil(x.f64); 
-      break;          
-   default: 
+      break;
+    case RMI_OPTION:
+      y.f64 = floor(x.f64);
+      break;
+    case RPI_OPTION:
+      y.f64 = ceil(x.f64);
+      break;
+    default:
       y.f64 = x.f64;
-      break; 
-   }
-   if (std::isnan(y.f64)) {
-      y.u64 = 0xfff8000000000000ull;
-   } else if (saturation_mode) {
-      y.f64 = cuda_math::__saturatef(y.f64); 
-   }
-   return y;
-}
-
-ptx_reg_t (*g_cvt_fn[11][11])( ptx_reg_t x, unsigned from_width, unsigned to_width, int to_sign, 
-                               int rounding_mode, int saturation_mode ) = {
-   { NULL, sext, sext, sext, NULL, sext, sext, sext, s2f, s2f, s2f}, 
-   { chop, NULL, sext, sext, chop, NULL, sext, sext, s2f, s2f, s2f}, 
-   { chop, sexd, NULL, sext, chop, chop, NULL, sext, s2f, s2f, s2f}, 
-   { chop, chop, chop, NULL, chop, chop, chop, NULL, s2f, s2f, s2f}, 
-   { NULL, zext, zext, zext, NULL, zext, zext, zext, u2f, u2f, u2f}, 
-   { chop, NULL, zext, zext, chop, NULL, zext, zext, u2f, u2f, u2f}, 
-   { chop, chop, NULL, zext, chop, chop, NULL, zext, u2f, u2f, u2f}, 
-   { chop, chop, chop, NULL, chop, chop, chop, NULL, u2f, u2f, u2f}, 
-   { f2x , f2x , f2x , f2x , f2x , f2x , f2x , f2x , NULL,f2f, f2x}, 
-   { f2x , f2x , f2x , f2x , f2x , f2x , f2x , f2x , f2x, f2f, f2x},
-   { d2x , d2x , d2x , d2x , d2x , d2x , d2x , d2x , d2x, d2x, d2d} 
-};
+      break;
+  }
+  if (std::isnan(y.f64)) {
+    y.u64 = 0xfff8000000000000ull;
+  } else if (saturation_mode) {
+    y.f64 = cuda_math::__saturatef(y.f64);
+  }
+  return y;
+}
 
-void ptx_round(ptx_reg_t& data, int rounding_mode, int type)
-{
-   if (rounding_mode == RN_OPTION) {
-      return;
-   }
-   switch ( rounding_mode ) {
-   case RZI_OPTION: 
-      switch ( type ) {
-      case S8_TYPE:
-      case S16_TYPE:
-      case S32_TYPE:
-      case S64_TYPE:
-      case U8_TYPE:
-      case U16_TYPE:
-      case U32_TYPE:
-      case U64_TYPE:
-         printf("Trying to round an integer??\n"); assert(0); break;
-      case F16_TYPE: data.f16=truncf(data.f16);break;//assert(0); break;
-      case F32_TYPE:
-         data.f32 = truncf(data.f32); 
-         break;          
-      case F64_TYPE:
-      case FF64_TYPE:
-         if (data.f64 < 0) data.f64 = ceil(data.f64); //negative
-         else data.f64 = floor(data.f64); //positive
-         break; 
-      default: assert(0); break;
+ptx_reg_t (*g_cvt_fn[11][11])(ptx_reg_t x, unsigned from_width,
+                              unsigned to_width, int to_sign, int rounding_mode,
+                              int saturation_mode) = {
+    {NULL, sext, sext, sext, NULL, sext, sext, sext, s2f, s2f, s2f},
+    {chop, NULL, sext, sext, chop, NULL, sext, sext, s2f, s2f, s2f},
+    {chop, sexd, NULL, sext, chop, chop, NULL, sext, s2f, s2f, s2f},
+    {chop, chop, chop, NULL, chop, chop, chop, NULL, s2f, s2f, s2f},
+    {NULL, zext, zext, zext, NULL, zext, zext, zext, u2f, u2f, u2f},
+    {chop, NULL, zext, zext, chop, NULL, zext, zext, u2f, u2f, u2f},
+    {chop, chop, NULL, zext, chop, chop, NULL, zext, u2f, u2f, u2f},
+    {chop, chop, chop, NULL, chop, chop, chop, NULL, u2f, u2f, u2f},
+    {f2x, f2x, f2x, f2x, f2x, f2x, f2x, f2x, NULL, f2f, f2x},
+    {f2x, f2x, f2x, f2x, f2x, f2x, f2x, f2x, f2x, f2f, f2x},
+    {d2x, d2x, d2x, d2x, d2x, d2x, d2x, d2x, d2x, d2x, d2d}};
+
+void ptx_round(ptx_reg_t &data, int rounding_mode, int type) {
+  if (rounding_mode == RN_OPTION) {
+    return;
+  }
+  switch (rounding_mode) {
+    case RZI_OPTION:
+      switch (type) {
+        case S8_TYPE:
+        case S16_TYPE:
+        case S32_TYPE:
+        case S64_TYPE:
+        case U8_TYPE:
+        case U16_TYPE:
+        case U32_TYPE:
+        case U64_TYPE:
+          printf("Trying to round an integer??\n");
+          assert(0);
+          break;
+        case F16_TYPE:
+          data.f16 = truncf(data.f16);
+          break;  // assert(0); break;
+        case F32_TYPE:
+          data.f32 = truncf(data.f32);
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          if (data.f64 < 0)
+            data.f64 = ceil(data.f64);  // negative
+          else
+            data.f64 = floor(data.f64);  // positive
+          break;
+        default:
+          assert(0);
+          break;
       }
       break;
-   case RNI_OPTION: 
-      switch ( type ) {
-      case S8_TYPE:
-      case S16_TYPE:
-      case S32_TYPE:
-      case S64_TYPE:
-      case U8_TYPE:
-      case U16_TYPE:
-      case U32_TYPE:
-      case U64_TYPE:
-         printf("Trying to round an integer??\n"); assert(0); break;
-      case F16_TYPE:// assert(0); break;
+    case RNI_OPTION:
+      switch (type) {
+        case S8_TYPE:
+        case S16_TYPE:
+        case S32_TYPE:
+        case S64_TYPE:
+        case U8_TYPE:
+        case U16_TYPE:
+        case U32_TYPE:
+        case U64_TYPE:
+          printf("Trying to round an integer??\n");
+          assert(0);
+          break;
+        case F16_TYPE:  // assert(0); break;
 #if CUDART_VERSION >= 3000
-         data.f16 = nearbyintf(data.f16); 
+          data.f16 = nearbyintf(data.f16);
 #else
-         data.f16 = cuda_math::__cuda_nearbyintf(data.f16); 
+          data.f16 = cuda_math::__cuda_nearbyintf(data.f16);
 #endif
-         break;          
-      case F32_TYPE: 
+          break;
+        case F32_TYPE:
 #if CUDART_VERSION >= 3000
-         data.f32 = nearbyintf(data.f32); 
+          data.f32 = nearbyintf(data.f32);
 #else
-         data.f32 = cuda_math::__cuda_nearbyintf(data.f32); 
+          data.f32 = cuda_math::__cuda_nearbyintf(data.f32);
 #endif
-         break;          
-      case F64_TYPE: case FF64_TYPE: data.f64 = round(data.f64); break; 
-      default: assert(0); break;
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          data.f64 = round(data.f64);
+          break;
+        default:
+          assert(0);
+          break;
       }
       break;
-   case RMI_OPTION: 
-      switch ( type ) {
-      case S8_TYPE:
-      case S16_TYPE:
-      case S32_TYPE:
-      case S64_TYPE:
-      case U8_TYPE:
-      case U16_TYPE:
-      case U32_TYPE:
-      case U64_TYPE:
-         printf("Trying to round an integer??\n"); assert(0); break;
-      case F16_TYPE: data.f16=floorf(data.f16);break;//assert(0); break;
-      case F32_TYPE: 
-         data.f32 = floorf(data.f32); 
-         break;          
-      case F64_TYPE: case FF64_TYPE: data.f64 = floor(data.f64); break; 
-      default: assert(0); break;
+    case RMI_OPTION:
+      switch (type) {
+        case S8_TYPE:
+        case S16_TYPE:
+        case S32_TYPE:
+        case S64_TYPE:
+        case U8_TYPE:
+        case U16_TYPE:
+        case U32_TYPE:
+        case U64_TYPE:
+          printf("Trying to round an integer??\n");
+          assert(0);
+          break;
+        case F16_TYPE:
+          data.f16 = floorf(data.f16);
+          break;  // assert(0); break;
+        case F32_TYPE:
+          data.f32 = floorf(data.f32);
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          data.f64 = floor(data.f64);
+          break;
+        default:
+          assert(0);
+          break;
       }
       break;
-   case RPI_OPTION: 
-      switch ( type ) {
-      case S8_TYPE:
-      case S16_TYPE:
-      case S32_TYPE:
-      case S64_TYPE:
-      case U8_TYPE:
-      case U16_TYPE:
-      case U32_TYPE:
-      case U64_TYPE:
-         printf("Trying to round an integer??\n"); assert(0); break;
-      case F16_TYPE: data.f16 = ceilf(data.f16); break;  //assert(0); break;
-      case F32_TYPE: data.f32 = ceilf(data.f32); break;          
-      case F64_TYPE: case FF64_TYPE: data.f64 = ceil(data.f64); break; 
-      default: assert(0); break;
+    case RPI_OPTION:
+      switch (type) {
+        case S8_TYPE:
+        case S16_TYPE:
+        case S32_TYPE:
+        case S64_TYPE:
+        case U8_TYPE:
+        case U16_TYPE:
+        case U32_TYPE:
+        case U64_TYPE:
+          printf("Trying to round an integer??\n");
+          assert(0);
+          break;
+        case F16_TYPE:
+          data.f16 = ceilf(data.f16);
+          break;  // assert(0); break;
+        case F32_TYPE:
+          data.f32 = ceilf(data.f32);
+          break;
+        case F64_TYPE:
+        case FF64_TYPE:
+          data.f64 = ceil(data.f64);
+          break;
+        default:
+          assert(0);
+          break;
       }
       break;
-   default:  break; 
-   }
+    default:
+      break;
+  }
 
-   if (type == F32_TYPE) {
+  if (type == F32_TYPE) {
 #if CUDART_VERSION >= 3000
-      if (isnanf(data.f32)) 
+    if (isnanf(data.f32))
 #else
-      if (cuda_math::__cuda___isnanf(data.f32)) 
+    if (cuda_math::__cuda___isnanf(data.f32))
 #endif
-      {
-         data.u32 = 0x7fffffff;
-      }
-   }
-   if ((type == F64_TYPE)||(type == FF64_TYPE)) {
-      if (std::isnan(data.f64)) {
-         data.u64 = 0xfff8000000000000ull;
-      }
-   }
+    {
+      data.u32 = 0x7fffffff;
+    }
+  }
+  if ((type == F64_TYPE) || (type == FF64_TYPE)) {
+    if (std::isnan(data.f64)) {
+      data.u64 = 0xfff8000000000000ull;
+    }
+  }
 }
 
-void ptx_saturate(ptx_reg_t& data, int saturation_mode, int type)
-{
-   if (!saturation_mode) {
-      return;
-   }
-   switch ( type ) {
-   case S8_TYPE:
-   case S16_TYPE:
-   case S32_TYPE:
-   case S64_TYPE:
-   case U8_TYPE:
-   case U16_TYPE:
-   case U32_TYPE:
-   case U64_TYPE:
-      printf("Trying to clamp an integer to 1??\n"); assert(0); break;
-   case F16_TYPE: //assert(0); break;
-      if (data.f16 > 1.0f) data.f16 = 1.0f; //negative
-      if (data.f16 < 0.0f) data.f16 = 0.0f; //positive
-      break;          
-   case F32_TYPE:
-      if (data.f32 > 1.0f) data.f32 = 1.0f; //negative
-      if (data.f32 < 0.0f) data.f32 = 0.0f; //positive
-      break;          
-   case F64_TYPE:
-   case FF64_TYPE:
-      if (data.f64 > 1.0f) data.f64 = 1.0f; //negative
-      if (data.f64 < 0.0f) data.f64 = 0.0f; //positive
-      break; 
-   default: assert(0); break;
-   }
-
+void ptx_saturate(ptx_reg_t &data, int saturation_mode, int type) {
+  if (!saturation_mode) {
+    return;
+  }
+  switch (type) {
+    case S8_TYPE:
+    case S16_TYPE:
+    case S32_TYPE:
+    case S64_TYPE:
+    case U8_TYPE:
+    case U16_TYPE:
+    case U32_TYPE:
+    case U64_TYPE:
+      printf("Trying to clamp an integer to 1??\n");
+      assert(0);
+      break;
+    case F16_TYPE:                           // assert(0); break;
+      if (data.f16 > 1.0f) data.f16 = 1.0f;  // negative
+      if (data.f16 < 0.0f) data.f16 = 0.0f;  // positive
+      break;
+    case F32_TYPE:
+      if (data.f32 > 1.0f) data.f32 = 1.0f;  // negative
+      if (data.f32 < 0.0f) data.f32 = 0.0f;  // positive
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      if (data.f64 > 1.0f) data.f64 = 1.0f;  // negative
+      if (data.f64 < 0.0f) data.f64 = 0.0f;  // positive
+      break;
+    default:
+      assert(0);
+      break;
+  }
 }
 
-void cvt_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   unsigned to_type = pI->get_type();
-   unsigned from_type = pI->get_type2();
-   unsigned rounding_mode = pI->rounding_mode();
-   unsigned saturation_mode = pI->saturation_mode();
+void cvt_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  unsigned to_type = pI->get_type();
+  unsigned from_type = pI->get_type2();
+  unsigned rounding_mode = pI->rounding_mode();
+  unsigned saturation_mode = pI->saturation_mode();
 
-//   if ( to_type == F16_TYPE || from_type == F16_TYPE )
-//      abort();
+  //   if ( to_type == F16_TYPE || from_type == F16_TYPE )
+  //      abort();
 
-   int to_sign, from_sign;
-   size_t from_width, to_width;
-   unsigned src_fmt = type_info_key::type_decode(from_type, from_width, from_sign);
-   unsigned dst_fmt = type_info_key::type_decode(to_type, to_width, to_sign);
+  int to_sign, from_sign;
+  size_t from_width, to_width;
+  unsigned src_fmt =
+      type_info_key::type_decode(from_type, from_width, from_sign);
+  unsigned dst_fmt = type_info_key::type_decode(to_type, to_width, to_sign);
 
-   ptx_reg_t data = thread->get_operand_value(src1, dst, from_type, thread, 1);
+  ptx_reg_t data = thread->get_operand_value(src1, dst, from_type, thread, 1);
 
-   if(pI->is_neg()){
-
-   switch( from_type ) {
+  if (pI->is_neg()) {
+    switch (from_type) {
       // Default to f32 for now, need to add support for others
       case S8_TYPE:
       case U8_TYPE:
       case B8_TYPE:
-         data.s8 = -data.s8;
-         break;
+        data.s8 = -data.s8;
+        break;
       case S16_TYPE:
       case U16_TYPE:
       case B16_TYPE:
-         data.s16 = -data.s16;
-         break;
+        data.s16 = -data.s16;
+        break;
       case S32_TYPE:
       case U32_TYPE:
       case B32_TYPE:
-         data.s32 = -data.s32;
-         break;
+        data.s32 = -data.s32;
+        break;
       case S64_TYPE:
       case U64_TYPE:
       case B64_TYPE:
-         data.s64 = -data.s64;
-         break;
+        data.s64 = -data.s64;
+        break;
       case F16_TYPE:
-         data.f16 = -data.f16;
-         break;
+        data.f16 = -data.f16;
+        break;
       case F32_TYPE:
-         data.f32 = -data.f32;
-         break;
+        data.f32 = -data.f32;
+        break;
       case F64_TYPE:
       case FF64_TYPE:
-         data.f64 = -data.f64;
-         break;
+        data.f64 = -data.f64;
+        break;
       default:
-         assert(0);
-      }
+        assert(0);
+    }
+  }
 
-   }
+  if (g_cvt_fn[src_fmt][dst_fmt] != NULL) {
+    ptx_reg_t result = g_cvt_fn[src_fmt][dst_fmt](
+        data, from_width, to_width, to_sign, rounding_mode, saturation_mode);
+    data = result;
+  }
 
+  thread->set_operand_value(dst, data, to_type, thread, pI);
+}
 
-   if ( g_cvt_fn[src_fmt][dst_fmt] != NULL ) {
-      ptx_reg_t result = g_cvt_fn[src_fmt][dst_fmt](data,from_width,to_width,to_sign, rounding_mode, saturation_mode);
-      data = result;
-   }
+void cvta_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t data;
+
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  memory_space_t space = pI->get_space();
+  bool to_non_generic = pI->is_to();
+
+  unsigned i_type = pI->get_type();
+  ptx_reg_t from_addr = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  addr_t from_addr_hw = (addr_t)from_addr.u64;
+  addr_t to_addr_hw = 0;
+  unsigned smid = thread->get_hw_sid();
+  unsigned hwtid = thread->get_hw_tid();
+
+  if (to_non_generic) {
+    switch (space.get_type()) {
+      case shared_space:
+        to_addr_hw = generic_to_shared(smid, from_addr_hw);
+        break;
+      case local_space:
+        to_addr_hw = generic_to_local(smid, hwtid, from_addr_hw);
+        break;
+      case global_space:
+        to_addr_hw = generic_to_global(from_addr_hw);
+        break;
+      default:
+        abort();
+    }
+  } else {
+    switch (space.get_type()) {
+      case shared_space:
+        to_addr_hw = shared_to_generic(smid, from_addr_hw);
+        break;
+      case local_space:
+        to_addr_hw = local_to_generic(smid, hwtid, from_addr_hw) +
+                     thread->get_local_mem_stack_pointer();
+        break;  // add stack ptr here so that it can be passed as a pointer at
+                // function call
+      case global_space:
+        to_addr_hw = global_to_generic(from_addr_hw);
+        break;
+      default:
+        abort();
+    }
+  }
 
-   thread->set_operand_value(dst, data, to_type, thread, pI );
+  ptx_reg_t to_addr;
+  to_addr.u64 = to_addr_hw;
+  thread->set_reg(dst.get_symbol(), to_addr);
 }
 
-void cvta_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t data;
+void div_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t data;
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   memory_space_t space = pI->get_space();
-   bool to_non_generic = pI->is_to();
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   ptx_reg_t from_addr = thread->get_operand_value(src1,dst,i_type,thread,1);
-   addr_t from_addr_hw = (addr_t)from_addr.u64;
-   addr_t to_addr_hw = 0;
-   unsigned smid = thread->get_hw_sid();
-   unsigned hwtid = thread->get_hw_tid();
+  unsigned i_type = pI->get_type();
 
-   if( to_non_generic ) {
-      switch( space.get_type() ) {
-      case shared_space: to_addr_hw = generic_to_shared( smid, from_addr_hw ); break;
-      case local_space:  to_addr_hw = generic_to_local( smid, hwtid, from_addr_hw ); break;
-      case global_space: to_addr_hw = generic_to_global(from_addr_hw ); break;
-      default: abort();
-      }
-   } else {
-      switch( space.get_type() ) {
-      case shared_space: to_addr_hw = shared_to_generic( smid, from_addr_hw ); break;
-      case local_space:  to_addr_hw =  local_to_generic( smid, hwtid, from_addr_hw )
-                                      + thread->get_local_mem_stack_pointer(); break; // add stack ptr here so that it can be passed as a pointer at function call 
-      case global_space: to_addr_hw = global_to_generic( from_addr_hw ); break;
-      default: abort();
-      }
-   }
-   
-   ptx_reg_t to_addr;
-   to_addr.u64 = to_addr_hw;
-   thread->set_reg(dst.get_symbol(),to_addr);
-}
-
-void div_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t data;
-
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-
-   unsigned i_type = pI->get_type();
-
-   ptx_reg_t src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   ptx_reg_t src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
-
-
-   switch ( i_type ) {
-   case S8_TYPE:
-      data.s8  = src1_data.s8  / src2_data.s8 ; break;
-   case S16_TYPE:
-      data.s16 = src1_data.s16 / src2_data.s16; break;
-   case S32_TYPE:
-      data.s32 = src1_data.s32 / src2_data.s32; break;
-   case S64_TYPE: 
-      data.s64 = src1_data.s64 / src2_data.s64; break;
-   case U8_TYPE:
-      data.u8  = src1_data.u8  / src2_data.u8 ; break;
-   case U16_TYPE:
-      data.u16 = src1_data.u16 / src2_data.u16; break;
-   case U32_TYPE:
-      data.u32 = src1_data.u32 / src2_data.u32; break;
-   case U64_TYPE: 
-      data.u64 = src1_data.u64 / src2_data.u64; break;
-   case B8_TYPE:
-      data.u8  = src1_data.u8  / src2_data.u8 ; break;
-   case B16_TYPE:
-      data.u16 = src1_data.u16 / src2_data.u16; break;
-   case B32_TYPE:
-      data.u32 = src1_data.u32 / src2_data.u32; break;
-   case B64_TYPE:
-      data.u64 = src1_data.u64 / src2_data.u64; break;
-   case F16_TYPE: data.f16 = src1_data.f16 / src2_data.f16; break;//assert(0); break;
-   case F32_TYPE: data.f32 = src1_data.f32 / src2_data.f32; break;
-   case F64_TYPE: case FF64_TYPE: data.f64 = src1_data.f64 / src2_data.f64; break;
-   default: assert(0); break;
-   }
-   thread->set_operand_value(dst,data, i_type, thread,pI);
-}
-
-void dp4a_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   printf("DP4A instruction not implemented yet");
-   assert(0);
+  ptx_reg_t src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  ptx_reg_t src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
+  switch (i_type) {
+    case S8_TYPE:
+      data.s8 = src1_data.s8 / src2_data.s8;
+      break;
+    case S16_TYPE:
+      data.s16 = src1_data.s16 / src2_data.s16;
+      break;
+    case S32_TYPE:
+      data.s32 = src1_data.s32 / src2_data.s32;
+      break;
+    case S64_TYPE:
+      data.s64 = src1_data.s64 / src2_data.s64;
+      break;
+    case U8_TYPE:
+      data.u8 = src1_data.u8 / src2_data.u8;
+      break;
+    case U16_TYPE:
+      data.u16 = src1_data.u16 / src2_data.u16;
+      break;
+    case U32_TYPE:
+      data.u32 = src1_data.u32 / src2_data.u32;
+      break;
+    case U64_TYPE:
+      data.u64 = src1_data.u64 / src2_data.u64;
+      break;
+    case B8_TYPE:
+      data.u8 = src1_data.u8 / src2_data.u8;
+      break;
+    case B16_TYPE:
+      data.u16 = src1_data.u16 / src2_data.u16;
+      break;
+    case B32_TYPE:
+      data.u32 = src1_data.u32 / src2_data.u32;
+      break;
+    case B64_TYPE:
+      data.u64 = src1_data.u64 / src2_data.u64;
+      break;
+    case F16_TYPE:
+      data.f16 = src1_data.f16 / src2_data.f16;
+      break;  // assert(0); break;
+    case F32_TYPE:
+      data.f32 = src1_data.f32 / src2_data.f32;
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      data.f64 = src1_data.f64 / src2_data.f64;
+      break;
+    default:
+      assert(0);
+      break;
+  }
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void ex2_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+void dp4a_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  printf("DP4A instruction not implemented yet");
+  assert(0);
+}
 
-   unsigned i_type = pI->get_type();
+void ex2_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
 
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case F32_TYPE: 
+  switch (i_type) {
+    case F32_TYPE:
       data.f32 = cuda_math::__powf(2.0, src1_data.f32);
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
-   
-   thread->set_operand_value(dst,data, i_type, thread,pI);
-}
+  }
 
-void exit_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   thread->set_done();
-   thread->exitCore();
-   thread->registerExit();
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void mad_def( const ptx_instruction *pI, ptx_thread_info *thread, bool use_carry = false );
-
-void fma_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   mad_def(pI,thread);
+void exit_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  thread->set_done();
+  thread->exitCore();
+  thread->registerExit();
 }
 
-void isspacep_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a;
-   bool t=false;
+void mad_def(const ptx_instruction *pI, ptx_thread_info *thread,
+             bool use_carry = false);
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   memory_space_t space = pI->get_space();
-
-   a = thread->get_reg(src1.get_symbol());
-   addr_t addr = (addr_t)a.u64;
-   unsigned smid = thread->get_hw_sid();
-   unsigned hwtid = thread->get_hw_tid();
+void fma_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  mad_def(pI, thread);
+}
 
-   switch( space.get_type() ) {
-   case shared_space: t = isspace_shared( smid, addr );
-   case local_space:  t = isspace_local( smid, hwtid, addr );
-   case global_space: t = isspace_global( addr );
-   default: abort();
-   }
+void isspacep_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a;
+  bool t = false;
+
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  memory_space_t space = pI->get_space();
+
+  a = thread->get_reg(src1.get_symbol());
+  addr_t addr = (addr_t)a.u64;
+  unsigned smid = thread->get_hw_sid();
+  unsigned hwtid = thread->get_hw_tid();
+
+  switch (space.get_type()) {
+    case shared_space:
+      t = isspace_shared(smid, addr);
+    case local_space:
+      t = isspace_local(smid, hwtid, addr);
+    case global_space:
+      t = isspace_global(addr);
+    default:
+      abort();
+  }
 
-   ptx_reg_t p;
-   p.pred = t?1:0;
+  ptx_reg_t p;
+  p.pred = t ? 1 : 0;
 
-   thread->set_reg(dst.get_symbol(),p);
+  thread->set_reg(dst.get_symbol(), p);
 }
 
-void decode_space( memory_space_t &space, ptx_thread_info *thread, const operand_info &op, memory_space *&mem, addr_t &addr)
-{
-   unsigned smid = thread->get_hw_sid();
-   unsigned hwtid = thread->get_hw_tid();
-
-   if( space == param_space_unclassified ) {
-      // need to op to determine whether it refers to a kernel param or local param
-      const symbol *s = op.get_symbol();
-      const type_info *t = s->type();
-      type_info_key ti = t->get_key();
-      if( ti.is_param_kernel() )
-         space = param_space_kernel;
-      else if( ti.is_param_local() ) {
-         space = param_space_local;
-      }
-      //mov r1, param-label
-      else if (ti.is_reg() ){
-         space = param_space_kernel;
-      }
-      else {
-         printf("GPGPU-Sim PTX: ERROR ** cannot resolve .param space for '%s'\n", s->name().c_str() );
-         abort(); 
-      }
-   }
-   switch ( space.get_type() ) {
-   case global_space: mem = thread->get_global_memory(); break;
-   case param_space_local:
-   case local_space:
-      mem = thread->m_local_mem; 
+void decode_space(memory_space_t &space, ptx_thread_info *thread,
+                  const operand_info &op, memory_space *&mem, addr_t &addr) {
+  unsigned smid = thread->get_hw_sid();
+  unsigned hwtid = thread->get_hw_tid();
+
+  if (space == param_space_unclassified) {
+    // need to op to determine whether it refers to a kernel param or local
+    // param
+    const symbol *s = op.get_symbol();
+    const type_info *t = s->type();
+    type_info_key ti = t->get_key();
+    if (ti.is_param_kernel())
+      space = param_space_kernel;
+    else if (ti.is_param_local()) {
+      space = param_space_local;
+    }
+    // mov r1, param-label
+    else if (ti.is_reg()) {
+      space = param_space_kernel;
+    } else {
+      printf("GPGPU-Sim PTX: ERROR ** cannot resolve .param space for '%s'\n",
+             s->name().c_str());
+      abort();
+    }
+  }
+  switch (space.get_type()) {
+    case global_space:
+      mem = thread->get_global_memory();
+      break;
+    case param_space_local:
+    case local_space:
+      mem = thread->m_local_mem;
       addr += thread->get_local_mem_stack_pointer();
-      break; 
-   case tex_space:    mem = thread->get_tex_memory(); break; 
-   case surf_space:   mem = thread->get_surf_memory(); break; 
-   case param_space_kernel:  mem = thread->get_param_memory(); break;
-   case shared_space:  mem = thread->m_shared_mem; break; 
-   case sstarr_space:	mem = thread->m_sstarr_mem; break;
-   case const_space:  mem = thread->get_global_memory(); break;
-   case generic_space:
-      if( thread->get_ptx_version().ver() >= 2.0 ) {
-         // convert generic address to memory space address
-         space = whichspace(addr);
-         switch ( space.get_type() ) {
-         case global_space: mem = thread->get_global_memory(); addr = generic_to_global(addr); break;
-         case local_space:  mem = thread->m_local_mem; addr = generic_to_local(smid,hwtid,addr); break; 
-         case shared_space: mem = thread->m_shared_mem; addr = generic_to_shared(smid,addr); break; 
-         default: abort();
-         }
+      break;
+    case tex_space:
+      mem = thread->get_tex_memory();
+      break;
+    case surf_space:
+      mem = thread->get_surf_memory();
+      break;
+    case param_space_kernel:
+      mem = thread->get_param_memory();
+      break;
+    case shared_space:
+      mem = thread->m_shared_mem;
+      break;
+    case sstarr_space:
+      mem = thread->m_sstarr_mem;
+      break;
+    case const_space:
+      mem = thread->get_global_memory();
+      break;
+    case generic_space:
+      if (thread->get_ptx_version().ver() >= 2.0) {
+        // convert generic address to memory space address
+        space = whichspace(addr);
+        switch (space.get_type()) {
+          case global_space:
+            mem = thread->get_global_memory();
+            addr = generic_to_global(addr);
+            break;
+          case local_space:
+            mem = thread->m_local_mem;
+            addr = generic_to_local(smid, hwtid, addr);
+            break;
+          case shared_space:
+            mem = thread->m_shared_mem;
+            addr = generic_to_shared(smid, addr);
+            break;
+          default:
+            abort();
+        }
       } else {
-         abort();
+        abort();
       }
       break;
-   case param_space_unclassified:
-   case undefined_space:
-   default:
+    case param_space_unclassified:
+    case undefined_space:
+    default:
       abort();
-   }
+  }
 }
 
-void ld_exec( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   const operand_info &dst = pI->dst();
-   const operand_info &src1 = pI->src1();
+void ld_exec(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+
+  unsigned type = pI->get_type();
 
-   unsigned type = pI->get_type();
+  ptx_reg_t src1_data = thread->get_operand_value(src1, dst, type, thread, 1);
+  ptx_reg_t data;
+  memory_space_t space = pI->get_space();
+  unsigned vector_spec = pI->get_vector();
 
-   ptx_reg_t src1_data = thread->get_operand_value(src1, dst, type, thread, 1);
-   ptx_reg_t data;
-   memory_space_t space = pI->get_space();
-   unsigned vector_spec = pI->get_vector();
+  memory_space *mem = NULL;
+  addr_t addr = src1_data.u32;
 
-   memory_space *mem = NULL;
-   addr_t addr = src1_data.u32;
+  decode_space(space, thread, src1, mem, addr);
 
-   decode_space(space,thread,src1,mem,addr);
+  thread->get_gpu()->gem5CudaGPU->getCudaCore(thread->get_hw_sid())->record_ld(space);
 
-   thread->get_gpu()->gem5CudaGPU->getCudaCore(thread->get_hw_sid())->record_ld(space);
-   if (space.get_type() != global_space &&
+  if (space.get_type() != global_space &&
        space.get_type() != const_space &&
        space.get_type() != local_space) {
    size_t size;
@@ -3067,8 +3398,7 @@ void ld_exec( const ptx_instruction *pI, ptx_thread_info *thread )
        // completes, it will write the operands a second time with the correct data
    if (!vector_spec) {
       mem->read(addr,size/8,&data.s64);
-      if( type == S16_TYPE || type == S32_TYPE ) 
-         sign_extend(data,size,dst);
+      if( type == S16_TYPE || type == S32_TYPE ) sign_extend(data,size,dst);
       thread->set_operand_value(dst,data, type, thread, pI);
    } else {
       ptx_reg_t data1, data2, data3, data4;
@@ -3084,2639 +3414,3209 @@ void ld_exec( const ptx_instruction *pI, ptx_thread_info *thread )
       } else //v2
          thread->set_vector_operand_values(dst,data1,data2,data2,data2);
    }
-   }
-   thread->m_last_effective_address = addr;
-   thread->m_last_memory_space = space; 
+  }
+  thread->m_last_effective_address = addr;
+  thread->m_last_memory_space = space;
 }
 
-void ld_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ld_exec(pI,thread);
-}
-void ldu_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ld_exec(pI,thread);
+void ld_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ld_exec(pI, thread);
+}
+void ldu_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ld_exec(pI, thread);
+}
+
+void mma_st_impl(const ptx_instruction *pI, core_t *core, warp_inst_t &inst) {
+  size_t size;
+  unsigned smid;
+  int t;
+  int thrd, k;
+  ptx_thread_info *thread;
+
+  const operand_info &src = pI->operand_lookup(1);
+  const operand_info &src1 = pI->operand_lookup(0);
+  const operand_info &src2 = pI->operand_lookup(2);
+  int tid;
+  unsigned type = pI->get_type();
+  unsigned wmma_type = pI->get_wmma_type();
+  unsigned wmma_layout = pI->get_wmma_layout(0);
+  int stride;
+
+  if(core->get_gpu()->is_functional_sim())
+    tid = inst.warp_id_func() * core->get_warp_size();
+  else
+    tid = inst.warp_id() * core->get_warp_size();
+
+  _memory_op_t insn_memory_op =
+      pI->has_memory_read() ? memory_load : memory_store;
+  for (thrd = 0; thrd < core->get_warp_size(); thrd++) {
+    thread = core->get_thread_info()[tid + thrd];
+    ptx_reg_t addr_reg = thread->get_operand_value(src1, src, type, thread, 1);
+    ptx_reg_t src2_data = thread->get_operand_value(src2, src, type, thread, 1);
+    const operand_info &src_a = pI->operand_lookup(1);
+    unsigned nelem = src_a.get_vect_nelem();
+    ptx_reg_t *v = new ptx_reg_t[8];
+    thread->get_vector_operand_values(src_a, v, nelem);
+    stride = src2_data.u32;
+
+   	memory_space_t space = pI->get_space();
+
+    memory_space *mem = NULL;
+    addr_t addr = addr_reg.u32;
+
+    new_addr_type mem_txn_addr[MAX_ACCESSES_PER_INSN_PER_THREAD];
+    int num_mem_txn = 0;
+
+    smid = thread->get_hw_sid();
+    if (whichspace(addr) == shared_space) {
+      addr = generic_to_shared(smid, addr);
+      space = shared_space;
+    }
+    decode_space(space, thread, src1, mem, addr);
+
+    type_info_key::type_decode(type, size, t);
+    if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+      printf("mma_st: thrd=%d, addr=%x, fp(size=%zu), stride=%d\n", thrd,
+             addr_reg.u32, size, src2_data.u32);
+    addr_t new_addr =
+        addr + thread_group_offset(thrd, wmma_type, wmma_layout, type, stride) *
+                   size / 8;
+    addr_t push_addr;
+
+    ptx_reg_t nw_v[8];
+    for (k = 0; k < 8; k++) {
+      if (k % 2 == 0)
+        nw_v[k].s64 = (v[k / 2].s64 & 0xffff);
+      else
+        nw_v[k].s64 = ((v[k / 2].s64 & 0xffff0000) >> 16);
+    }
+
+    for (k = 0; k < 8; k++) {
+      if (type == F32_TYPE) {
+        // mem->write(new_addr+4*acc_float_offset(k,wmma_layout,stride),size/8,&v[k].s64,thread,pI);
+        push_addr = new_addr + 4 * acc_float_offset(k, wmma_layout, stride);
+        mem->write(push_addr, size / 8, &v[k].s64, thread, pI);
+        mem_txn_addr[num_mem_txn++] = push_addr;
+        if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+          printf(
+              "wmma:store:thread%d=%llx,%llx,%llx,%llx,%llx,%llx,%llx,%llx\n",
+              thrd, v[0].s64, v[1].s64, v[2].s64, v[3].s64, v[4].s64, v[5].s64,
+              v[6].s64, v[7].s64);
+          float temp;
+          int l;
+          printf("thread=%d:", thrd);
+          for (l = 0; l < 8; l++) {
+            temp = v[l].f32;
+            printf("%.2f", temp);
+          }
+          printf("\n");
+        }
+      } else if (type == F16_TYPE) {
+        if (wmma_layout == ROW) {
+          // mem->write(new_addr+k*2,size/8,&nw_v[k].s64,thread,pI);
+          push_addr = new_addr + k * 2;
+          mem->write(push_addr, size / 8, &nw_v[k].s64, thread, pI);
+          if (k % 2 == 0) mem_txn_addr[num_mem_txn++] = push_addr;
+        } else if (wmma_layout == COL) {
+          // mem->write(new_addr+k*2*stride,size/8,&nw_v[k].s64,thread,pI);
+          push_addr = new_addr + k * 2 * stride;
+          mem->write(push_addr, size / 8, &nw_v[k].s64, thread, pI);
+          mem_txn_addr[num_mem_txn++] = push_addr;
+        }
+	
+        if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+          printf(
+              "wmma:store:thread%d=%llx,%llx,%llx,%llx,%llx,%llx,%llx,%llx\n",
+              thrd, nw_v[0].s64, nw_v[1].s64, nw_v[2].s64, nw_v[3].s64,
+              nw_v[4].s64, nw_v[5].s64, nw_v[6].s64, nw_v[7].s64);
+      }
+    }
+
+    delete[] v;
+    inst.space = space;
+    inst.set_addr(thrd, (new_addr_type *)mem_txn_addr, num_mem_txn);
+
+    if ((type == F16_TYPE) &&
+        (wmma_layout == COL))  // check the profiling xls for details
+      inst.data_size = 2;      // 2 byte transaction
+    else
+      inst.data_size = 4;  // 4 byte transaction
+
+    assert(inst.memory_op == insn_memory_op);
+    // thread->m_last_effective_address = addr;
+    // thread->m_last_memory_space = space;
+  }
+}
+
+void mma_ld_impl(const ptx_instruction *pI, core_t *core, warp_inst_t &inst) {
+  size_t size;
+  int t, i;
+  unsigned smid;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+
+  unsigned type = pI->get_type();
+  unsigned wmma_type = pI->get_wmma_type();
+  unsigned wmma_layout = pI->get_wmma_layout(0);
+  int tid;
+  int thrd, stride;
+  ptx_thread_info *thread;
+
+  if (core->get_gpu()->is_functional_sim())
+    tid = inst.warp_id_func() * core->get_warp_size();
+  else
+    tid = inst.warp_id() * core->get_warp_size();
+
+  _memory_op_t insn_memory_op =
+      pI->has_memory_read() ? memory_load : memory_store;
+
+  for (thrd = 0; thrd < core->get_warp_size(); thrd++) {
+    thread = core->get_thread_info()[tid + thrd];
+    ptx_reg_t src1_data =
+        thread->get_operand_value(src1, dst, U32_TYPE, thread, 1);
+    ptx_reg_t src2_data =
+        thread->get_operand_value(src2, dst, U32_TYPE, thread, 1);
+    stride = src2_data.u32;
+    memory_space_t space = pI->get_space();
+
+    memory_space *mem = NULL;
+    addr_t addr = src1_data.u32;
+    smid = thread->get_hw_sid();
+    if (whichspace(addr) == shared_space) {
+      addr = generic_to_shared(smid, addr);
+      space = shared_space;
+    }
+
+    decode_space(space, thread, src1, mem, addr);
+    type_info_key::type_decode(type, size, t);
+
+    ptx_reg_t data[16];
+    if (core->get_gpu()->gpgpu_ctx->debug_tensorcore)
+      printf("mma_ld: thrd=%d,addr=%x, fpsize=%zu, stride=%d\n", thrd,
+             src1_data.u32, size, src2_data.u32);
+
+    addr_t new_addr =
+        addr + thread_group_offset(thrd, wmma_type, wmma_layout, type, stride) *
+                   size / 8;
+    addr_t fetch_addr;
+    new_addr_type mem_txn_addr[MAX_ACCESSES_PER_INSN_PER_THREAD];
+    int num_mem_txn = 0;
+
+    if (wmma_type == LOAD_A) {
+      for (i = 0; i < 16; i++) {
+        if (wmma_layout == ROW) {
+          // mem->read(new_addr+2*i,size/8,&data[i].s64);
+          fetch_addr = new_addr + 2 * i;
+          mem->read(fetch_addr, size / 8, &data[i].s64);
+        } else if (wmma_layout == COL) {
+          // mem->read(new_addr+2*(i%4)+2*stride*4*(i/4),size/8,&data[i].s64);
+          fetch_addr = new_addr + 2 * (i % 4) + 2 * stride * 4 * (i / 4);
+          mem->read(fetch_addr, size / 8, &data[i].s64);
+        } else {
+          printf("mma_ld:wrong_layout_type\n");
+          abort();
+        }
+        if (i % 2 == 0) mem_txn_addr[num_mem_txn++] = fetch_addr;
+      }
+    } else if (wmma_type == LOAD_B) {
+      for (i = 0; i < 16; i++) {
+        if (wmma_layout == COL) {
+          // mem->read(new_addr+2*i,size/8,&data[i].s64);
+          fetch_addr = new_addr + 2 * i;
+          mem->read(fetch_addr, size / 8, &data[i].s64);
+        } else if (wmma_layout == ROW) {
+          // mem->read(new_addr+2*(i%4)+2*stride*4*(i/4),size/8,&data[i].s64);
+          fetch_addr = new_addr + 2 * (i % 4) + 2 * stride * 4 * (i / 4);
+          mem->read(fetch_addr, size / 8, &data[i].s64);
+        } else {
+          printf("mma_ld:wrong_layout_type\n");
+          abort();
+        }
+        if (i % 2 == 0) mem_txn_addr[num_mem_txn++] = fetch_addr;
+      }
+    } else if (wmma_type == LOAD_C) {
+      for (i = 0; i < 8; i++) {
+        if (type == F16_TYPE) {
+          if (wmma_layout == ROW) {
+            // mem->read(new_addr+2*i,size/8,&data[i].s64);
+            fetch_addr = new_addr + 2 * i;
+            mem->read(fetch_addr, size / 8, &data[i].s64);
+            if (i % 2 == 0) mem_txn_addr[num_mem_txn++] = fetch_addr;
+          } else if (wmma_layout == COL) {
+            // mem->read(new_addr+2*stride*i,size/8,&data[i].s64);
+            fetch_addr = new_addr + 2 * stride * i;
+            mem->read(fetch_addr, size / 8, &data[i].s64);
+            mem_txn_addr[num_mem_txn++] = fetch_addr;
+          } else {
+            printf("mma_ld:wrong_type\n");
+            abort();
+          }
+        } else if (type == F32_TYPE) {
+          // mem->read(new_addr+4*acc_float_offset(i,wmma_layout,stride),size/8,&data[i].s64);
+          fetch_addr = new_addr + 4 * acc_float_offset(i, wmma_layout, stride);
+          mem->read(fetch_addr, size / 8, &data[i].s64);
+          mem_txn_addr[num_mem_txn++] = fetch_addr;
+        } else {
+          printf("wrong type");
+          abort();
+        }
+      }
+    } else {
+      printf("wrong wmma type\n");
+      ;
+      abort();
+    }
+    // generate timing memory request
+    inst.space = space;
+    inst.set_addr(thrd, (new_addr_type *)mem_txn_addr, num_mem_txn);
+
+    if ((wmma_type == LOAD_C) && (type == F16_TYPE) &&
+        (wmma_layout == COL))  // memory address is scattered, check the
+                               // profiling xls for more detail.
+      inst.data_size = 2;      // 2 byte transaction
+    else
+      inst.data_size = 4;  // 4 byte transaction
+    assert(inst.memory_op == insn_memory_op);
+
+    if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+      if (type == F16_TYPE) {
+        printf("\nmma_ld:thread%d= ", thrd);
+        for (i = 0; i < 16; i++) {
+          printf("%llx ", data[i].u64);
+        }
+        printf("\n");
+
+        printf("\nmma_ld:thread%d= ", thrd);
+        float temp;
+        for (i = 0; i < 16; i++) {
+          temp = data[i].f16;
+          printf("%.2f ", temp);
+        }
+        printf("\n");
+      } else {
+        printf("\nmma_ld:thread%d= ", thrd);
+        for (i = 0; i < 8; i++) {
+          printf("%.2f ", data[i].f32);
+        }
+        printf("\n");
+        printf("\nmma_ld:thread%d= ", thrd);
+        for (i = 0; i < 8; i++) {
+          printf("%llx ", data[i].u64);
+        }
+        printf("\n");
+      }
+    }
+
+    if ((wmma_type == LOAD_C) && (type == F32_TYPE)) {
+      thread->set_wmma_vector_operand_values(dst, data[0], data[1], data[2],
+                                             data[3], data[4], data[5], data[6],
+                                             data[7]);
+    } else {
+      ptx_reg_t nw_data[8];
+      int num_reg;
+
+      if (wmma_type == LOAD_C)
+        num_reg = 4;
+      else
+        num_reg = 8;
+
+      for (i = 0; i < num_reg; i++) {
+        nw_data[i].s64 = ((data[2 * i].s64 & 0xffff) << 16) |
+                         ((data[2 * i + 1].s64 & 0xffff));
+      }
+
+      if (wmma_type == LOAD_C)
+        thread->set_vector_operand_values(dst, nw_data[0], nw_data[1],
+                                          nw_data[2], nw_data[3]);
+      else
+        thread->set_wmma_vector_operand_values(
+            dst, nw_data[0], nw_data[1], nw_data[2], nw_data[3], nw_data[4],
+            nw_data[5], nw_data[6], nw_data[7]);
+      if (core->get_gpu()->gpgpu_ctx->debug_tensorcore) {
+        printf(
+            "mma_ld:data[0].s64=%llx,data[1].s64=%llx,new_data[0].s64=%llx\n",
+            data[0].u64, data[1].u64, nw_data[0].u64);
+        printf(
+            "mma_ld:data[2].s64=%llx,data[3].s64=%llx,new_data[1].s64=%llx\n",
+            data[2].u64, data[3].u64, nw_data[1].u64);
+        printf(
+            "mma_ld:data[4].s64=%llx,data[5].s64=%llx,new_data[2].s64=%llx\n",
+            data[4].u64, data[5].u64, nw_data[2].u64);
+        printf(
+            "mma_ld:data[6].s64=%llx,data[7].s64=%llx,new_data[3].s64=%llx\n",
+            data[6].u64, data[7].u64, nw_data[3].u64);
+        if (wmma_type != LOAD_C) {
+          printf(
+              "mma_ld:data[8].s64=%llx,data[9].s64=%llx,new_data[4].s64=%llx\n",
+              data[8].u64, data[9].u64, nw_data[4].s64);
+          printf(
+              "mma_ld:data[10].s64=%llx,data[11].s64=%llx,new_data[5].s64=%"
+              "llx\n",
+              data[10].u64, data[11].u64, nw_data[5].u64);
+          printf(
+              "mma_ld:data[12].s64=%llx,data[13].s64=%llx,new_data[6].s64=%"
+              "llx\n",
+              data[12].u64, data[13].u64, nw_data[6].u64);
+          printf(
+              "mma_ld:data[14].s64=%llx,data[15].s64=%llx,new_data[7].s64=%"
+              "llx\n",
+              data[14].u64, data[15].u64, nw_data[3].u64);
+        }
+      }
+    }
+
+    // thread->m_last_effective_address = addr;
+    // thread->m_last_memory_space = space;
+  }
 }
 
-void mma_st_impl( const ptx_instruction *pI, core_t *core, warp_inst_t &inst )
-{
-   size_t size;
-   unsigned smid;
-   int t;
-   int thrd,odd,inx,k;
-   ptx_thread_info *thread;
-   
-   const operand_info &src  = pI->operand_lookup(1);
-   const operand_info &src1 = pI->operand_lookup(0);
-   const operand_info &src2 = pI->operand_lookup(2);
-   int tid ;
-   unsigned type = pI->get_type();
-   unsigned wmma_type = pI->get_wmma_type();
-   unsigned wmma_layout = pI->get_wmma_layout(0);
-   int stride; 
-
-   if(core->get_gpu()->is_functional_sim())
-    	tid= inst.warp_id_func()*core->get_warp_size();
-   else
-    	tid= inst.warp_id()*core->get_warp_size();
-
-   _memory_op_t insn_memory_op = pI->has_memory_read() ? memory_load : memory_store;
-   for (thrd=0; thrd < core->get_warp_size(); thrd++) {
-	thread = core->get_thread_info()[tid+thrd];
-	odd=thrd%2;
-	inx=thrd/2;
-    	ptx_reg_t addr_reg = thread->get_operand_value(src1, src, type, thread, 1);
-   	ptx_reg_t src2_data = thread->get_operand_value(src2, src, type, thread, 1);
-	const operand_info &src_a=  pI->operand_lookup(1);
-        unsigned nelem = src_a.get_vect_nelem();
-        ptx_reg_t* v= new ptx_reg_t[8]; 
-       	thread->get_vector_operand_values( src_a, v, nelem );
-  	stride=src2_data.u32;
- 
-   	memory_space_t space = pI->get_space();
+void lg2_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
-   	memory_space *mem = NULL;
-   	addr_t addr = addr_reg.u32;
-	
-	new_addr_type mem_txn_addr[MAX_ACCESSES_PER_INSN_PER_THREAD];
-	int num_mem_txn=0;
-        
-        smid = thread->get_hw_sid();
-   	if( whichspace(addr) == shared_space ) {
-          addr= generic_to_shared(smid,addr);
-          space = shared_space;
-	}
-   	decode_space(space,thread,src1,mem,addr);
-
-   	type_info_key::type_decode(type,size,t);
-   	if(debug_tensorcore)
-		printf("mma_st: thrd=%d,addr=%x, fp(size=%d), stride=%d\n",thrd,addr_reg.u32,size,src2_data.u32);
-	addr_t new_addr = addr+thread_group_offset(thrd,wmma_type,wmma_layout,type,stride)*size/8;  
-	addr_t push_addr;
-
-	ptx_reg_t nw_v[8];
-	for(k=0;k<8;k++){
-		if(k%2==0)
-			nw_v[k].s64=(v[k/2].s64&0xffff);
-		else
-			nw_v[k].s64=((v[k/2].s64&0xffff0000)>>16);
-	}
-
-	for(k=0;k<8;k++){
-		if(type==F32_TYPE){
-       			//mem->write(new_addr+4*acc_float_offset(k,wmma_layout,stride),size/8,&v[k].s64,thread,pI);
-       			push_addr=new_addr+4*acc_float_offset(k,wmma_layout,stride);
-       			mem->write(push_addr,size/8,&v[k].s64,thread,pI);
-			mem_txn_addr[num_mem_txn++]=push_addr;
-	
-			if(debug_tensorcore){
-				printf("wmma:store:thread%d=%x,%x,%x,%x,%x,%x,%x,%x\n",thrd,v[0].s64,v[1].s64,v[2].s64,v[3].s64,v[4].s64,v[5].s64,v[6].s64,v[7].s64);   
-				float temp;
-				int l;
-				printf("thread=%d:",thrd);
-				for(l=0;l<8;l++){
-					temp=v[l].f32;
-					printf("%.2f",temp);	
-				}
-				printf("\n");
-			}
-		}
-		else if(type==F16_TYPE){
-			if(wmma_layout==ROW){
-       				//mem->write(new_addr+k*2,size/8,&nw_v[k].s64,thread,pI);
-       				push_addr=new_addr+k*2;
-       				mem->write(push_addr,size/8,&nw_v[k].s64,thread,pI);
-				if(k%2==0)
-					mem_txn_addr[num_mem_txn++]=push_addr;
-			}
-			else if(wmma_layout==COL){
-       				//mem->write(new_addr+k*2*stride,size/8,&nw_v[k].s64,thread,pI);
-       				push_addr=new_addr+k*2*stride;
-       				mem->write(push_addr,size/8,&nw_v[k].s64,thread,pI);
-				mem_txn_addr[num_mem_txn++]=push_addr;
-			}
-	
-			if(debug_tensorcore)
-				printf("wmma:store:thread%d=%x,%x,%x,%x,%x,%x,%x,%x\n",thrd,nw_v[0].s64,nw_v[1].s64,nw_v[2].s64,nw_v[3].s64,nw_v[4].s64,nw_v[5].s64,nw_v[6].s64,nw_v[7].s64);   
-		}
-	}
-   	
-	delete [] v;
-   	inst.space = space;
-   	inst.set_addr(thrd, (new_addr_type *)mem_txn_addr , num_mem_txn);
-
-	if((type==F16_TYPE)&&(wmma_layout==COL))//check the profiling xls for details
-   		inst.data_size = 2; // 2 byte transaction 
-   	else
-		inst.data_size = 4; // 4 byte transaction 
-
-   	assert( inst.memory_op == insn_memory_op );
-   	//thread->m_last_effective_address = addr;
-   	//thread->m_last_memory_space = space;
-   } 
-}
-
-void mma_ld_impl( const ptx_instruction *pI, core_t *core, warp_inst_t &inst )
-{
-   size_t size;
-   int t,i;
-   unsigned smid;
-   const operand_info &dst = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-
-   unsigned type = pI->get_type();
-   unsigned wmma_type = pI->get_wmma_type();
-   unsigned wmma_layout = pI->get_wmma_layout(0);
-   int tid;
-   int thrd,stride;
-   ptx_thread_info *thread;
- 
-
-   if(core->get_gpu()->is_functional_sim())
-    	tid= inst.warp_id_func()*core->get_warp_size();
-   else
-    	tid= inst.warp_id()*core->get_warp_size();
-
-   _memory_op_t insn_memory_op = pI->has_memory_read() ? memory_load : memory_store;
-   
-   for (thrd=0; thrd < core->get_warp_size(); thrd++){
-   	thread = core->get_thread_info()[tid+thrd];
-   	ptx_reg_t src1_data = thread->get_operand_value(src1, dst, U32_TYPE, thread, 1);
-   	ptx_reg_t src2_data = thread->get_operand_value(src2, dst, U32_TYPE, thread, 1);
-	stride=src2_data.u32;
-   	memory_space_t space = pI->get_space();
+  unsigned i_type = pI->get_type();
 
-   	memory_space *mem = NULL;
-   	addr_t addr = src1_data.u32;
-        smid = thread->get_hw_sid();
-        if( whichspace(addr) == shared_space ) {
-          addr= generic_to_shared(smid,addr);
-          space = shared_space;
-	}
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-	decode_space(space,thread,src1,mem,addr);
-   	type_info_key::type_decode(type,size,t);
-	
-	ptx_reg_t data[16];
-   	if(debug_tensorcore)	
-		printf("mma_ld: thrd=%d,addr=%x, fpsize=%d, stride=%d\n",thrd,src1_data.u32,size,src2_data.u32);
-	
-	addr_t new_addr = addr+thread_group_offset(thrd,wmma_type,wmma_layout,type,stride)*size/8;  
-	addr_t fetch_addr;
-	new_addr_type mem_txn_addr[MAX_ACCESSES_PER_INSN_PER_THREAD];
-	int num_mem_txn=0;
-
-	if(wmma_type==LOAD_A){
-		for(i=0;i<16;i++){
-			if(wmma_layout==ROW){
-				//mem->read(new_addr+2*i,size/8,&data[i].s64);
-				fetch_addr=new_addr+2*i;
-				mem->read(fetch_addr,size/8,&data[i].s64);
-			}
-			else if(wmma_layout==COL){
-				//mem->read(new_addr+2*(i%4)+2*stride*4*(i/4),size/8,&data[i].s64);
-				fetch_addr=new_addr+2*(i%4)+2*stride*4*(i/4);
-				mem->read(fetch_addr,size/8,&data[i].s64);
-			}
-			else{
-				printf("mma_ld:wrong_layout_type\n");
-				abort();
-			
-			}
-			if(i%2==0)
-				mem_txn_addr[num_mem_txn++]=fetch_addr;	
-		}
-	}
-	else if(wmma_type==LOAD_B){
-		for(i=0;i<16;i++){
-			if(wmma_layout==COL){
-				//mem->read(new_addr+2*i,size/8,&data[i].s64);
-				fetch_addr=new_addr+2*i;
-				mem->read(fetch_addr,size/8,&data[i].s64);
-			}
-			else if(wmma_layout==ROW){
-				//mem->read(new_addr+2*(i%4)+2*stride*4*(i/4),size/8,&data[i].s64);
-				fetch_addr=new_addr+2*(i%4)+2*stride*4*(i/4);
-				mem->read(fetch_addr,size/8,&data[i].s64);
-			}
-			else{
-				printf("mma_ld:wrong_layout_type\n");
-				abort();
-			}
-			if(i%2==0)
-				mem_txn_addr[num_mem_txn++]=fetch_addr;	
-		}
-	}
-	else if(wmma_type==LOAD_C){
-		for(i=0;i<8;i++){
-			if(type==F16_TYPE){
-				if(wmma_layout==ROW){
-					//mem->read(new_addr+2*i,size/8,&data[i].s64);
-					fetch_addr=new_addr+2*i;
-					mem->read(fetch_addr,size/8,&data[i].s64);
-					if(i%2==0)
-						mem_txn_addr[num_mem_txn++]=fetch_addr;	
-				}
-				else if(wmma_layout==COL){
-					//mem->read(new_addr+2*stride*i,size/8,&data[i].s64);
-					fetch_addr=new_addr+2*stride*i;
-					mem->read(fetch_addr,size/8,&data[i].s64);
-					mem_txn_addr[num_mem_txn++]=fetch_addr;	
-				}
-				else{
-					printf("mma_ld:wrong_type\n");
-					abort();
-				}
-			}
-			else if(type==F32_TYPE){
-				//mem->read(new_addr+4*acc_float_offset(i,wmma_layout,stride),size/8,&data[i].s64);
-				fetch_addr=new_addr+4*acc_float_offset(i,wmma_layout,stride);
-				mem->read(fetch_addr,size/8,&data[i].s64);
-				mem_txn_addr[num_mem_txn++]=fetch_addr;	
-			}
-			else{
-				printf("wrong type");
-				abort();
-			}
-		}
-	}
-	else{
-		printf("wrong wmma type\n");;
-		abort();
-	}
-	//generate timing memory request
-   	inst.space = space;
-   	inst.set_addr(thrd, (new_addr_type *)mem_txn_addr , num_mem_txn);
-
-	if((wmma_type==LOAD_C)&&(type==F16_TYPE)&&(wmma_layout==COL))//memory address is scattered, check the profiling xls for more detail.
-   		inst.data_size = 2; // 2 byte transaction 
-	else	
-   		inst.data_size = 4; // 4 byte transaction 
-   	assert( inst.memory_op == insn_memory_op );
-
-	if(debug_tensorcore){
-		if(type==F16_TYPE){
-			printf("\nmma_ld:thread%d= ",thrd);
-			for(i=0;i<16;i++){
-				printf("%x ",data[i].u64);
-			}
-			printf("\n");
-			
-			printf("\nmma_ld:thread%d= ",thrd);
-			float temp;
-			for(i=0;i<16;i++){
-			temp=data[i].f16;
-				printf("%.2f ",temp);
-			}
-			printf("\n");
-		}
-		else{
-			printf("\nmma_ld:thread%d= ",thrd);
-			for(i=0;i<8;i++){
-				printf("%.2f ",data[i].f32);
-			}
-			printf("\n");
-			printf("\nmma_ld:thread%d= ",thrd);
-			for(i=0;i<8;i++){
-				printf("%x ",data[i].u64);
-			}
-			printf("\n");
-		}
-	}
-
-	if((wmma_type==LOAD_C)&&(type==F32_TYPE)){
-   		thread->set_wmma_vector_operand_values(dst,data[0],data[1],data[2],data[3],data[4],data[5],data[6],data[7]);
-	}
-	else{
-		ptx_reg_t nw_data[8];
-		int num_reg;
-		
-		if(wmma_type==LOAD_C)
-			num_reg=4;
-		else
-			num_reg=8;
-
-		for(i=0;i<num_reg;i++){
-			nw_data[i].s64= ((data[2*i].s64 & 0xffff)<<16)| ((data[2*i+1].s64 & 0xffff));
-		}
-
-		if(wmma_type==LOAD_C)
-   			thread->set_vector_operand_values(dst,nw_data[0],nw_data[1],nw_data[2],nw_data[3]);
-		else
-   			thread->set_wmma_vector_operand_values(dst,nw_data[0],nw_data[1],nw_data[2],nw_data[3],nw_data[4],nw_data[5],nw_data[6],nw_data[7]);
-		if(debug_tensorcore){	
-			printf("mma_ld:data[0].s64=%x,data[1].s64=%x,new_data[0].s64=%x\n",data[0].u64,data[1].u64,nw_data[0].u64);	
-			printf("mma_ld:data[2].s64=%x,data[3].s64=%x,new_data[1].s64=%x\n",data[2].u64,data[3].u64,nw_data[1].u64);	
-			printf("mma_ld:data[4].s64=%x,data[5].s64=%x,new_data[2].s64=%x\n",data[4].u64,data[5].u64,nw_data[2].u64);	
-			printf("mma_ld:data[6].s64=%x,data[7].s64=%x,new_data[3].s64=%x\n",data[6].u64,data[7].u64,nw_data[3].u64);	
-			if(wmma_type!=LOAD_C){
-			printf("mma_ld:data[8].s64=%x,data[9].s64=%x,new_data[4].s64=%x\n",data[8].u64,data[9].u64,nw_data[4].s64);	
-			printf("mma_ld:data[10].s64=%x,data[11].s64=%x,new_data[5].s64=%x\n",data[10].u64,data[11].u64,nw_data[5].u64);	
-			printf("mma_ld:data[12].s64=%x,data[13].s64=%x,new_data[6].s64=%x\n",data[12].u64,data[13].u64,nw_data[6].u64);	
-			printf("mma_ld:data[14].s64=%x,data[15].s64=%x,new_data[7].s64=%x\n",data[14].u64,data[15].u64,nw_data[3].u64);	
-			}
-		}
-	}
-
-   	//thread->m_last_effective_address = addr;
-   	//thread->m_last_memory_space = space;
-   } 
-}
-
-void lg2_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-
-   unsigned i_type = pI->get_type();
-
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-
-
-   switch ( i_type ) {
-   case F32_TYPE: 
-      d.f32 = log(a.f32)/log(2);
-      break;
-   default:
+  switch (i_type) {
+    case F32_TYPE:
+      d.f32 = log(a.f32) / log(2);
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void mad24_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
-   ptx_reg_t d, t;
+void mad24_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+  ptx_reg_t d, t;
 
-   unsigned i_type = pI->get_type();
-   ptx_reg_t a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   ptx_reg_t c = thread->get_operand_value(src3, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  ptx_reg_t a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  ptx_reg_t c = thread->get_operand_value(src3, dst, i_type, thread, 1);
 
-   unsigned sat_mode = pI->saturation_mode();
+  unsigned sat_mode = pI->saturation_mode();
 
-   assert( !pI->is_wide() );
+  assert(!pI->is_wide());
 
-   switch ( i_type ) {
-   case S32_TYPE: 
+  switch (i_type) {
+    case S32_TYPE:
       t.s64 = a.s32 * b.s32;
-      if ( pI->is_hi() ) {
-         d.s64 = (t.s64>>16) + c.s32;
-         if ( sat_mode ) {
-            if ( d.s64 > (int)0x7FFFFFFF )
-               d.s64 = (int)0x7FFFFFFF;
-            else if ( d.s64 < (int)0x80000000 )
-               d.s64 = (int)0x80000000;
-         }
-      } else if ( pI->is_lo() ) d.s64 = t.s32 + c.s32;
-      else assert(0);
-      break;
-   case U32_TYPE: 
+      if (pI->is_hi()) {
+        d.s64 = (t.s64 >> 16) + c.s32;
+        if (sat_mode) {
+          if (d.s64 > (int)0x7FFFFFFF)
+            d.s64 = (int)0x7FFFFFFF;
+          else if (d.s64 < (int)0x80000000)
+            d.s64 = (int)0x80000000;
+        }
+      } else if (pI->is_lo())
+        d.s64 = t.s32 + c.s32;
+      else
+        assert(0);
+      break;
+    case U32_TYPE:
       t.u64 = a.u32 * b.u32;
-      if ( pI->is_hi() ) d.u64 = (t.u64>>16) + c.u32;
-      else if ( pI->is_lo() ) d.u64 = t.u32 + c.u32;
-      else assert(0);
+      if (pI->is_hi())
+        d.u64 = (t.u64 >> 16) + c.u32;
+      else if (pI->is_lo())
+        d.u64 = t.u32 + c.u32;
+      else
+        assert(0);
       break;
-   default: 
+    default:
       assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst, d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void mad_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   mad_def(pI, thread, false);
+void mad_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  mad_def(pI, thread, false);
 }
 
-void madp_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   mad_def(pI, thread, true);
+void madp_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  mad_def(pI, thread, true);
 }
 
-void madc_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   mad_def(pI, thread, true);
+void madc_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  mad_def(pI, thread, true);
 }
 
-void mad_def( const ptx_instruction *pI, ptx_thread_info *thread, bool use_carry ) 
-{ 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
-   ptx_reg_t d, t;
-
-   int carry=0;
-   int overflow=0;
-
-   unsigned i_type = pI->get_type();
-   ptx_reg_t a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   ptx_reg_t c = thread->get_operand_value(src3, dst, i_type, thread, 1);
-
-   // take the carry bit, it should be the 4th operand 
-   ptx_reg_t carry_bit; 
-   carry_bit.u64 = 0;
-   if (use_carry) {
-      const operand_info &carry = pI->operand_lookup(4);
-      carry_bit = thread->get_operand_value(carry, dst, PRED_TYPE, thread, 0);
-      carry_bit.pred &= 0x4;
-      carry_bit.pred >>=2;
-   }
+void mad_def(const ptx_instruction *pI, ptx_thread_info *thread,
+             bool use_carry) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+  ptx_reg_t d, t;
+
+  int carry = 0;
+  int overflow = 0;
+
+  unsigned i_type = pI->get_type();
+  ptx_reg_t a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  ptx_reg_t c = thread->get_operand_value(src3, dst, i_type, thread, 1);
+
+  // take the carry bit, it should be the 4th operand
+  ptx_reg_t carry_bit;
+  carry_bit.u64 = 0;
+  if (use_carry) {
+    const operand_info &carry = pI->operand_lookup(4);
+    carry_bit = thread->get_operand_value(carry, dst, PRED_TYPE, thread, 0);
+    carry_bit.pred &= 0x4;
+    carry_bit.pred >>= 2;
+  }
 
-   unsigned rounding_mode = pI->rounding_mode();
+  unsigned rounding_mode = pI->rounding_mode();
 
-   switch ( i_type ) {
-   case S16_TYPE: 
+  switch (i_type) {
+    case S16_TYPE:
       t.s32 = a.s16 * b.s16;
-      if ( pI->is_wide() ) d.s32 = t.s32 + c.s32 + carry_bit.pred;
-      else if ( pI->is_hi() ) d.s16 = (t.s32>>16) + c.s16 + carry_bit.pred;
-      else if ( pI->is_lo() ) d.s16 = t.s16 + c.s16 + carry_bit.pred;
-      else assert(0);
-      carry = ((long long int)(t.s32 + c.s32 + carry_bit.pred)&0x100000000)>>32;
+      if (pI->is_wide())
+        d.s32 = t.s32 + c.s32 + carry_bit.pred;
+      else if (pI->is_hi())
+        d.s16 = (t.s32 >> 16) + c.s16 + carry_bit.pred;
+      else if (pI->is_lo())
+        d.s16 = t.s16 + c.s16 + carry_bit.pred;
+      else
+        assert(0);
+      carry =
+          ((long long int)(t.s32 + c.s32 + carry_bit.pred) & 0x100000000) >> 32;
       break;
-   case S32_TYPE: 
+    case S32_TYPE:
       t.s64 = a.s32 * b.s32;
-      if ( pI->is_wide() ) d.s64 = t.s64 + c.s64 + carry_bit.pred;
-      else if ( pI->is_hi() ) d.s32 = (t.s64>>32) + c.s32 + carry_bit.pred;
-      else if ( pI->is_lo() ) d.s32 = t.s32 + c.s32 + carry_bit.pred;
-      else assert(0);
+      if (pI->is_wide())
+        d.s64 = t.s64 + c.s64 + carry_bit.pred;
+      else if (pI->is_hi())
+        d.s32 = (t.s64 >> 32) + c.s32 + carry_bit.pred;
+      else if (pI->is_lo())
+        d.s32 = t.s32 + c.s32 + carry_bit.pred;
+      else
+        assert(0);
       break;
-   case S64_TYPE: 
+    case S64_TYPE:
       t.s64 = a.s64 * b.s64;
-      assert( !pI->is_wide() );
-      assert( !pI->is_hi() );
-      assert( use_carry == false); 
-      if ( pI->is_lo() ) d.s64 = t.s64 + c.s64 + carry_bit.pred;
-      else assert(0);
+      assert(!pI->is_wide());
+      assert(!pI->is_hi());
+      assert(use_carry == false);
+      if (pI->is_lo())
+        d.s64 = t.s64 + c.s64 + carry_bit.pred;
+      else
+        assert(0);
       break;
-   case U16_TYPE: 
+    case U16_TYPE:
       t.u32 = a.u16 * b.u16;
-      if ( pI->is_wide() ) d.u32 = t.u32 + c.u32 + carry_bit.pred;
-      else if ( pI->is_hi() ) d.u16 = (t.u32 + c.u16 + carry_bit.pred)>>16;
-      else if ( pI->is_lo() ) d.u16 = t.u16 + c.u16 + carry_bit.pred;
-      else assert(0);
-      carry = ((long long int)((long long int)t.u32 + c.u32 + carry_bit.pred)&0x100000000)>>32;
+      if (pI->is_wide())
+        d.u32 = t.u32 + c.u32 + carry_bit.pred;
+      else if (pI->is_hi())
+        d.u16 = (t.u32 + c.u16 + carry_bit.pred) >> 16;
+      else if (pI->is_lo())
+        d.u16 = t.u16 + c.u16 + carry_bit.pred;
+      else
+        assert(0);
+      carry = ((long long int)((long long int)t.u32 + c.u32 + carry_bit.pred) &
+               0x100000000) >>
+              32;
       break;
-   case U32_TYPE: 
+    case U32_TYPE:
       t.u64 = a.u32 * b.u32;
-      if ( pI->is_wide() ) d.u64 = t.u64 + c.u64 + carry_bit.pred;
-      else if ( pI->is_hi() ) d.u32 = (t.u64 + c.u32 + carry_bit.pred)>>32;
-      else if ( pI->is_lo() ) d.u32 = t.u32 + c.u32 + carry_bit.pred;
-      else assert(0);
+      if (pI->is_wide())
+        d.u64 = t.u64 + c.u64 + carry_bit.pred;
+      else if (pI->is_hi())
+        d.u32 = (t.u64 + c.u32 + carry_bit.pred) >> 32;
+      else if (pI->is_lo())
+        d.u32 = t.u32 + c.u32 + carry_bit.pred;
+      else
+        assert(0);
       break;
-   case U64_TYPE: 
+    case U64_TYPE:
       t.u64 = a.u64 * b.u64;
-      assert( !pI->is_wide() );
-      assert( !pI->is_hi() );
-      assert( use_carry == false); 
-      if ( pI->is_lo() ) d.u64 = t.u64 + c.u64 + carry_bit.pred;
-      else assert(0);
-      break;
-   case F16_TYPE:{ 
-     // assert(0); 
-     // break;
-         assert( use_carry == false); 
-         int orig_rm = fegetround();
-         switch ( rounding_mode ) {
-         case RN_OPTION: break;
-         case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-         default: assert(0); break;
-         }
-         d.f16 = a.f16 * b.f16 + c.f16;
-         if ( pI->saturation_mode() ) {
-            if ( d.f16 < 0 ) d.f16 = 0;
-            else if ( d.f16 > 1.0f ) d.f16 = 1.0f;
-         }
-         fesetround( orig_rm );
-         break;
-      }  
-   case F32_TYPE: {
-         assert( use_carry == false); 
-         int orig_rm = fegetround();
-         switch ( rounding_mode ) {
-         case RN_OPTION: break;
-         case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-         default: assert(0); break;
-         }
-         d.f32 = a.f32 * b.f32 + c.f32;
-         if ( pI->saturation_mode() ) {
-            if ( d.f32 < 0 ) d.f32 = 0;
-            else if ( d.f32 > 1.0f ) d.f32 = 1.0f;
-         }
-         fesetround( orig_rm );
-         break;
-      }  
-   case F64_TYPE: case FF64_TYPE: {
-         assert( use_carry == false); 
-         int orig_rm = fegetround();
-         switch ( rounding_mode ) {
-         case RN_OPTION: break;
-         case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-         default: assert(0); break;
-         }
-         d.f64 = a.f64 * b.f64 + c.f64;
-         if ( pI->saturation_mode() ) {
-            if ( d.f64 < 0 ) d.f64 = 0;
-            else if ( d.f64 > 1.0f ) d.f64 = 1.0;
-         }
-         fesetround( orig_rm );
-         break;
+      assert(!pI->is_wide());
+      assert(!pI->is_hi());
+      assert(use_carry == false);
+      if (pI->is_lo())
+        d.u64 = t.u64 + c.u64 + carry_bit.pred;
+      else
+        assert(0);
+      break;
+    case F16_TYPE: {
+      // assert(0);
+      // break;
+      assert(use_carry == false);
+      int orig_rm = fegetround();
+      switch (rounding_mode) {
+        case RN_OPTION:
+          break;
+        case RZ_OPTION:
+          fesetround(FE_TOWARDZERO);
+          break;
+        default:
+          assert(0);
+          break;
+      }
+      d.f16 = a.f16 * b.f16 + c.f16;
+      if (pI->saturation_mode()) {
+        if (d.f16 < 0)
+          d.f16 = 0;
+        else if (d.f16 > 1.0f)
+          d.f16 = 1.0f;
+      }
+      fesetround(orig_rm);
+      break;
+    }
+    case F32_TYPE: {
+      assert(use_carry == false);
+      int orig_rm = fegetround();
+      switch (rounding_mode) {
+        case RN_OPTION:
+          break;
+        case RZ_OPTION:
+          fesetround(FE_TOWARDZERO);
+          break;
+        default:
+          //assert(0);
+          break;
+      }
+      d.f32 = a.f32 * b.f32 + c.f32;
+      if (pI->saturation_mode()) {
+        if (d.f32 < 0)
+          d.f32 = 0;
+        else if (d.f32 > 1.0f)
+          d.f32 = 1.0f;
+      }
+      fesetround(orig_rm);
+      break;
+    }
+    case F64_TYPE:
+    case FF64_TYPE: {
+      assert(use_carry == false);
+      int orig_rm = fegetround();
+      switch (rounding_mode) {
+        case RN_OPTION:
+          break;
+        case RZ_OPTION:
+          fesetround(FE_TOWARDZERO);
+          break;
+        default:
+          assert(0);
+          break;
+      }
+      d.f64 = a.f64 * b.f64 + c.f64;
+      if (pI->saturation_mode()) {
+        if (d.f64 < 0)
+          d.f64 = 0;
+        else if (d.f64 > 1.0f)
+          d.f64 = 1.0;
       }
-   default: 
+      fesetround(orig_rm);
+      break;
+    }
+    default:
       assert(0);
       break;
-   }
-   thread->set_operand_value(dst, d, i_type, thread, pI, overflow, carry);
-}
-
-bool isNaN(float x)
-{
-   return std::isnan(x);
+  }
+  thread->set_operand_value(dst, d, i_type, thread, pI, overflow, carry);
 }
 
-bool isNaN(double x)
-{
-   return std::isnan(x);
-}
+bool isNaN(float x) { return std::isnan(x); }
 
-void max_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, b, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+bool isNaN(double x) { return std::isnan(x); }
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+void max_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case U16_TYPE: d.u16 = MY_MAX_I(a.u16,b.u16); break;
-   case U32_TYPE: d.u32 = MY_MAX_I(a.u32,b.u32); break;
-   case U64_TYPE: d.u64 = MY_MAX_I(a.u64,b.u64); break;
-   case S16_TYPE: d.s16 = MY_MAX_I(a.s16,b.s16); break;
-   case S32_TYPE: d.s32 = MY_MAX_I(a.s32,b.s32); break;
-   case S64_TYPE: d.s64 = MY_MAX_I(a.s64,b.s64); break;
-   case F32_TYPE: d.f32 = MY_MAX_F(a.f32,b.f32); break;
-   case F64_TYPE: case FF64_TYPE: d.f64 = MY_MAX_F(a.f64,b.f64); break;
-   default:
+  switch (i_type) {
+    case U16_TYPE:
+      d.u16 = MY_MAX_I(a.u16, b.u16);
+      break;
+    case U32_TYPE:
+      d.u32 = MY_MAX_I(a.u32, b.u32);
+      break;
+    case U64_TYPE:
+      d.u64 = MY_MAX_I(a.u64, b.u64);
+      break;
+    case S16_TYPE:
+      d.s16 = MY_MAX_I(a.s16, b.s16);
+      break;
+    case S32_TYPE:
+      d.s32 = MY_MAX_I(a.s32, b.s32);
+      break;
+    case S64_TYPE:
+      d.s64 = MY_MAX_I(a.s64, b.s64);
+      break;
+    case F32_TYPE:
+      d.f32 = MY_MAX_F(a.f32, b.f32);
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      d.f64 = MY_MAX_F(a.f64, b.f64);
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void membar_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   // handled by timing simulator 
+void membar_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  // handled by timing simulator
 }
 
-void min_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, b, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+void min_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-
-   switch ( i_type ) {
-   case U16_TYPE: d.u16 = MY_MIN_I(a.u16,b.u16); break;
-   case U32_TYPE: d.u32 = MY_MIN_I(a.u32,b.u32); break;
-   case U64_TYPE: d.u64 = MY_MIN_I(a.u64,b.u64); break;
-   case S16_TYPE: d.s16 = MY_MIN_I(a.s16,b.s16); break;
-   case S32_TYPE: d.s32 = MY_MIN_I(a.s32,b.s32); break;
-   case S64_TYPE: d.s64 = MY_MIN_I(a.s64,b.s64); break;
-   case F32_TYPE: d.f32 = MY_MIN_F(a.f32,b.f32); break;
-   case F64_TYPE: case FF64_TYPE: d.f64 = MY_MIN_F(a.f64,b.f64); break;
-   default:
+  switch (i_type) {
+    case U16_TYPE:
+      d.u16 = MY_MIN_I(a.u16, b.u16);
+      break;
+    case U32_TYPE:
+      d.u32 = MY_MIN_I(a.u32, b.u32);
+      break;
+    case U64_TYPE:
+      d.u64 = MY_MIN_I(a.u64, b.u64);
+      break;
+    case S16_TYPE:
+      d.s16 = MY_MIN_I(a.s16, b.s16);
+      break;
+    case S32_TYPE:
+      d.s32 = MY_MIN_I(a.s32, b.s32);
+      break;
+    case S64_TYPE:
+      d.s64 = MY_MIN_I(a.s64, b.s64);
+      break;
+    case F32_TYPE:
+      d.f32 = MY_MIN_F(a.f32, b.f32);
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      d.f64 = MY_MIN_F(a.f64, b.f64);
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void mov_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t data;
+void mov_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t data;
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   unsigned i_type = pI->get_type();
-   assert( src1.is_param_local() == 0 );
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  unsigned i_type = pI->get_type();
+  assert(src1.is_param_local() == 0);
 
-   if( (src1.is_vector() || dst.is_vector()) && (i_type != BB64_TYPE) && (i_type != BB128_TYPE) && (i_type != FF64_TYPE) ) {
-      // pack or unpack operation
-      unsigned nbits_to_move;
-      ptx_reg_t tmp_bits;
+  if ((src1.is_vector() || dst.is_vector()) && (i_type != BB64_TYPE) &&
+      (i_type != BB128_TYPE) && (i_type != FF64_TYPE)) {
+    // pack or unpack operation
+    unsigned nbits_to_move;
+    ptx_reg_t tmp_bits;
 
-      switch( pI->get_type() ) {
-      case B16_TYPE: nbits_to_move = 16; break;
-      case B32_TYPE: nbits_to_move = 32; break;
-      case B64_TYPE: nbits_to_move = 64; break;
-      default: printf("Execution error: mov pack/unpack with unsupported type qualifier\n"); assert(0); break;
-      }
+    switch (pI->get_type()) {
+      case B16_TYPE:
+        nbits_to_move = 16;
+        break;
+      case B32_TYPE:
+        nbits_to_move = 32;
+        break;
+      case B64_TYPE:
+        nbits_to_move = 64;
+        break;
+      default:
+        printf(
+            "Execution error: mov pack/unpack with unsupported type "
+            "qualifier\n");
+        assert(0);
+        break;
+    }
 
-      if( src1.is_vector() ) {
-         unsigned nelem = src1.get_vect_nelem();
-         ptx_reg_t v[4];
-         thread->get_vector_operand_values(src1, v, nelem );
-
-         unsigned bits_per_src_elem = nbits_to_move / nelem;
-         for( unsigned i=0; i < nelem; i++ ) {
-            switch(bits_per_src_elem) {
-            case 8:   tmp_bits.u64 |= ((unsigned long long)(v[i].u8)  << (8*i));  break;
-            case 16:  tmp_bits.u64 |= ((unsigned long long)(v[i].u16) << (16*i)); break;
-            case 32:  tmp_bits.u64 |= ((unsigned long long)(v[i].u32) << (32*i)); break;
-            default: printf("Execution error: mov pack/unpack with unsupported source/dst size ratio (src)\n"); assert(0); break;
-            }
-         }
-      } else {
-         data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-
-         switch( pI->get_type() ) {
-         case B16_TYPE: tmp_bits.u16 = data.u16; break;
-         case B32_TYPE: tmp_bits.u32 = data.u32; break;
-         case B64_TYPE: tmp_bits.u64 = data.u64; break;
-         default: assert(0); break;
-         }
-      }
+    if (src1.is_vector()) {
+      unsigned nelem = src1.get_vect_nelem();
+      ptx_reg_t v[4];
+      thread->get_vector_operand_values(src1, v, nelem);
 
-      if( dst.is_vector() ) {
-         unsigned nelem = dst.get_vect_nelem();
-         ptx_reg_t v[4];
-         unsigned bits_per_dst_elem = nbits_to_move / nelem;
-         for( unsigned i=0; i < nelem; i++ ) {
-            switch(bits_per_dst_elem) {
-            case 8:  v[i].u8  = (tmp_bits.u64 >> (8*i)) & ((unsigned long long) 0xFF); break;
-            case 16: v[i].u16 = (tmp_bits.u64 >> (16*i)) & ((unsigned long long) 0xFFFF); break;
-            case 32: v[i].u32 = (tmp_bits.u64 >> (32*i)) & ((unsigned long long) 0xFFFFFFFF); break;
-            default:
-               printf("Execution error: mov pack/unpack with unsupported source/dst size ratio (dst)\n");
-               assert(0);
-               break;
-            }
-         }
-         thread->set_vector_operand_values(dst,v[0],v[1],v[2],v[3]);
-      } else {
-         thread->set_operand_value(dst,tmp_bits, i_type, thread, pI);
+      unsigned bits_per_src_elem = nbits_to_move / nelem;
+      for (unsigned i = 0; i < nelem; i++) {
+        switch (bits_per_src_elem) {
+          case 8:
+            tmp_bits.u64 |= ((unsigned long long)(v[i].u8) << (8 * i));
+            break;
+          case 16:
+            tmp_bits.u64 |= ((unsigned long long)(v[i].u16) << (16 * i));
+            break;
+          case 32:
+            tmp_bits.u64 |= ((unsigned long long)(v[i].u32) << (32 * i));
+            break;
+          default:
+            printf(
+                "Execution error: mov pack/unpack with unsupported source/dst "
+                "size ratio (src)\n");
+            assert(0);
+            break;
+        }
       }
-   } else if (i_type == PRED_TYPE and src1.is_literal() == true) {
-      // in ptx, literal input translate to predicate as 0 = false and 1 = true 
-      // we have adopted the opposite to simplify implementation of zero flags in ptxplus 
+    } else {
       data = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-      ptx_reg_t finaldata; 
-      finaldata.pred = (data.u32 == 0)? 1 : 0;  // setting zero-flag in predicate 
-      thread->set_operand_value(dst, finaldata, i_type, thread, pI);
-   } else {
-
-      data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+      switch (pI->get_type()) {
+        case B16_TYPE:
+          tmp_bits.u16 = data.u16;
+          break;
+        case B32_TYPE:
+          tmp_bits.u32 = data.u32;
+          break;
+        case B64_TYPE:
+          tmp_bits.u64 = data.u64;
+          break;
+        default:
+          assert(0);
+          break;
+      }
+    }
 
-     thread->set_operand_value(dst, data, i_type, thread, pI);
+    if (dst.is_vector()) {
+      unsigned nelem = dst.get_vect_nelem();
+      ptx_reg_t v[4];
+      unsigned bits_per_dst_elem = nbits_to_move / nelem;
+      for (unsigned i = 0; i < nelem; i++) {
+        switch (bits_per_dst_elem) {
+          case 8:
+            v[i].u8 = (tmp_bits.u64 >> (8 * i)) & ((unsigned long long)0xFF);
+            break;
+          case 16:
+            v[i].u16 =
+                (tmp_bits.u64 >> (16 * i)) & ((unsigned long long)0xFFFF);
+            break;
+          case 32:
+            v[i].u32 =
+                (tmp_bits.u64 >> (32 * i)) & ((unsigned long long)0xFFFFFFFF);
+            break;
+          default:
+            printf(
+                "Execution error: mov pack/unpack with unsupported source/dst "
+                "size ratio (dst)\n");
+            assert(0);
+            break;
+        }
+      }
+      thread->set_vector_operand_values(dst, v[0], v[1], v[2], v[3]);
+    } else {
+      thread->set_operand_value(dst, tmp_bits, i_type, thread, pI);
+    }
+  } else if (i_type == PRED_TYPE and src1.is_literal() == true) {
+    // in ptx, literal input translate to predicate as 0 = false and 1 = true
+    // we have adopted the opposite to simplify implementation of zero flags in
+    // ptxplus
+    data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+
+    ptx_reg_t finaldata;
+    finaldata.pred = (data.u32 == 0) ? 1 : 0;  // setting zero-flag in predicate
+    thread->set_operand_value(dst, finaldata, i_type, thread, pI);
+  } else {
+    data = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   }
+    thread->set_operand_value(dst, data, i_type, thread, pI);
+  }
 }
 
-void mul24_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t src1_data, src2_data, data;
-
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+void mul24_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   //src1_data = srcOperandModifiers(src1_data, src1, dst, i_type, thread);
-   //src2_data = srcOperandModifiers(src2_data, src2, dst, i_type, thread);
+  // src1_data = srcOperandModifiers(src1_data, src1, dst, i_type, thread);
+  // src2_data = srcOperandModifiers(src2_data, src2, dst, i_type, thread);
 
-   src1_data.mask_and(0,0x00FFFFFF);
-   src2_data.mask_and(0,0x00FFFFFF);
+  src1_data.mask_and(0, 0x00FFFFFF);
+  src2_data.mask_and(0, 0x00FFFFFF);
 
-   switch ( i_type ) {
-   case S32_TYPE: 
-      if( src1_data.get_bit(23) ) 
-         src1_data.mask_or(0xFFFFFFFF,0xFF000000);
-      if( src2_data.get_bit(23) ) 
-         src2_data.mask_or(0xFFFFFFFF,0xFF000000);
+  switch (i_type) {
+    case S32_TYPE:
+      if (src1_data.get_bit(23)) src1_data.mask_or(0xFFFFFFFF, 0xFF000000);
+      if (src2_data.get_bit(23)) src2_data.mask_or(0xFFFFFFFF, 0xFF000000);
       data.s64 = src1_data.s64 * src2_data.s64;
       break;
-   case U32_TYPE:
+    case U32_TYPE:
       data.u64 = src1_data.u64 * src2_data.u64;
       break;
-   default:
-      printf("GPGPU-Sim PTX: Execution error - type mismatch with instruction\n");
+    default:
+      printf(
+          "GPGPU-Sim PTX: Execution error - type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   if ( pI->is_hi() ) {
-      data.u64 = data.u64 >> 16;
-      data.mask_and(0,0xFFFFFFFF);
-   } else if (pI->is_lo()) {
-      data.mask_and(0,0xFFFFFFFF);
-   }
+  if (pI->is_hi()) {
+    data.u64 = data.u64 >> 16;
+    data.mask_and(0, 0xFFFFFFFF);
+  } else if (pI->is_lo()) {
+    data.mask_and(0, 0xFFFFFFFF);
+  }
 
-   thread->set_operand_value(dst, data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void mul_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t data;
+void mul_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t data;
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-   ptx_reg_t d, t;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  ptx_reg_t d, t;
 
-   unsigned i_type = pI->get_type();
-   ptx_reg_t a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  ptx_reg_t a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  ptx_reg_t b = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   unsigned rounding_mode = pI->rounding_mode();
+  unsigned rounding_mode = pI->rounding_mode();
 
-   switch ( i_type ) {
-   case S16_TYPE: 
+  switch (i_type) {
+    case S16_TYPE:
       t.s32 = ((int)a.s16) * ((int)b.s16);
-      if ( pI->is_wide() ) d.s32 = t.s32;
-      else if ( pI->is_hi() ) d.s16 = (t.s32>>16);
-      else if ( pI->is_lo() ) d.s16 = t.s16;
-      else assert(0);
+      if (pI->is_wide())
+        d.s32 = t.s32;
+      else if (pI->is_hi())
+        d.s16 = (t.s32 >> 16);
+      else if (pI->is_lo())
+        d.s16 = t.s16;
+      else
+        assert(0);
       break;
-   case S32_TYPE: 
+    case S32_TYPE:
       t.s64 = ((long long)a.s32) * ((long long)b.s32);
-      if ( pI->is_wide() ) d.s64 = t.s64;
-      else if ( pI->is_hi() ) d.s32 = (t.s64>>32);
-      else if ( pI->is_lo() ) d.s32 = t.s32;
-      else assert(0);
+      if (pI->is_wide())
+        d.s64 = t.s64;
+      else if (pI->is_hi())
+        d.s32 = (t.s64 >> 32);
+      else if (pI->is_lo())
+        d.s32 = t.s32;
+      else
+        assert(0);
       break;
-   case S64_TYPE: 
+    case S64_TYPE:
       t.s64 = a.s64 * b.s64;
-      assert( !pI->is_wide() );
-      assert( !pI->is_hi() );
-      if ( pI->is_lo() ) d.s64 = t.s64;
-      else assert(0);
+      assert(!pI->is_wide());
+      //assert(!pI->is_hi());
+      d.s64 = t.s64;
       break;
-   case U16_TYPE: 
+    case U16_TYPE:
       t.u32 = ((unsigned)a.u16) * ((unsigned)b.u16);
-      if ( pI->is_wide() ) d.u32 = t.u32;
-      else if ( pI->is_lo() ) d.u16 = t.u16;
-      else if ( pI->is_hi() ) d.u16 = (t.u32>>16);
-      else assert(0);
+      if (pI->is_wide())
+        d.u32 = t.u32;
+      else if (pI->is_lo())
+        d.u16 = t.u16;
+      else if (pI->is_hi())
+        d.u16 = (t.u32 >> 16);
+      else
+        assert(0);
       break;
-   case U32_TYPE: 
+    case U32_TYPE:
       t.u64 = ((unsigned long long)a.u32) * ((unsigned long long)b.u32);
-      if ( pI->is_wide() ) d.u64 = t.u64;
-      else if ( pI->is_lo() ) d.u32 = t.u32;
-      else if ( pI->is_hi() ) d.u32 = (t.u64>>32);
-      else assert(0);
+      if (pI->is_wide())
+        d.u64 = t.u64;
+      else if (pI->is_lo())
+        d.u32 = t.u32;
+      else if (pI->is_hi())
+        d.u32 = (t.u64 >> 32);
+      else
+        assert(0);
       break;
-   case U64_TYPE: 
+    case U64_TYPE:
       t.u64 = a.u64 * b.u64;
-      assert( !pI->is_wide() );
-      assert( !pI->is_hi() );
-      if ( pI->is_lo() ) d.u64 = t.u64;
-      else assert(0);
-      break;
-   case F16_TYPE:{ 
-      //assert(0); 
-      //break;
-         int orig_rm = fegetround();
-         switch ( rounding_mode ) {
-         case RN_OPTION: break;
-         case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-         default: assert(0); break;
-         }
-
-         d.f16 = a.f16 * b.f16;
-
-         if ( pI->saturation_mode() ) {
-            if ( d.f16 < 0 ) d.f16 = 0;
-            else if ( d.f16 > 1.0f ) d.f16 = 1.0f;
-         }
-         fesetround( orig_rm );
-         break;
-      }  
-   case F32_TYPE: {
-         int orig_rm = fegetround();
-         switch ( rounding_mode ) {
-         case RN_OPTION: break;
-         case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-         default: assert(0); break;
-         }
-
-         d.f32 = a.f32 * b.f32;
-
-         if ( pI->saturation_mode() ) {
-            if ( d.f32 < 0 ) d.f32 = 0;
-            else if ( d.f32 > 1.0f ) d.f32 = 1.0f;
-         }
-         fesetround( orig_rm );
-         break;
-      }  
-   case F64_TYPE: case FF64_TYPE:{
-         int orig_rm = fegetround();
-         switch ( rounding_mode ) {
-         case RN_OPTION: break;
-         case RZ_OPTION: fesetround( FE_TOWARDZERO ); break;
-         default: assert(0); break;
-         }
-         d.f64 = a.f64 * b.f64;
-         if ( pI->saturation_mode() ) {
-            if ( d.f64 < 0 ) d.f64 = 0;
-            else if ( d.f64 > 1.0f ) d.f64 = 1.0;
-         }
-         fesetround( orig_rm );
-         break;
-      }
-   default: 
-      assert(0); 
+      assert(!pI->is_wide());
+      assert(!pI->is_hi());
+      if (pI->is_lo())
+        d.u64 = t.u64;
+      else
+        assert(0);
       break;
-   }
-
-   thread->set_operand_value(dst, d, i_type, thread, pI);
-}
-
-void neg_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
+    case F16_TYPE: {
+      // assert(0);
+      // break;
+      int orig_rm = fegetround();
+      switch (rounding_mode) {
+        case RN_OPTION:
+          break;
+        case RZ_OPTION:
+          fesetround(FE_TOWARDZERO);
+          break;
+        default:
+          assert(0);
+          break;
+      }
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+      d.f16 = a.f16 * b.f16;
 
-   unsigned to_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, to_type, thread, 1);
+      if (pI->saturation_mode()) {
+        if (d.f16 < 0)
+          d.f16 = 0;
+        else if (d.f16 > 1.0f)
+          d.f16 = 1.0f;
+      }
+      fesetround(orig_rm);
+      break;
+    }
+    case F32_TYPE: {
+      int orig_rm = fegetround();
+      switch (rounding_mode) {
+        case RN_OPTION:
+          break;
+        case RZ_OPTION:
+          fesetround(FE_TOWARDZERO);
+          break;
+        default:
+          assert(0);
+          break;
+      }
 
+      d.f32 = a.f32 * b.f32;
 
-   switch ( to_type ) {
-   case S8_TYPE:
-   case S16_TYPE:
-   case S32_TYPE:
-   case S64_TYPE: 
-      data.s64 = 0 - src1_data.s64; break; // seems buggy, but not (just ignore higher bits)
-   case U8_TYPE:
-   case U16_TYPE:
-   case U32_TYPE:
-   case U64_TYPE: 
-      assert(0); break;
-   case F16_TYPE: data.f16 =0.0f  - src1_data.f16; break;//assert(0); break;
-   case F32_TYPE: data.f32 = 0.0f - src1_data.f32; break;
-   case F64_TYPE: case FF64_TYPE: data.f64 = 0.0f - src1_data.f64; break;
-   default: assert(0); break;
-   }
+      if (pI->saturation_mode()) {
+        if (d.f32 < 0)
+          d.f32 = 0;
+        else if (d.f32 > 1.0f)
+          d.f32 = 1.0f;
+      }
+      fesetround(orig_rm);
+      break;
+    }
+    case F64_TYPE:
+    case FF64_TYPE: {
+      int orig_rm = fegetround();
+      switch (rounding_mode) {
+        case RN_OPTION:
+          break;
+        case RZ_OPTION:
+          fesetround(FE_TOWARDZERO);
+          break;
+        default:
+          assert(0);
+          break;
+      }
+      d.f64 = a.f64 * b.f64;
+      if (pI->saturation_mode()) {
+        if (d.f64 < 0)
+          d.f64 = 0;
+        else if (d.f64 > 1.0f)
+          d.f64 = 1.0;
+      }
+      fesetround(orig_rm);
+      break;
+    }
+    default:
+      assert(0);
+      break;
+  }
 
-   thread->set_operand_value(dst,data, to_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-//nandn bitwise negates second operand then bitwise nands with the first operand
-void nandn_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t src1_data, src2_data, data;
+void neg_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
+
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+
+  unsigned to_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, to_type, thread, 1);
+
+  switch (to_type) {
+    case S8_TYPE:
+    case S16_TYPE:
+    case S32_TYPE:
+    case S64_TYPE:
+      data.s64 = 0 - src1_data.s64;
+      break;  // seems buggy, but not (just ignore higher bits)
+    case U8_TYPE:
+    case U16_TYPE:
+    case U32_TYPE:
+    case U64_TYPE:
+      assert(0);
+      break;
+    case F16_TYPE:
+      data.f16 = 0.0f - src1_data.f16;
+      break;  // assert(0); break;
+    case F32_TYPE:
+      data.f32 = 0.0f - src1_data.f32;
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      data.f64 = 0.0f - src1_data.f64;
+      break;
+    default:
+      assert(0);
+      break;
+  }
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+  thread->set_operand_value(dst, data, to_type, thread, pI);
+}
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+// nandn bitwise negates second operand then bitwise nands with the first
+// operand
+void nandn_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
 
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   //the way ptxplus handles predicates: 1 = false and 0 = true
-   if(i_type == PRED_TYPE)
-      data.pred = (~src1_data.pred & src2_data.pred);
-   else
-      data.u64 = ~(src1_data.u64 & ~src2_data.u64);
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  // the way ptxplus handles predicates: 1 = false and 0 = true
+  if (i_type == PRED_TYPE)
+    data.pred = (~src1_data.pred & src2_data.pred);
+  else
+    data.u64 = ~(src1_data.u64 & ~src2_data.u64);
 
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-//norn bitwise negates first operand then bitwise ands with the second operand
-void norn_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t src1_data, src2_data, data;
-
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+// norn bitwise negates first operand then bitwise ands with the second operand
+void norn_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   //the way ptxplus handles predicates: 1 = false and 0 = true
-   if(i_type == PRED_TYPE)
-      data.pred = ~(src1_data.pred & ~(src2_data.pred));
-   else
-      data.u64 = ~(src1_data.u64) & src2_data.u64;
-
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  // the way ptxplus handles predicates: 1 = false and 0 = true
+  if (i_type == PRED_TYPE)
+    data.pred = ~(src1_data.pred & ~(src2_data.pred));
+  else
+    data.u64 = ~(src1_data.u64) & src2_data.u64;
 
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void not_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t a, b, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+void not_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case PRED_TYPE: d.pred = (~(a.pred) & 0x000F); break;
-   case B16_TYPE:  d.u16  = ~a.u16; break;
-   case B32_TYPE:  d.u32  = ~a.u32; break;
-   case B64_TYPE:  d.u64  = ~a.u64; break;
-   default:
+  switch (i_type) {
+    case PRED_TYPE:
+      d.pred = (~(a.pred) & 0x000F);
+      break;
+    case B16_TYPE:
+      d.u16 = ~a.u16;
+      break;
+    case B32_TYPE:
+      d.u32 = ~a.u32;
+      break;
+    case B64_TYPE:
+      d.u64 = ~a.u64;
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void or_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+void or_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   //the way ptxplus handles predicates: 1 = false and 0 = true
-   if(i_type == PRED_TYPE)
-      data.pred = ~(~(src1_data.pred) | ~(src2_data.pred));
-   else
-      data.u64 = src1_data.u64 | src2_data.u64;
+  // the way ptxplus handles predicates: 1 = false and 0 = true
+  if (i_type == PRED_TYPE)
+    data.pred = ~(~(src1_data.pred) | ~(src2_data.pred));
+  else
+    data.u64 = src1_data.u64 | src2_data.u64;
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void orn_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+void orn_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   //the way ptxplus handles predicates: 1 = false and 0 = true
-   if(i_type == PRED_TYPE)
-      data.pred = ~(~(src1_data.pred) | (src2_data.pred));
-   else
-      data.u64 = src1_data.u64 | ~src2_data.u64;
+  // the way ptxplus handles predicates: 1 = false and 0 = true
+  if (i_type == PRED_TYPE)
+    data.pred = ~(~(src1_data.pred) | (src2_data.pred));
+  else
+    data.u64 = src1_data.u64 | ~src2_data.u64;
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void pmevent_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void popc_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src_data, data;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src = pI->src1();
+void pmevent_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void popc_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src_data, data;
+  const operand_info &dst = pI->dst();
+  const operand_info &src = pI->src1();
 
-   unsigned i_type = pI->get_type();
-   src_data = thread->get_operand_value(src, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  src_data = thread->get_operand_value(src, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case B32_TYPE: {
-      std::bitset<32> mask(src_data.u32); 
-      data.u32 = mask.count(); 
-      } break;
-   case B64_TYPE: {
-      std::bitset<64> mask(src_data.u64); 
+  switch (i_type) {
+    case B32_TYPE: {
+      std::bitset<32> mask(src_data.u32);
       data.u32 = mask.count();
-      } break;
-   default:
+    } break;
+    case B64_TYPE: {
+      std::bitset<64> mask(src_data.u64);
+      data.u32 = mask.count();
+    } break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
-
-   thread->set_operand_value(dst,data, i_type, thread, pI);
-}
-void prefetch_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void prefetchu_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-
-int prmt_mode_present(int mode)
-{
-	int returnval=0;
-	switch(mode){
-		case PRMT_F4E_MODE:
-		case PRMT_B4E_MODE:
-		case PRMT_RC8_MODE:
-		case PRMT_RC16_MODE:
-		case PRMT_ECL_MODE:
-		case PRMT_ECR_MODE:	
-			returnval=1;
-			break;
-		default:	
-			break;
-	}
-	return returnval;
-}
-int read_byte(int mode,int control,int d_sel_index,signed long long value){
-
-	int returnval;
-	int prmt_f4e_mode[4][4]={{0,1,2,3},{1,2,3,4},{2,3,4,5},{3,4,5,6}};
-	int prmt_b4e_mode[4][4]={{0,7,6,5},{1,0,7,6},{2,1,0,7},{3,2,1,0}};
-	int prmt_rc8_mode[4][4]={{0,0,0,0},{1,1,1,1},{2,2,2,2},{3,3,3,3}};
-	int prmt_ecl_mode[4][4]={{0,1,2,3},{1,1,2,3},{2,2,2,3},{3,3,3,3}};
-	int prmt_ecr_mode[4][4]={{0,0,0,0},{0,1,1,1},{0,1,2,2},{0,1,2,3}};
-	int prmt_rc16_mode[4][4]={{0,1,0,1},{2,3,2,3},{0,1,0,1},{2,3,2,3}};
-
-	if(!prmt_mode_present(mode)){
-		if(control&0x8){
-			returnval=0xff;
-		}
-		else{
-			returnval= (value>>(8*control)) & 0xff;
-		}
-	}
-	else{
-		switch(mode){
-			case PRMT_F4E_MODE:	returnval=prmt_f4e_mode[control][d_sel_index];break;
-			case PRMT_B4E_MODE:	returnval=prmt_b4e_mode[control][d_sel_index];break;
-			case PRMT_RC8_MODE:	returnval=prmt_rc8_mode[control][d_sel_index];break;
-			case PRMT_ECL_MODE:	returnval=prmt_ecl_mode[control][d_sel_index];break;
-			case PRMT_ECR_MODE:	returnval=prmt_ecr_mode[control][d_sel_index];break;
-			case PRMT_RC16_MODE:	returnval=prmt_rc16_mode[control][d_sel_index];break;
-			default: printf("ERROR\n");break;
-		}
-	}	
-	return (returnval<<8*d_sel_index);
-}
-
-void prmt_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { 
-   
-   ptx_reg_t src1_data, src2_data, src3_data,tmpdata,data;
-   const operand_info &dst  = pI->dst();  
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
-
-   unsigned mode   = pI->prmt_op();
-   unsigned i_type = pI->get_type();
-
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   src3_data = thread->get_operand_value(src3, dst, i_type, thread, 1);
-
-   tmpdata.s64=src1_data.s32|(src2_data.s64<<32);
-   int ctl[4];
-
-   if(!prmt_mode_present(mode)){
-	ctl[0]=(src3_data.s32>>0)&0xf;
-	ctl[1]=(src3_data.s32>>4)&0xf;
-	ctl[2]=(src3_data.s32>>8)&0xf;
-	ctl[3]=(src3_data.s32>>12)&0xf;
-   }
-   else{
-	ctl[0]=ctl[1]=ctl[2]=ctl[3]=(src3_data.s32>>0)&0x3;	
-   }
-   
-   data.s32=0;
-   data.s32=data.s32|read_byte(mode,ctl[0],0,tmpdata.s64);   //First byte-0
-   data.s32=data.s32|read_byte(mode,ctl[1],1,tmpdata.s64);   //Second byte-1
-   data.s32=data.s32|read_byte(mode,ctl[2],2,tmpdata.s64);   //Third byte-2
-   data.s32=data.s32|read_byte(mode,ctl[3],3,tmpdata.s64);   //Fourth byte-3
-	
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  }
 
+  thread->set_operand_value(dst, data, i_type, thread, pI);
+}
+void prefetch_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void prefetchu_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
 
+int prmt_mode_present(int mode) {
+  int returnval = 0;
+  switch (mode) {
+    case PRMT_F4E_MODE:
+    case PRMT_B4E_MODE:
+    case PRMT_RC8_MODE:
+    case PRMT_RC16_MODE:
+    case PRMT_ECL_MODE:
+    case PRMT_ECR_MODE:
+      returnval = 1;
+      break;
+    default:
+      break;
+  }
+  return returnval;
+}
+int read_byte(int mode, int control, int d_sel_index, signed long long value) {
+  int returnval = 0;
+  int prmt_f4e_mode[4][4] = {
+      {0, 1, 2, 3}, {1, 2, 3, 4}, {2, 3, 4, 5}, {3, 4, 5, 6}};
+  int prmt_b4e_mode[4][4] = {
+      {0, 7, 6, 5}, {1, 0, 7, 6}, {2, 1, 0, 7}, {3, 2, 1, 0}};
+  int prmt_rc8_mode[4][4] = {
+      {0, 0, 0, 0}, {1, 1, 1, 1}, {2, 2, 2, 2}, {3, 3, 3, 3}};
+  int prmt_ecl_mode[4][4] = {
+      {0, 1, 2, 3}, {1, 1, 2, 3}, {2, 2, 2, 3}, {3, 3, 3, 3}};
+  int prmt_ecr_mode[4][4] = {
+      {0, 0, 0, 0}, {0, 1, 1, 1}, {0, 1, 2, 2}, {0, 1, 2, 3}};
+  int prmt_rc16_mode[4][4] = {
+      {0, 1, 0, 1}, {2, 3, 2, 3}, {0, 1, 0, 1}, {2, 3, 2, 3}};
+
+  if (!prmt_mode_present(mode)) {
+    if (control & 0x8) {
+      returnval = 0xff;
+    } else {
+      returnval = (value >> (8 * control)) & 0xff;
+    }
+  } else {
+    switch (mode) {
+      case PRMT_F4E_MODE:
+        returnval = prmt_f4e_mode[control][d_sel_index];
+        break;
+      case PRMT_B4E_MODE:
+        returnval = prmt_b4e_mode[control][d_sel_index];
+        break;
+      case PRMT_RC8_MODE:
+        returnval = prmt_rc8_mode[control][d_sel_index];
+        break;
+      case PRMT_ECL_MODE:
+        returnval = prmt_ecl_mode[control][d_sel_index];
+        break;
+      case PRMT_ECR_MODE:
+        returnval = prmt_ecr_mode[control][d_sel_index];
+        break;
+      case PRMT_RC16_MODE:
+        returnval = prmt_rc16_mode[control][d_sel_index];
+        break;
+        // Change the default from printing "ERROR" to just asserting
+      default:
+        assert(false);
+    }
+  }
+  return (returnval << 8 * d_sel_index);
 }
 
-void rcp_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+void prmt_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, src3_data, tmpdata, data;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+
+  unsigned mode = pI->prmt_op();
+  unsigned i_type = pI->get_type();
+
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  src3_data = thread->get_operand_value(src3, dst, i_type, thread, 1);
+
+  tmpdata.s64 = src1_data.s32 | (src2_data.s64 << 32);
+  int ctl[4];
+
+  if (!prmt_mode_present(mode)) {
+    ctl[0] = (src3_data.s32 >> 0) & 0xf;
+    ctl[1] = (src3_data.s32 >> 4) & 0xf;
+    ctl[2] = (src3_data.s32 >> 8) & 0xf;
+    ctl[3] = (src3_data.s32 >> 12) & 0xf;
+  } else {
+    ctl[0] = ctl[1] = ctl[2] = ctl[3] = (src3_data.s32 >> 0) & 0x3;
+  }
+
+  data.s32 = 0;
+  data.s32 = data.s32 | read_byte(mode, ctl[0], 0, tmpdata.s64);  // First
+                                                                  // byte-0
+  data.s32 =
+      data.s32 | read_byte(mode, ctl[1], 1, tmpdata.s64);  // Second byte-1
+  data.s32 = data.s32 | read_byte(mode, ctl[2], 2, tmpdata.s64);  // Third
+                                                                  // byte-2
+  data.s32 =
+      data.s32 | read_byte(mode, ctl[3], 3, tmpdata.s64);  // Fourth byte-3
+
+  thread->set_operand_value(dst, data, i_type, thread, pI);
+}
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+void rcp_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case F32_TYPE: 
+  switch (i_type) {
+    case F32_TYPE:
       data.f32 = 1.0f / src1_data.f32;
       break;
-   case F64_TYPE:
-   case FF64_TYPE:
+    case F64_TYPE:
+    case FF64_TYPE:
       data.f64 = 1.0f / src1_data.f64;
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void red_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
+void red_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
 
-void rem_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
+void rem_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case S32_TYPE:
+  switch (i_type) {
+    case S32_TYPE:
       data.s32 = src1_data.s32 % src2_data.s32;
       break;
-   case S64_TYPE:
+    case S64_TYPE:
       data.s64 = src1_data.s64 % src2_data.s64;
       break;
-   case U32_TYPE:
+    case U32_TYPE:
       data.u32 = src1_data.u32 % src2_data.u32;
       break;
-   case U64_TYPE:
+    case U64_TYPE:
       data.u64 = src1_data.u64 % src2_data.u64;
       break;
-   default: assert(0); break;
-   }
+    default:
+      assert(0);
+      break;
+  }
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void ret_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   bool empty = thread->callstack_pop();
-   if( empty ) {
-   thread->set_done();
-   thread->exitCore();
-   thread->registerExit();
-   }
+void ret_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  bool empty = thread->callstack_pop();
+  if (empty) {
+    thread->set_done();
+    thread->exitCore();
+    thread->registerExit();
+  }
 }
 
-//Ptxplus version of ret instruction.
-void retp_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-   bool empty = thread->callstack_pop_plus();
-   if( empty ) {
-   thread->set_done();
-   thread->exitCore();
-   thread->registerExit();
-   }
+// Ptxplus version of ret instruction.
+void retp_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  bool empty = thread->callstack_pop_plus();
+  if (empty) {
+    thread->set_done();
+    thread->exitCore();
+    thread->registerExit();
+  }
 }
 
-void rsqrt_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-
-
-   switch ( i_type ) {
-   case F32_TYPE:
-      if ( a.f32 < 0 ) {
-         d.u64 = 0;
-         d.u64 = 0x7fc00000; // NaN
-      } else if ( a.f32 == 0 ) {
-         d.u64 = 0;
-         d.u32 = 0x7f800000; // Inf
+void rsqrt_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+
+  switch (i_type) {
+    case F32_TYPE:
+      if (a.f32 < 0) {
+        d.u64 = 0;
+        d.u64 = 0x7fc00000;  // NaN
+      } else if (a.f32 == 0) {
+        d.u64 = 0;
+        d.u32 = 0x7f800000;  // Inf
       } else
-         d.f32 = cuda_math::__internal_accurate_fdividef(1.0f, sqrtf(a.f32));
-      break;
-   case F64_TYPE: 
-   case FF64_TYPE:
-      if ( a.f32 < 0 ) {
-         d.u64 = 0;
-	      d.u32 = 0x7fc00000; // NaN
-         float x = d.f32; 
-         d.f64 = (double)x;
-      } else if ( a.f32 == 0 ) {
-         d.u64 = 0;
-	      d.u32 = 0x7f800000; // Inf
-         float x = d.f32; 
-         d.f64 = (double)x;
+        d.f32 = cuda_math::__internal_accurate_fdividef(1.0f, sqrtf(a.f32));
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      if (a.f32 < 0) {
+        d.u64 = 0;
+        d.u32 = 0x7fc00000;  // NaN
+        float x = d.f32;
+        d.f64 = (double)x;
+      } else if (a.f32 == 0) {
+        d.u64 = 0;
+        d.u32 = 0x7f800000;  // Inf
+        float x = d.f32;
+        d.f64 = (double)x;
       } else
-         d.f64 = 1.0 / sqrt(a.f64); 
+        d.f64 = 1.0 / sqrt(a.f64);
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-#define SAD(d,a,b,c) d = c + ((a<b) ? (b-a) : (a-b))
-
-void sad_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, b, c, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
+#define SAD(d, a, b, c) d = c + ((a < b) ? (b - a) : (a - b))
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   c = thread->get_operand_value(src3, dst, i_type, thread, 1);
+void sad_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b, c, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
 
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  c = thread->get_operand_value(src3, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case U16_TYPE: SAD(d.u16,a.u16,b.u16,c.u16); break;
-   case U32_TYPE: SAD(d.u32,a.u32,b.u32,c.u32); break;
-   case U64_TYPE: SAD(d.u64,a.u64,b.u64,c.u64); break;
-   case S16_TYPE: SAD(d.s16,a.s16,b.s16,c.s16); break;
-   case S32_TYPE: SAD(d.s32,a.s32,b.s32,c.s32); break;
-   case S64_TYPE: SAD(d.s64,a.s64,b.s64,c.s64); break;
-   case F32_TYPE: SAD(d.f32,a.f32,b.f32,c.f32); break;
-   case F64_TYPE: case FF64_TYPE: SAD(d.f64,a.f64,b.f64,c.f64); break;
-   default:
+  switch (i_type) {
+    case U16_TYPE:
+      SAD(d.u16, a.u16, b.u16, c.u16);
+      break;
+    case U32_TYPE:
+      SAD(d.u32, a.u32, b.u32, c.u32);
+      break;
+    case U64_TYPE:
+      SAD(d.u64, a.u64, b.u64, c.u64);
+      break;
+    case S16_TYPE:
+      SAD(d.s16, a.s16, b.s16, c.s16);
+      break;
+    case S32_TYPE:
+      SAD(d.s32, a.s32, b.s32, c.s32);
+      break;
+    case S64_TYPE:
+      SAD(d.s64, a.s64, b.s64, c.s64);
+      break;
+    case F32_TYPE:
+      SAD(d.f32, a.f32, b.f32, c.f32);
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      SAD(d.f64, a.f64, b.f64, c.f64);
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void selp_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
+void selp_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
 
-   ptx_reg_t a, b, c, d;
+  ptx_reg_t a, b, c, d;
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   c = thread->get_operand_value(src3, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  c = thread->get_operand_value(src3, dst, i_type, thread, 1);
 
-   //predicate value was changed so the lowest bit being set means the zero flag is set.
-   //As a result, the value of c.pred must be inverted to get proper behavior
-   d = (!(c.pred & 0x0001))?a:b;
+  // predicate value was changed so the lowest bit being set means the zero flag
+  // is set. As a result, the value of c.pred must be inverted to get proper
+  // behavior
+  d = (!(c.pred & 0x0001)) ? a : b;
 
-   thread->set_operand_value(dst,d, PRED_TYPE, thread, pI);
+  thread->set_operand_value(dst, d, PRED_TYPE, thread, pI);
 }
 
-bool isFloat(int type) 
-{
-   switch ( type ) {
-   case F16_TYPE:
-   case F32_TYPE:
-   case F64_TYPE:
-   case FF64_TYPE:
+bool isFloat(int type) {
+  switch (type) {
+    case F16_TYPE:
+    case F32_TYPE:
+    case F64_TYPE:
+    case FF64_TYPE:
       return true;
-   default:
+    default:
       return false;
-   }
+  }
 }
 
-bool CmpOp( int type, ptx_reg_t a, ptx_reg_t b, unsigned cmpop )
-{
-   bool t = false;
+bool CmpOp(int type, ptx_reg_t a, ptx_reg_t b, unsigned cmpop) {
+  bool t = false;
 
-   switch ( type ) {
-   case B16_TYPE: 
+  switch (type) {
+    case B16_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.u16 == b.u16); break;
-      case NE_OPTION: t = (a.u16 != b.u16); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.u16 == b.u16);
+          break;
+        case NE_OPTION:
+          t = (a.u16 != b.u16);
+          break;
+        default:
+          assert(0);
       }
 
-   case B32_TYPE: 
+    case B32_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.u32 == b.u32); break;
-      case NE_OPTION: t = (a.u32 != b.u32); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.u32 == b.u32);
+          break;
+        case NE_OPTION:
+          t = (a.u32 != b.u32);
+          break;
+        default:
+          assert(0);
       }
-   case B64_TYPE:
+    case B64_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.u64 == b.u64); break;
-      case NE_OPTION: t = (a.u64 != b.u64); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.u64 == b.u64);
+          break;
+        case NE_OPTION:
+          t = (a.u64 != b.u64);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case S8_TYPE: 
-   case S16_TYPE:
+    case S8_TYPE:
+    case S16_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.s16 == b.s16); break;
-      case NE_OPTION: t = (a.s16 != b.s16); break;
-      case LT_OPTION: t = (a.s16 < b.s16); break;
-      case LE_OPTION: t = (a.s16 <= b.s16); break;
-      case GT_OPTION: t = (a.s16 > b.s16); break;
-      case GE_OPTION: t = (a.s16 >= b.s16); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.s16 == b.s16);
+          break;
+        case NE_OPTION:
+          t = (a.s16 != b.s16);
+          break;
+        case LT_OPTION:
+          t = (a.s16 < b.s16);
+          break;
+        case LE_OPTION:
+          t = (a.s16 <= b.s16);
+          break;
+        case GT_OPTION:
+          t = (a.s16 > b.s16);
+          break;
+        case GE_OPTION:
+          t = (a.s16 >= b.s16);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case S32_TYPE: 
+    case S32_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.s32 == b.s32); break;
-      case NE_OPTION: t = (a.s32 != b.s32); break;
-      case LT_OPTION: t = (a.s32 < b.s32); break;
-      case LE_OPTION: t = (a.s32 <= b.s32); break;
-      case GT_OPTION: t = (a.s32 > b.s32); break;
-      case GE_OPTION: t = (a.s32 >= b.s32); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.s32 == b.s32);
+          break;
+        case NE_OPTION:
+          t = (a.s32 != b.s32);
+          break;
+        case LT_OPTION:
+          t = (a.s32 < b.s32);
+          break;
+        case LE_OPTION:
+          t = (a.s32 <= b.s32);
+          break;
+        case GT_OPTION:
+          t = (a.s32 > b.s32);
+          break;
+        case GE_OPTION:
+          t = (a.s32 >= b.s32);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case S64_TYPE:
+    case S64_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.s64 == b.s64); break;
-      case NE_OPTION: t = (a.s64 != b.s64); break;
-      case LT_OPTION: t = (a.s64 < b.s64); break;
-      case LE_OPTION: t = (a.s64 <= b.s64); break;
-      case GT_OPTION: t = (a.s64 > b.s64); break;
-      case GE_OPTION: t = (a.s64 >= b.s64); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.s64 == b.s64);
+          break;
+        case NE_OPTION:
+          t = (a.s64 != b.s64);
+          break;
+        case LT_OPTION:
+          t = (a.s64 < b.s64);
+          break;
+        case LE_OPTION:
+          t = (a.s64 <= b.s64);
+          break;
+        case GT_OPTION:
+          t = (a.s64 > b.s64);
+          break;
+        case GE_OPTION:
+          t = (a.s64 >= b.s64);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case U8_TYPE: 
-   case U16_TYPE: 
+    case U8_TYPE:
+    case U16_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.u16 == b.u16); break;
-      case NE_OPTION: t = (a.u16 != b.u16); break;
-      case LT_OPTION: t = (a.u16 < b.u16); break;
-      case LE_OPTION: t = (a.u16 <= b.u16); break;
-      case GT_OPTION: t = (a.u16 > b.u16); break;
-      case GE_OPTION: t = (a.u16 >= b.u16); break;
-      case LO_OPTION: t = (a.u16 < b.u16); break;
-      case LS_OPTION: t = (a.u16 <= b.u16); break;
-      case HI_OPTION: t = (a.u16 > b.u16); break;
-      case HS_OPTION: t = (a.u16 >= b.u16); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.u16 == b.u16);
+          break;
+        case NE_OPTION:
+          t = (a.u16 != b.u16);
+          break;
+        case LT_OPTION:
+          t = (a.u16 < b.u16);
+          break;
+        case LE_OPTION:
+          t = (a.u16 <= b.u16);
+          break;
+        case GT_OPTION:
+          t = (a.u16 > b.u16);
+          break;
+        case GE_OPTION:
+          t = (a.u16 >= b.u16);
+          break;
+        case LO_OPTION:
+          t = (a.u16 < b.u16);
+          break;
+        case LS_OPTION:
+          t = (a.u16 <= b.u16);
+          break;
+        case HI_OPTION:
+          t = (a.u16 > b.u16);
+          break;
+        case HS_OPTION:
+          t = (a.u16 >= b.u16);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case U32_TYPE: 
+    case U32_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.u32 == b.u32); break;
-      case NE_OPTION: t = (a.u32 != b.u32); break;
-      case LT_OPTION: t = (a.u32 < b.u32); break;
-      case LE_OPTION: t = (a.u32 <= b.u32); break;
-      case GT_OPTION: t = (a.u32 > b.u32); break;
-      case GE_OPTION: t = (a.u32 >= b.u32); break;
-      case LO_OPTION: t = (a.u32 < b.u32); break;
-      case LS_OPTION: t = (a.u32 <= b.u32); break;
-      case HI_OPTION: t = (a.u32 > b.u32); break;
-      case HS_OPTION: t = (a.u32 >= b.u32); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.u32 == b.u32);
+          break;
+        case NE_OPTION:
+          t = (a.u32 != b.u32);
+          break;
+        case LT_OPTION:
+          t = (a.u32 < b.u32);
+          break;
+        case LE_OPTION:
+          t = (a.u32 <= b.u32);
+          break;
+        case GT_OPTION:
+          t = (a.u32 > b.u32);
+          break;
+        case GE_OPTION:
+          t = (a.u32 >= b.u32);
+          break;
+        case LO_OPTION:
+          t = (a.u32 < b.u32);
+          break;
+        case LS_OPTION:
+          t = (a.u32 <= b.u32);
+          break;
+        case HI_OPTION:
+          t = (a.u32 > b.u32);
+          break;
+        case HS_OPTION:
+          t = (a.u32 >= b.u32);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case U64_TYPE:
+    case U64_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.u64 == b.u64); break;
-      case NE_OPTION: t = (a.u64 != b.u64); break;
-      case LT_OPTION: t = (a.u64 < b.u64); break;
-      case LE_OPTION: t = (a.u64 <= b.u64); break;
-      case GT_OPTION: t = (a.u64 > b.u64); break;
-      case GE_OPTION: t = (a.u64 >= b.u64); break;
-      case LO_OPTION: t = (a.u64 < b.u64); break;
-      case LS_OPTION: t = (a.u64 <= b.u64); break;
-      case HI_OPTION: t = (a.u64 > b.u64); break;
-      case HS_OPTION: t = (a.u64 >= b.u64); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.u64 == b.u64);
+          break;
+        case NE_OPTION:
+          t = (a.u64 != b.u64);
+          break;
+        case LT_OPTION:
+          t = (a.u64 < b.u64);
+          break;
+        case LE_OPTION:
+          t = (a.u64 <= b.u64);
+          break;
+        case GT_OPTION:
+          t = (a.u64 > b.u64);
+          break;
+        case GE_OPTION:
+          t = (a.u64 >= b.u64);
+          break;
+        case LO_OPTION:
+          t = (a.u64 < b.u64);
+          break;
+        case LS_OPTION:
+          t = (a.u64 <= b.u64);
+          break;
+        case HI_OPTION:
+          t = (a.u64 > b.u64);
+          break;
+        case HS_OPTION:
+          t = (a.u64 >= b.u64);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case F16_TYPE: assert(0); break;
-   case F32_TYPE: 
+    case F16_TYPE:
+      assert(0);
+      break;
+    case F32_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.f32 == b.f32) && !isNaN(a.f32) && !isNaN(b.f32); break;
-      case NE_OPTION: t = (a.f32 != b.f32) && !isNaN(a.f32) && !isNaN(b.f32); break;
-      case LT_OPTION: t = (a.f32 < b.f32 ) && !isNaN(a.f32) && !isNaN(b.f32); break;
-      case LE_OPTION: t = (a.f32 <= b.f32) && !isNaN(a.f32) && !isNaN(b.f32); break;
-      case GT_OPTION: t = (a.f32 > b.f32 ) && !isNaN(a.f32) && !isNaN(b.f32); break;
-      case GE_OPTION: t = (a.f32 >= b.f32) && !isNaN(a.f32) && !isNaN(b.f32); break;
-      case EQU_OPTION: t = (a.f32 == b.f32) || isNaN(a.f32) || isNaN(b.f32); break;
-      case NEU_OPTION: t = (a.f32 != b.f32) || isNaN(a.f32) || isNaN(b.f32); break;
-      case LTU_OPTION: t = (a.f32 < b.f32 ) || isNaN(a.f32) || isNaN(b.f32); break;
-      case LEU_OPTION: t = (a.f32 <= b.f32) || isNaN(a.f32) || isNaN(b.f32); break;
-      case GTU_OPTION: t = (a.f32 > b.f32 ) || isNaN(a.f32) || isNaN(b.f32); break;
-      case GEU_OPTION: t = (a.f32 >= b.f32) || isNaN(a.f32) || isNaN(b.f32); break;
-      case NUM_OPTION: t = !isNaN(a.f32) && !isNaN(b.f32); break;
-      case NAN_OPTION: t = isNaN(a.f32) || isNaN(b.f32); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.f32 == b.f32) && !isNaN(a.f32) && !isNaN(b.f32);
+          break;
+        case NE_OPTION:
+          t = (a.f32 != b.f32) && !isNaN(a.f32) && !isNaN(b.f32);
+          break;
+        case LT_OPTION:
+          t = (a.f32 < b.f32) && !isNaN(a.f32) && !isNaN(b.f32);
+          break;
+        case LE_OPTION:
+          t = (a.f32 <= b.f32) && !isNaN(a.f32) && !isNaN(b.f32);
+          break;
+        case GT_OPTION:
+          t = (a.f32 > b.f32) && !isNaN(a.f32) && !isNaN(b.f32);
+          break;
+        case GE_OPTION:
+          t = (a.f32 >= b.f32) && !isNaN(a.f32) && !isNaN(b.f32);
+          break;
+        case EQU_OPTION:
+          t = (a.f32 == b.f32) || isNaN(a.f32) || isNaN(b.f32);
+          break;
+        case NEU_OPTION:
+          t = (a.f32 != b.f32) || isNaN(a.f32) || isNaN(b.f32);
+          break;
+        case LTU_OPTION:
+          t = (a.f32 < b.f32) || isNaN(a.f32) || isNaN(b.f32);
+          break;
+        case LEU_OPTION:
+          t = (a.f32 <= b.f32) || isNaN(a.f32) || isNaN(b.f32);
+          break;
+        case GTU_OPTION:
+          t = (a.f32 > b.f32) || isNaN(a.f32) || isNaN(b.f32);
+          break;
+        case GEU_OPTION:
+          t = (a.f32 >= b.f32) || isNaN(a.f32) || isNaN(b.f32);
+          break;
+        case NUM_OPTION:
+          t = !isNaN(a.f32) && !isNaN(b.f32);
+          break;
+        case NAN_OPTION:
+          t = isNaN(a.f32) || isNaN(b.f32);
+          break;
+        default:
+          assert(0);
       }
       break;
-   case F64_TYPE: 
-   case FF64_TYPE:
+    case F64_TYPE:
+    case FF64_TYPE:
       switch (cmpop) {
-      case EQ_OPTION: t = (a.f64 == b.f64) && !isNaN(a.f64) && !isNaN(b.f64); break;
-      case NE_OPTION: t = (a.f64 != b.f64) && !isNaN(a.f64) && !isNaN(b.f64); break;
-      case LT_OPTION: t = (a.f64 < b.f64 ) && !isNaN(a.f64) && !isNaN(b.f64); break;
-      case LE_OPTION: t = (a.f64 <= b.f64) && !isNaN(a.f64) && !isNaN(b.f64); break;
-      case GT_OPTION: t = (a.f64 > b.f64 ) && !isNaN(a.f64) && !isNaN(b.f64); break;
-      case GE_OPTION: t = (a.f64 >= b.f64) && !isNaN(a.f64) && !isNaN(b.f64); break;
-      case EQU_OPTION: t = (a.f64 == b.f64) || isNaN(a.f64) || isNaN(b.f64); break;
-      case NEU_OPTION: t = (a.f64 != b.f64) || isNaN(a.f64) || isNaN(b.f64); break;
-      case LTU_OPTION: t = (a.f64 < b.f64 ) || isNaN(a.f64) || isNaN(b.f64); break;
-      case LEU_OPTION: t = (a.f64 <= b.f64) || isNaN(a.f64) || isNaN(b.f64); break;
-      case GTU_OPTION: t = (a.f64 > b.f64 ) || isNaN(a.f64) || isNaN(b.f64); break;
-      case GEU_OPTION: t = (a.f64 >= b.f64) || isNaN(a.f64) || isNaN(b.f64); break;
-      case NUM_OPTION: t = !isNaN(a.f64) && !isNaN(b.f64); break;
-      case NAN_OPTION: t = isNaN(a.f64) || isNaN(b.f64); break;
-      default:
-         assert(0);
+        case EQ_OPTION:
+          t = (a.f64 == b.f64) && !isNaN(a.f64) && !isNaN(b.f64);
+          break;
+        case NE_OPTION:
+          t = (a.f64 != b.f64) && !isNaN(a.f64) && !isNaN(b.f64);
+          break;
+        case LT_OPTION:
+          t = (a.f64 < b.f64) && !isNaN(a.f64) && !isNaN(b.f64);
+          break;
+        case LE_OPTION:
+          t = (a.f64 <= b.f64) && !isNaN(a.f64) && !isNaN(b.f64);
+          break;
+        case GT_OPTION:
+          t = (a.f64 > b.f64) && !isNaN(a.f64) && !isNaN(b.f64);
+          break;
+        case GE_OPTION:
+          t = (a.f64 >= b.f64) && !isNaN(a.f64) && !isNaN(b.f64);
+          break;
+        case EQU_OPTION:
+          t = (a.f64 == b.f64) || isNaN(a.f64) || isNaN(b.f64);
+          break;
+        case NEU_OPTION:
+          t = (a.f64 != b.f64) || isNaN(a.f64) || isNaN(b.f64);
+          break;
+        case LTU_OPTION:
+          t = (a.f64 < b.f64) || isNaN(a.f64) || isNaN(b.f64);
+          break;
+        case LEU_OPTION:
+          t = (a.f64 <= b.f64) || isNaN(a.f64) || isNaN(b.f64);
+          break;
+        case GTU_OPTION:
+          t = (a.f64 > b.f64) || isNaN(a.f64) || isNaN(b.f64);
+          break;
+        case GEU_OPTION:
+          t = (a.f64 >= b.f64) || isNaN(a.f64) || isNaN(b.f64);
+          break;
+        case NUM_OPTION:
+          t = !isNaN(a.f64) && !isNaN(b.f64);
+          break;
+        case NAN_OPTION:
+          t = isNaN(a.f64) || isNaN(b.f64);
+          break;
+        default:
+          assert(0);
       }
       break;
-   default: assert(0); break;
-   }
+    default:
+      assert(0);
+      break;
+  }
 
-   return t;
+  return t;
 }
 
-void setp_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t a, b;
+void setp_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b;
 
-   int t=0;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+  int t = 0;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   assert( pI->get_num_operands() < 4 ); // or need to deal with "c" operand / boolOp
+  assert(pI->get_num_operands() <
+         4);  // or need to deal with "c" operand / boolOp
 
-   unsigned type = pI->get_type();
-   unsigned cmpop = pI->get_cmpop();
-   a = thread->get_operand_value(src1, dst, type, thread, 1);
-   b = thread->get_operand_value(src2, dst, type, thread, 1);
+  unsigned type = pI->get_type();
+  unsigned cmpop = pI->get_cmpop();
+  a = thread->get_operand_value(src1, dst, type, thread, 1);
+  b = thread->get_operand_value(src2, dst, type, thread, 1);
 
-   t = CmpOp(type,a,b,cmpop);
+  t = CmpOp(type, a, b, cmpop);
 
-   ptx_reg_t data;
+  ptx_reg_t data;
 
-   //the way ptxplus handles the zero flag, 1 = false and 0 = true
-   data.pred = (t==0); //inverting predicate since ptxplus uses "1" for a set zero flag
+  // the way ptxplus handles the zero flag, 1 = false and 0 = true
+  data.pred =
+      (t ==
+       0);  // inverting predicate since ptxplus uses "1" for a set zero flag
 
-   thread->set_operand_value(dst,data, PRED_TYPE, thread, pI);
+  thread->set_operand_value(dst, data, PRED_TYPE, thread, pI);
 }
 
-void set_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, b;
+void set_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b;
 
-   int t=0;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+  int t = 0;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   assert( pI->get_num_operands() < 4 ); // or need to deal with "c" operand / boolOp
+  assert(pI->get_num_operands() <
+         4);  // or need to deal with "c" operand / boolOp
 
-   unsigned src_type = pI->get_type2();
-   unsigned cmpop = pI->get_cmpop();
+  unsigned src_type = pI->get_type2();
+  unsigned cmpop = pI->get_cmpop();
 
-   a = thread->get_operand_value(src1, dst, src_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, src_type, thread, 1);
+  a = thread->get_operand_value(src1, dst, src_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, src_type, thread, 1);
 
-   // Take abs of first operand if needed
-   if(pI->is_abs()) {
-      switch ( src_type ) {
-      case S16_TYPE: a.s16 = my_abs(a.s16); break;
-      case S32_TYPE: a.s32 = my_abs(a.s32); break;
-      case S64_TYPE: a.s64 = my_abs(a.s64); break;
-      case U16_TYPE: a.u16 = a.u16; break;
-      case U32_TYPE: a.u32 = my_abs(a.u32); break;
-      case U64_TYPE: a.u64 = my_abs(a.u64); break;
-      case F32_TYPE: a.f32 = my_abs(a.f32); break;
-      case F64_TYPE: case FF64_TYPE: a.f64 = my_abs(a.f64); break;
+  // Take abs of first operand if needed
+  if (pI->is_abs()) {
+    switch (src_type) {
+      case S16_TYPE:
+        a.s16 = my_abs(a.s16);
+        break;
+      case S32_TYPE:
+        a.s32 = my_abs(a.s32);
+        break;
+      case S64_TYPE:
+        a.s64 = my_abs(a.s64);
+        break;
+      case U16_TYPE:
+        a.u16 = a.u16;
+        break;
+      case U32_TYPE:
+        a.u32 = my_abs(a.u32);
+        break;
+      case U64_TYPE:
+        a.u64 = my_abs(a.u64);
+        break;
+      case F32_TYPE:
+        a.f32 = my_abs(a.f32);
+        break;
+      case F64_TYPE:
+      case FF64_TYPE:
+        a.f64 = my_abs(a.f64);
+        break;
       default:
-         printf("Execution error: type mismatch with instruction\n");
-         assert(0);
-         break;
-      }
-   }
-
-   t = CmpOp(src_type,a,b,cmpop);
+        printf("Execution error: type mismatch with instruction\n");
+        assert(0);
+        break;
+    }
+  }
 
-   ptx_reg_t data;
-   if ( isFloat(pI->get_type()) ) {
-      data.f32 = (t!=0)?1.0f:0.0f;
-   } else {
-      data.u32 = (t!=0)?0xFFFFFFFF:0;
-   }
+  t = CmpOp(src_type, a, b, cmpop);
 
-   thread->set_operand_value(dst, data, pI->get_type(), thread, pI);
+  ptx_reg_t data;
+  if (isFloat(pI->get_type())) {
+    data.f32 = (t != 0) ? 1.0f : 0.0f;
+  } else {
+    data.u32 = (t != 0) ? 0xFFFFFFFF : 0;
+  }
 
+  thread->set_operand_value(dst, data, pI->get_type(), thread, pI);
 }
 
-void shfl_impl( const ptx_instruction *pI, core_t *core, warp_inst_t inst )
-{
-	unsigned i_type = pI->get_type();
-  	int tid;
+void shfl_impl(const ptx_instruction *pI, core_t *core, warp_inst_t inst) {
+  unsigned i_type = pI->get_type();
+  int tid;
+
+  if (core->get_gpu()->is_functional_sim())
+    tid = inst.warp_id_func() * core->get_warp_size();
+  else
+    tid = inst.warp_id() * core->get_warp_size();
+
+  ptx_thread_info *thread = core->get_thread_info()[tid];
+  ptx_warp_info *warp_info = thread->m_warp_info;
+  int lane = warp_info->get_done_threads();
+  thread = core->get_thread_info()[tid + lane];
+
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+  int bval = (thread->get_operand_value(src2, dst, i_type, thread, 1)).u32;
+  int cval = (thread->get_operand_value(src3, dst, i_type, thread, 1)).u32;
+  int mask = cval >> 8;
+  bval &= 0x1F;
+  cval &= 0x1F;
+
+  int maxLane = (lane & mask) | (cval & ~mask);
+  int minLane = lane & mask;
+
+  int src_idx;
+  unsigned p;
+  switch (pI->shfl_op()) {
+    case UP_OPTION:
+      src_idx = lane - bval;
+      p = (src_idx >= maxLane);
+      break;
+    case DOWN_OPTION:
+      src_idx = lane + bval;
+      p = (src_idx <= maxLane);
+      break;
+    case BFLY_OPTION:
+      src_idx = lane ^ bval;
+      p = (src_idx <= maxLane);
+      break;
+    case IDX_OPTION:
+      src_idx = minLane | (bval & ~mask);
+      p = (src_idx <= maxLane);
+      break;
+    default:
+      printf("GPGPU-Sim PTX: ERROR: Invalid shfl option\n");
+      assert(0);
+      break;
+  }
+  // copy from own lane
+  if (!p) src_idx = lane;
+
+  // copy input from lane src_idx
+  ptx_reg_t data;
+  if (inst.active(src_idx)) {
+    ptx_thread_info *source = core->get_thread_info()[tid + src_idx];
+    data = source->get_operand_value(src1, dst, i_type, source, 1);
+  } else {
+    printf(
+        "GPGPU-Sim PTX: WARNING: shfl input value unpredictable for inactive "
+        "threads in a warp\n");
+    data.u32 = 0;
+  }
+  thread->set_operand_value(dst, data, i_type, thread, pI);
+
+  /*
+  TODO: deal with predicates appropriately using the following pseudocode:
+  if (!isGuardPredicateTrue(src_idx)) {
+          printf("GPGPU-Sim PTX: WARNING: shfl input value unpredictable for
+  predicated-off threads in a warp\n");
+  }
+  if (dest predicate selected) data.pred = p;
+  */
+
+  // keep track of the number of threads that have executed in the warp
+  warp_info->inc_done_threads();
+  if (warp_info->get_done_threads() == inst.active_count()) {
+    warp_info->reset_done_threads();
+  }
+}
 
-  	if(core->get_gpu()->is_functional_sim())
-    		tid = inst.warp_id_func() * core->get_warp_size();
-  	else
-	 	tid = inst.warp_id() * core->get_warp_size();
-	
-	ptx_thread_info *thread = core->get_thread_info()[tid];
-	ptx_warp_info *warp_info = thread->m_warp_info;
-	int lane = warp_info->get_done_threads();
-	thread = core->get_thread_info()[tid + lane];
-
-	const operand_info &dst = pI->dst();
-	const operand_info &src1 = pI->src1();
-	const operand_info &src2 = pI->src2();
-	const operand_info &src3 = pI->src3();
-	int bval = (thread->get_operand_value(src2, dst, i_type, thread, 1)).u32;
-	int cval = (thread->get_operand_value(src3, dst, i_type, thread, 1)).u32;
-	int mask = cval >> 8;
-	bval &= 0x1F;
-	cval &= 0x1F;
-
-	int maxLane = (lane & mask) | (cval & ~mask);
-	int minLane = lane & mask;
-
-	int src_idx;
-	unsigned p;
-	switch(pI->shfl_op()) {
-	case UP_OPTION:
-		src_idx = lane - bval;
-		p = (src_idx >= maxLane);
-		break;
-	case DOWN_OPTION:
-		src_idx = lane + bval;
-		p = (src_idx <= maxLane);
-		break;
-	case BFLY_OPTION:
-		src_idx = lane ^ bval;
-		p = (src_idx <= maxLane);
-		break;
-	case IDX_OPTION:
-		src_idx = minLane | (bval & ~mask);
-		p = (src_idx <= maxLane);
-		break;
-	default:
-		printf("GPGPU-Sim PTX: ERROR: Invalid shfl option\n");
-		assert(0);
-		break;
-	}
-	// copy from own lane
-	if (!p) src_idx = lane;
-
-	// copy input from lane src_idx
-	ptx_reg_t data;
-	if (inst.active(src_idx)) {
-		ptx_thread_info *source = core->get_thread_info()[tid + src_idx];
-		data = source->get_operand_value(src1, dst, i_type, source, 1);
-	} else {
-		printf("GPGPU-Sim PTX: WARNING: shfl input value unpredictable for inactive threads in a warp\n");
-		data.u32 = 0;
-	}
-	thread->set_operand_value(dst, data, i_type, thread, pI);
-
-	/*
-	TODO: deal with predicates appropriately using the following pseudocode:
-	if (!isGuardPredicateTrue(src_idx)) {
-		printf("GPGPU-Sim PTX: WARNING: shfl input value unpredictable for predicated-off threads in a warp\n");
-	}
-	if (dest predicate selected) data.pred = p;
-	*/
-
-	// keep track of the number of threads that have executed in the warp
-	warp_info->inc_done_threads();
-	if (warp_info->get_done_threads() == inst.active_count()) {
-		warp_info->reset_done_threads();
-	}
-}
-
-void shl_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t a, b, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-
-   switch ( i_type ) {
-   case B16_TYPE:
-   case U16_TYPE:
-      if ( b.u16 >= 16 )
-         d.u16 = 0;
+void shl_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+
+  switch (i_type) {
+    case B16_TYPE:
+    case U16_TYPE:
+      if (b.u16 >= 16)
+        d.u16 = 0;
       else
-         d.u16 = (unsigned short) ((a.u16 << b.u16) & 0xFFFF); 
+        d.u16 = (unsigned short)((a.u16 << b.u16) & 0xFFFF);
       break;
-   case B32_TYPE: 
-   case U32_TYPE: 
-      if ( b.u32 >= 32 )
-         d.u32 = 0;
+    case B32_TYPE:
+    case U32_TYPE:
+      if (b.u32 >= 32)
+        d.u32 = 0;
       else
-         d.u32 = (unsigned) ((a.u32 << b.u32) & 0xFFFFFFFF); 
+        d.u32 = (unsigned)((a.u32 << b.u32) & 0xFFFFFFFF);
       break;
-   case B64_TYPE: 
-   case U64_TYPE: 
-      if ( b.u32 >= 64 )
-         d.u64 = 0;
+    case B64_TYPE:
+    case U64_TYPE:
+      if (b.u32 >= 64)
+        d.u64 = 0;
       else
-         d.u64 = (a.u64 << b.u64); 
+        d.u64 = (a.u64 << b.u64);
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst, d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void shr_impl( const ptx_instruction *pI, ptx_thread_info *thread )
-{
-   ptx_reg_t a, b, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-
-
-   switch ( i_type ) {
-   case U16_TYPE:
-   case B16_TYPE: 
-      if ( b.u16 < 16 )
-         d.u16 = (unsigned short) ((a.u16 >> b.u16) & 0xFFFF);
+void shr_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, b, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+
+  switch (i_type) {
+    case U16_TYPE:
+    case B16_TYPE:
+      if (b.u16 < 16)
+        d.u16 = (unsigned short)((a.u16 >> b.u16) & 0xFFFF);
       else
-         d.u16 = 0;
+        d.u16 = 0;
       break;
-   case U32_TYPE:
-   case B32_TYPE: 
-      if ( b.u32 < 32 )
-         d.u32 = (unsigned) ((a.u32 >> b.u32) & 0xFFFFFFFF);
+    case U32_TYPE:
+    case B32_TYPE:
+      if (b.u32 < 32)
+        d.u32 = (unsigned)((a.u32 >> b.u32) & 0xFFFFFFFF);
       else
-         d.u32 = 0;
+        d.u32 = 0;
       break;
-   case U64_TYPE:
-   case B64_TYPE: 
-      if ( b.u32 < 64 )
-         d.u64 = (a.u64 >> b.u64);
+    case U64_TYPE:
+    case B64_TYPE:
+      if (b.u32 < 64)
+        d.u64 = (a.u64 >> b.u64);
       else
-         d.u64 = 0;
+        d.u64 = 0;
       break;
-   case S16_TYPE: 
-      if ( b.u16 < 16 )
-         d.s64 = (a.s16 >> b.s16);
+    case S16_TYPE:
+      if (b.u16 < 16)
+        d.s64 = (a.s16 >> b.s16);
       else {
-         if ( a.s16 < 0 ) {
-            d.s64 = -1;
-         } else {
-            d.s64 = 0;
-         }
+        if (a.s16 < 0) {
+          d.s64 = -1;
+        } else {
+          d.s64 = 0;
+        }
       }
       break;
-   case S32_TYPE: 
-      if ( b.u32 < 32 )
-         d.s64 = (a.s32 >> b.s32);
+    case S32_TYPE:
+      if (b.u32 < 32)
+        d.s64 = (a.s32 >> b.s32);
       else {
-         if ( a.s32 < 0 ) {
-            d.s64 = -1;
-         } else {
-            d.s64 = 0;
-         }
+        if (a.s32 < 0) {
+          d.s64 = -1;
+        } else {
+          d.s64 = 0;
+        }
       }
       break;
-   case S64_TYPE: 
-      if ( b.u64 < 64 )
-         d.s64 = (a.s64 >> b.u64);
+    case S64_TYPE:
+      if (b.u64 < 64)
+        d.s64 = (a.s64 >> b.u64);
       else {
-         if ( a.s64 < 0 ) {
-            if ( b.s32 < 0 ) {
-               d.u64 = -1;
-               d.s32 = 0;
-            } else {
-               d.s64 = -1;
-            }
-         } else {
-            d.s64 = 0;
-         }
+        if (a.s64 < 0) {
+          if (b.s32 < 0) {
+            d.u64 = -1;
+            d.s32 = 0;
+          } else {
+            d.s64 = -1;
+          }
+        } else {
+          d.s64 = 0;
+        }
       }
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void sin_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t a, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+void sin_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-   switch ( i_type ) {
-   case F32_TYPE: 
+  switch (i_type) {
+    case F32_TYPE:
       d.f32 = sin(a.f32);
       break;
-   default:
+    default:
       printf("Execution error: type mismatch with instruction\n");
-      assert(0); 
+      assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void slct_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
-   const operand_info &src3 = pI->src3();
+void slct_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
 
-   ptx_reg_t a, b, c, d;
+  ptx_reg_t a, b, c, d;
 
-   unsigned i_type = pI->get_type();
-   unsigned c_type = pI->get_type2();
-   bool t = false;
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   b = thread->get_operand_value(src2, dst, i_type, thread, 1);
-   c = thread->get_operand_value(src3, dst, c_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  unsigned c_type = pI->get_type2();
+  bool t = false;
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  c = thread->get_operand_value(src3, dst, c_type, thread, 1);
 
-   switch ( c_type ) {
-   case S32_TYPE: t = c.s32 >= 0; break;
-   case F32_TYPE: t = c.f32 >= 0; break;
-   default: assert(0);
-   }
+  switch (c_type) {
+    case S32_TYPE:
+      t = c.s32 >= 0;
+      break;
+    case F32_TYPE:
+      t = c.f32 >= 0;
+      break;
+    default:
+      assert(0);
+  }
 
-   switch ( i_type ) {
-   case B16_TYPE:
-   case S16_TYPE:
-   case U16_TYPE: d.u16 = t?a.u16:b.u16; break;
-   case F32_TYPE:
-   case B32_TYPE:
-   case S32_TYPE:
-   case U32_TYPE: d.u32 = t?a.u32:b.u32; break;
-   case F64_TYPE:
-   case FF64_TYPE:
-   case B64_TYPE:
-   case S64_TYPE:
-   case U64_TYPE: d.u64 = t?a.u64:b.u64; break;
-   default: assert(0);
-   }
+  switch (i_type) {
+    case B16_TYPE:
+    case S16_TYPE:
+    case U16_TYPE:
+      d.u16 = t ? a.u16 : b.u16;
+      break;
+    case F32_TYPE:
+    case B32_TYPE:
+    case S32_TYPE:
+    case U32_TYPE:
+      d.u32 = t ? a.u32 : b.u32;
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+    case B64_TYPE:
+    case S64_TYPE:
+    case U64_TYPE:
+      d.u64 = t ? a.u64 : b.u64;
+      break;
+    default:
+      assert(0);
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
+  thread->set_operand_value(dst, d, i_type, thread, pI);
 }
 
-void sqrt_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t a, d;
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
+void sqrt_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t a, d;
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
 
-   unsigned i_type = pI->get_type();
-   a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
 
-
-   switch ( i_type ) {
-   case F32_TYPE: 
-      if ( a.f32 < 0 )
-         d.f32 = nanf("");
+  switch (i_type) {
+    case F32_TYPE:
+      if (a.f32 < 0)
+        d.f32 = nanf("");
       else
-         d.f32 = sqrt(a.f32); break;
-   case F64_TYPE: 
-   case FF64_TYPE:
-      if ( a.f64 < 0 )
-         d.f64 = nan("");
+        d.f32 = sqrt(a.f32);
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      if (a.f64 < 0)
+        d.f64 = nan("");
       else
-         d.f64 = sqrt(a.f64); break;
-   default:
+        d.f64 = sqrt(a.f64);
+      break;
+    default:
       printf("Execution error: type mismatch with instruction\n");
       assert(0);
       break;
-   }
+  }
 
-   thread->set_operand_value(dst,d, i_type, thread, pI);
-}
-
-void sst_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-	ptx_instruction * cpI = const_cast<ptx_instruction *>(pI); // constant
-	const operand_info &dst = cpI->dst();
-	const operand_info &src1 = pI->src1();
-	const operand_info &src2 = pI->src2();
-	const operand_info &src3 = pI->src3();
-	unsigned type = pI->get_type();
-	ptx_reg_t dst_data = thread->get_operand_value(dst, dst, type, thread, 1);
-	ptx_reg_t src1_data = thread->get_operand_value(src1, src1, type, thread, 1);
-	ptx_reg_t src2_data = thread->get_operand_value(src2, src1, type, thread, 1);
-	ptx_reg_t src3_data = thread->get_operand_value(src3, src1, type, thread, 1);
-	memory_space_t space = pI->get_space();
-	memory_space *mem = NULL;
-	addr_t addr = src2_data.u32 * 4; // this assumes sstarr memory starts at address 0
-	ptx_cta_info *cta_info = thread->m_cta_info;
-
-	decode_space(space,thread,src1,mem,addr);
-
-	size_t size;
-	int t;
-	type_info_key::type_decode(type,size,t);
-
-	// store data in sstarr memory
-	mem->write(addr,size/8,&src3_data.s64,thread,pI);
-
-	// sync threads
-	cpI->set_bar_id(16); // use 16 for sst because bar uses an int from 0-15
-
-	thread->m_last_effective_address = addr;
-	thread->m_last_memory_space = space;
-	thread->m_last_dram_callback.function = bar_callback;
-	thread->m_last_dram_callback.instruction = cpI;
-
-	// the last thread that executes loads all of the data back from sstarr memory
-	int NUM_THREADS = cta_info->num_threads();
-	cta_info->inc_bar_threads();
-	if (NUM_THREADS == cta_info->get_bar_threads()) {
-		unsigned offset = 0;
-		addr = 0;
-		ptx_reg_t data;
-		float sstarr_fdata[NUM_THREADS];
-		signed long long sstarr_ldata[NUM_THREADS];
-		// loop through all of the threads
-		for (int tid = 0; tid < NUM_THREADS; tid++) {
-			data.u64=0;
-			mem->read(addr+(tid*4),size/8,&data.s64);
-			sstarr_fdata[tid] = data.f32;
-			sstarr_ldata[tid] = data.s64;
-		}
-
-		// squeeze the zeros out of the array and store data back into original array
-		mem = NULL;
-		addr = src1_data.u32;
-		space.set_type(global_space);
-		decode_space(space,thread,src1,mem,addr);
-		// store nonzero entries and indices
-		for (int tid = 0; tid < NUM_THREADS; tid++) {
-			if (sstarr_fdata[tid] != 0) {
-				float ftid = (float)tid;
-				mem->write(addr+(offset*4),size/8,&sstarr_ldata[tid],thread,pI);
-				mem->write(addr+((NUM_THREADS+offset)*4),size/8,&ftid,thread,pI);
-				offset++;
-			}
-		}
-		// store the number of nonzero elements in the array
-		data = thread->get_operand_value(src1, dst, type, thread, 1);
-		data.s64 += 4*(offset-1);
-		thread->set_operand_value(dst, data, type, thread, pI);
-
-		// fill the rest of the array with zeros (dst should always have a 0 in it)
-		while (offset < NUM_THREADS) {
-			mem->write(addr+(offset*4),size/8,&dst_data.s64,thread,pI);
-			offset++;
-		}
-
-		cta_info->reset_bar_threads();
-		thread->m_last_effective_address = addr+(NUM_THREADS-1)*4;
-		thread->m_last_memory_space = space;
-	}
-}
-
-void ssy_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   //printf("Execution Warning: unimplemented ssy instruction is treated as a nop\n");
-   // TODO: add implementation
-}
-
-void st_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   const operand_info &dst = pI->dst();
-   const operand_info &src1 = pI->src1(); //may be scalar or vector of regs
-   unsigned type = pI->get_type();
-   ptx_reg_t addr_reg = thread->get_operand_value(dst, dst, type, thread, 1);
-   ptx_reg_t data;
-   memory_space_t space = pI->get_space();
-   unsigned vector_spec = pI->get_vector();
-
-   memory_space *mem = NULL;
-   addr_t addr = addr_reg.u32;
-
-   decode_space(space,thread,dst,mem,addr);
-   thread->get_gpu()->gem5CudaGPU->getCudaCore(thread->get_hw_sid())->record_st(space);
-   if (space.get_type() != global_space &&
-       space.get_type() != const_space &&
-       space.get_type() != local_space) {
+  thread->set_operand_value(dst, d, i_type, thread, pI);
+}
 
-   size_t size;
-   int t;
-   type_info_key::type_decode(type,size,t);
+void sst_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_instruction *cpI = const_cast<ptx_instruction *>(pI);  // constant
+  const operand_info &dst = cpI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
+  const operand_info &src3 = pI->src3();
+  unsigned type = pI->get_type();
+  ptx_reg_t dst_data = thread->get_operand_value(dst, dst, type, thread, 1);
+  ptx_reg_t src1_data = thread->get_operand_value(src1, src1, type, thread, 1);
+  ptx_reg_t src2_data = thread->get_operand_value(src2, src1, type, thread, 1);
+  ptx_reg_t src3_data = thread->get_operand_value(src3, src1, type, thread, 1);
+  memory_space_t space = pI->get_space();
+  memory_space *mem = NULL;
+  addr_t addr =
+      src2_data.u32 * 4;  // this assumes sstarr memory starts at address 0
+  ptx_cta_info *cta_info = thread->m_cta_info;
+
+  decode_space(space, thread, src1, mem, addr);
+
+  size_t size;
+  int t;
+  type_info_key::type_decode(type, size, t);
+
+  // store data in sstarr memory
+  mem->write(addr, size / 8, &src3_data.s64, thread, pI);
+
+  // sync threads
+  cpI->set_bar_id(16);  // use 16 for sst because bar uses an int from 0-15
+
+  thread->m_last_effective_address = addr;
+  thread->m_last_memory_space = space;
+  thread->m_last_dram_callback.function = bar_callback;
+  thread->m_last_dram_callback.instruction = cpI;
+
+  // the last thread that executes loads all of the data back from sstarr memory
+  int NUM_THREADS = cta_info->num_threads();
+  cta_info->inc_bar_threads();
+  if (NUM_THREADS == cta_info->get_bar_threads()) {
+    unsigned offset = 0;
+    addr = 0;
+    ptx_reg_t data;
+    float sstarr_fdata[NUM_THREADS];
+    signed long long sstarr_ldata[NUM_THREADS];
+    // loop through all of the threads
+    for (int tid = 0; tid < NUM_THREADS; tid++) {
+      data.u64 = 0;
+      mem->read(addr + (tid * 4), size / 8, &data.s64);
+      sstarr_fdata[tid] = data.f32;
+      sstarr_ldata[tid] = data.s64;
+    }
 
-   if (!vector_spec) {
-      data = thread->get_operand_value(src1, dst, type, thread, 1);
-      mem->write(addr,size/8,&data.s64,thread,pI);
-    } else {
-      if (vector_spec == V2_TYPE) {
-         ptx_reg_t* ptx_regs = new ptx_reg_t[2]; 
-         thread->get_vector_operand_values(src1, ptx_regs, 2); 
-         mem->write(addr,size/8,&ptx_regs[0].s64,thread,pI);
-         mem->write(addr+size/8,size/8,&ptx_regs[1].s64,thread,pI);
-         delete [] ptx_regs;
-      }
-      if (vector_spec == V3_TYPE) {
-         ptx_reg_t* ptx_regs = new ptx_reg_t[3]; 
-         thread->get_vector_operand_values(src1, ptx_regs, 3); 
-         mem->write(addr,size/8,&ptx_regs[0].s64,thread,pI);
-         mem->write(addr+size/8,size/8,&ptx_regs[1].s64,thread,pI);
-         mem->write(addr+2*size/8,size/8,&ptx_regs[2].s64,thread,pI);
-         delete [] ptx_regs;
+    // squeeze the zeros out of the array and store data back into original
+    // array
+    mem = NULL;
+    addr = src1_data.u32;
+    space.set_type(global_space);
+    decode_space(space, thread, src1, mem, addr);
+    // store nonzero entries and indices
+    for (int tid = 0; tid < NUM_THREADS; tid++) {
+      if (sstarr_fdata[tid] != 0) {
+        float ftid = (float)tid;
+        mem->write(addr + (offset * 4), size / 8, &sstarr_ldata[tid], thread,
+                   pI);
+        mem->write(addr + ((NUM_THREADS + offset) * 4), size / 8, &ftid, thread,
+                   pI);
+        offset++;
       }
-      if (vector_spec == V4_TYPE) {
-         ptx_reg_t* ptx_regs = new ptx_reg_t[4]; 
-         thread->get_vector_operand_values(src1, ptx_regs, 4); 
-         mem->write(addr,size/8,&ptx_regs[0].s64,thread,pI);
-         mem->write(addr+size/8,size/8,&ptx_regs[1].s64,thread,pI);
-         mem->write(addr+2*size/8,size/8,&ptx_regs[2].s64,thread,pI);
-         mem->write(addr+3*size/8,size/8,&ptx_regs[3].s64,thread,pI);
-         delete [] ptx_regs;
-      }
-   }
+    }
+    // store the number of nonzero elements in the array
+    data = thread->get_operand_value(src1, dst, type, thread, 1);
+    data.s64 += 4 * (offset - 1);
+    thread->set_operand_value(dst, data, type, thread, pI);
+
+    // fill the rest of the array with zeros (dst should always have a 0 in it)
+    while (offset < NUM_THREADS) {
+      mem->write(addr + (offset * 4), size / 8, &dst_data.s64, thread, pI);
+      offset++;
+    }
+
+    cta_info->reset_bar_threads();
+    thread->m_last_effective_address = addr + (NUM_THREADS - 1) * 4;
+    thread->m_last_memory_space = space;
+  }
+}
+
+void ssy_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  // printf("Execution Warning: unimplemented ssy instruction is treated as a
+  // nop\n");
+  // TODO: add implementation
+}
+
+void st_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();  // may be scalar or vector of regs
+  unsigned type = pI->get_type();
+  ptx_reg_t addr_reg = thread->get_operand_value(dst, dst, type, thread, 1);
+  ptx_reg_t data;
+  memory_space_t space = pI->get_space();
+  unsigned vector_spec = pI->get_vector();
+
+  memory_space *mem = NULL;
+  addr_t addr = addr_reg.u32;
+
+  decode_space(space, thread, dst, mem, addr);
+  thread->get_gpu()->gem5CudaGPU->getCudaCore(thread->get_hw_sid())->record_st(space);
+  if (space.get_type() != global_space &&
+       space.get_type() != const_space &&
+       space.get_type() != local_space) {
+
+  size_t size;
+  int t;
+  type_info_key::type_decode(type, size, t);
+
+  if (!vector_spec) {
+    data = thread->get_operand_value(src1, dst, type, thread, 1);
+    mem->write(addr, size / 8, &data.s64, thread, pI);
+  } else {
+    if (vector_spec == V2_TYPE) {
+      ptx_reg_t *ptx_regs = new ptx_reg_t[2];
+      thread->get_vector_operand_values(src1, ptx_regs, 2);
+      mem->write(addr, size / 8, &ptx_regs[0].s64, thread, pI);
+      mem->write(addr + size / 8, size / 8, &ptx_regs[1].s64, thread, pI);
+      delete[] ptx_regs;
+    }
+    if (vector_spec == V3_TYPE) {
+      ptx_reg_t *ptx_regs = new ptx_reg_t[3];
+      thread->get_vector_operand_values(src1, ptx_regs, 3);
+      mem->write(addr, size / 8, &ptx_regs[0].s64, thread, pI);
+      mem->write(addr + size / 8, size / 8, &ptx_regs[1].s64, thread, pI);
+      mem->write(addr + 2 * size / 8, size / 8, &ptx_regs[2].s64, thread, pI);
+      delete[] ptx_regs;
+    }
+    if (vector_spec == V4_TYPE) {
+      ptx_reg_t *ptx_regs = new ptx_reg_t[4];
+      thread->get_vector_operand_values(src1, ptx_regs, 4);
+      mem->write(addr, size / 8, &ptx_regs[0].s64, thread, pI);
+      mem->write(addr + size / 8, size / 8, &ptx_regs[1].s64, thread, pI);
+      mem->write(addr + 2 * size / 8, size / 8, &ptx_regs[2].s64, thread, pI);
+      mem->write(addr + 3 * size / 8, size / 8, &ptx_regs[3].s64, thread, pI);
+      delete[] ptx_regs;
+    }
    }
-   thread->m_last_effective_address = addr;
-   thread->m_last_memory_space = space; 
+  }
+  thread->m_last_effective_address = addr;
+  thread->m_last_memory_space = space;
 }
 
-void sub_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   ptx_reg_t data;
-   int overflow = 0;
-   int carry = 0;
+void sub_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t data;
+  int overflow = 0;
+  int carry = 0;
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   ptx_reg_t src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   ptx_reg_t src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  ptx_reg_t src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  ptx_reg_t src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   //performs addition. Sets carry and overflow if needed.
-   //the constant is added in during subtraction so the carry bit is set properly.
-   switch ( i_type ) {
-   case S8_TYPE:
+  // performs addition. Sets carry and overflow if needed.
+  // the constant is added in during subtraction so the carry bit is set
+  // properly.
+  switch (i_type) {
+    case S8_TYPE:
       data.s64 = (src1_data.s64 & 0xFF) - (src2_data.s64 & 0xFF) + 0x100;
-      if(((src1_data.s64 & 0x80)-(src2_data.s64 & 0x80)) != 0) {overflow=((src1_data.s64 & 0x80)-(data.s64 & 0x80))==0?0:1; }
-      carry = (data.s32 & 0x100)>>8;
+      if (((src1_data.s64 & 0x80) - (src2_data.s64 & 0x80)) != 0) {
+        overflow = ((src1_data.s64 & 0x80) - (data.s64 & 0x80)) == 0 ? 0 : 1;
+      }
+      carry = (data.s32 & 0x100) >> 8;
       break;
-   case S16_TYPE:
+    case S16_TYPE:
       data.s64 = (src1_data.s64 & 0xFFFF) - (src2_data.s64 & 0xFFFF) + 0x10000;
-      if(((src1_data.s64 & 0x8000)-(src2_data.s64 & 0x8000)) != 0) {overflow=((src1_data.s64 & 0x8000)-(data.s64 & 0x8000))==0?0:1; }
-      carry = (data.s32 & 0x10000)>>16;
-      break;
-   case S32_TYPE:
-      data.s64 = (src1_data.s64 & 0xFFFFFFFF) - (src2_data.s64 & 0xFFFFFFFF) + 0x100000000;
-      if(((src1_data.s64 & 0x80000000)-(src2_data.s64 & 0x80000000)) != 0) {overflow=((src1_data.s64 & 0x80000000)-(data.s64 & 0x80000000))==0?0:1; }
-      carry = ((data.u64)>>32) & 0x0001;
-      break;
-   case S64_TYPE: 
-      data.s64 = src1_data.s64 - src2_data.s64; break;
-   case B8_TYPE:
-   case U8_TYPE:
+      if (((src1_data.s64 & 0x8000) - (src2_data.s64 & 0x8000)) != 0) {
+        overflow =
+            ((src1_data.s64 & 0x8000) - (data.s64 & 0x8000)) == 0 ? 0 : 1;
+      }
+      carry = (data.s32 & 0x10000) >> 16;
+      break;
+    case S32_TYPE:
+      data.s64 = (src1_data.s64 & 0xFFFFFFFF) - (src2_data.s64 & 0xFFFFFFFF) +
+                 0x100000000;
+      if (((src1_data.s64 & 0x80000000) - (src2_data.s64 & 0x80000000)) != 0) {
+        overflow = ((src1_data.s64 & 0x80000000) - (data.s64 & 0x80000000)) == 0
+                       ? 0
+                       : 1;
+      }
+      carry = ((data.u64) >> 32) & 0x0001;
+      break;
+    case S64_TYPE:
+      data.s64 = src1_data.s64 - src2_data.s64;
+      break;
+    case B8_TYPE:
+    case U8_TYPE:
       data.u64 = (src1_data.u64 & 0xFF) - (src2_data.u64 & 0xFF) + 0x100;
-      carry = (data.u64 & 0x100)>>8;
+      carry = (data.u64 & 0x100) >> 8;
       break;
-   case B16_TYPE:
-   case U16_TYPE:
+    case B16_TYPE:
+    case U16_TYPE:
       data.u64 = (src1_data.u64 & 0xFFFF) - (src2_data.u64 & 0xFFFF) + 0x10000;
-      carry = (data.u64 & 0x10000)>>16;
-      break;
-   case B32_TYPE:
-   case U32_TYPE:
-      data.u64 = (src1_data.u64 & 0xFFFFFFFF) - (src2_data.u64 & 0xFFFFFFFF) + 0x100000000;
-      carry = (data.u64 & 0x100000000)>>32;
-      break;
-   case B64_TYPE:
-   case U64_TYPE: 
-      data.u64 = src1_data.u64 - src2_data.u64; break;
-   case F16_TYPE: data.f16 = src1_data.f16 - src2_data.f16; break;//assert(0); break;
-   case F32_TYPE: data.f32 = src1_data.f32 - src2_data.f32; break;
-   case F64_TYPE: case FF64_TYPE: data.f64 = src1_data.f64 - src2_data.f64; break;
-   default: assert(0); break;
-   }
+      carry = (data.u64 & 0x10000) >> 16;
+      break;
+    case B32_TYPE:
+    case U32_TYPE:
+      data.u64 = (src1_data.u64 & 0xFFFFFFFF) - (src2_data.u64 & 0xFFFFFFFF) +
+                 0x100000000;
+      carry = (data.u64 & 0x100000000) >> 32;
+      break;
+    case B64_TYPE:
+    case U64_TYPE:
+      data.u64 = src1_data.u64 - src2_data.u64;
+      break;
+    case F16_TYPE:
+      data.f16 = src1_data.f16 - src2_data.f16;
+      break;  // assert(0); break;
+    case F32_TYPE:
+      data.f32 = src1_data.f32 - src2_data.f32;
+      break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      data.f64 = src1_data.f64 - src2_data.f64;
+      break;
+    default:
+      assert(0);
+      break;
+  }
 
-   thread->set_operand_value(dst,data, i_type, thread, pI, overflow, carry);
+  thread->set_operand_value(dst, data, i_type, thread, pI, overflow, carry);
 }
 
-void nop_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   // Do nothing
+void nop_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  // Do nothing
 }
 
-void subc_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void suld_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void sured_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void sust_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void suq_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-
-ptx_reg_t* ptx_tex_regs = NULL;
+void subc_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void suld_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void sured_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void sust_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void suq_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
 
 union intfloat {
-   int a;
-   float b;
+  int a;
+  float b;
 };
 
-float reduce_precision( float x, unsigned bits )
-{
-   intfloat tmp;
-   tmp.b = x;
-   int v = tmp.a;
-   int man = v & ((1<<23)-1);
-   int mask =  ((1<<bits)-1) << (23-bits);
-   int nv = (v & ((-1)-((1<<23)-1))) | (mask&man);
-   tmp.a = nv;
-   float result = tmp.b;
-   return result;
-}
-
-unsigned wrap( unsigned x, unsigned y, unsigned mx, unsigned my, size_t elem_size )
-{
-   unsigned nx = (mx+x)%mx;
-   unsigned ny = (my+y)%my;
-   return nx + mx*ny;
-}
-
-unsigned clamp( unsigned x, unsigned y, unsigned mx, unsigned my, size_t elem_size )
-{
-   unsigned nx = x;
-   while (nx >= mx) nx -= elem_size;
-   unsigned ny = (y >= my)? my - 1 : y;
-   return nx + mx*ny;
-}
-
-typedef unsigned (*texAddr_t) (unsigned x, unsigned y, unsigned mx, unsigned my, size_t elem_size);
-float tex_linf_sampling(memory_space* mem, unsigned tex_array_base, 
-                        int x, int y, unsigned int width, unsigned int height, size_t elem_size,
-                        float alpha, float beta, texAddr_t b_lim)
-{
-   float Tij;
-   float Ti1j;
-   float Tij1;
-   float Ti1j1;
-
-   mem->read(tex_array_base + b_lim(x,y,width,height,elem_size), 4, &Tij);
-   mem->read(tex_array_base + b_lim(x+elem_size,y,width,height,elem_size), 4, &Ti1j);
-   mem->read(tex_array_base + b_lim(x,y+1,width,height,elem_size), 4, &Tij1);
-   mem->read(tex_array_base + b_lim(x+elem_size,y+1,width,height,elem_size), 4, &Ti1j1);
-
-   float sample = (1-alpha)*(1-beta)*Tij + 
-                   alpha*(1-beta)*Ti1j +
-                   (1-alpha)*beta*Tij1 +
-                   alpha*beta*Ti1j1;
-   
-   return sample;
-}
-
-float textureNormalizeElementSigned(int element, int bits)
-{
-   if (bits) {
-      int maxN = (1 << bits) - 1; 
-      // removing upper bits 
-      element &= maxN;
-      // normalizing the number to [-1.0,1.0]
-      maxN >>= 1;
-      float output = (float) element / maxN;  
-      if (output < -1.0f) output = -1.0f; 
-      return output; 
-   } else {
-      return 0.0f; 
-   }
+float reduce_precision(float x, unsigned bits) {
+  intfloat tmp;
+  tmp.b = x;
+  int v = tmp.a;
+  int man = v & ((1 << 23) - 1);
+  int mask = ((1 << bits) - 1) << (23 - bits);
+  int nv = (v & ((-1) - ((1 << 23) - 1))) | (mask & man);
+  tmp.a = nv;
+  float result = tmp.b;
+  return result;
 }
 
-float textureNormalizeElementUnsigned(unsigned int element, int bits)
-{
-   if (bits) {
-      unsigned int maxN = (1 << bits) - 1; 
-      // removing upper bits and normalizing the number to [0.0,1.0]
-      return (float)(element & maxN) / maxN;  
-   } else {
-      return 0.0f; 
-   }
+unsigned wrap(unsigned x, unsigned y, unsigned mx, unsigned my,
+              size_t elem_size) {
+  unsigned nx = (mx + x) % mx;
+  unsigned ny = (my + y) % my;
+  return nx + mx * ny;
 }
 
-void textureNormalizeOutput( const struct cudaChannelFormatDesc& desc, ptx_reg_t& datax, ptx_reg_t& datay, ptx_reg_t& dataz, ptx_reg_t& dataw ) 
-{
-   if (desc.f == cudaChannelFormatKindSigned) {
-      datax.f32 = textureNormalizeElementSigned( datax.s32, desc.x ); 
-      datay.f32 = textureNormalizeElementSigned( datay.s32, desc.y ); 
-      dataz.f32 = textureNormalizeElementSigned( dataz.s32, desc.z ); 
-      dataw.f32 = textureNormalizeElementSigned( dataw.s32, desc.w ); 
-   } else if (desc.f == cudaChannelFormatKindUnsigned) {
-      datax.f32 = textureNormalizeElementUnsigned( datax.u32, desc.x ); 
-      datay.f32 = textureNormalizeElementUnsigned( datay.u32, desc.y ); 
-      dataz.f32 = textureNormalizeElementUnsigned( dataz.u32, desc.z ); 
-      dataw.f32 = textureNormalizeElementUnsigned( dataw.u32, desc.w ); 
-   } else {
-      assert(0 && "Undefined texture read mode: cudaReadModeNormalizedFloat expect integer elements"); 
-   }
+unsigned clamp(unsigned x, unsigned y, unsigned mx, unsigned my,
+               size_t elem_size) {
+  unsigned nx = x;
+  while (nx >= mx) nx -= elem_size;
+  unsigned ny = (y >= my) ? my - 1 : y;
+  return nx + mx * ny;
 }
 
-void tex_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   unsigned dimension = pI->dimension();
-   const operand_info &dst = pI->dst(); //the registers to which fetched texel will be placed
-   const operand_info &src1 = pI->src1(); //the name of the texture
-   const operand_info &src2 = pI->src2(); //the vector registers containing coordinates of the texel to be fetched
-
-   std::string texname = src1.name();
-   unsigned to_type = pI->get_type();
-   unsigned c_type = pI->get_type2();
-   fflush(stdout);
-   ptx_reg_t data1, data2, data3, data4;
-   if (!ptx_tex_regs) ptx_tex_regs = new ptx_reg_t[4];
-   unsigned nelem = src2.get_vect_nelem();
-   thread->get_vector_operand_values(src2, ptx_tex_regs, nelem); //ptx_reg should be 4 entry vector type...coordinates into texture
-   /*
-     For programs with many streams, textures can be bound and unbound
-     asynchronously.  This means we need to use the kernel's "snapshot" of
-     the state of the texture mappings when it was launched (so that we
-     don't try to access the incorrect texture mapping if it's been updated,
-     or that we don't access a mapping that has been unbound).
-   */
-   gpgpu_t *gpu = thread->get_gpu();
-   kernel_info_t &k = thread->get_kernel();
-   const struct textureReference* texref = gpu->get_texref(texname);
-   const struct cudaArray* cuArray = k.get_texarray(texname); 
-   const struct textureInfo* texInfo = k.get_texinfo(texname);
-   const struct textureReferenceAttr* texAttr = gpu->get_texattr(texname);
-
-   //assume always 2D f32 input
-   //access array with src2 coordinates
-   memory_space *mem = thread->get_global_memory();
-   float x_f32,  y_f32;
-   size_t size;
-   int t;
-   unsigned tex_array_base;
-   unsigned int width = 0, height = 0;
-   int x = 0;
-   int y = 0;
-   unsigned tex_array_index;
-   float alpha=0, beta=0;
-
-   type_info_key::type_decode(to_type,size,t);
-   tex_array_base = cuArray->devPtr32;
-
-   switch (dimension) {
-   case GEOM_MODIFIER_1D:
+typedef unsigned (*texAddr_t)(unsigned x, unsigned y, unsigned mx, unsigned my,
+                              size_t elem_size);
+float tex_linf_sampling(memory_space *mem, unsigned tex_array_base, int x,
+                        int y, unsigned int width, unsigned int height,
+                        size_t elem_size, float alpha, float beta,
+                        texAddr_t b_lim) {
+  float Tij;
+  float Ti1j;
+  float Tij1;
+  float Ti1j1;
+
+  mem->read(tex_array_base + b_lim(x, y, width, height, elem_size), 4, &Tij);
+  mem->read(tex_array_base + b_lim(x + elem_size, y, width, height, elem_size),
+            4, &Ti1j);
+  mem->read(tex_array_base + b_lim(x, y + 1, width, height, elem_size), 4,
+            &Tij1);
+  mem->read(
+      tex_array_base + b_lim(x + elem_size, y + 1, width, height, elem_size), 4,
+      &Ti1j1);
+
+  float sample = (1 - alpha) * (1 - beta) * Tij + alpha * (1 - beta) * Ti1j +
+                 (1 - alpha) * beta * Tij1 + alpha * beta * Ti1j1;
+
+  return sample;
+}
+
+float textureNormalizeElementSigned(int element, int bits) {
+  if (bits) {
+    int maxN = (1 << bits) - 1;
+    // removing upper bits
+    element &= maxN;
+    // normalizing the number to [-1.0,1.0]
+    maxN >>= 1;
+    float output = (float)element / maxN;
+    if (output < -1.0f) output = -1.0f;
+    return output;
+  } else {
+    return 0.0f;
+  }
+}
+
+float textureNormalizeElementUnsigned(unsigned int element, int bits) {
+  if (bits) {
+    unsigned int maxN = (1 << bits) - 1;
+    // removing upper bits and normalizing the number to [0.0,1.0]
+    return (float)(element & maxN) / maxN;
+  } else {
+    return 0.0f;
+  }
+}
+
+void textureNormalizeOutput(const struct cudaChannelFormatDesc &desc,
+                            ptx_reg_t &datax, ptx_reg_t &datay,
+                            ptx_reg_t &dataz, ptx_reg_t &dataw) {
+  if (desc.f == cudaChannelFormatKindSigned) {
+    datax.f32 = textureNormalizeElementSigned(datax.s32, desc.x);
+    datay.f32 = textureNormalizeElementSigned(datay.s32, desc.y);
+    dataz.f32 = textureNormalizeElementSigned(dataz.s32, desc.z);
+    dataw.f32 = textureNormalizeElementSigned(dataw.s32, desc.w);
+  } else if (desc.f == cudaChannelFormatKindUnsigned) {
+    datax.f32 = textureNormalizeElementUnsigned(datax.u32, desc.x);
+    datay.f32 = textureNormalizeElementUnsigned(datay.u32, desc.y);
+    dataz.f32 = textureNormalizeElementUnsigned(dataz.u32, desc.z);
+    dataw.f32 = textureNormalizeElementUnsigned(dataw.u32, desc.w);
+  } else {
+    assert(0 &&
+           "Undefined texture read mode: cudaReadModeNormalizedFloat expect "
+           "integer elements");
+  }
+}
+
+void tex_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  unsigned dimension = pI->dimension();
+  const operand_info &dst =
+      pI->dst();  // the registers to which fetched texel will be placed
+  const operand_info &src1 = pI->src1();  // the name of the texture
+  const operand_info &src2 =
+      pI->src2();  // the vector registers containing coordinates of the texel
+                   // to be fetched
+
+  std::string texname = src1.name();
+  unsigned to_type = pI->get_type();
+  unsigned c_type = pI->get_type2();
+  fflush(stdout);
+  ptx_reg_t data1, data2, data3, data4;
+  if (!thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs)
+    thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs = new ptx_reg_t[4];
+  unsigned nelem = src2.get_vect_nelem();
+  thread->get_vector_operand_values(
+      src2, thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs,
+      nelem);  // ptx_reg should be 4 entry vector type...coordinates into
+               // texture
+  /*
+    For programs with many streams, textures can be bound and unbound
+    asynchronously.  This means we need to use the kernel's "snapshot" of
+    the state of the texture mappings when it was launched (so that we
+    don't try to access the incorrect texture mapping if it's been updated,
+    or that we don't access a mapping that has been unbound).
+  */
+  gpgpu_t *gpu = thread->get_gpu();
+  kernel_info_t &k = thread->get_kernel();
+  const struct textureReference *texref = gpu->get_texref(texname);
+  const struct cudaArray *cuArray = k.get_texarray(texname);
+  const struct textureInfo *texInfo = k.get_texinfo(texname);
+  const struct textureReferenceAttr *texAttr = gpu->get_texattr(texname);
+
+  // assume always 2D f32 input
+  // access array with src2 coordinates
+  memory_space *mem = thread->get_global_memory();
+  float x_f32, y_f32;
+  size_t size;
+  int t;
+  unsigned tex_array_base;
+  unsigned int width = 0, height = 0;
+  int x = 0;
+  int y = 0;
+  unsigned tex_array_index;
+  float alpha = 0, beta = 0;
+
+  type_info_key::type_decode(to_type, size, t);
+  tex_array_base = cuArray->devPtr32;
+
+  switch (dimension) {
+    case GEOM_MODIFIER_1D:
       width = cuArray->width;
       height = cuArray->height;
       if (texref->normalized) {
-         assert(c_type == F32_TYPE); 
-         x_f32 = ptx_tex_regs[0].f32;
-         if (texref->addressMode[0] == cudaAddressModeClamp) {
-            x_f32 = (x_f32 > 1.0)? 1.0 : x_f32;
-            x_f32 = (x_f32 < 0.0)? 0.0 : x_f32;
-         } else if (texref->addressMode[0] == cudaAddressModeWrap) {
-            x_f32 = x_f32 - floor(x_f32);
-         }
-
-         if( texref->filterMode == cudaFilterModeLinear ) {
-            float xb = x_f32 * width - 0.5;
-            alpha = xb - floor(xb);
-            alpha = reduce_precision(alpha,9);
-            beta = 0.0;
-
-            x = (int)floor(xb);
-            y = 0;
-         } else {
-            x = (int) floor(x_f32 * width);
-            y = 0;
-         }
+        assert(c_type == F32_TYPE);
+        x_f32 = thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs[0].f32;
+        if (texref->addressMode[0] == cudaAddressModeClamp) {
+          x_f32 = (x_f32 > 1.0) ? 1.0 : x_f32;
+          x_f32 = (x_f32 < 0.0) ? 0.0 : x_f32;
+        } else if (texref->addressMode[0] == cudaAddressModeWrap) {
+          x_f32 = x_f32 - floor(x_f32);
+        }
+
+        if (texref->filterMode == cudaFilterModeLinear) {
+          float xb = x_f32 * width - 0.5;
+          alpha = xb - floor(xb);
+          alpha = reduce_precision(alpha, 9);
+          beta = 0.0;
+
+          x = (int)floor(xb);
+          y = 0;
+        } else {
+          x = (int)floor(x_f32 * width);
+          y = 0;
+        }
       } else {
-         switch ( c_type ) {
-         case S32_TYPE: 
-            x = ptx_tex_regs[0].s32; 
-            assert(texref->filterMode == cudaFilterModePoint); 
-            break; 
-         case F32_TYPE: 
-            x_f32 = ptx_tex_regs[0].f32; 
-            alpha = x_f32 - floor(x_f32); // offset into subtexel (for linear sampling)
-            x = (int) x_f32; 
-            break; 
-         default: assert(0 && "Unsupported texture coordinate type."); 
-         }
-         // handle texture fetch that exceeded boundaries
-         if (texref->addressMode[0] == cudaAddressModeClamp) {
-            x = (x > width - 1)? (width - 1) : x;
-            x = (x < 0)? 0 : x;
-         } else if (texref->addressMode[0] == cudaAddressModeWrap) {
-            x = x % width;
-         }
+        switch (c_type) {
+          case S32_TYPE:
+            x = thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs[0].s32;
+            assert(texref->filterMode == cudaFilterModePoint);
+            break;
+          case F32_TYPE:
+            x_f32 = thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs[0].f32;
+            alpha = x_f32 -
+                    floor(x_f32);  // offset into subtexel (for linear sampling)
+            x = (int)x_f32;
+            break;
+          default:
+            assert(0 && "Unsupported texture coordinate type.");
+        }
+        // handle texture fetch that exceeded boundaries
+        if (texref->addressMode[0] == cudaAddressModeClamp) {
+          x = (x > width - 1) ? (width - 1) : x;
+          x = (x < 0) ? 0 : x;
+        } else if (texref->addressMode[0] == cudaAddressModeWrap) {
+          x = x % width;
+        }
       }
-      width *= (cuArray->desc.w+cuArray->desc.x+cuArray->desc.y+cuArray->desc.z)/8;
-      x *= (cuArray->desc.w+cuArray->desc.x+cuArray->desc.y+cuArray->desc.z)/8;
+      width *= (cuArray->desc.w + cuArray->desc.x + cuArray->desc.y +
+                cuArray->desc.z) /
+               8;
+      x *= (cuArray->desc.w + cuArray->desc.x + cuArray->desc.y +
+            cuArray->desc.z) /
+           8;
       tex_array_index = tex_array_base + x;
 
       break;
-   case GEOM_MODIFIER_2D:
+    case GEOM_MODIFIER_2D:
       width = cuArray->width;
       height = cuArray->height;
       if (texref->normalized) {
-         x_f32 = reduce_precision(ptx_tex_regs[0].f32,16);
-         y_f32 = reduce_precision(ptx_tex_regs[1].f32,15);
-
-         if (texref->addressMode[0]) {//clamp
-            if (x_f32<0) x_f32 = 0;
-            if (x_f32>=1) x_f32 = 1 - 1/x_f32;
-         } else {//wrap
-            x_f32 = x_f32 - floor(x_f32);
-         }
-         if (texref->addressMode[1]) {//clamp
-            if (y_f32<0) y_f32 = 0;
-            if (y_f32>=1) y_f32 = 1 - 1/y_f32;
-         } else {//wrap
-            y_f32 = y_f32 - floor(y_f32);
-         }
-
-         if( texref->filterMode == cudaFilterModeLinear ) {
-            float xb = x_f32 * width - 0.5;
-            float yb = y_f32 * height - 0.5;
-            alpha = xb - floor(xb);
-            beta = yb - floor(yb);
-            alpha = reduce_precision(alpha,9);
-            beta = reduce_precision(beta,9);
-
-            x = (int)floor(xb);
-            y = (int)floor(yb);
-         } else {
-            x = (int) floor(x_f32 * width);
-            y = (int) floor(y_f32 * height);
-         }
+        x_f32 = reduce_precision(
+            thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs[0].f32, 16);
+        y_f32 = reduce_precision(
+            thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs[1].f32, 15);
+
+        if (texref->addressMode[0]) {  // clamp
+          if (x_f32 < 0) x_f32 = 0;
+          if (x_f32 >= 1) x_f32 = 1 - 1 / x_f32;
+        } else {  // wrap
+          x_f32 = x_f32 - floor(x_f32);
+        }
+        if (texref->addressMode[1]) {  // clamp
+          if (y_f32 < 0) y_f32 = 0;
+          if (y_f32 >= 1) y_f32 = 1 - 1 / y_f32;
+        } else {  // wrap
+          y_f32 = y_f32 - floor(y_f32);
+        }
+
+        if (texref->filterMode == cudaFilterModeLinear) {
+          float xb = x_f32 * width - 0.5;
+          float yb = y_f32 * height - 0.5;
+          alpha = xb - floor(xb);
+          beta = yb - floor(yb);
+          alpha = reduce_precision(alpha, 9);
+          beta = reduce_precision(beta, 9);
+
+          x = (int)floor(xb);
+          y = (int)floor(yb);
+        } else {
+          x = (int)floor(x_f32 * width);
+          y = (int)floor(y_f32 * height);
+        }
       } else {
-         x_f32 = ptx_tex_regs[0].f32;
-         y_f32 = ptx_tex_regs[1].f32;
-
-         alpha = x_f32 - floor(x_f32);
-         beta = y_f32 - floor(y_f32);
-
-         x = (int) x_f32;
-         y = (int) y_f32;
-         if (texref->addressMode[0]) {//clamp
-            if (x<0) x = 0;
-            if (x>= (int)width) x = width-1;
-         } else {//wrap
-            x = x % width;
-            if (x < 0) x*= -1;
-         }
-         if (texref->addressMode[1]) {//clamp
-            if (y<0) y = 0;
-            if (y>= (int)height) y = height -1;
-         } else {//wrap
-            y = y % height;
-            if (y < 0) y *= -1;
-         }
+        x_f32 = thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs[0].f32;
+        y_f32 = thread->get_gpu()->gpgpu_ctx->func_sim->ptx_tex_regs[1].f32;
+
+        alpha = x_f32 - floor(x_f32);
+        beta = y_f32 - floor(y_f32);
+
+        x = (int)x_f32;
+        y = (int)y_f32;
+        if (texref->addressMode[0]) {  // clamp
+          if (x < 0) x = 0;
+          if (x >= (int)width) x = width - 1;
+        } else {  // wrap
+          x = x % width;
+          if (x < 0) x *= -1;
+        }
+        if (texref->addressMode[1]) {  // clamp
+          if (y < 0) y = 0;
+          if (y >= (int)height) y = height - 1;
+        } else {  // wrap
+          y = y % height;
+          if (y < 0) y *= -1;
+        }
       }
 
-      width *= (cuArray->desc.w+cuArray->desc.x+cuArray->desc.y+cuArray->desc.z)/8;
-      x *= (cuArray->desc.w+cuArray->desc.x+cuArray->desc.y+cuArray->desc.z)/8;
-      tex_array_index = tex_array_base + (x + width*y);
+      width *= (cuArray->desc.w + cuArray->desc.x + cuArray->desc.y +
+                cuArray->desc.z) /
+               8;
+      x *= (cuArray->desc.w + cuArray->desc.x + cuArray->desc.y +
+            cuArray->desc.z) /
+           8;
+      tex_array_index = tex_array_base + (x + width * y);
       break;
-   default:
-      assert(0); break;
-   }
-   switch ( to_type ) {
-   case U8_TYPE:
-   case U16_TYPE:
-   case U32_TYPE: 
-   case B8_TYPE:
-   case B16_TYPE:
-   case B32_TYPE: 
-   case S8_TYPE:
-   case S16_TYPE:
-   case S32_TYPE: {
-      unsigned long long elementOffset = 0; // offset into the next element 
-      mem->read( tex_array_index, cuArray->desc.x/8, &data1.u32);
-      elementOffset += cuArray->desc.x/8;  
+    default:
+      assert(0);
+      break;
+  }
+  switch (to_type) {
+    case U8_TYPE:
+    case U16_TYPE:
+    case U32_TYPE:
+    case B8_TYPE:
+    case B16_TYPE:
+    case B32_TYPE:
+    case S8_TYPE:
+    case S16_TYPE:
+    case S32_TYPE: {
+      unsigned long long elementOffset = 0;  // offset into the next element
+      mem->read(tex_array_index, cuArray->desc.x / 8, &data1.u32);
+      elementOffset += cuArray->desc.x / 8;
       if (cuArray->desc.y) {
-         mem->read( tex_array_index + elementOffset, cuArray->desc.y/8, &data2.u32);
-         elementOffset += cuArray->desc.y/8; 
-         if (cuArray->desc.z) {
-            mem->read( tex_array_index + elementOffset, cuArray->desc.z/8, &data3.u32);
-            elementOffset += cuArray->desc.z/8; 
-            if (cuArray->desc.w) 
-               mem->read( tex_array_index + elementOffset, cuArray->desc.w/8, &data4.u32);
-         }
+        mem->read(tex_array_index + elementOffset, cuArray->desc.y / 8,
+                  &data2.u32);
+        elementOffset += cuArray->desc.y / 8;
+        if (cuArray->desc.z) {
+          mem->read(tex_array_index + elementOffset, cuArray->desc.z / 8,
+                    &data3.u32);
+          elementOffset += cuArray->desc.z / 8;
+          if (cuArray->desc.w)
+            mem->read(tex_array_index + elementOffset, cuArray->desc.w / 8,
+                      &data4.u32);
+        }
       }
       break;
-   }
-   case B64_TYPE:
-   case U64_TYPE:
-   case S64_TYPE:
-      mem->read( tex_array_index, 8, &data1.u64);
+    }
+    case B64_TYPE:
+    case U64_TYPE:
+    case S64_TYPE:
+      mem->read(tex_array_index, 8, &data1.u64);
       if (cuArray->desc.y) {
-         mem->read( tex_array_index+8, 8, &data2.u64);
-         if (cuArray->desc.z) {
-            mem->read( tex_array_index+16, 8, &data3.u64);
-            if (cuArray->desc.w) 
-               mem->read( tex_array_index+24, 8, &data4.u64);
-         }
+        mem->read(tex_array_index + 8, 8, &data2.u64);
+        if (cuArray->desc.z) {
+          mem->read(tex_array_index + 16, 8, &data3.u64);
+          if (cuArray->desc.w) mem->read(tex_array_index + 24, 8, &data4.u64);
+        }
       }
       break;
-   case F16_TYPE: assert(0); break;
-   case F32_TYPE:  {
-      if( texref->filterMode == cudaFilterModeLinear ) {
-         texAddr_t b_lim = wrap;
-         if ( texref->addressMode[0] == cudaAddressModeClamp ) {
-            b_lim = clamp;
-         }
-         size_t elem_size = (cuArray->desc.x + cuArray->desc.y + cuArray->desc.z + cuArray->desc.w) / 8;
-         size_t elem_ofst = 0;
-
-         data1.f32 = tex_linf_sampling(mem, tex_array_base, x + elem_ofst, y, width, height, elem_size, alpha, beta, b_lim);
-         elem_ofst += cuArray->desc.x / 8; 
-         if (cuArray->desc.y) {
-            data2.f32 = tex_linf_sampling(mem, tex_array_base, x + elem_ofst, y, width, height, elem_size, alpha, beta, b_lim);
-            elem_ofst += cuArray->desc.y / 8; 
-            if (cuArray->desc.z) {
-               data3.f32 = tex_linf_sampling(mem, tex_array_base, x + elem_ofst, y, width, height, elem_size, alpha, beta, b_lim);
-               elem_ofst += cuArray->desc.z / 8; 
-               if (cuArray->desc.w) 
-                  data4.f32 = tex_linf_sampling(mem, tex_array_base, x + elem_ofst, y, width, height, elem_size, alpha, beta, b_lim);
-            }
-         }
+    case F16_TYPE:
+      assert(0);
+      break;
+    case F32_TYPE: {
+      if (texref->filterMode == cudaFilterModeLinear) {
+        texAddr_t b_lim = wrap;
+        if (texref->addressMode[0] == cudaAddressModeClamp) {
+          b_lim = clamp;
+        }
+        size_t elem_size = (cuArray->desc.x + cuArray->desc.y +
+                            cuArray->desc.z + cuArray->desc.w) /
+                           8;
+        size_t elem_ofst = 0;
+
+        data1.f32 =
+            tex_linf_sampling(mem, tex_array_base, x + elem_ofst, y, width,
+                              height, elem_size, alpha, beta, b_lim);
+        elem_ofst += cuArray->desc.x / 8;
+        if (cuArray->desc.y) {
+          data2.f32 =
+              tex_linf_sampling(mem, tex_array_base, x + elem_ofst, y, width,
+                                height, elem_size, alpha, beta, b_lim);
+          elem_ofst += cuArray->desc.y / 8;
+          if (cuArray->desc.z) {
+            data3.f32 =
+                tex_linf_sampling(mem, tex_array_base, x + elem_ofst, y, width,
+                                  height, elem_size, alpha, beta, b_lim);
+            elem_ofst += cuArray->desc.z / 8;
+            if (cuArray->desc.w)
+              data4.f32 = tex_linf_sampling(mem, tex_array_base, x + elem_ofst,
+                                            y, width, height, elem_size, alpha,
+                                            beta, b_lim);
+          }
+        }
       } else {
-         mem->read( tex_array_index, cuArray->desc.x/8, &data1.f32);
-         if (cuArray->desc.y) {
-            mem->read( tex_array_index+4, cuArray->desc.y/8, &data2.f32);
-            if (cuArray->desc.z) {
-               mem->read( tex_array_index+8, cuArray->desc.z/8, &data3.f32);
-               if (cuArray->desc.w) 
-                  mem->read( tex_array_index+12, cuArray->desc.w/8, &data4.f32);
-            }
-         }
+        mem->read(tex_array_index, cuArray->desc.x / 8, &data1.f32);
+        if (cuArray->desc.y) {
+          mem->read(tex_array_index + 4, cuArray->desc.y / 8, &data2.f32);
+          if (cuArray->desc.z) {
+            mem->read(tex_array_index + 8, cuArray->desc.z / 8, &data3.f32);
+            if (cuArray->desc.w)
+              mem->read(tex_array_index + 12, cuArray->desc.w / 8, &data4.f32);
+          }
+        }
       }
-   } break;
-   case F64_TYPE: 
-   case FF64_TYPE:
-      mem->read( tex_array_index, 8, &data1.f64);
+    } break;
+    case F64_TYPE:
+    case FF64_TYPE:
+      mem->read(tex_array_index, 8, &data1.f64);
       if (cuArray->desc.y) {
-         mem->read( tex_array_index+8, 8, &data2.f64);
-         if (cuArray->desc.z) {
-            mem->read( tex_array_index+16, 8, &data3.f64);
-            if (cuArray->desc.w) 
-               mem->read( tex_array_index+24, 8, &data4.f64);
-         }
+        mem->read(tex_array_index + 8, 8, &data2.f64);
+        if (cuArray->desc.z) {
+          mem->read(tex_array_index + 16, 8, &data3.f64);
+          if (cuArray->desc.w) mem->read(tex_array_index + 24, 8, &data4.f64);
+        }
       }
       break;
-   default: assert(0); break;
-   }
-   int x_block_coord, y_block_coord, memreqindex, blockoffset;
+    default:
+      assert(0);
+      break;
+  }
+  int x_block_coord, y_block_coord, memreqindex, blockoffset;
 
-   switch (dimension) {
-   case GEOM_MODIFIER_1D:
+  switch (dimension) {
+    case GEOM_MODIFIER_1D:
       thread->m_last_effective_address = tex_array_index;
       break;
-   case GEOM_MODIFIER_2D: 
+    case GEOM_MODIFIER_2D:
       x_block_coord = x >> (texInfo->Tx_numbits + texInfo->texel_size_numbits);
       y_block_coord = y >> texInfo->Ty_numbits;
 
-      memreqindex = ((y_block_coord*cuArray->width/texInfo->Tx)+x_block_coord)<<6;
+      memreqindex =
+          ((y_block_coord * cuArray->width / texInfo->Tx) + x_block_coord) << 6;
 
-      blockoffset = (x%(texInfo->Tx*texInfo->texel_size) + (y%(texInfo->Ty)<<(texInfo->Tx_numbits + texInfo->texel_size_numbits)));
+      blockoffset = (x % (texInfo->Tx * texInfo->texel_size) +
+                     (y % (texInfo->Ty)
+                      << (texInfo->Tx_numbits + texInfo->texel_size_numbits)));
       memreqindex += blockoffset;
-      thread->m_last_effective_address = tex_array_base + memreqindex;//tex_array_index;
+      thread->m_last_effective_address =
+          tex_array_base + memreqindex;  // tex_array_index;
       break;
-   default:
+    default:
       assert(0);
-   }
-   thread->m_last_memory_space = tex_space; 
+  }
+  thread->m_last_memory_space = tex_space;
+
+  // normalize output into floating point numbers according to the texture read
+  // mode
+  if (texAttr->m_readmode == cudaReadModeNormalizedFloat) {
+    textureNormalizeOutput(cuArray->desc, data1, data2, data3, data4);
+  } else {
+    assert(texAttr->m_readmode == cudaReadModeElementType);
+  }
 
-   // normalize output into floating point numbers according to the texture read mode
-   if (texAttr->m_readmode == cudaReadModeNormalizedFloat) {
-      textureNormalizeOutput(cuArray->desc, data1, data2, data3, data4); 
-   } else {
-      assert(texAttr->m_readmode == cudaReadModeElementType); 
-   }
+  thread->set_vector_operand_values(dst, data1, data2, data3, data4);
+}
 
-   thread->set_vector_operand_values(dst,data1,data2,data3,data4);
-}
-
-void txq_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void trap_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vabsdiff_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vadd_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vmad_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vmax_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vmin_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vset_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vshl_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vshr_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-void vsub_impl( const ptx_instruction *pI, ptx_thread_info *thread ) { inst_not_implemented(pI); }
-
-void vote_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{
-   static bool first_in_warp = true;
-   static bool and_all;
-   static bool or_all;
-   static unsigned int ballot_result;
-   static std::list<ptx_thread_info*> threads_in_warp;
-   static unsigned last_tid;
-
-   if( first_in_warp ) {
-      first_in_warp = false;
-      threads_in_warp.clear();
-      and_all = true;
-      or_all = false;
-      ballot_result = 0;
-      int offset=31;
-      while( (offset>=0) && !pI->active(offset) ) 
-         offset--;
-      assert( offset >= 0 );
-      last_tid = (thread->get_hw_tid() - (thread->get_hw_tid()%pI->warp_size())) + offset;
-   }
+void txq_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void trap_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void vabsdiff_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void vadd_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void vmad_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+
+#define VMAX 0
+#define VMIN 1
+
+void vmax_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  video_mem_instruction(pI, thread, VMAX);
+}
+void vmin_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  video_mem_instruction(pI, thread, VMIN);
+}
+void vset_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void vshl_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void vshr_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
+void vsub_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  inst_not_implemented(pI);
+}
 
-   ptx_reg_t src1_data;
-   const operand_info &src1 = pI->src1();
-   src1_data = thread->get_operand_value(src1, pI->dst(), PRED_TYPE, thread, 1);
+void vote_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  static bool first_in_warp = true;
+  static bool and_all;
+  static bool or_all;
+  static unsigned int ballot_result;
+  static std::list<ptx_thread_info *> threads_in_warp;
+  static unsigned last_tid;
+
+  if (first_in_warp) {
+    first_in_warp = false;
+    threads_in_warp.clear();
+    and_all = true;
+    or_all = false;
+    ballot_result = 0;
+    int offset = 31;
+    while ((offset >= 0) && !pI->active(offset)) offset--;
+    assert(offset >= 0);
+    last_tid =
+        (thread->get_hw_tid() - (thread->get_hw_tid() % pI->warp_size())) +
+        offset;
+  }
 
-   //predicate value was changed so the lowest bit being set means the zero flag is set.
-   //As a result, the value of src1_data.pred must be inverted to get proper behavior
-   bool pred_value = !(src1_data.pred & 0x0001);
-   bool invert = src1.is_neg_pred();
+  ptx_reg_t src1_data;
+  const operand_info &src1 = pI->src1();
+  src1_data = thread->get_operand_value(src1, pI->dst(), PRED_TYPE, thread, 1);
 
-   threads_in_warp.push_back(thread);
-   and_all &= (invert ^ pred_value);
-   or_all |= (invert ^ pred_value);
+  // predicate value was changed so the lowest bit being set means the zero flag
+  // is set. As a result, the value of src1_data.pred must be inverted to get
+  // proper behavior
+  bool pred_value = !(src1_data.pred & 0x0001);
+  bool invert = src1.is_neg_pred();
 
-   // vote.ballot
-   if (invert ^ pred_value) {
-      int lane_id = thread->get_hw_tid() % pI->warp_size(); 
-      ballot_result |= (1 << lane_id); 
-   }
+  threads_in_warp.push_back(thread);
+  and_all &= (invert ^ pred_value);
+  or_all |= (invert ^ pred_value);
 
-   if( thread->get_hw_tid() == last_tid ) {
-      if (pI->vote_mode() == ptx_instruction::vote_ballot) {
-         ptx_reg_t data = ballot_result; 
-         for( std::list<ptx_thread_info*>::iterator t=threads_in_warp.begin(); t!=threads_in_warp.end(); ++t ) {
-            const operand_info &dst = pI->dst();
-            (*t)->set_operand_value(dst,data, pI->get_type(), (*t), pI);
-         }
-      } else {
-         bool pred_value = false; 
+  // vote.ballot
+  if (invert ^ pred_value) {
+    int lane_id = thread->get_hw_tid() % pI->warp_size();
+    ballot_result |= (1 << lane_id);
+  }
 
-         switch( pI->vote_mode() ) {
-         case ptx_instruction::vote_any: pred_value = or_all; break;
-         case ptx_instruction::vote_all: pred_value = and_all; break;
-         case ptx_instruction::vote_uni: pred_value = (or_all ^ and_all); break;
-         default:
-            abort();
-         }
-         ptx_reg_t data;
-         data.pred = pred_value?0:1; //the way ptxplus handles the zero flag, 1 = false and 0 = true
-
-         for( std::list<ptx_thread_info*>::iterator t=threads_in_warp.begin(); t!=threads_in_warp.end(); ++t ) {
-            const operand_info &dst = pI->dst();
-            (*t)->set_operand_value(dst,data, PRED_TYPE, (*t), pI);
-         }
+  if (thread->get_hw_tid() == last_tid) {
+    if (pI->vote_mode() == ptx_instruction::vote_ballot) {
+      ptx_reg_t data = ballot_result;
+      for (std::list<ptx_thread_info *>::iterator t = threads_in_warp.begin();
+           t != threads_in_warp.end(); ++t) {
+        const operand_info &dst = pI->dst();
+        (*t)->set_operand_value(dst, data, pI->get_type(), (*t), pI);
       }
-      first_in_warp = true;
-   }
+    } else {
+      bool pred_value = false;
+
+      switch (pI->vote_mode()) {
+        case ptx_instruction::vote_any:
+          pred_value = or_all;
+          break;
+        case ptx_instruction::vote_all:
+          pred_value = and_all;
+          break;
+        case ptx_instruction::vote_uni:
+          pred_value = (or_all ^ and_all);
+          break;
+        default:
+          abort();
+      }
+      ptx_reg_t data;
+      data.pred = pred_value ? 0 : 1;  // the way ptxplus handles the zero flag,
+                                       // 1 = false and 0 = true
+
+      for (std::list<ptx_thread_info *>::iterator t = threads_in_warp.begin();
+           t != threads_in_warp.end(); ++t) {
+        const operand_info &dst = pI->dst();
+        (*t)->set_operand_value(dst, data, PRED_TYPE, (*t), pI);
+      }
+    }
+    first_in_warp = true;
+  }
+}
+
+void activemask_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  active_mask_t l_activemask_bitset = pI->get_warp_active_mask();
+  uint32_t l_activemask_uint =
+      static_cast<uint32_t>(l_activemask_bitset.to_ulong());
+
+  const operand_info &dst = pI->dst();
+  thread->set_operand_value(dst, l_activemask_uint, U32_TYPE, thread, pI);
 }
 
-void xor_impl( const ptx_instruction *pI, ptx_thread_info *thread ) 
-{ 
-   ptx_reg_t src1_data, src2_data, data;
+void xor_impl(const ptx_instruction *pI, ptx_thread_info *thread) {
+  ptx_reg_t src1_data, src2_data, data;
 
-   const operand_info &dst  = pI->dst();
-   const operand_info &src1 = pI->src1();
-   const operand_info &src2 = pI->src2();
+  const operand_info &dst = pI->dst();
+  const operand_info &src1 = pI->src1();
+  const operand_info &src2 = pI->src2();
 
-   unsigned i_type = pI->get_type();
-   src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
-   src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  unsigned i_type = pI->get_type();
+  src1_data = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  src2_data = thread->get_operand_value(src2, dst, i_type, thread, 1);
 
-   //the way ptxplus handles predicates: 1 = false and 0 = true
-   if(i_type == PRED_TYPE)
-      data.pred = ~(~(src1_data.pred) ^ ~(src2_data.pred));
-   else
-      data.u64 = src1_data.u64 ^ src2_data.u64;
+  // the way ptxplus handles predicates: 1 = false and 0 = true
+  if (i_type == PRED_TYPE)
+    data.pred = ~(~(src1_data.pred) ^ ~(src2_data.pred));
+  else
+    data.u64 = src1_data.u64 ^ src2_data.u64;
 
-   thread->set_operand_value(dst,data, i_type, thread, pI);
+  thread->set_operand_value(dst, data, i_type, thread, pI);
 }
 
-void inst_not_implemented( const ptx_instruction * pI ) 
-{
-   printf("GPGPU-Sim PTX: ERROR (%s:%u) instruction \"%s\" not (yet) implemented\n",
-          pI->source_file(), 
-          pI->source_line(), 
-          pI->get_opcode_cstr() );
-   abort();
+void inst_not_implemented(const ptx_instruction *pI) {
+  printf(
+      "GPGPU-Sim PTX: ERROR (%s:%u) instruction \"%s\" not (yet) implemented\n",
+      pI->source_file(), pI->source_line(), pI->get_opcode_cstr());
+  abort();
 }
 
-ptx_reg_t srcOperandModifiers(ptx_reg_t opData, operand_info opInfo, operand_info dstInfo, unsigned type, ptx_thread_info *thread)
-{
-   ptx_reg_t result;
-   memory_space *mem = NULL;
-   size_t size;
-   int t;
-   result.u64=0;
-
-   //complete other cases for reading from memory, such as reading from other const memory
-   if(opInfo.get_addr_space() == global_space)
-   {
-       mem = thread->get_global_memory();
-       type_info_key::type_decode(type,size,t);
-       mem->read(opData.u32,size/8,&result.u64);
-       if( type == S16_TYPE || type == S32_TYPE ) 
-         sign_extend(result,size,dstInfo);
-   }
-   else if(opInfo.get_addr_space() == shared_space)
-   {
-       mem = thread->m_shared_mem;
-       type_info_key::type_decode(type,size,t);
-       mem->read(opData.u32,size/8,&result.u64);
+ptx_reg_t srcOperandModifiers(ptx_reg_t opData, operand_info opInfo,
+                              operand_info dstInfo, unsigned type,
+                              ptx_thread_info *thread) {
+  ptx_reg_t result;
+  memory_space *mem = NULL;
+  size_t size;
+  int t;
+  result.u64 = 0;
+
+  // complete other cases for reading from memory, such as reading from other
+  // const memory
+  if (opInfo.get_addr_space() == global_space) {
+    mem = thread->get_global_memory();
+    type_info_key::type_decode(type, size, t);
+    mem->read(opData.u32, size / 8, &result.u64);
+    if (type == S16_TYPE || type == S32_TYPE)
+      sign_extend(result, size, dstInfo);
+  } else if (opInfo.get_addr_space() == shared_space) {
+    mem = thread->m_shared_mem;
+    type_info_key::type_decode(type, size, t);
+    mem->read(opData.u32, size / 8, &result.u64);
+
+    if (type == S16_TYPE || type == S32_TYPE)
+      sign_extend(result, size, dstInfo);
+
+  } else if (opInfo.get_addr_space() == const_space) {
+    mem = thread->get_global_memory();
+    type_info_key::type_decode(type, size, t);
+
+    mem->read((opData.u32 + opInfo.get_const_mem_offset()), size / 8,
+              &result.u64);
+
+    if (type == S16_TYPE || type == S32_TYPE)
+      sign_extend(result, size, dstInfo);
+  } else {
+    result = opData;
+  }
 
-       if( type == S16_TYPE || type == S32_TYPE ) 
-         sign_extend(result,size,dstInfo);
+  if (opInfo.get_operand_lohi() == 1) {
+    result.u64 = result.u64 & 0xFFFF;
+  } else if (opInfo.get_operand_lohi() == 2) {
+    result.u64 = (result.u64 >> 16) & 0xFFFF;
+  }
 
-   }
-   else if(opInfo.get_addr_space() == const_space)
-   {
-       mem = thread->get_global_memory();
-       type_info_key::type_decode(type,size,t);
+  if (opInfo.get_operand_neg() == true) {
+    result.f32 = -result.f32;
+  }
 
-       mem->read((opData.u32 + opInfo.get_const_mem_offset()),size/8,&result.u64);
+  return result;
+}
 
-       if( type == S16_TYPE || type == S32_TYPE ) 
-         sign_extend(result,size,dstInfo);
-   }
-   else
-   {
-       result = opData;
-   }
+void video_mem_instruction(const ptx_instruction *pI, ptx_thread_info *thread,
+                           int op_code) {
+  const operand_info &dst = pI->dst();    // d
+  const operand_info &src1 = pI->src1();  // a
+  const operand_info &src2 = pI->src2();  // b
+  const operand_info &src3 = pI->src3();  // c
 
-   if(opInfo.get_operand_lohi() == 1)
-   {
-        result.u64 = result.u64 & 0xFFFF;
-   }
-   else if(opInfo.get_operand_lohi() == 2)
-   {
-        result.u64 = (result.u64>>16) & 0xFFFF;
-   }
+  const unsigned i_type = pI->get_type();
 
-   if(opInfo.get_operand_neg() == true) {
-      result.f32 = -result.f32;
-   }
+  std::list<int> scalar_type;
+  std::list<int> options;
 
-   return result;
-}
+  ptx_reg_t a, b, ta, tb, c, data;
 
-#endif
+  a = thread->get_operand_value(src1, dst, i_type, thread, 1);
+  b = thread->get_operand_value(src2, dst, i_type, thread, 1);
+  c = thread->get_operand_value(src3, dst, i_type, thread, 1);
+
+  // TODO: implement this
+  // ta = partSelectSignExtend( a, atype );
+  // tb = partSelectSignExtend( b, btype );
+  ta = a;
+  tb = b;
+
+  options = pI->get_options();
+  assert(options.size() == 1);
+
+  auto option = options.begin();
+  assert(*option == ATOMIC_MAX || *option == ATOMIC_MIN);
+
+  switch (i_type) {
+    case S32_TYPE: {
+      // assert all operands are S32_TYPE:
+      scalar_type = pI->get_scalar_type();
+      for (std::list<int>::iterator scalar = scalar_type.begin();
+           scalar != scalar_type.end(); scalar++) {
+        assert(*scalar == S32_TYPE);
+      }
+      assert(scalar_type.size() == 3);
+      scalar_type.clear();
+
+      switch (op_code) {
+        case VMAX:
+          data.s32 = MY_MAX_I(ta.s32, tb.s32);
+          break;
+        case VMIN:
+          data.s32 = MY_MIN_I(ta.s32, tb.s32);
+          break;
+        default:
+          assert(0);
+      }
+
+      switch (*option) {
+        case ATOMIC_MAX:
+          data.s32 = MY_MAX_I(data.s32, c.s32);
+          break;
+        case ATOMIC_MIN:
+          data.s32 = MY_MIN_I(data.s32, c.s32);
+          break;
+        default:
+          assert(0);  // not yet implemented
+      }
+      break;
+    }
+    default:
+      assert(0);  // not yet implemented
+  }
+
+  thread->set_operand_value(dst, data, i_type, thread, pI);
+
+  return;
+}
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.h
index cfbf4976c0..7373611246 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/instructions.h
@@ -99,4 +99,5 @@ void vshr_impl( const ptx_instruction *pI, ptx_thread_info *thread ) ;
 void vsub_impl( const ptx_instruction *pI, ptx_thread_info *thread ) ;
 void vote_impl( const ptx_instruction *pI, ptx_thread_info *thread ) ;
 void xor_impl( const ptx_instruction *pI, ptx_thread_info *thread ) ;
+void activemask_impl(const ptx_instruction *pI, ptx_thread_info *thread);
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptx_.c b/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptx_.c
index 754be0f65d..2d8102c095 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptx_.c
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptx_.c
@@ -1,36 +1,239 @@
-#line 2 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptx_.c"
+#line 2 "lex.ptx_c.c"
 
-#line 4 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptx_.c"
+#line 4 "lex.ptx_c.c"
 
 #define  YY_INT_ALIGNED short int
 
 /* A lexical scanner generated by flex */
 
+#define FLEX_SCANNER
+#define YY_FLEX_MAJOR_VERSION 2
+#define YY_FLEX_MINOR_VERSION 6
+#define YY_FLEX_SUBMINOR_VERSION 4
+#if YY_FLEX_SUBMINOR_VERSION > 0
+#define FLEX_BETA
+#endif
+
+#ifdef yy_create_buffer
+#define ptx__create_buffer_ALREADY_DEFINED
+#else
 #define yy_create_buffer ptx__create_buffer
+#endif
+
+#ifdef yy_delete_buffer
+#define ptx__delete_buffer_ALREADY_DEFINED
+#else
 #define yy_delete_buffer ptx__delete_buffer
-#define yy_flex_debug ptx__flex_debug
+#endif
+
+#ifdef yy_scan_buffer
+#define ptx__scan_buffer_ALREADY_DEFINED
+#else
+#define yy_scan_buffer ptx__scan_buffer
+#endif
+
+#ifdef yy_scan_string
+#define ptx__scan_string_ALREADY_DEFINED
+#else
+#define yy_scan_string ptx__scan_string
+#endif
+
+#ifdef yy_scan_bytes
+#define ptx__scan_bytes_ALREADY_DEFINED
+#else
+#define yy_scan_bytes ptx__scan_bytes
+#endif
+
+#ifdef yy_init_buffer
+#define ptx__init_buffer_ALREADY_DEFINED
+#else
 #define yy_init_buffer ptx__init_buffer
+#endif
+
+#ifdef yy_flush_buffer
+#define ptx__flush_buffer_ALREADY_DEFINED
+#else
 #define yy_flush_buffer ptx__flush_buffer
+#endif
+
+#ifdef yy_load_buffer_state
+#define ptx__load_buffer_state_ALREADY_DEFINED
+#else
 #define yy_load_buffer_state ptx__load_buffer_state
+#endif
+
+#ifdef yy_switch_to_buffer
+#define ptx__switch_to_buffer_ALREADY_DEFINED
+#else
 #define yy_switch_to_buffer ptx__switch_to_buffer
-#define yyin ptx_in
-#define yyleng ptx_leng
+#endif
+
+#ifdef yypush_buffer_state
+#define ptx_push_buffer_state_ALREADY_DEFINED
+#else
+#define yypush_buffer_state ptx_push_buffer_state
+#endif
+
+#ifdef yypop_buffer_state
+#define ptx_pop_buffer_state_ALREADY_DEFINED
+#else
+#define yypop_buffer_state ptx_pop_buffer_state
+#endif
+
+#ifdef yyensure_buffer_stack
+#define ptx_ensure_buffer_stack_ALREADY_DEFINED
+#else
+#define yyensure_buffer_stack ptx_ensure_buffer_stack
+#endif
+
+#ifdef yylex
+#define ptx_lex_ALREADY_DEFINED
+#else
 #define yylex ptx_lex
-#define yylineno ptx_lineno
-#define yyout ptx_out
+#endif
+
+#ifdef yyrestart
+#define ptx_restart_ALREADY_DEFINED
+#else
 #define yyrestart ptx_restart
-#define yytext ptx_text
+#endif
+
+#ifdef yylex_init
+#define ptx_lex_init_ALREADY_DEFINED
+#else
+#define yylex_init ptx_lex_init
+#endif
+
+#ifdef yylex_init_extra
+#define ptx_lex_init_extra_ALREADY_DEFINED
+#else
+#define yylex_init_extra ptx_lex_init_extra
+#endif
+
+#ifdef yylex_destroy
+#define ptx_lex_destroy_ALREADY_DEFINED
+#else
+#define yylex_destroy ptx_lex_destroy
+#endif
+
+#ifdef yyget_debug
+#define ptx_get_debug_ALREADY_DEFINED
+#else
+#define yyget_debug ptx_get_debug
+#endif
+
+#ifdef yyset_debug
+#define ptx_set_debug_ALREADY_DEFINED
+#else
+#define yyset_debug ptx_set_debug
+#endif
+
+#ifdef yyget_extra
+#define ptx_get_extra_ALREADY_DEFINED
+#else
+#define yyget_extra ptx_get_extra
+#endif
+
+#ifdef yyset_extra
+#define ptx_set_extra_ALREADY_DEFINED
+#else
+#define yyset_extra ptx_set_extra
+#endif
+
+#ifdef yyget_in
+#define ptx_get_in_ALREADY_DEFINED
+#else
+#define yyget_in ptx_get_in
+#endif
+
+#ifdef yyset_in
+#define ptx_set_in_ALREADY_DEFINED
+#else
+#define yyset_in ptx_set_in
+#endif
+
+#ifdef yyget_out
+#define ptx_get_out_ALREADY_DEFINED
+#else
+#define yyget_out ptx_get_out
+#endif
+
+#ifdef yyset_out
+#define ptx_set_out_ALREADY_DEFINED
+#else
+#define yyset_out ptx_set_out
+#endif
+
+#ifdef yyget_leng
+#define ptx_get_leng_ALREADY_DEFINED
+#else
+#define yyget_leng ptx_get_leng
+#endif
+
+#ifdef yyget_text
+#define ptx_get_text_ALREADY_DEFINED
+#else
+#define yyget_text ptx_get_text
+#endif
+
+#ifdef yyget_lineno
+#define ptx_get_lineno_ALREADY_DEFINED
+#else
+#define yyget_lineno ptx_get_lineno
+#endif
+
+#ifdef yyset_lineno
+#define ptx_set_lineno_ALREADY_DEFINED
+#else
+#define yyset_lineno ptx_set_lineno
+#endif
+
+#ifdef yyget_column
+#define ptx_get_column_ALREADY_DEFINED
+#else
+#define yyget_column ptx_get_column
+#endif
+
+#ifdef yyset_column
+#define ptx_set_column_ALREADY_DEFINED
+#else
+#define yyset_column ptx_set_column
+#endif
+
+#ifdef yywrap
+#define ptx_wrap_ALREADY_DEFINED
+#else
 #define yywrap ptx_wrap
+#endif
+
+#ifdef yyget_lval
+#define ptx_get_lval_ALREADY_DEFINED
+#else
+#define yyget_lval ptx_get_lval
+#endif
+
+#ifdef yyset_lval
+#define ptx_set_lval_ALREADY_DEFINED
+#else
+#define yyset_lval ptx_set_lval
+#endif
+
+#ifdef yyalloc
+#define ptx_alloc_ALREADY_DEFINED
+#else
 #define yyalloc ptx_alloc
+#endif
+
+#ifdef yyrealloc
+#define ptx_realloc_ALREADY_DEFINED
+#else
 #define yyrealloc ptx_realloc
-#define yyfree ptx_free
+#endif
 
-#define FLEX_SCANNER
-#define YY_FLEX_MAJOR_VERSION 2
-#define YY_FLEX_MINOR_VERSION 6
-#define YY_FLEX_SUBMINOR_VERSION 0
-#if YY_FLEX_SUBMINOR_VERSION > 0
-#define FLEX_BETA
+#ifdef yyfree
+#define ptx_free_ALREADY_DEFINED
+#else
+#define yyfree ptx_free
 #endif
 
 /* First, we deal with  platform-specific or compiler-specific issues. */
@@ -103,60 +306,65 @@ typedef unsigned int flex_uint32_t;
 #define UINT32_MAX             (4294967295U)
 #endif
 
+#ifndef SIZE_MAX
+#define SIZE_MAX               (~(size_t)0)
+#endif
+
 #endif /* ! C99 */
 
 #endif /* ! FLEXINT_H */
 
-#ifdef __cplusplus
-
-/* The "const" storage-class-modifier is valid. */
-#define YY_USE_CONST
-
-#else	/* ! __cplusplus */
-
-/* C99 requires __STDC__ to be defined as 1. */
-#if defined (__STDC__)
+/* begin standard C++ headers. */
 
-#define YY_USE_CONST
-
-#endif	/* defined (__STDC__) */
-#endif	/* ! __cplusplus */
-
-#ifdef YY_USE_CONST
+/* TODO: this is always defined, so inline it */
 #define yyconst const
+
+#if defined(__GNUC__) && __GNUC__ >= 3
+#define yynoreturn __attribute__((__noreturn__))
 #else
-#define yyconst
+#define yynoreturn
 #endif
 
 /* Returned upon end-of-file. */
 #define YY_NULL 0
 
-/* Promotes a possibly negative, possibly signed char to an unsigned
- * integer for use as an array index.  If the signed char is negative,
- * we want to instead treat it as an 8-bit unsigned char, hence the
- * double cast.
+/* Promotes a possibly negative, possibly signed char to an
+ *   integer in range [0..255] for use as an array index.
  */
-#define YY_SC_TO_UI(c) ((unsigned int) (unsigned char) c)
+#define YY_SC_TO_UI(c) ((YY_CHAR) (c))
+
+/* An opaque pointer. */
+#ifndef YY_TYPEDEF_YY_SCANNER_T
+#define YY_TYPEDEF_YY_SCANNER_T
+typedef void* yyscan_t;
+#endif
+
+/* For convenience, these vars (plus the bison vars far below)
+   are macros in the reentrant scanner. */
+#define yyin yyg->yyin_r
+#define yyout yyg->yyout_r
+#define yyextra yyg->yyextra_r
+#define yyleng yyg->yyleng_r
+#define yytext yyg->yytext_r
+#define yylineno (YY_CURRENT_BUFFER_LVALUE->yy_bs_lineno)
+#define yycolumn (YY_CURRENT_BUFFER_LVALUE->yy_bs_column)
+#define yy_flex_debug yyg->yy_flex_debug_r
 
 /* Enter a start condition.  This macro really ought to take a parameter,
  * but we do it the disgusting crufty way forced on us by the ()-less
  * definition of BEGIN.
  */
-#define BEGIN (yy_start) = 1 + 2 *
-
+#define BEGIN yyg->yy_start = 1 + 2 *
 /* Translate the current start state into a value that can be later handed
  * to BEGIN to return to the state.  The YYSTATE alias is for lex
  * compatibility.
  */
-#define YY_START (((yy_start) - 1) / 2)
+#define YY_START ((yyg->yy_start - 1) / 2)
 #define YYSTATE YY_START
-
 /* Action number for EOF rule of a given start state. */
 #define YY_STATE_EOF(state) (YY_END_OF_BUFFER + state + 1)
-
 /* Special action meaning "start processing a new file". */
-#define YY_NEW_FILE ptx_restart(ptx_in  )
-
+#define YY_NEW_FILE yyrestart( yyin , yyscanner )
 #define YY_END_OF_BUFFER_CHAR 0
 
 /* Size of default input buffer. */
@@ -186,51 +394,46 @@ typedef struct yy_buffer_state *YY_BUFFER_STATE;
 typedef size_t yy_size_t;
 #endif
 
-extern yy_size_t ptx_leng;
-
-extern FILE *ptx_in, *ptx_out;
-
 #define EOB_ACT_CONTINUE_SCAN 0
 #define EOB_ACT_END_OF_FILE 1
 #define EOB_ACT_LAST_MATCH 2
-
+    
     /* Note: We specifically omit the test for yy_rule_can_match_eol because it requires
      *       access to the local variable yy_act. Since yyless() is a macro, it would break
-     *       existing scanners that call yyless() from OUTSIDE ptx_lex. 
+     *       existing scanners that call yyless() from OUTSIDE yylex.
      *       One obvious solution it to make yy_act a global. I tried that, and saw
-     *       a 5% performance hit in a non-ptx_lineno scanner, because yy_act is
+     *       a 5% performance hit in a non-yylineno scanner, because yy_act is
      *       normally declared as a register variable-- so it is not worth it.
      */
     #define  YY_LESS_LINENO(n) \
             do { \
                 int yyl;\
-                for ( yyl = n; yyl < ptx_leng; ++yyl )\
-                    if ( ptx_text[yyl] == '\n' )\
-                        --ptx_lineno;\
+                for ( yyl = n; yyl < yyleng; ++yyl )\
+                    if ( yytext[yyl] == '\n' )\
+                        --yylineno;\
             }while(0)
     #define YY_LINENO_REWIND_TO(dst) \
             do {\
                 const char *p;\
                 for ( p = yy_cp-1; p >= (dst); --p)\
                     if ( *p == '\n' )\
-                        --ptx_lineno;\
+                        --yylineno;\
             }while(0)
     
 /* Return all but the first "n" matched characters back to the input stream. */
 #define yyless(n) \
 	do \
 		{ \
-		/* Undo effects of setting up ptx_text. */ \
+		/* Undo effects of setting up yytext. */ \
         int yyless_macro_arg = (n); \
         YY_LESS_LINENO(yyless_macro_arg);\
-		*yy_cp = (yy_hold_char); \
+		*yy_cp = yyg->yy_hold_char; \
 		YY_RESTORE_YY_MORE_OFFSET \
-		(yy_c_buf_p) = yy_cp = yy_bp + yyless_macro_arg - YY_MORE_ADJ; \
-		YY_DO_BEFORE_ACTION; /* set up ptx_text again */ \
+		yyg->yy_c_buf_p = yy_cp = yy_bp + yyless_macro_arg - YY_MORE_ADJ; \
+		YY_DO_BEFORE_ACTION; /* set up yytext again */ \
 		} \
 	while ( 0 )
-
-#define unput(c) yyunput( c, (yytext_ptr)  )
+#define unput(c) yyunput( c, yyg->yytext_ptr , yyscanner )
 
 #ifndef YY_STRUCT_YY_BUFFER_STATE
 #define YY_STRUCT_YY_BUFFER_STATE
@@ -244,7 +447,7 @@ struct yy_buffer_state
 	/* Size of input buffer in bytes, not including room for EOB
 	 * characters.
 	 */
-	yy_size_t yy_buf_size;
+	int yy_buf_size;
 
 	/* Number of characters read into yy_ch_buf, not including EOB
 	 * characters.
@@ -272,7 +475,7 @@ struct yy_buffer_state
 
     int yy_bs_lineno; /**< The line count. */
     int yy_bs_column; /**< The column count. */
-    
+
 	/* Whether to try to fill the input buffer when we reach the
 	 * end of it.
 	 */
@@ -289,136 +492,96 @@ struct yy_buffer_state
 	 * possible backing-up.
 	 *
 	 * When we actually see the EOF, we change the status to "new"
-	 * (via ptx_restart()), so that the user can continue scanning by
-	 * just pointing ptx_in at a new input file.
+	 * (via yyrestart()), so that the user can continue scanning by
+	 * just pointing yyin at a new input file.
 	 */
 #define YY_BUFFER_EOF_PENDING 2
 
 	};
 #endif /* !YY_STRUCT_YY_BUFFER_STATE */
 
-/* Stack of input buffers. */
-static size_t yy_buffer_stack_top = 0; /**< index of top of stack. */
-static size_t yy_buffer_stack_max = 0; /**< capacity of stack. */
-static YY_BUFFER_STATE * yy_buffer_stack = 0; /**< Stack as an array. */
-
 /* We provide macros for accessing buffer states in case in the
  * future we want to put the buffer states in a more general
  * "scanner state".
  *
  * Returns the top of the stack, or NULL.
  */
-#define YY_CURRENT_BUFFER ( (yy_buffer_stack) \
-                          ? (yy_buffer_stack)[(yy_buffer_stack_top)] \
+#define YY_CURRENT_BUFFER ( yyg->yy_buffer_stack \
+                          ? yyg->yy_buffer_stack[yyg->yy_buffer_stack_top] \
                           : NULL)
-
 /* Same as previous macro, but useful when we know that the buffer stack is not
  * NULL or when we need an lvalue. For internal use only.
  */
-#define YY_CURRENT_BUFFER_LVALUE (yy_buffer_stack)[(yy_buffer_stack_top)]
-
-/* yy_hold_char holds the character lost when ptx_text is formed. */
-static char yy_hold_char;
-static int yy_n_chars;		/* number of characters read into yy_ch_buf */
-yy_size_t ptx_leng;
-
-/* Points to current character in buffer. */
-static char *yy_c_buf_p = (char *) 0;
-static int yy_init = 0;		/* whether we need to initialize */
-static int yy_start = 0;	/* start state number */
-
-/* Flag which is used to allow ptx_wrap()'s to do buffer switches
- * instead of setting up a fresh ptx_in.  A bit of a hack ...
- */
-static int yy_did_buffer_switch_on_eof;
-
-void ptx_restart (FILE *input_file  );
-void ptx__switch_to_buffer (YY_BUFFER_STATE new_buffer  );
-YY_BUFFER_STATE ptx__create_buffer (FILE *file,int size  );
-void ptx__delete_buffer (YY_BUFFER_STATE b  );
-void ptx__flush_buffer (YY_BUFFER_STATE b  );
-void ptx_push_buffer_state (YY_BUFFER_STATE new_buffer  );
-void ptx_pop_buffer_state (void );
-
-static void ptx_ensure_buffer_stack (void );
-static void ptx__load_buffer_state (void );
-static void ptx__init_buffer (YY_BUFFER_STATE b,FILE *file  );
-
-#define YY_FLUSH_BUFFER ptx__flush_buffer(YY_CURRENT_BUFFER )
-
-YY_BUFFER_STATE ptx__scan_buffer (char *base,yy_size_t size  );
-YY_BUFFER_STATE ptx__scan_string (yyconst char *yy_str  );
-YY_BUFFER_STATE ptx__scan_bytes (yyconst char *bytes,yy_size_t len  );
-
-void *ptx_alloc (yy_size_t  );
-void *ptx_realloc (void *,yy_size_t  );
-void ptx_free (void *  );
-
-#define yy_new_buffer ptx__create_buffer
-
+#define YY_CURRENT_BUFFER_LVALUE yyg->yy_buffer_stack[yyg->yy_buffer_stack_top]
+
+void yyrestart ( FILE *input_file , yyscan_t yyscanner );
+void yy_switch_to_buffer ( YY_BUFFER_STATE new_buffer , yyscan_t yyscanner );
+YY_BUFFER_STATE yy_create_buffer ( FILE *file, int size , yyscan_t yyscanner );
+void yy_delete_buffer ( YY_BUFFER_STATE b , yyscan_t yyscanner );
+void yy_flush_buffer ( YY_BUFFER_STATE b , yyscan_t yyscanner );
+void yypush_buffer_state ( YY_BUFFER_STATE new_buffer , yyscan_t yyscanner );
+void yypop_buffer_state ( yyscan_t yyscanner );
+
+static void yyensure_buffer_stack ( yyscan_t yyscanner );
+static void yy_load_buffer_state ( yyscan_t yyscanner );
+static void yy_init_buffer ( YY_BUFFER_STATE b, FILE *file , yyscan_t yyscanner );
+#define YY_FLUSH_BUFFER yy_flush_buffer( YY_CURRENT_BUFFER , yyscanner)
+
+YY_BUFFER_STATE yy_scan_buffer ( char *base, yy_size_t size , yyscan_t yyscanner );
+YY_BUFFER_STATE yy_scan_string ( const char *yy_str , yyscan_t yyscanner );
+YY_BUFFER_STATE yy_scan_bytes ( const char *bytes, int len , yyscan_t yyscanner );
+
+void *yyalloc ( yy_size_t , yyscan_t yyscanner );
+void *yyrealloc ( void *, yy_size_t , yyscan_t yyscanner );
+void yyfree ( void * , yyscan_t yyscanner );
+
+#define yy_new_buffer yy_create_buffer
 #define yy_set_interactive(is_interactive) \
 	{ \
 	if ( ! YY_CURRENT_BUFFER ){ \
-        ptx_ensure_buffer_stack (); \
+        yyensure_buffer_stack (yyscanner); \
 		YY_CURRENT_BUFFER_LVALUE =    \
-            ptx__create_buffer(ptx_in,YY_BUF_SIZE ); \
+            yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner); \
 	} \
 	YY_CURRENT_BUFFER_LVALUE->yy_is_interactive = is_interactive; \
 	}
-
 #define yy_set_bol(at_bol) \
 	{ \
 	if ( ! YY_CURRENT_BUFFER ){\
-        ptx_ensure_buffer_stack (); \
+        yyensure_buffer_stack (yyscanner); \
 		YY_CURRENT_BUFFER_LVALUE =    \
-            ptx__create_buffer(ptx_in,YY_BUF_SIZE ); \
+            yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner); \
 	} \
 	YY_CURRENT_BUFFER_LVALUE->yy_at_bol = at_bol; \
 	}
-
 #define YY_AT_BOL() (YY_CURRENT_BUFFER_LVALUE->yy_at_bol)
 
 /* Begin user sect3 */
 
-#define ptx_wrap() (/*CONSTCOND*/1)
+#define ptx_wrap(yyscanner) (/*CONSTCOND*/1)
 #define YY_SKIP_YYWRAP
-
-typedef unsigned char YY_CHAR;
-
-FILE *ptx_in = (FILE *) 0, *ptx_out = (FILE *) 0;
+typedef flex_uint8_t YY_CHAR;
 
 typedef int yy_state_type;
 
-extern int ptx_lineno;
-
-int ptx_lineno = 1;
+#define yytext_ptr yytext_r
 
-extern char *ptx_text;
-#ifdef yytext_ptr
-#undef yytext_ptr
-#endif
-#define yytext_ptr ptx_text
-
-static yy_state_type yy_get_previous_state (void );
-static yy_state_type yy_try_NUL_trans (yy_state_type current_state  );
-static int yy_get_next_buffer (void );
-#if defined(__GNUC__) && __GNUC__ >= 3
-__attribute__((__noreturn__))
-#endif
-static void yy_fatal_error (yyconst char msg[]  );
+static yy_state_type yy_get_previous_state ( yyscan_t yyscanner );
+static yy_state_type yy_try_NUL_trans ( yy_state_type current_state  , yyscan_t yyscanner);
+static int yy_get_next_buffer ( yyscan_t yyscanner );
+static void yynoreturn yy_fatal_error ( const char* msg , yyscan_t yyscanner );
 
 /* Done after the current pattern has been matched and before the
- * corresponding action - sets up ptx_text.
+ * corresponding action - sets up yytext.
  */
 #define YY_DO_BEFORE_ACTION \
-	(yytext_ptr) = yy_bp; \
-	ptx_leng = (size_t) (yy_cp - yy_bp); \
-	(yy_hold_char) = *yy_cp; \
+	yyg->yytext_ptr = yy_bp; \
+	yyleng = (int) (yy_cp - yy_bp); \
+	yyg->yy_hold_char = *yy_cp; \
 	*yy_cp = '\0'; \
-	(yy_c_buf_p) = yy_cp;
-
-#define YY_NUM_RULES 329
-#define YY_END_OF_BUFFER 330
+	yyg->yy_c_buf_p = yy_cp;
+#define YY_NUM_RULES 339
+#define YY_END_OF_BUFFER 340
 /* This struct is not used in this scanner,
    but its presence is necessary. */
 struct yy_trans_info
@@ -426,172 +589,188 @@ struct yy_trans_info
 	flex_int32_t yy_verify;
 	flex_int32_t yy_nxt;
 	};
-static yyconst flex_int16_t yy_accept[1471] =
+static const flex_int16_t yy_accept[1603] =
     {   0,
-        0,    0,  326,  326,    0,    0,    0,    0,    0,    0,
-      330,  328,  314,  312,  313,  306,  324,  328,  328,  302,
-      303,  294,  295,  293,  309,  310,  182,  182,  304,  305,
-      300,  307,  301,  296,  176,  176,  176,  298,  299,  176,
-      176,  176,  176,  176,  176,  176,  176,  176,  176,  176,
-      176,  176,  176,  176,  176,  176,  176,  316,  297,  308,
-      326,  326,  325,  326,  326,  326,  326,  326,  326,  326,
-      326,  326,  326,  326,  326,  326,  326,  326,  326,  326,
-      326,  326,  320,  320,  323,  322,  321,  320,  320,  320,
-      320,  320,  320,  320,  320,  320,  320,  320,  320,  320,
-
-      320,  320,  320,  320,  320,  310,  315,    0,  312,  177,
-      177,  177,  177,  177,  177,  177,  177,  177,  177,  177,
-      182,  287,  288,  289,    0,    0,    0,    0,    0,    0,
+        0,    0,  336,  336,    0,    0,    0,    0,    0,    0,
+      340,  338,  324,  322,  323,  316,  334,  338,  338,  312,
+      313,  304,  305,  303,  319,  320,  192,  192,  314,  315,
+      310,  317,  311,  306,  186,  186,  186,  308,  309,  186,
+      186,  186,  186,  186,  186,  186,  186,  186,  186,  186,
+      186,  186,  186,  186,  186,  186,  186,  326,  307,  318,
+      336,  336,  335,  336,  336,  336,  336,  336,  336,  336,
+      336,  336,  336,  336,  336,  336,  336,  336,  336,  336,
+      336,  336,  330,  330,  333,  332,  331,  330,  330,  330,
+      330,  330,  330,  330,  330,  330,  330,  330,  330,  330,
+
+      330,  330,  330,  330,  330,  320,  325,    0,  322,  187,
+      187,  187,  187,  187,  187,  187,  187,  187,  187,  187,
+      192,  297,  298,  299,    0,    0,    0,    0,    0,    0,
+        0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+        0,    0,    0,    0,    0,  300,  301,  302,  327,  321,
+        0,  190,  192,    0,    0,    0,  192,    0,  186,  186,
+      186,  186,  186,  186,  186,  186,  186,  186,  186,  186,
+      186,  186,  186,  186,  186,  186,  186,  186,  186,   34,
+      186,  186,  186,  186,  186,  186,  186,  186,  186,   52,
+      186,  186,  186,  186,  186,  186,  186,  186,  186,  186,
+
+      186,  186,  186,   77,  186,  186,  186,  186,  186,  186,
+      186,  186,  186,  186,  336,  336,  336,  336,  336,  336,
+      336,  336,  336,  336,  336,  336,  336,  336,  336,  336,
+      336,  336,  336,  336,  336,   34,  336,  336,  336,  336,
+      336,  336,  336,  336,  336,   52,  336,  336,  336,  336,
+      336,  336,  336,  336,  336,  336,  336,  336,  336,   77,
+      336,  336,  336,  336,  336,  336,  336,  336,  336,  336,
+      330,  330,  328,    0,  330,  330,  330,  330,  330,  330,
+      330,  330,  330,  330,  330,  330,  330,  330,  330,  330,
+      330,  330,   34,  330,  330,  330,  330,  330,  330,  330,
+
+      330,  330,   52,  330,  330,  330,  330,  330,  330,  330,
+      330,  330,  330,  330,  330,  330,   77,  330,  330,  330,
+      330,  330,  330,  330,  330,  330,  330,    0,  187,  187,
+      187,  187,  187,  187,  187,  187,  187,  187,  187,  187,
+      187,  187,  294,  295,  296,    0,    0,    0,    0,    0,
+        0,    0,    0,    0,    0,    0,    0,  208,    0,    0,
+        0,    0,    0,    0,  271,  222,  238,  272,    0,  273,
+        0,  275,    0,    0,    0,    0,    0,  232,    0,    0,
+        0,    0,    0,    0,    0,    0,    0,  237,  265,  236,
+        0,  243,  244,    0,    0,  235,  241,  242,  234,  274,
+
+        0,    0,    0,    0,    0,    0,    0,  278,  233,    0,
+        0,  285,    0,    0,    0,    0,    0,    0,  251,  249,
+        0,  252,  250,    0,    0,    0,  195,    0,    0,  239,
+        0,    0,    0,    0,    0,    0,    0,    0,  270,    0,
+        0,    0,  199,    0,  279,  218,  219,  220,    0,    0,
+      276,    0,    0,  277,    0,  321,  188,  190,  191,    0,
+        0,  189,  186,  186,    1,  186,    2,    5,  186,    9,
+       11,   12,   14,  186,  186,   15,  186,   23,  186,   25,
+       26,   28,  186,   30,  186,   32,  186,    0,  186,   36,
+       37,   39,   42,  186,   44,   45,   47,  186,   48,  100,
+
+      186,   51,   53,  186,  186,  186,  186,   59,   60,   61,
+       62,  186,   65,  186,   68,  186,   70,   71,   72,  186,
+      186,   75,   76,    0,  186,   79,  186,   84,  186,   85,
+      186,   86,  186,  186,  186,  186,  186,  186,  186,  186,
+      186,   99,  336,  336,    1,  336,    2,    5,  336,    9,
+       11,   12,   14,  336,  336,   15,  336,   23,  336,   25,
+       26,   28,  336,   30,  336,   32,  336,  336,   36,   37,
+       39,   42,  336,   44,   45,   47,  336,   48,  100,  336,
+       51,   53,  336,  336,  336,  336,   59,   60,   61,   62,
+      336,   65,  336,   68,  336,   70,   71,   72,  336,  336,
+
+       75,   76,  336,   79,  336,   84,  336,   85,  336,   86,
+      336,  336,  336,  336,  336,  336,  336,  336,  336,   99,
+      330,    0,    1,  330,    2,    5,  330,    9,   11,   12,
+       14,  330,  330,   15,  330,   23,  330,   25,   26,   28,
+      330,   30,  330,   32,  330,  330,   36,   37,   39,   42,
+      330,   44,   45,   47,  330,   48,  100,  330,   51,   53,
+      330,  330,  330,  330,   59,   60,   61,   62,  330,   65,
+      330,   68,  330,   70,   71,   72,  330,  330,   75,   76,
+      330,   79,  330,   84,  330,   85,  330,   86,  330,  330,
+      330,  330,  330,  330,  330,  330,  330,   99,    0,  187,
+
+      187,  187,  187,  187,  187,  187,  187,  187,  187,  181,
+      187,  183,  187,    0,  269,  289,    0,  263,  284,  262,
+        0,    0,    0,  209,  210,  121,  211,    0,    0,    0,
+        0,    0,    0,    0,    0,  287,  110,    0,  266,    0,
+      291,    0,  123,  124,    0,  223,    0,    0,    0,  203,
+      205,  120,  206,    0,    0,  253,    0,    0,  228,    0,
+      227,    0,  282,  290,  226,  140,  225,    0,    0,    0,
+      293,  292,    0,  230,  254,  224,  240,  229,    0,    0,
+        0,    0,  164,    0,  122,  259,  147,    0,  247,  245,
+      109,  248,  246,  196,  197,  198,    0,  231,    0,    0,
+
+        0,    0,    0,    0,    0,  267,    0,  156,  200,  201,
+      202,  256,    0,    0,    0,    0,  286,  191,    0,    0,
+      189,  186,  186,  186,    4,    3,    6,    7,    0,  186,
+      186,  186,  186,   16,  186,   21,   24,   27,   29,   31,
+      186,    0,  186,  186,   40,   41,  186,  186,  186,   50,
+      186,   55,  186,   58,   63,  186,   66,   67,   69,   73,
+       74,    0,  186,   80,   81,  186,  186,   87,  186,   89,
+       90,   91,   92,   97,   93,   94,   95,   96,   18,  336,
+      336,  336,    4,    3,    6,    7,  336,  336,  336,  336,
+       16,  336,   21,   24,   27,   29,   31,  336,  336,  336,
+
+       40,   41,  336,  336,  336,   50,  336,   55,  336,   58,
+       63,  336,   66,   67,   69,   73,   74,  336,   80,   81,
+      336,  336,   87,  336,   89,   90,   91,   92,   97,   93,
+       94,   95,   96,   18,  330,    0,  330,    4,    3,    6,
+        7,  330,  330,  330,  330,   16,  330,   21,   24,   27,
+       29,   31,  330,  330,  330,   40,   41,  330,  330,  330,
+       50,  330,   55,  330,   58,   63,  330,   66,   67,   69,
+       73,   74,  330,   80,   81,  330,  330,   87,  330,   89,
+       90,   91,   92,   97,   93,   94,   95,   96,   18,    0,
+      187,  187,  187,  187,  187,  187,  187,  187,  178,  187,
+
+      182,  187,    0,    0,    0,    0,    0,    0,    0,    0,
+      212,  281,    0,  128,    0,    0,    0,    0,  280,    0,
+      288,  268,    0,    0,  207,  135,  261,  136,    0,  221,
+        0,    0,    0,    0,    0,    0,    0,    0,  283,    0,
+      214,  125,    0,    0,    0,    0,  151,    0,    0,  154,
+      257,    0,    0,    0,    0,    0,  160,  255,    0,    0,
+      186,  186,  186,    0,  186,  186,   13,  101,   17,   22,
+      186,    0,  186,   38,  186,   46,   49,  186,  186,   64,
+        0,  186,   82,   83,  186,    0,  336,  336,  336,  336,
+      336,   13,  101,   17,   22,  336,  336,   38,  336,   46,
+
+       49,  336,  336,   64,  336,   82,   83,  336,  336,  330,
+        0,  330,  330,  330,   13,  101,   17,   22,  330,  330,
+       38,  330,   46,   49,  330,  330,   64,  330,   82,   83,
+      330,  330,    0,  165,  168,  187,  187,  187,  187,  187,
+      187,  179,  187,  187,    0,    0,  126,    0,    0,    0,
+        0,  213,    0,    0,    0,    0,  132,    0,  133,    0,
+      204,    0,  139,    0,    0,    0,    0,    0,    0,    0,
+        0,  145,    0,    0,    0,    0,    0,    0,    0,    0,
+        0,    0,  157,    0,    0,    0,    0,  186,  186,  186,
+        0,  186,  186,  186,  186,    0,  186,   43,  186,  186,
+
+        0,  186,  186,    0,    0,  336,  336,  336,  336,  336,
+      336,  336,  336,   43,  336,  336,  336,  336,  336,  336,
+      330,    0,    0,  330,  330,  330,  330,  330,  330,   43,
+      330,  330,  330,  330,  330,  330,    0,  187,  187,  170,
+      187,  171,  187,  177,  187,  184,    0,    0,  260,  258,
+        0,  264,    0,    0,    0,    0,    0,    0,    0,  134,
+      137,    0,    0,    0,    0,    0,    0,    0,    0,  146,
+        0,    0,    0,  150,  152,  153,    0,  155,  215,    0,
+        0,    0,    0,  186,  185,  186,    0,  186,   10,  186,
+      186,    0,  186,   54,  186,    0,  186,  186,    0,    0,
+
+      336,  336,  336,  336,   10,  336,  336,  336,   54,  336,
+      336,  336,  336,  336,  330,    0,    0,  330,  330,   10,
+      330,  330,  330,   54,  330,  330,  330,  330,  330,    0,
+      167,  169,  187,  187,  180,    0,    0,    0,    0,    0,
+        0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+      142,  143,    0,    0,  148,    0,  149,  217,  158,  159,
+        0,    0,  103,  186,    8,    8,  186,   33,    0,  186,
+       56,    0,  186,   88,    0,    0,  336,  103,  336,    8,
+      336,   33,  336,   56,  336,   88,  336,  336,  330,    0,
+      103,  330,    8,  330,   33,  330,   56,  330,   88,  330,
+
+      330,    0,  337,  187,  187,    0,    0,    0,    0,    0,
+        0,    0,  131,  163,    0,    0,    0,  118,  119,    0,
+        0,    0,    0,    0,    0,  186,  102,    0,  186,   57,
+        0,  186,   19,    0,  336,  336,  336,  102,  336,   57,
+      336,   19,  336,  330,    0,  330,  102,  330,   57,  330,
+       19,  330,  166,  187,    0,    0,    0,    0,    0,    0,
+        0,    0,    0,  117,    0,    0,    0,    0,    0,    0,
+        0,  193,   98,    0,  186,    0,  186,   20,   98,  336,
+      336,   20,  329,   98,  330,  330,   20,  187,  187,  187,
+        0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
+
+        0,    0,    0,    0,    0,    0,    0,  216,    0,   35,
+       35,   78,   78,   35,   78,   35,   78,  172,  175,  176,
+      173,  174,    0,    0,    0,    0,    0,    0,  130,    0,
         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
-        0,    0,    0,    0,    0,  290,  291,  292,  317,  311,
-        0,  180,  182,    0,    0,    0,  182,    0,  176,  176,
-      176,  176,  176,  176,  176,  176,  176,  176,  176,  176,
-      176,  176,  176,  176,  176,  176,  176,  176,   33,  176,
-      176,  176,  176,  176,  176,  176,  176,  176,   51,  176,
-      176,  176,  176,  176,  176,  176,  176,  176,  176,  176,
-
-      176,  176,   76,  176,  176,  176,  176,  176,  176,  176,
-      176,  176,  176,  326,  326,  326,  326,  326,  326,  326,
-      326,  326,  326,  326,  326,  326,  326,  326,  326,  326,
-      326,  326,  326,   33,  326,  326,  326,  326,  326,  326,
-      326,  326,  326,   51,  326,  326,  326,  326,  326,  326,
-      326,  326,  326,  326,  326,  326,  326,   76,  326,  326,
-      326,  326,  326,  326,  326,  326,  326,  326,  320,  320,
-      318,    0,  320,  320,  320,  320,  320,  320,  320,  320,
-      320,  320,  320,  320,  320,  320,  320,  320,  320,   33,
-      320,  320,  320,  320,  320,  320,  320,  320,  320,   51,
-
-      320,  320,  320,  320,  320,  320,  320,  320,  320,  320,
-      320,  320,  320,   76,  320,  320,  320,  320,  320,  320,
-      320,  320,  320,  320,    0,  177,  177,  177,  177,  177,
-      177,  177,  177,  177,  177,  177,  177,  177,  177,  284,
-      285,  286,    0,    0,    0,    0,    0,    0,    0,    0,
-        0,    0,    0,    0,  198,    0,    0,    0,    0,    0,
-        0,  261,  212,  228,  262,    0,  263,    0,  265,    0,
-        0,    0,    0,    0,  222,    0,    0,    0,    0,    0,
-        0,    0,    0,    0,  227,  255,  226,    0,  233,  234,
-        0,    0,  225,  231,  232,  224,  264,    0,    0,    0,
-
-        0,    0,  268,  223,    0,    0,  275,    0,    0,    0,
-        0,    0,    0,  241,  239,    0,  242,  240,    0,    0,
-        0,  185,    0,    0,  229,    0,    0,    0,    0,    0,
-        0,    0,    0,  260,    0,    0,    0,  189,    0,  269,
-      208,  209,  210,    0,    0,  266,    0,    0,  267,    0,
-      311,  178,  180,  181,    0,    0,  179,  176,  176,    1,
-        2,    5,  176,    9,   10,   11,   13,  176,  176,   14,
-      176,   22,  176,   24,   25,   27,  176,   29,  176,   31,
-      176,    0,  176,   35,   36,   38,   41,  176,   43,   44,
-       46,  176,   47,   98,  176,   50,   52,  176,  176,  176,
-
-      176,   58,   59,   60,   61,  176,   64,  176,   67,  176,
-       69,   70,   71,  176,  176,   74,   75,    0,  176,   78,
-      176,   83,  176,   84,  176,   85,  176,  176,  176,  176,
-      176,  176,  176,  176,  176,   97,  326,  326,    1,    2,
-        5,  326,    9,   10,   11,   13,  326,  326,   14,  326,
-       22,  326,   24,   25,   27,  326,   29,  326,   31,  326,
-      326,   35,   36,   38,   41,  326,   43,   44,   46,  326,
-       47,   98,  326,   50,   52,  326,  326,  326,  326,   58,
-       59,   60,   61,  326,   64,  326,   67,  326,   69,   70,
-       71,  326,  326,   74,   75,  326,   78,  326,   83,  326,
-
-       84,  326,   85,  326,  326,  326,  326,  326,  326,  326,
-      326,  326,   97,  320,    0,    1,    2,    5,  320,    9,
-       10,   11,   13,  320,  320,   14,  320,   22,  320,   24,
-       25,   27,  320,   29,  320,   31,  320,  320,   35,   36,
-       38,   41,  320,   43,   44,   46,  320,   47,   98,  320,
-       50,   52,  320,  320,  320,  320,   58,   59,   60,   61,
-      320,   64,  320,   67,  320,   69,   70,   71,  320,  320,
-       74,   75,  320,   78,  320,   83,  320,   84,  320,   85,
-      320,  320,  320,  320,  320,  320,  320,  320,  320,   97,
-        0,  177,  177,  177,  177,  177,  177,  177,  177,  177,
-
-      177,  171,  177,  173,  177,    0,  259,  279,    0,  253,
-      274,  252,    0,    0,    0,  199,  200,  111,  201,    0,
-        0,    0,    0,    0,    0,    0,    0,  277,  108,    0,
-      256,    0,  281,    0,  113,  114,    0,  213,    0,    0,
-        0,  193,  195,  110,  196,    0,    0,  243,    0,    0,
-      218,    0,  217,    0,  272,  280,  216,  130,  215,    0,
-      283,  282,    0,  220,  244,  214,  230,  219,    0,    0,
-        0,    0,  154,    0,  112,  249,  137,    0,  237,  235,
-      107,  238,  236,  186,  187,  188,    0,  221,    0,    0,
-        0,    0,    0,    0,    0,  257,    0,  146,  190,  191,
-
-      192,  246,    0,    0,    0,    0,  276,  181,    0,    0,
-      179,  176,  176,    4,    3,    6,    7,    0,  176,  176,
-      176,   15,  176,   20,   23,   26,   28,   30,  176,    0,
-      176,  176,   39,   40,  176,  176,  176,   49,  176,   54,
-      176,   57,   62,  176,   65,   66,   68,   72,   73,    0,
-      176,   79,   80,  176,  176,   86,  176,   88,   89,   90,
-       91,   96,   92,   93,   94,   95,   17,  326,  326,    4,
-        3,    6,    7,  326,  326,  326,   15,  326,   20,   23,
-       26,   28,   30,  326,  326,  326,   39,   40,  326,  326,
-      326,   49,  326,   54,  326,   57,   62,  326,   65,   66,
-
-       68,   72,   73,  326,   79,   80,  326,  326,   86,  326,
-       88,   89,   90,   91,   96,   92,   93,   94,   95,   17,
-      320,    0,    4,    3,    6,    7,  320,  320,  320,   15,
-      320,   20,   23,   26,   28,   30,  320,  320,  320,   39,
-       40,  320,  320,  320,   49,  320,   54,  320,   57,   62,
-      320,   65,   66,   68,   72,   73,  320,   79,   80,  320,
-      320,   86,  320,   88,   89,   90,   91,   96,   92,   93,
-       94,   95,   17,    0,  177,  177,  177,  177,  177,  177,
-      177,  177,  168,  177,  172,  177,    0,    0,    0,    0,
-        0,    0,    0,    0,  202,  271,    0,  118,    0,    0,
-
-        0,    0,  270,    0,  278,  258,    0,    0,  197,  125,
-      251,  126,    0,  211,    0,    0,    0,    0,    0,    0,
-      273,    0,  204,  115,    0,    0,    0,    0,  141,    0,
-        0,  144,  247,    0,    0,    0,    0,    0,  150,  245,
-        0,    0,  176,  176,    0,  176,   12,   99,   16,   21,
-      176,    0,  176,   37,  176,   45,   48,  176,  176,   63,
-        0,  176,   81,   82,  176,    0,  326,  326,  326,   12,
-       99,   16,   21,  326,  326,   37,  326,   45,   48,  326,
-      326,   63,  326,   81,   82,  326,  326,  320,    0,  320,
-       12,   99,   16,   21,  320,  320,   37,  320,   45,   48,
-
-      320,  320,   63,  320,   81,   82,  320,  320,    0,  155,
-      158,  177,  177,  177,  177,  177,  177,  169,  177,  177,
-        0,    0,  116,    0,    0,    0,    0,  203,    0,    0,
-        0,    0,  122,    0,  123,    0,  194,    0,  129,    0,
-        0,    0,    0,    0,    0,  135,    0,    0,    0,    0,
-        0,    0,    0,    0,    0,    0,  147,    0,    0,    0,
-        0,  176,  176,    0,  176,  176,  176,    0,  176,   42,
-      176,  176,    0,  176,  176,    0,    0,  326,  326,  326,
-      326,  326,  326,   42,  326,  326,  326,  326,  326,  326,
-      320,    0,    0,  320,  320,  320,  320,   42,  320,  320,
-
-      320,  320,  320,  320,    0,  177,  177,  160,  177,  161,
-      177,  167,  177,  174,  102,    0,  250,  248,  103,  254,
-        0,  104,    0,    0,    0,    0,  105,  124,  127,    0,
-        0,    0,    0,    0,    0,  136,    0,    0,    0,  140,
-      142,  143,    0,  145,  205,    0,    0,    0,    0,  176,
-      175,    0,  176,  176,  176,    0,  176,   53,  176,    0,
-      176,  176,    0,    0,  326,  326,  326,  326,  326,  326,
-       53,  326,  326,  326,  326,  326,  320,    0,    0,  320,
-      320,  320,  320,   53,  320,  320,  320,  320,  320,    0,
-      157,  159,  177,  177,  170,    0,    0,    0,    0,    0,
-
-        0,    0,    0,    0,  132,  133,    0,    0,  138,    0,
-      139,  207,  148,  149,    0,    0,  101,    8,    8,  176,
-       32,    0,  176,   55,    0,  176,   87,    0,    0,  326,
-      101,    8,  326,   32,  326,   55,  326,   87,  326,  326,
-      320,    0,  101,    8,  320,   32,  320,   55,  320,   87,
-      320,  320,    0,  327,  177,  177,    0,    0,    0,    0,
-      121,  153,    0,    0,    0,    0,  106,    0,    0,    0,
-      100,    0,  176,   56,    0,  176,   18,    0,  326,  326,
-      100,  326,   56,  326,   18,  326,  320,    0,  100,  320,
-       56,  320,   18,  320,  156,  177,    0,    0,    0,    0,
-
-        0,  109,    0,    0,    0,    0,  183,    0,  176,    0,
-      176,   19,  326,  326,   19,  319,  320,  320,   19,  177,
-      177,  177,    0,    0,    0,    0,    0,    0,    0,  206,
-        0,   34,   34,   77,   77,   34,   77,   34,   77,  162,
-      165,  166,  163,  164,    0,    0,    0,  120,    0,    0,
-        0,    0,  151,    0,    0,    0,  131,  134,    0,  117,
-      119,    0,    0,    0,    0,  128,    0,    0,  184,    0
+        0,    0,  161,    0,    0,    0,    0,    0,    0,    0,
+        0,    0,    0,    0,    0,  141,  144,    0,    0,    0,
+        0,  127,    0,  129,    0,    0,    0,    0,    0,    0,
+        0,    0,    0,    0,  104,  105,  106,  107,    0,    0,
+        0,    0,    0,    0,    0,    0,    0,  138,    0,    0,
+      112,  115,  113,  116,    0,    0,  111,  114,  108,    0,
+
+      194,    0
     } ;
 
-static yyconst YY_CHAR yy_ec[256] =
+static const YY_CHAR yy_ec[256] =
     {   0,
         1,    1,    1,    1,    1,    1,    1,    1,    2,    3,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
@@ -623,7 +802,7 @@ static yyconst YY_CHAR yy_ec[256] =
         1,    1,    1,    1,    1
     } ;
 
-static yyconst YY_CHAR yy_meta[81] =
+static const YY_CHAR yy_meta[81] =
     {   0,
         1,    1,    2,    1,    1,    3,    4,    1,    1,    1,
         5,    1,    1,    1,    1,    1,    6,    6,    6,    6,
@@ -635,349 +814,377 @@ static yyconst YY_CHAR yy_meta[81] =
         4,    4,    4,    4,    4,    4,    4,    1,    1,    1
     } ;
 
-static yyconst flex_uint16_t yy_base[1517] =
+static const flex_int16_t yy_base[1649] =
     {   0,
-        0,    0,   79,  153,  227,  301,  371,    0, 1823, 1822,
-     1899, 1902, 1867,    0, 1902, 1902, 1902,    0,   27, 1902,
-     1902, 1902, 1902,   85,  433,   72,  496,  101, 1902, 1902,
-     1902, 1902, 1902, 1902,    0, 1855, 1864, 1902, 1902,  102,
-       42,   98,   32, 1820, 1830, 1823,   56,  109,  112, 1823,
-      108,  125,  131,  129,  178, 1827, 1824, 1902, 1902, 1902,
-        0, 1858, 1902, 1846,  178,  182,  192,  118, 1812, 1822,
-     1815,   58,  180,  197, 1815,  198,  216,  253,  199,  206,
-     1819, 1816,    0, 1850, 1902, 1864, 1837,  255,  264,  242,
-      122, 1803, 1813, 1806,   71,  275,  274, 1806,  273,  291,
-
-      325,  294,  328, 1810, 1807, 1856, 1902, 1840,    0,    0,
-       68, 1805, 1800, 1815, 1814,  279, 1802, 1801, 1803, 1810,
-      393, 1902, 1806, 1805, 1804,  404,  525,  406,  221,  334,
-      504,  281,  330,   73,  466,  402,  498, 1790,  337,  525,
-      585,  390,  549,  538,  424, 1792, 1902, 1902, 1902,    0,
-      600,  612,  645,  179,    0,    0, 1902,    0,    0, 1812,
-     1813, 1785, 1798, 1797, 1786, 1782,  164,  531, 1787, 1772,
-     1782, 1777, 1775, 1772, 1823,  186, 1790, 1772,  691, 1822,
-       85, 1776, 1774, 1765, 1774, 1771, 1776,  208, 1769, 1776,
-     1765,   32, 1764,  470, 1762, 1773,  105,  399, 1763, 1772,
-
-     1757,  308,  771,  544, 1750, 1771, 1755,  159,  369, 1751,
-      587, 1757, 1751,    0, 1788, 1773, 1747, 1760, 1759, 1748,
-     1744,  413,  619, 1749, 1734, 1744, 1739, 1737, 1734, 1785,
-      299, 1752, 1734,  643, 1784,  137, 1738, 1736, 1727, 1736,
-     1733, 1738,  413, 1731, 1738, 1727,  196, 1726,  510, 1724,
-     1735,  360,  615, 1725, 1734, 1719,  459,  263,  617, 1712,
-     1733, 1717,  289,  422, 1713,  617, 1719, 1713,    0, 1750,
-     1902, 1735, 1709, 1722, 1721, 1710, 1706,  499,  670, 1711,
-     1696, 1706, 1701, 1699, 1696, 1747,  567, 1714, 1696,  576,
-     1746,  472, 1700, 1698, 1689, 1698, 1695, 1700,  641, 1693,
-
-     1700, 1689,  316, 1688,  540, 1686, 1697,  514,  666, 1687,
-     1696, 1681,  611,  641,  662, 1674, 1695, 1679,  507,  637,
-     1675,  654, 1681, 1675, 1708, 1676, 1688, 1667, 1678, 1675,
-     1672, 1665, 1671, 1673, 1680,  719, 1671, 1675, 1661, 1902,
-     1902, 1902, 1659, 1658, 1671,  578,  243, 1659, 1656, 1654,
-     1700, 1703, 1664, 1699, 1902, 1656,  665, 1655, 1664, 1645,
-     1645,  670, 1902, 1902, 1902,  588, 1902, 1661, 1902, 1643,
-     1657, 1637,  338, 1639, 1637,  673, 1685, 1688, 1649, 1684,
-     1681, 1640, 1625,  683, 1629, 1634, 1627, 1635, 1902, 1902,
-     1622, 1641, 1623, 1639, 1902, 1621, 1902, 1669, 1616, 1625,
-
-     1636, 1623, 1902,  684, 1629, 1622, 1902, 1616, 1617,  656,
-     1614,  430,  691, 1621, 1620, 1606, 1618, 1617, 1654, 1657,
-     1654, 1902,  687, 1619, 1902, 1620, 1615, 1600, 1601, 1600,
-      684, 1599, 1592, 1902, 1643, 1646, 1643, 1902, 1602, 1902,
-     1902, 1902, 1902, 1593, 1591, 1902, 1607, 1603, 1902, 1589,
-        0,  771, 1902,  786,    0,    0, 1611, 1608, 1613,    0,
-      697, 1589, 1589,  851,    0, 1587,    0,  752, 1584,    0,
-     1587,    0, 1578,    0, 1595,    0, 1594,    0, 1575,    0,
-     1578, 1571, 1570, 1569,    0,  759,    0, 1587,    0,    0,
-     1621, 1583,    0,    0, 1573,    0,    0, 1564, 1581, 1577,
-
-     1563,    0,    0,    0, 1566, 1563,    0, 1564, 1563, 1566,
-        0,    0,    0, 1557, 1556,    0,    0, 1553, 1552, 1569,
-     1567,    0,  696,    0, 1555,    0, 1551, 1564,  699, 1554,
-     1561, 1546,  698, 1562, 1562,    0, 1579, 1565,    0,  751,
-     1547, 1547,  594,    0, 1545,    0,  754, 1542,    0, 1545,
-        0, 1536,    0, 1553,    0, 1552,    0, 1533,    0, 1536,
-     1529, 1528,    0,  791,    0, 1546,    0,    0, 1580, 1542,
-        0,    0, 1532,    0,    0, 1523, 1540, 1536, 1522,    0,
-        0,    0, 1525, 1522,    0, 1523, 1522, 1525,    0,    0,
-        0, 1516, 1515,    0,    0, 1512, 1529, 1527,    0,  754,
-
-        0, 1515,    0, 1511, 1524,  753, 1514, 1521, 1506,  699,
-     1522, 1522,    0, 1539, 1525,    0,  753, 1507, 1507,  801,
-        0, 1505,    0,  760, 1502,    0, 1505,    0, 1496,    0,
-     1513,    0, 1512,    0, 1493,    0, 1496, 1489, 1488,    0,
-      817,    0, 1506,    0,    0, 1540, 1502,    0,    0, 1492,
-        0,    0, 1483, 1500, 1496, 1482,    0,    0,    0, 1485,
-     1482,    0, 1483, 1482, 1485,    0,    0,    0, 1476, 1475,
-        0,    0, 1472, 1489, 1487,    0,  759,    0, 1475,    0,
-     1471, 1484,  759, 1474, 1481, 1466,  754, 1482, 1482,    0,
-     1487, 1478, 1471, 1462, 1474, 1471, 1471, 1474, 1465, 1469,
-
-     1455,    0, 1467,    0, 1455, 1445, 1902, 1451, 1460, 1902,
-     1902, 1902, 1449, 1456, 1440, 1902, 1902, 1902, 1902, 1452,
-     1495, 1492, 1436, 1446, 1453, 1433, 1445, 1902, 1902, 1437,
-     1902, 1430, 1902, 1440, 1902, 1902, 1435, 1902, 1443, 1431,
-     1444, 1425, 1902, 1902, 1902, 1478, 1441, 1902, 1434, 1441,
-     1902, 1441, 1902, 1436, 1902, 1902, 1902, 1440, 1902, 1427,
-     1426, 1425, 1474, 1902, 1902, 1902, 1902, 1902, 1435, 1432,
-     1427, 1429, 1902, 1461, 1902, 1902, 1902, 1418, 1902, 1902,
-     1902, 1902, 1902, 1902, 1902, 1902, 1415, 1902, 1410, 1411,
-     1420, 1425, 1405, 1418, 1420, 1902, 1415, 1404, 1902, 1902,
-
-     1902, 1406, 1401, 1409, 1407, 1411, 1902, 1902,    0,    0,
-     1902, 1415, 1414,    0,    0,    0,    0, 1391, 1390, 1407,
-     1400,    0, 1390, 1393,    0,    0,    0,    0, 1406, 1392,
-     1391, 1435,    0,    0, 1402, 1433, 1388,    0, 1395,    0,
-     1394,    0,    0, 1379,    0,    0,    0,    0,    0, 1383,
-     1382,    0,    0, 1391, 1375,    0, 1389,    0,    0,    0,
-        0,    0,    0,    0,    0,    0, 1429, 1396, 1390,    0,
-        0,    0,    0, 1367, 1384, 1377,    0, 1367, 1370,    0,
-        0,    0,    0, 1383, 1369, 1413,    0,    0, 1380, 1411,
-     1366,    0, 1373,    0, 1372,    0,    0, 1357,    0,    0,
-
-        0,    0,    0, 1361,    0,    0, 1370, 1354,    0, 1368,
-        0,    0,    0,    0,    0,    0,    0,    0,    0, 1408,
-     1375, 1369,    0,    0,    0,    0, 1346, 1363, 1356,    0,
-     1346, 1295,    0,    0,    0,    0, 1290, 1263, 1297,    0,
-        0, 1258, 1269, 1218,    0, 1213,    0, 1210,    0,    0,
-     1193,    0,    0,    0,    0,    0, 1192,    0,    0, 1197,
-     1178,    0, 1186,    0,    0,    0,    0,    0,    0,    0,
-        0,    0,   76,   96,   95,  134,  143,  178,  234,  696,
-      268,  274,    0,  337,    0,  372,  436,  455,  474,  498,
-      542,  561,  574,  626, 1902, 1902,  624, 1902,  700,  770,
-
-      748,  765, 1902,  755, 1902, 1902,  769,  820, 1902, 1902,
-     1902, 1902,  787, 1902,  779,  825,  799,  789,  775,  783,
-     1902,  784, 1902, 1902,  798,  808,  812,  817, 1902,  806,
-      821,  814, 1902,  828,  829,  822,  827,  835, 1902, 1902,
-        0,    0,  854,  847,  839,  840,    0,  841,    0,    0,
-      840,  833,  834,    0,  829,    0,    0,  834,  829,    0,
-      840,  841,    0,    0,  844,  843,  875,  872,  856,    0,
-      857,    0,    0,  856,  849,    0,  845,    0,    0,  850,
-      845,    0,  854,    0,    0,  857,  856,  888,  888,  869,
-        0,  870,    0,    0,  870,  864,    0,  859,    0,    0,
-
-      867,  862,    0,  871,    0,    0,  874,  873,  894,  915,
-        0,  880,  884,  878,  886,  891,  889,    0,  885,  891,
-      893,  879, 1902,  875,  894,  897,  882, 1902,  894,  900,
-      887,  904,  908,  904, 1902,  895, 1902,  898, 1902,  939,
-      892,  907,  904,  895,  891, 1902,  915,  908,  913,  905,
-      916,  904,  903,  919,  906,  920, 1902,  913,  917,    0,
-        0,  940,  933,  914,  915,  929,  929,  934,  935,    0,
-      918,  935,  938,  939,  935,  928,  924,  953,  956,  929,
-      943,  943,  948,    0,  931,  948,  951,  947,  940,  936,
-      965,  973,  969,  942,  956,  956,  961,    0,  944,  961,
-
-      964,  960,  953,  949,  984, 1001, 1006,    0,  966,    0,
-      963,    0,  978,    0, 1902,  965, 1902, 1902, 1902, 1902,
-      965, 1902,  971,  969, 1022,  977, 1902, 1902,    0,  987,
-      997,  992,  996, 1000,  989, 1902,  999,  987,  992, 1902,
-     1902, 1902, 1000, 1902, 1902,  994, 1003,    0,    0, 1026,
-        0,  995,  996, 1008,  998,  995,  996,    0, 1008,  998,
-      999, 1013, 1019, 1007, 1037, 1040, 1009, 1021, 1011, 1008,
-        0, 1020, 1010, 1024, 1030, 1018, 1048, 1048, 1052, 1021,
-     1033, 1023, 1020,    0, 1032, 1022, 1036, 1042, 1030, 1094,
-        0, 1081, 1052, 1046,    0, 1057, 1057, 1040, 1053, 1096,
-
-     1053, 1050, 1106, 1058, 1902, 1902, 1059, 1072, 1902, 1059,
-     1902, 1902, 1902, 1902,    0,    0,    0, 1902,    0, 1060,
-        0, 1069, 1070, 1060, 1072, 1073,    0, 1079, 1067, 1134,
-        0,    0, 1069,    0, 1078, 1069, 1081,    0, 1087, 1075,
-     1142, 1106, 1902,    0, 1079,    0, 1088, 1078, 1090,    0,
-     1096, 1085, 1152, 1902, 1094, 1105, 1088, 1090, 1094, 1104,
-     1902, 1902, 1096, 1140, 1107, 1108, 1902, 1109,    0,    0,
-        0, 1104, 1105,    0, 1106, 1107, 1902, 1114, 1169,    0,
-        0, 1110,    0, 1111,    0, 1120, 1175, 1138,    0, 1117,
-        0, 1118,    0, 1125,    0, 1126, 1123, 1128, 1117, 1119,
-
-     1128, 1902, 1123, 1124, 1136,    0, 1902, 1138, 1139, 1140,
-     1141, 1902, 1142, 1143,    0, 1902, 1144, 1145,    0, 1135,
-     1147, 1148, 1129, 1150, 1132, 1140, 1158, 1142, 1143, 1902,
-        0, 1902,    0, 1902,    0,    0,    0,    0,    0,    0,
-        0,    0,    0,    0, 1157, 1144, 1149, 1902, 1146, 1156,
-     1157,    0, 1902, 1152, 1166, 1163, 1902, 1902,    0, 1902,
-     1902, 1162,    0, 1169,    0, 1902,    0,    0, 1902, 1902,
-     1226, 1233, 1240, 1244, 1248, 1255, 1262, 1268, 1270, 1272,
-     1274, 1281, 1288, 1295, 1302, 1309, 1316, 1318, 1320, 1327,
-     1334, 1341, 1343, 1345, 1347, 1349, 1351, 1353, 1360, 1362,
-
-     1364, 1371, 1373, 1375, 1382, 1389, 1391, 1393, 1395, 1397,
-     1399, 1401, 1403, 1405, 1407, 1409
+        0,    0,   79,  153,  227,  301,  371,    0, 1958, 1957,
+     2034, 2037, 2002,    0, 2037, 2037, 2037,    0,   27, 2037,
+     2037, 2037, 2037,   85,  433,   72,  496,  101, 2037, 2037,
+     2037, 2037, 2037, 2037,    0, 1990, 1999, 2037, 2037,  102,
+       42,   98,   32, 1955, 1965, 1958,   56,  109,  112, 1958,
+      108,  125,  131,  129,  178, 1962, 1959, 2037, 2037, 2037,
+        0, 1993, 2037, 1981,  178,  182,  192,  118, 1947, 1957,
+     1950,   58,  203,  184, 1950,  202,  197,  253,  220,  206,
+     1954, 1951,    0, 1985, 2037, 1999, 1972,  263,  262,  273,
+      122, 1938, 1948, 1941,   71,  276,  221, 1941,  281,  252,
+
+      325,  294,  328, 1945, 1942, 1991, 2037, 1975,    0,    0,
+       68, 1940, 1935, 1950, 1949,  329, 1937, 1936, 1938, 1945,
+      393, 2037, 1941, 1940, 1939,  404,  525,  406,  365,  457,
+      516,  391,  279,   73,  490,  506,  413, 1925,  338,  525,
+      585,  390,  545,  565,  543, 1927, 2037, 2037, 2037,    0,
+      610,  645,  656,  179,    0,    0, 2037,    0,    0, 1947,
+     1948, 1920, 1918, 1932, 1931, 1920, 1916,  164,  562, 1921,
+     1906, 1916, 1911, 1909, 1906, 1957,  186, 1924, 1906,  702,
+     1956,   85, 1910, 1908, 1899, 1908, 1905, 1910,  411, 1903,
+     1910, 1899,   32, 1898,  502, 1896, 1907,  105,  548, 1897,
+
+     1906, 1891,  330,  782,  590, 1884, 1905, 1889,  159,  291,
+     1885,  481, 1891, 1885,    0, 1922, 1907, 1881, 1879, 1893,
+     1892, 1881, 1877,  322,  630, 1882, 1867, 1877, 1872, 1870,
+     1867, 1918,  244, 1885, 1867,  569, 1917,  137, 1871, 1869,
+     1860, 1869, 1866, 1871,  549, 1864, 1871, 1860,  182, 1859,
+      626, 1857, 1868,  290,  626, 1858, 1867, 1852,  452,  305,
+      631, 1845, 1866, 1850,  287,  402, 1846,  579, 1852, 1846,
+        0, 1883, 2037, 1868, 1842, 1840, 1854, 1853, 1842, 1838,
+      494,  681, 1843, 1828, 1838, 1833, 1831, 1828, 1879,  341,
+     1846, 1828,  614, 1878,  333, 1832, 1830, 1821, 1830, 1827,
+
+     1832,  652, 1825, 1832, 1821,  222, 1820,  671, 1818, 1829,
+      464,  678, 1819, 1828, 1813,  505,  587,  686, 1806, 1827,
+     1811,  539,  546, 1807,  665, 1813, 1807, 1840, 1808, 1820,
+     1799, 1810, 1807, 1804, 1797, 1803, 1805, 1812,  740, 1803,
+     1807, 1793, 2037, 2037, 2037, 1791, 1790, 1803,  586,  197,
+     1791, 1788, 1786, 1832, 1835, 1796, 1831, 2037, 1788,  670,
+     1787, 1796, 1777, 1777,  360, 2037, 2037, 2037,  587, 2037,
+     1793, 2037, 1775, 1789, 1769,  412, 1771, 1769,  690, 1817,
+     1820, 1781, 1816, 1813, 1772, 1757,  633, 1761, 1766, 1759,
+     1767, 2037, 2037, 1754, 1773, 1755, 1771, 2037, 1753, 2037,
+
+     1801, 1804, 1757, 1746, 1755, 1766, 1753, 2037,  361, 1759,
+     1752, 2037, 1746, 1747,  667, 1744,  477,  707, 1751, 1750,
+     1736, 1748, 1747, 1784, 1787, 1784, 2037,  493, 1749, 2037,
+     1750, 1745, 1730, 1731, 1730,  674, 1729, 1722, 2037, 1773,
+     1776, 1773, 2037, 1732, 2037, 2037, 2037, 2037, 1723, 1721,
+     2037, 1737, 1733, 2037, 1719,    0,  782, 2037,  797,    0,
+        0, 1741, 1738, 1743,    0, 1723,  695, 1718, 1718,  862,
+        0, 1716,    0,  673, 1713,    0, 1716,    0, 1707,    0,
+     1724,    0, 1723,    0, 1704,    0, 1707, 1700, 1699, 1698,
+        0,  770,    0, 1716,    0,    0, 1750, 1712,    0,    0,
+
+     1702,    0,    0, 1693, 1710, 1706, 1692,    0,    0,    0,
+     1695, 1692,    0, 1693, 1692, 1695,    0,    0,    0, 1686,
+     1685,    0,    0, 1682, 1681, 1698, 1696,    0,  707,    0,
+     1684,    0, 1680, 1693,  692, 1683, 1690, 1675,  577, 1691,
+     1691,    0, 1708, 1694,    0, 1680,  718, 1675, 1675,  762,
+        0, 1673,    0,  763, 1670,    0, 1673,    0, 1664,    0,
+     1681,    0, 1680,    0, 1661,    0, 1664, 1657, 1656,    0,
+      798,    0, 1674,    0,    0, 1708, 1670,    0,    0, 1660,
+        0,    0, 1651, 1668, 1664, 1650,    0,    0,    0, 1653,
+     1650,    0, 1651, 1650, 1653,    0,    0,    0, 1644, 1643,
+
+        0,    0, 1640, 1657, 1655,    0,  764,    0, 1643,    0,
+     1639, 1652,  763, 1642, 1649, 1634,  707, 1650, 1650,    0,
+     1667, 1653,    0, 1639,  763, 1634, 1634,  811,    0, 1632,
+        0,  767, 1629,    0, 1632,    0, 1623,    0, 1640,    0,
+     1639,    0, 1620,    0, 1623, 1616, 1615,    0,  827,    0,
+     1633,    0,    0, 1667, 1629,    0,    0, 1619,    0,    0,
+     1610, 1627, 1623, 1609,    0,    0,    0, 1612, 1609,    0,
+     1610, 1609, 1612,    0,    0,    0, 1603, 1602,    0,    0,
+     1599, 1616, 1614,    0,  769,    0, 1602,    0, 1598, 1611,
+      767, 1601, 1608, 1593,  709, 1609, 1609,    0, 1614, 1605,
+
+     1598, 1589, 1601, 1598, 1598, 1601, 1592, 1596, 1582,    0,
+     1594,    0, 1582, 1572, 2037, 1578, 1587, 2037, 2037, 2037,
+     1576, 1583, 1567, 2037, 2037, 2037, 2037, 1579, 1622, 1619,
+     1563, 1573, 1580, 1560, 1572, 2037, 2037, 1564, 2037, 1557,
+     2037, 1567, 2037, 2037, 1562, 2037, 1570, 1558, 1571, 1552,
+     2037, 2037, 2037, 1605, 1568, 2037, 1561, 1568, 2037, 1568,
+     2037, 1563, 2037, 2037, 2037, 1567, 2037, 1554, 1553, 1597,
+     1551, 1550, 1599, 2037, 2037, 2037, 2037, 2037, 1560, 1557,
+     1552, 1554, 2037, 1586, 2037, 2037, 2037, 1543, 2037, 2037,
+     2037, 2037, 2037, 2037, 2037, 2037, 1540, 2037, 1535, 1536,
+
+     1545, 1550, 1530, 1543, 1545, 2037, 1540, 1529, 2037, 2037,
+     2037, 1531, 1526, 1534, 1532, 1536, 2037, 2037,    0,    0,
+     2037, 1540, 1539, 1517,    0,    0,    0,    0, 1515, 1514,
+      767, 1531, 1524,    0, 1514, 1517,    0,    0,    0,    0,
+     1530, 1516, 1515, 1559,    0,    0, 1526, 1557, 1512,    0,
+     1519,    0, 1518,    0,    0, 1503,    0,    0,    0,    0,
+        0, 1507, 1506,    0,    0, 1515, 1499,    0, 1513,    0,
+        0,    0,    0,    0,    0,    0,    0,    0, 1553, 1520,
+     1514, 1492,    0,    0,    0,    0, 1490,  774, 1507, 1500,
+        0, 1490, 1493,    0,    0,    0,    0, 1506, 1492, 1536,
+
+        0,    0, 1503, 1534, 1489,    0, 1496,    0, 1495,    0,
+        0, 1426,    0,    0,    0,    0,    0, 1412,    0,    0,
+     1408, 1382,    0, 1390,    0,    0,    0,    0,    0,    0,
+        0,    0,    0, 1410, 1371, 1353, 1329,    0,    0,    0,
+        0, 1325,  783, 1337, 1326,    0, 1313, 1310,    0,    0,
+        0,    0,   38,   62,  169,    0,    0,  147,  215,  197,
+        0,  242,    0,  270,    0,    0,  259,    0,    0,    0,
+        0,    0,  307,    0,    0,  334,  386,    0,  418,    0,
+        0,    0,    0,    0,    0,    0,    0,    0,  469,  564,
+      551,  567,  615,  640,  654,  763,  708,  715,    0,  712,
+
+        0,  768,  782,  792,  785,  785,  779,  789,  789,  832,
+     2037, 2037,  803, 2037,  794,  812,  798,  816, 2037,  808,
+     2037, 2037,  816,  867, 2037, 2037, 2037, 2037,  834, 2037,
+      832,  878,  873,  879,  845,  844,  831,  838, 2037,  839,
+     2037, 2037,  833,  842,  845,  850, 2037,  839,  854,  841,
+     2037,  854,  858,  852,  857,  865, 2037, 2037,    0,    0,
+      884,  877,  865,  870,  871,  868,    0,  873,    0,    0,
+      872,  865,  866,    0,  861,    0,    0,  867,  862,    0,
+      871,  872,    0,    0,  875,  874,  906,  903,  886,  892,
+      889,    0,  894,    0,    0,  893,  886,    0,  881,    0,
+
+        0,  886,  881,    0,  890,    0,    0,  893,  892,  924,
+      924,  901,  906,  904,    0,  910,    0,    0,  909,  902,
+        0,  897,    0,    0,  902,  897,    0,  906,    0,    0,
+      909,  908,  929,  950,    0,  915,  919,  913,  921,  926,
+      924,    0,  920,  926,  928,  914, 2037,  910,  929,  932,
+      917, 2037,  929,  935,  922,  939,  943,  939, 2037,  930,
+     2037,  933, 2037,  974,  936,  937,  929,  944,  941,  932,
+      928, 2037,  952,  945,  950,  942,  953,  942,  941,  956,
+      943,  957, 2037,  950,  954,    0,    0,  977,  970,  956,
+      952,  953,  954,  968,  968,  973,  974,    0,  957,  974,
+
+      977,  978,  974,  967,  963,  992,  995,  973,  969,  970,
+      984,  984,  989,    0,  972,  989,  992,  988,  981,  977,
+     1006, 1014, 1010,  988,  984,  985,  999,  999, 1004,    0,
+      987, 1004, 1007, 1003,  996,  992, 1027, 1044, 1049,    0,
+     1009,    0, 1006,    0, 1021,    0, 1063, 1009, 2037, 2037,
+     1065, 2037, 1010, 1067, 1017, 1015, 1068, 1023, 1080, 2037,
+        0, 1034, 1079, 1080, 1046, 1041, 1045, 1049, 1038, 2037,
+     1048, 1036, 1041, 2037, 2037, 2037, 1049, 2037, 2037, 1043,
+     1052,    0,    0, 1075,    0, 1058, 1045, 1046,    0, 1058,
+     1048, 1045, 1046,    0, 1058, 1048, 1049, 1063, 1069, 1057,
+
+     1087, 1090, 1073, 1060,    0, 1072, 1062, 1059,    0, 1071,
+     1061, 1075, 1081, 1069, 1099, 1099, 1103, 1086, 1073,    0,
+     1085, 1075, 1072,    0, 1084, 1074, 1088, 1094, 1082, 1146,
+        0, 1133, 1104, 1098,    0, 1108, 1110, 1110, 1111, 1112,
+     1095, 1108, 1151, 1108, 1125, 1106, 1162, 1158, 1159, 1116,
+     2037, 2037, 1117, 1130, 2037, 1117, 2037, 2037, 2037, 2037,
+        0,    0,    0, 1117, 2037,    0, 1119,    0, 1128, 1129,
+     1119, 1131, 1132,    0, 1138, 1126, 1193,    0, 1127,    0,
+     1129,    0, 1139, 1129, 1142,    0, 1148, 1136, 1203, 1167,
+     2037, 1138,    0, 1140,    0, 1149, 1139, 1151,    0, 1157,
+
+     1146, 1213, 2037, 1155, 1166, 1156, 1150, 1158, 1153, 1160,
+     1158, 1168, 2037, 2037, 1163, 1161, 1205, 1214, 1215, 1174,
+     1175, 1218, 1177,    0,    0, 1173,    0, 1173, 1175,    0,
+     1176, 1177, 2037, 1184, 1239,    0, 1181,    0, 1181,    0,
+     1183,    0, 1190, 1245, 1208, 1188,    0, 1188,    0, 1189,
+        0, 1196,    0, 1198, 1193, 1197, 1198, 1201, 1201, 1192,
+     1193, 1204, 1203, 1252, 1210, 1211, 1199, 1202, 1219, 1215,
+        0, 2037,    0, 1217, 1218, 1219, 1220, 2037,    0, 1221,
+     1225,    0, 2037,    0, 1226, 1227,    0, 1217, 1229, 1230,
+     1229, 1212, 1231, 1234, 1233, 1217, 1224, 1236, 1243, 1238,
+
+     1235, 1239, 1239, 1243, 1234, 1235, 1243, 2037,    0, 2037,
+        0, 2037,    0,    0,    0,    0,    0,    0,    0,    0,
+        0,    0, 1242, 1252, 1245, 1240, 1247, 1246, 2037, 1249,
+     1244, 1253, 1257, 1252, 1266, 1254, 1268, 1258, 1259, 1263,
+        0, 1268, 2037, 1269, 1257, 1271, 1272, 1273, 1270, 1266,
+     1280, 1280, 1266, 1282, 1268, 2037, 2037, 1279,    0, 1283,
+     1284, 2037, 1285, 2037, 1286, 1280, 1290, 1276, 1293, 1290,
+     1295, 1292, 1285,    0, 2037, 2037, 2037, 2037, 1294, 1299,
+     1296, 1291, 1299, 1293, 1301, 1301,    0, 2037, 1296, 1304,
+     2037, 2037, 2037, 2037, 1305,    0, 2037, 2037, 2037,    0,
+
+     2037, 2037, 1361, 1368, 1375, 1379, 1383, 1390, 1397, 1403,
+     1405, 1407, 1409, 1416, 1423, 1430, 1437, 1444, 1451, 1453,
+     1455, 1462, 1469, 1476, 1478, 1480, 1482, 1484, 1486, 1488,
+     1495, 1497, 1499, 1506, 1508, 1510, 1517, 1524, 1526, 1528,
+     1530, 1532, 1534, 1536, 1538, 1540, 1542, 1544
     } ;
 
-static yyconst flex_int16_t yy_def[1517] =
+static const flex_int16_t yy_def[1649] =
     {   0,
-     1470,    1, 1471, 1471, 1472, 1472,    1,    7,    7,    7,
-     1470, 1470, 1470, 1473, 1470, 1470, 1470, 1474, 1474, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1475, 1475, 1475, 1470, 1470, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1470, 1470, 1470,
-     1476, 1476, 1470, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1477, 1477, 1470, 1470, 1470, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-
-     1477, 1477, 1477, 1477, 1477, 1470, 1470, 1470, 1473, 1474,
-     1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1478,
-     1470, 1470, 1470, 1470, 1479, 1480, 1470, 1481, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1482, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-
-     1475, 1475, 1483, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1484, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1485, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1477, 1477,
-     1470, 1470, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1486,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1487, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1470, 1474, 1474, 1474, 1474, 1474,
-     1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1478, 1470, 1470, 1470, 1488, 1489, 1481, 1475, 1475, 1475,
-     1475, 1475, 1475, 1490, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1470, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1470, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1476, 1476, 1476, 1476,
-     1476, 1476, 1491, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1477, 1470, 1477, 1477, 1477, 1477, 1492,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1470, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474,
-
-     1474, 1474, 1474, 1474, 1474, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1493, 1494,
-     1470, 1475, 1475, 1475, 1475, 1475, 1475, 1470, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1470,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1470,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1475, 1475, 1475, 1475, 1475, 1475, 1475, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1477, 1470, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1477, 1470, 1474, 1474, 1474, 1474, 1474, 1474,
-     1474, 1474, 1474, 1474, 1474, 1474, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1495, 1496, 1475, 1475, 1470, 1475, 1475, 1475, 1475, 1475,
-     1475, 1470, 1475, 1475, 1475, 1475, 1475, 1475, 1475, 1475,
-     1470, 1475, 1475, 1475, 1475, 1470, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1477, 1470, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1470, 1474,
-     1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1497,
-     1498, 1475, 1475, 1470, 1475, 1475, 1475, 1470, 1475, 1475,
-     1475, 1475, 1470, 1475, 1475, 1470, 1470, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1477, 1470, 1470, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-
-     1477, 1477, 1477, 1477, 1470, 1474, 1474, 1474, 1474, 1474,
-     1474, 1474, 1474, 1474, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1499, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1500, 1501, 1475,
-     1475, 1470, 1475, 1475, 1475, 1470, 1475, 1475, 1475, 1470,
-     1475, 1475, 1470, 1470, 1476, 1476, 1476, 1476, 1476, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1477, 1470, 1470, 1477,
-     1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1502,
-     1474, 1474, 1474, 1474, 1474, 1470, 1470, 1470, 1470, 1470,
-
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1503, 1504, 1475, 1470, 1475, 1475,
-     1475, 1470, 1475, 1475, 1470, 1475, 1475, 1470, 1470, 1505,
-     1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476, 1476,
-     1506, 1470, 1470, 1477, 1477, 1477, 1477, 1477, 1477, 1477,
-     1477, 1477, 1502, 1470, 1474, 1474, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1507, 1508,
-     1475, 1470, 1475, 1475, 1470, 1475, 1470, 1470, 1505, 1476,
-     1476, 1476, 1476, 1476, 1476, 1476, 1506, 1470, 1477, 1477,
-     1477, 1477, 1477, 1477, 1474, 1474, 1470, 1470, 1470, 1470,
-
-     1470, 1470, 1470, 1470, 1470, 1509, 1470, 1470, 1475, 1470,
-     1475, 1470, 1476, 1476, 1476, 1470, 1477, 1477, 1477, 1474,
-     1474, 1474, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1510, 1470, 1475, 1470, 1475, 1476, 1476, 1477, 1477, 1474,
-     1474, 1474, 1474, 1474, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1511, 1470, 1470, 1470, 1470, 1470, 1470, 1512, 1470,
-     1470, 1470, 1513, 1470, 1514, 1470, 1515, 1516, 1470,    0,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470
+     1602,    1, 1603, 1603, 1604, 1604,    1,    7,    7,    7,
+     1602, 1602, 1602, 1605, 1602, 1602, 1602, 1606, 1606, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1607, 1607, 1607, 1602, 1602, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1602, 1602, 1602,
+     1608, 1608, 1602, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1609, 1609, 1602, 1602, 1602, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+
+     1609, 1609, 1609, 1609, 1609, 1602, 1602, 1602, 1605, 1606,
+     1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1610,
+     1602, 1602, 1602, 1602, 1611, 1612, 1602, 1613, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1614,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+
+     1607, 1607, 1607, 1615, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1616, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1617,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1609, 1609, 1602, 1602, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1618, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1619, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1602, 1606, 1606,
+     1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606,
+     1606, 1606, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1610, 1602, 1602, 1602, 1620,
+     1621, 1613, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1622,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1602, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1602, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1623,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1609, 1602, 1609, 1609, 1609, 1609, 1609, 1624, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1602, 1606,
+
+     1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606,
+     1606, 1606, 1606, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1625, 1626,
+     1602, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1602, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1602, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1602, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1609, 1602, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1602,
+     1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606,
+
+     1606, 1606, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1627, 1628,
+     1607, 1607, 1607, 1602, 1607, 1607, 1607, 1607, 1607, 1607,
+     1607, 1602, 1607, 1607, 1607, 1607, 1607, 1607, 1607, 1607,
+     1602, 1607, 1607, 1607, 1607, 1602, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1609,
+     1602, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1602, 1606, 1606, 1606, 1606, 1606, 1606, 1606,
+     1606, 1606, 1606, 1606, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1629, 1630, 1607, 1607, 1607,
+     1602, 1607, 1607, 1607, 1607, 1602, 1607, 1607, 1607, 1607,
+
+     1602, 1607, 1607, 1602, 1602, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1609, 1602, 1602, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1602, 1606, 1606, 1606,
+     1606, 1606, 1606, 1606, 1606, 1606, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1631, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1632, 1633, 1607, 1607, 1607, 1602, 1607, 1607, 1607,
+     1607, 1602, 1607, 1607, 1607, 1602, 1607, 1607, 1602, 1602,
+
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1609, 1602, 1602, 1609, 1609, 1609,
+     1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1634,
+     1606, 1606, 1606, 1606, 1606, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1635, 1636, 1607, 1607, 1602, 1607, 1607, 1607, 1602, 1607,
+     1607, 1602, 1607, 1607, 1602, 1602, 1637, 1608, 1608, 1608,
+     1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1638, 1602,
+     1602, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609,
+
+     1609, 1634, 1602, 1606, 1606, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1639, 1640, 1607, 1607, 1602, 1607, 1607,
+     1602, 1607, 1602, 1602, 1637, 1608, 1608, 1608, 1608, 1608,
+     1608, 1608, 1608, 1638, 1602, 1609, 1609, 1609, 1609, 1609,
+     1609, 1609, 1606, 1606, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1641, 1602, 1607, 1602, 1607, 1602, 1607, 1602, 1608, 1608,
+     1608, 1608, 1602, 1609, 1609, 1609, 1609, 1606, 1606, 1606,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1642, 1602,
+     1607, 1602, 1607, 1608, 1608, 1609, 1609, 1606, 1606, 1606,
+     1606, 1606, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1643, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1644, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1645, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1646, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1647, 1602, 1602, 1602, 1648,
+
+     1602,    0, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602
     } ;
 
-static yyconst flex_uint16_t yy_nxt[1983] =
+static const flex_int16_t yy_nxt[2118] =
     {   0,
        12,   13,   14,   15,   16,   17,   18,   19,   20,   21,
        12,   22,   23,   24,   25,   26,   27,   28,   28,   28,
@@ -987,219 +1194,234 @@ static yyconst flex_uint16_t yy_nxt[1983] =
        39,   35,   40,   41,   42,   43,   44,   45,   35,   35,
        46,   35,   47,   48,   49,   50,   51,   35,   52,   53,
        54,   35,   55,   56,   57,   35,   35,   58,   59,   60,
-       62,  111,  149,  112,   63,  113,  114,  150,  500,  115,
-     1108,  116,  174,  117,  166,  501,  118,  119,  175,  167,
+       62,  111,  149,  112,   63,  113,  114,  150,  506,  115,
+     1119,  116,  175,  117,  167,  507,  118,  119,  176,  168,
 
       120,  121,  121,  121,  121,  121,  121,  121,  121,  121,
-      168,  179,   64,  234,  180,  151,  235,  153,  153,  153,
-      153,  153,  153,  153,  153,  153,  290, 1109,  391,  291,
-      326,   65,   66,   67,   68,   69,   70,  392,  327,   71,
-      486,   72,   73,   74,   75,   76,  157,   77,   78,   79,
-      169,   80,   81,   82,   62,  162, 1110,  163,   63,  487,
-      170,  181,  171,  172,  186,  182,  164,  508,  187,  183,
-      173,  190,  165,  191,  184,  509,  192,  188,  229,  193,
-      185,  194,  285,  196,  230,  205,   64,  197,  286, 1111,
-      198,  199,  564,  200,  195,  454,  454,  206,  201, 1112,
-
-      202,  203,  204,  207,  478,   65,   66,   67,   68,   69,
-       70,  565,  527,   71,  528,   72,   73,   74,   75,   76,
-      465,   77,   78,   79,  466,   80,   81,   82,   84,   85,
-      208,  217,  236,  218,  221,  370,  237,   86, 1113,  222,
-      238,  209,  219,  210,  224,  239,  479,  211,  220,  241,
-      223,  240,  578,  242,  225,  260,  226,  227,  263,  579,
-       87,  245,  243,  246,  228,  214,  247,  261,  518,  264,
-      248,  265,  249,  262,  494,  266,  495,  371,  496,   88,
-       89,   90,   91,   92,   93,  250,  372,   94, 1114,   95,
-       96,   97,   98,   99,  280,  100,  101,  102,  711,  103,
-
-      104,  105,   84,   85,  281,  251,  282,  283,  273,  252,
-      274,   86,  253,  254,  284,  255,  277,  557,  712,  275,
-      256,  278,  257,  258,  259,  276,  297,  292, 1117, 1118,
-      298,  293,  279,  332,   87,  294,  301,  385,  302,  299,
-      295,  303,  604,  386,  605,  304,  296,  305,  333,  334,
-      316,  387,  335,   88,   89,   90,   91,   92,   93,  558,
-      306,   94,  317,   95,   96,   97,   98,   99,  318,  100,
-      101,  102,  655,  103,  104,  105,   12,  307,  516,  656,
-      319,  308,  388,  517,  309,  310,  106,  311,  373,  408,
-      389,  320,  312,  321,  313,  314,  315,  322,  374,  390,
-
-      735,  375,  409, 1119,   35,  410,  736,  411,  376,  121,
-      121,  121,  121,  121,  121,  121,  121,  121,  343,  398,
-      361,  529,  586,   35,   35,   35,   35,   35,   35,  530,
-      587,   35, 1120,   35,   35,   35,   35,   35,  157,   35,
-       35,   35,  432,   35,   35,   35,  433,  774,  107,  122,
-      123,  124,  125,  775,  399,  434,  510,  344,  362,  345,
-      363,  511,  400,  364,  365,  401,  346,  512,  347,  544,
-      348,  366,  349,  545,  606,  367,  368,  446,  369,  572,
-      447,  573,  607,  574,  448,  126,  127,  128,  129,  130,
-      131,  132,  133,  134,  449,  135,  136,  137,  138,  139,
-
-     1121,  140,  141,  142,  143,  144,  145,  146,  147,  148,
-      151, 1122,  152,  152,  152,  152,  152,  152,  152,  153,
-      153,  377,  393,  378,  379,  503,  380,  641,  154,  594,
-      155,  394,  156,  504,  595,  395,  396,  397, 1123,  350,
-      505,  157,  351,  158,  352,  353,  642,  354,  355,  154,
-      402,  155,  403,  156,  404,  621,  441,  442,  443,  622,
-      681,  381,  682, 1124,  382,  581,  435,  405,  436,  406,
-      158,  437,  438,  582,  383,  384,  663,  356,  357,  412,
-      583,  413,  358,  467,  664,  634,  482,  468,  414,  415,
-      416,  417,  469,  359,  444,  658,  214,  520,  445,  818,
-
-      360,  418,  419,  659,  420,  470,  521,  421,  422,  482,
-      660,  522,  523,  439, 1125,  440,  452,  452,  452,  452,
-      452,  452,  452,  452,  452, 1126,  151,  635,  152,  152,
-      152,  152,  152,  152,  152,  153,  153,  423,  709, 1127,
-      710,  424,  425,  532,  426,  214,  533,  639,  482, 1128,
-      729,  518,  730,  427,  428,  429,  430,  453,  534,  151,
-      431,  153,  153,  153,  153,  153,  153,  153,  153,  153,
-      597,  546,  588,  609,  518,  547,  610,  589, 1129,  598,
-      548,  671,  721,  590,  599,  600,  672,  722,  611,  683,
-      157,  482,  482,  549,  482,  482,  482,  684,  482,  482,
-
-      482,  482,  482,  482,  482,  482,  482,  649,  771,  650,
-      686,  651,  772,  687,  562,  674,  482,  482,  482,  482,
-      482,  482,  623,  665,  675,  688,  624,  739,  666,  676,
-      677,  625,  727,  740,  667,  702,  702,  702,  702,  728,
-      482,  482,  765,  741,  626,  749,  776,  750,  795,  777,
-      787,  814,  854,  796,  859,  766, 1115,  788,  778, 1116,
-      864,  917,  484,  815, 1130,  855,  865,  918,  482,  482,
-      482,  518,  518,  860,  518,  518,  518,  832,  518,  518,
-      518,  518,  518,  518,  518,  518,  518,  452,  452,  452,
-      452,  452,  452,  452,  452,  452,  518,  518,  518,  518,
-
-      518,  518,  454,  454,  821,  870,  876,  923,  912,  886,
-      907,  818,  929,  833,  965,  960,  970,  871, 1133,  924,
-      518,  518,  971,  908,  822,  834,  877,  913,  961, 1134,
-     1135,  808,  930,  966,  818,  939, 1131, 1136, 1137, 1138,
-     1132, 1139, 1140, 1144, 1145,  887, 1146, 1147,  518,  518,
-      518,  818,  818, 1141,  818,  818,  818,  888,  818,  818,
-      818,  818,  818,  818,  818,  818,  818, 1142, 1148, 1143,
-     1149,  940, 1150, 1151, 1152, 1153,  818,  818,  818,  818,
-      818,  818, 1154,  941, 1155, 1156, 1157, 1158, 1159, 1162,
-     1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172,
-
-      818,  818, 1173, 1174, 1175, 1176, 1178, 1179, 1180, 1181,
-     1182, 1183, 1177, 1184, 1185, 1186, 1187, 1188, 1189, 1191,
-     1192, 1194, 1195, 1193, 1196, 1190, 1197, 1198,  818,  818,
-      818, 1199, 1200, 1201, 1202, 1203, 1205, 1206, 1207, 1208,
-     1209, 1210, 1204, 1211, 1212, 1213, 1214, 1215, 1216, 1217,
-     1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1227, 1228,
-     1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238,
-     1239, 1240, 1241, 1242, 1226, 1243, 1244, 1245, 1246, 1247,
-     1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,
-     1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269,
-
-     1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279,
-     1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289,
-     1290, 1291, 1292, 1292, 1292, 1292, 1292, 1292, 1292, 1292,
-     1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1300,
-     1300, 1300, 1300, 1300, 1300, 1300, 1300, 1301, 1303, 1304,
-     1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,
+      169,  180,   64,  236,  181,  151,  237,  153,  153,  153,
+      153,  153,  153,  153,  153,  153,  293, 1120,  394,  294,
+      329,   65,   66,   67,   68,   69,   70,  395,  330,   71,
+      492,   72,   73,   74,   75,   76,  157,   77,   78,   79,
+      170,   80,   81,   82,   62,  162,  163,  164,   63,  493,
+      171,  182,  172,  173,  187,  183,  165,  514,  188,  184,
+      174,  191,  166,  192,  185,  515,  193,  189,  231,  194,
+      186,  195,  288,  197,  232,  206,   64,  198,  289, 1121,
+      199,  200,  571,  201,  196,  459,  459,  207,  202, 1122,
+
+      203,  204,  205,  208,  484,   65,   66,   67,   68,   69,
+       70,  572,  533,   71,  534,   72,   73,   74,   75,   76,
+      471,   77,   78,   79,  472,   80,   81,   82,   84,   85,
+      209,  218,  219,  220,  223, 1123,  243,   86,  585,  224,
+      244,  210,  221,  211,  226,  586,  485,  212,  222,  245,
+      225,  250,  719,  251,  227,  238,  228,  229,  265,  239,
+       87, 1124,  564,  240,  230,  247,  252,  248,  241,  266,
+      249,  267,  720,  300,  242,  268,  262,  301,  663,   88,
+       89,   90,   91,   92,   93,  664,  302,   94,  263,   95,
+       96,   97,   98,   99,  264,  100,  101,  102, 1125,  103,
+
+      104,  105,   84,   85,  565,  253,  307,  215,  308,  254,
+      524,   86,  255,  256,  280,  257,  275,  276,  277,  281,
+      258,  309,  259,  260,  261,  283, 1126,  278,  295, 1127,
+      282,  391,  296,  279,   87,  284,  297,  285,  286,  392,
+      611,  298,  612,  535,  304,  287,  305,  299,  393,  306,
+      319,  536,  593,   88,   89,   90,   91,   92,   93,  642,
+      594,   94,  320,   95,   96,   97,   98,   99,  321,  100,
+      101,  102, 1128,  103,  104,  105,   12,  310,  551,  373,
+      322,  311,  552,  335,  312,  313,  106,  314,  649, 1129,
+      413,  323,  315,  324,  316,  317,  318,  325,  336,  337,
+
+      522,  643,  338,  414,   35,  523,  415,  650,  416,  121,
+      121,  121,  121,  121,  121,  121,  121,  121,  346,  775,
+      364,  374,  735,   35,   35,   35,   35,   35,   35,  736,
+      375,   35,  776,   35,   35,   35,   35,   35,  157,   35,
+       35,   35,  437,   35,   35,   35,  438,  388,  107,  122,
+      123,  124,  125,  389,  613,  439, 1130,  347,  365,  348,
+      366,  390,  614,  367,  368,  407,  349,  408,  350,  409,
+      351,  369,  352, 1131,  743,  370,  371,  500,  372,  501,
+      744,  502,  410, 1132,  411,  126,  127,  128,  129,  130,
+      131,  132,  133,  134,  784,  135,  136,  137,  138,  139,
+
+      785,  140,  141,  142,  143,  144,  145,  146,  147,  148,
+      151,  376,  152,  152,  152,  152,  152,  152,  152,  153,
+      153,  377,  601,  401,  378,  402,  671,  602,  154,  403,
+      155,  379,  156,  380,  672,  381,  382,  538,  383,  353,
+      539,  157,  354,  158,  355,  356,  396,  357,  358,  154,
+      629,  155,  540,  156,  630,  397,  797,  509,  404,  398,
+      399,  400,  440,  798,  441,  510,  405,  442,  443,  406,
+      158,  215,  511,  384,  488,  679,  385,  359,  360,  417,
+      680,  418,  361,  446,  447,  448,  386,  387,  419,  420,
+      421,  422,  689,  362,  690, 1133,  451,  524,  691,  452,
+
+      363,  423,  424,  453,  425,  516,  692,  426,  427,  444,
+      517,  445, 1134,  454,  473,  579,  518,  580,  474,  581,
+      524,  449, 1135,  475,  488,  450,  457,  457,  457,  457,
+      457,  457,  457,  457,  457,  616,  476,  428,  617,  876,
+      569,  429,  430,  526,  431,  877,  717,  488,  718,  737,
+      618,  738,  527,  432,  433,  434,  435,  528,  529,  151,
+      436,  152,  152,  152,  152,  152,  152,  152,  153,  153,
+      151, 1136,  153,  153,  153,  153,  153,  153,  153,  153,
+      153,  588,  553,  595,  604,  647,  554,  729,  596,  589,
+      458,  555,  730,  605,  597,  757,  590,  758,  606,  607,
+
+     1137,  157,  488,  488,  556,  488,  488,  488, 1138,  488,
+      488,  488,  488,  488,  488,  488,  488,  488,  657,  781,
+      658,  694,  659,  782,  695,  833,  666,  488,  488,  488,
+      488,  488,  488,  631,  667,  673,  696,  632,  805,  682,
+      674,  668,  633,  806,  747,  834,  675,  871,  683,  825,
+      748,  488,  488,  684,  685,  634,  710,  710,  710,  710,
+      749,  826,  786,  866,  215,  787,  872,  829, 1141,  931,
+     1142,  986,  883,  490,  788,  932,  867,  987, 1143,  488,
+      488,  488,  524,  524,  884,  524,  524,  524,  844,  524,
+      524,  524,  524,  524,  524,  524,  524,  524,  457,  457,
+
+      457,  457,  457,  457,  457,  457,  457,  524,  524,  524,
+      524,  524,  524,  459,  459,  890,  900,  938,  926,  945,
+      921,  829,  981, 1139,  845,  976, 1140, 1066, 1144,  939,
+      888,  524,  524,  922, 1091,  891,  846,  927,  977,  946,
+     1065,  982,  818, 1114,  829,  955, 1145, 1090, 1146, 1147,
+     1148, 1149,  901, 1150, 1151, 1152, 1113, 1153, 1154,  524,
+      524,  524,  829,  829,  902,  829,  829,  829, 1157,  829,
+      829,  829,  829,  829,  829,  829,  829,  829, 1155,  943,
+     1158,  956, 1156, 1159, 1160, 1161, 1162,  829,  829,  829,
+      829,  829,  829,  957, 1163, 1164, 1165, 1166, 1170, 1167,
+
+     1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,
+     1181,  829,  829, 1168, 1182, 1169, 1183, 1184, 1185, 1188,
+     1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198,
+      831, 1199, 1200, 1201, 1202, 1203, 1204, 1206, 1207,  829,
+      829,  829, 1208, 1205, 1209, 1210, 1211, 1212, 1213, 1214,
+     1215, 1216, 1217, 1218, 1219, 1221, 1222, 1224, 1225, 1223,
+     1226, 1220, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234,
+     1235, 1237, 1238, 1239, 1240, 1241, 1242, 1236, 1243, 1244,
+     1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254,
+     1255, 1256, 1257, 1259, 1260, 1261, 1262, 1263, 1264, 1265,
+
+     1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1258,
+     1275, 1276, 1277, 1278, 1279, 1280, 1281, 1284, 1285, 1286,
+     1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296,
+     1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306,
+     1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316,
      1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326,
-     1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,
-     1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346,
-     1347, 1348, 1349, 1350, 1351, 1352, 1354, 1292, 1292, 1292,
-
-     1292, 1292, 1292, 1292, 1292, 1292, 1355, 1356, 1357, 1358,
-     1359, 1360, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300,
-     1300, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1371, 1372,
-     1373, 1374, 1375, 1376, 1377, 1378, 1380, 1381, 1382, 1353,
-     1383, 1384, 1385, 1386, 1354, 1388, 1361, 1389, 1390, 1391,
-     1392, 1393, 1353, 1394, 1354, 1395, 1396, 1397, 1398, 1399,
-     1400, 1401, 1402, 1403, 1404, 1405, 1408, 1409, 1410, 1411,
-     1412, 1380, 1413, 1414, 1353, 1353, 1415, 1354, 1416, 1417,
-     1418, 1419, 1420, 1423, 1421, 1353, 1424, 1425, 1422, 1426,
-     1427, 1428, 1429, 1430, 1432, 1433, 1434, 1435, 1436, 1437,
-
-     1438, 1439, 1440, 1441, 1443, 1445, 1446, 1447, 1353, 1448,
-     1449, 1450, 1451, 1453, 1454, 1455, 1456, 1442, 1444, 1457,
-     1458, 1460, 1461, 1462, 1464, 1466,   61,   61,   61,   61,
-       61,   61,   61,   83,   83,   83,   83,   83,   83,   83,
-      109, 1107,  109,  109,  109,  109,  109,  110, 1106,  110,
-      110,  159, 1105,  159,  159,  214,  214, 1104,  214,  214,
-      214,  214,  269, 1103,  269,  269, 1102,  269,  451, 1101,
-      451,  451,  451,  451,  451,  455,  455,  456,  456,  457,
-      457,  483, 1100,  483,  483,  483,  483,  483,  519, 1099,
-      519,  519,  519,  519,  519,  561,  561,  561,  561,  561,
-
-      561,  561,  596,  596,  596,  596,  596,  596,  596,  638,
-     1098,  638,  638,  638,  638,  638,  673, 1097,  673,  673,
-      673,  673,  673,  809,  809,  810,  810,  819, 1096,  819,
-      819,  819,  819,  819,  874,  874,  874,  874,  874,  874,
-      874,  927, 1095,  927,  927,  927,  927,  927, 1041, 1041,
-     1042, 1042, 1160, 1160, 1161, 1161, 1248, 1248, 1249, 1249,
-     1302, 1094, 1302, 1302, 1302, 1302, 1302, 1315, 1315, 1316,
-     1316, 1353, 1353, 1353, 1353, 1353, 1353, 1353, 1369, 1369,
-     1370, 1370, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1387,
-     1387, 1387, 1387, 1387, 1387, 1387, 1406, 1406, 1407, 1407,
-
-     1431, 1431, 1452, 1452, 1459, 1459, 1463, 1463, 1465, 1465,
-     1467, 1467, 1468, 1468, 1469, 1469, 1093, 1092, 1091, 1090,
-     1089, 1088, 1087, 1086, 1085, 1084, 1083, 1082, 1081, 1080,
-     1079, 1078, 1077, 1076, 1075, 1074, 1073, 1072, 1071, 1070,
-     1069, 1068, 1067, 1066, 1065, 1064, 1063, 1062, 1061, 1060,
-     1059, 1058, 1057, 1056, 1055, 1054, 1053, 1052, 1051, 1050,
-     1049, 1048, 1047, 1046, 1045, 1044, 1043, 1040, 1039, 1038,
-     1037, 1036, 1035, 1034, 1033, 1032, 1031, 1030, 1029, 1028,
-     1027, 1026, 1025, 1024, 1023, 1022, 1021, 1020, 1019, 1018,
-     1017, 1016, 1015, 1014, 1013, 1012, 1011, 1010, 1009, 1008,
-
-     1007, 1006, 1005, 1004, 1003, 1002, 1001, 1000,  999,  998,
-      997,  996,  995,  994,  993,  992,  991,  990,  989,  988,
-      987,  986,  985,  984,  983,  982,  981,  980,  979,  978,
-      977,  976,  975,  974,  973,  972,  969,  968,  967,  964,
-      963,  962,  959,  958,  957,  956,  955,  954,  953,  952,
-      951,  950,  949,  948,  947,  946,  945,  944,  943,  942,
-      938,  938,  937,  936,  935,  934,  933,  932,  931,  928,
-      926,  925,  922,  921,  920,  919,  916,  915,  914,  911,
-      910,  909,  906,  905,  904,  903,  902,  901,  900,  899,
-      898,  897,  896,  895,  894,  893,  892,  891,  890,  889,
-
-      885,  885,  884,  883,  882,  881,  880,  879,  878,  875,
-      873,  872,  869,  868,  867,  866,  863,  862,  861,  858,
-      857,  856,  853,  852,  851,  850,  849,  848,  847,  846,
-      845,  844,  843,  842,  841,  840,  839,  838,  837,  836,
-      835,  831,  831,  830,  829,  828,  827,  826,  825,  824,
-      823,  820,  817,  816,  813,  812,  811,  807,  806,  805,
-      804,  803,  802,  801,  800,  799,  798,  797,  794,  793,
-      792,  791,  790,  789,  786,  785,  784,  783,  782,  781,
-      780,  779,  773,  770,  769,  768,  767,  764,  763,  762,
-      761,  760,  759,  758,  757,  756,  755,  754,  753,  752,
-
-      751,  748,  747,  746,  745,  744,  743,  742,  738,  737,
-      734,  733,  732,  731,  726,  725,  724,  723,  720,  719,
-      718,  717,  716,  715,  714,  713,  708,  707,  706,  705,
-      704,  703,  701,  700,  699,  698,  697,  696,  695,  694,
-      693,  692,  691,  690,  689,  685,  680,  679,  678,  670,
-      669,  668,  662,  661,  657,  654,  653,  652,  648,  647,
-      646,  645,  644,  643,  640,  637,  636,  633,  632,  631,
-      630,  629,  628,  627,  620,  619,  618,  617,  616,  615,
-      614,  613,  612,  608,  603,  602,  601,  593,  592,  591,
-      585,  584,  580,  577,  576,  575,  571,  570,  569,  568,
-
-      567,  566,  563,  560,  559,  556,  555,  554,  553,  552,
-      551,  550,  543,  542,  541,  540,  539,  538,  537,  536,
-      535,  531,  526,  525,  524,  515,  514,  513,  507,  506,
-      502,  499,  498,  497,  493,  492,  491,  490,  489,  488,
-      485,  481,  480,  477,  476,  475,  474,  473,  472,  471,
-      464,  463,  462,  461,  460,  459,  458,  450,  407,  342,
-      341,  340,  339,  338,  337,  336,  331,  330,  329,  328,
-      325,  150,  324,  323,  300,  289,  288,  287,  272,  271,
-      270,  268,  267,  244,  233,  232,  231,  216,  215,  213,
-      212,  189,  178,  177,  176,  161,  160,  108, 1470,   58,
-
-       58,   11, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470
+     1327, 1328, 1329, 1330, 1331, 1332, 1332, 1332, 1332, 1332,
+     1332, 1332, 1332, 1332, 1333, 1334, 1335, 1336, 1337, 1338,
+     1339, 1340, 1341, 1342, 1343, 1343, 1343, 1343, 1343, 1343,
+     1343, 1343, 1343, 1344, 1345, 1347, 1348, 1349, 1350, 1351,
+
+     1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1363,
+     1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373,
+     1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383,
+     1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393,
+     1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1403, 1332,
+     1332, 1332, 1332, 1332, 1332, 1332, 1332, 1332, 1404, 1405,
+     1406, 1407, 1408, 1409, 1410, 1411, 1412, 1343, 1343, 1343,
+     1343, 1343, 1343, 1343, 1343, 1343, 1414, 1415, 1416, 1417,
+     1418, 1419, 1420, 1421, 1422, 1423, 1426, 1427, 1428, 1429,
+     1430, 1431, 1432, 1433, 1434, 1436, 1437, 1438, 1402, 1439,
+
+     1440, 1413, 1441, 1442, 1443, 1403, 1445, 1446, 1447, 1448,
+     1449, 1450, 1451, 1402, 1452, 1403, 1453, 1454, 1455, 1456,
+     1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466,
+     1467, 1468, 1469, 1470, 1473, 1474, 1402, 1475, 1476, 1477,
+     1478, 1436, 1479, 1480, 1402, 1481, 1482, 1403, 1483, 1484,
+     1485, 1486, 1487, 1491, 1488, 1402, 1489, 1492, 1493, 1494,
+     1490, 1495, 1496, 1497, 1498, 1499, 1500, 1505, 1501, 1503,
+     1506, 1507, 1508, 1510, 1511, 1512, 1513, 1514, 1402, 1502,
+     1504, 1515, 1516, 1517, 1518, 1519, 1521, 1523, 1524, 1525,
+     1526, 1527, 1528, 1529, 1530, 1531, 1532, 1534, 1535, 1520,
+
+     1522, 1536, 1537, 1538, 1539, 1540, 1542, 1533, 1543, 1544,
+     1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554,
+     1555, 1556, 1557, 1558, 1560, 1561, 1562, 1563, 1564, 1565,
+     1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1575, 1576,
+     1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586,
+     1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1597, 1598,
+     1599,   61,   61,   61,   61,   61,   61,   61,   83,   83,
+       83,   83,   83,   83,   83,  109, 1118,  109,  109,  109,
+      109,  109,  110, 1117,  110,  110,  159, 1116,  159,  159,
+      215,  215, 1115,  215,  215,  215,  215,  271, 1113,  271,
+
+      271, 1112,  271,  456, 1111,  456,  456,  456,  456,  456,
+      460,  460,  461,  461,  462,  462,  489, 1110,  489,  489,
+      489,  489,  489,  525, 1109,  525,  525,  525,  525,  525,
+      568,  568,  568,  568,  568,  568,  568,  603,  603,  603,
+      603,  603,  603,  603,  646, 1108,  646,  646,  646,  646,
+      646,  681, 1107,  681,  681,  681,  681,  681,  819,  819,
+      820,  820,  830, 1106,  830,  830,  830,  830,  830,  887,
+      887,  887,  887,  887,  887,  887,  942, 1105,  942,  942,
+      942,  942,  942, 1059, 1059, 1060, 1060, 1186, 1186, 1187,
+     1187, 1282, 1282, 1283, 1283, 1346, 1104, 1346, 1346, 1346,
+
+     1346, 1346, 1361, 1361, 1362, 1362, 1402, 1402, 1402, 1402,
+     1402, 1402, 1402, 1424, 1424, 1425, 1425, 1435, 1435, 1435,
+     1435, 1435, 1435, 1435, 1444, 1444, 1444, 1444, 1444, 1444,
+     1444, 1471, 1471, 1472, 1472, 1509, 1509, 1541, 1541, 1559,
+     1559, 1574, 1574, 1587, 1587, 1596, 1596, 1600, 1600, 1601,
+     1601, 1103, 1102, 1101, 1100, 1099, 1098, 1097, 1096, 1095,
+     1094, 1093, 1092, 1090, 1089, 1088, 1087, 1086, 1085, 1084,
+     1083, 1082, 1081, 1080, 1079, 1078, 1077, 1076, 1075, 1074,
+     1073, 1072, 1071, 1070, 1069, 1068, 1067, 1065, 1064, 1063,
+     1062, 1061, 1058, 1057, 1056, 1055, 1054, 1053, 1052, 1051,
+
+     1050, 1049, 1048, 1047, 1046, 1045, 1044, 1043, 1042, 1041,
+     1040, 1039, 1038, 1037, 1036, 1035, 1034, 1033, 1032, 1031,
+     1030, 1029, 1028, 1027, 1026, 1025, 1024, 1023, 1022, 1021,
+     1020, 1019, 1018, 1017, 1016, 1015, 1014, 1013, 1012, 1011,
+     1010, 1009, 1008, 1007, 1006, 1005, 1004, 1003, 1002, 1001,
+     1000,  999,  998,  997,  996,  995,  994,  993,  992,  991,
+      990,  989,  988,  985,  984,  983,  980,  979,  978,  975,
+      974,  973,  972,  971,  970,  969,  968,  967,  966,  965,
+      964,  963,  962,  961,  960,  959,  958,  954,  954,  953,
+      952,  951,  950,  949,  948,  947,  944,  941,  940,  937,
+
+      936,  935,  934,  933,  930,  929,  928,  925,  924,  923,
+      920,  919,  918,  917,  916,  915,  914,  913,  912,  911,
+      910,  909,  908,  907,  906,  905,  904,  903,  899,  899,
+      898,  897,  896,  895,  894,  893,  892,  889,  886,  885,
+      882,  881,  880,  879,  878,  875,  874,  873,  870,  869,
+      868,  865,  864,  863,  862,  861,  860,  859,  858,  857,
+      856,  855,  854,  853,  852,  851,  850,  849,  848,  847,
+      843,  843,  842,  841,  840,  839,  838,  837,  836,  835,
+      832,  828,  827,  824,  823,  822,  821,  817,  816,  815,
+      814,  813,  812,  811,  810,  809,  808,  807,  804,  803,
+
+      802,  801,  800,  799,  796,  795,  794,  793,  792,  791,
+      790,  789,  783,  780,  779,  778,  777,  774,  773,  772,
+      771,  770,  769,  768,  767,  766,  765,  764,  763,  762,
+      761,  760,  759,  756,  755,  754,  753,  752,  751,  750,
+      746,  745,  742,  741,  740,  739,  734,  733,  732,  731,
+      728,  727,  726,  725,  724,  723,  722,  721,  716,  715,
+      714,  713,  712,  711,  709,  708,  707,  706,  705,  704,
+      703,  702,  701,  700,  699,  698,  697,  693,  688,  687,
+      686,  678,  677,  676,  670,  669,  665,  662,  661,  660,
+      656,  655,  654,  653,  652,  651,  648,  645,  644,  641,
+
+      640,  639,  638,  637,  636,  635,  628,  627,  626,  625,
+      624,  623,  622,  621,  620,  619,  615,  610,  609,  608,
+      600,  599,  598,  592,  591,  587,  584,  583,  582,  578,
+      577,  576,  575,  574,  573,  570,  567,  566,  563,  562,
+      561,  560,  559,  558,  557,  550,  549,  548,  547,  546,
+      545,  544,  543,  542,  541,  537,  532,  531,  530,  521,
+      520,  519,  513,  512,  508,  505,  504,  503,  499,  498,
+      497,  496,  495,  494,  491,  487,  486,  483,  482,  481,
+      480,  479,  478,  477,  470,  469,  468,  467,  466,  465,
+      464,  463,  455,  412,  345,  344,  343,  342,  341,  340,
+
+      339,  334,  333,  332,  331,  328,  150,  327,  326,  303,
+      292,  291,  290,  274,  273,  272,  270,  269,  246,  235,
+      234,  233,  217,  216,  214,  213,  190,  179,  178,  177,
+      161,  160,  108, 1602,   58,   58,   11, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602
     } ;
 
-static yyconst flex_int16_t yy_chk[1983] =
+static const flex_int16_t yy_chk[2118] =
     {   0,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
@@ -1209,220 +1431,235 @@ static yyconst flex_int16_t yy_chk[1983] =
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
-        3,   19,   26,   19,    3,   19,   19,   26,  192,   19,
-      973,   19,   43,   19,   41,  192,   19,   19,   43,   41,
+        3,   19,   26,   19,    3,   19,   19,   26,  193,   19,
+      953,   19,   43,   19,   41,  193,   19,   19,   43,   41,
 
        19,   24,   24,   24,   24,   24,   24,   24,   24,   24,
        41,   47,    3,   72,   47,   28,   72,   28,   28,   28,
-       28,   28,   28,   28,   28,   28,   95,  974,  134,   95,
+       28,   28,   28,   28,   28,   28,   95,  954,  134,   95,
       111,    3,    3,    3,    3,    3,    3,  134,  111,    3,
-      181,    3,    3,    3,    3,    3,   28,    3,    3,    3,
-       42,    3,    3,    3,    4,   40,  975,   40,    4,  181,
-       42,   48,   42,   42,   49,   48,   40,  197,   49,   48,
-       42,   51,   40,   51,   48,  197,   51,   49,   68,   52,
-       48,   52,   91,   53,   68,   54,    4,   53,   91,  976,
-       53,   53,  236,   53,   52,  154,  154,   54,   53,  977,
-
-       53,   53,   53,   54,  176,    4,    4,    4,    4,    4,
-        4,  236,  208,    4,  208,    4,    4,    4,    4,    4,
-      167,    4,    4,    4,  167,    4,    4,    4,    5,    5,
-       55,   65,   73,   65,   66,  129,   73,    5,  978,   66,
-       73,   55,   65,   55,   67,   73,  176,   55,   65,   74,
-       66,   73,  247,   74,   67,   79,   67,   67,   80,  247,
-        5,   76,   74,   76,   67,  258,   76,   79,  258,   80,
-       77,   80,   77,   79,  188,   80,  188,  129,  188,    5,
-        5,    5,    5,    5,    5,   77,  129,    5,  979,    5,
-        5,    5,    5,    5,   90,    5,    5,    5,  347,    5,
-
-        5,    5,    6,    6,   90,   78,   90,   90,   88,   78,
-       88,    6,   78,   78,   90,   78,   89,  231,  347,   88,
-       78,   89,   78,   78,   78,   88,   97,   96,  981,  982,
-       97,   96,   89,  116,    6,   96,   99,  132,   99,   97,
-       96,   99,  263,  132,  263,  100,   96,  100,  116,  116,
-      102,  132,  116,    6,    6,    6,    6,    6,    6,  231,
-      100,    6,  102,    6,    6,    6,    6,    6,  102,    6,
-        6,    6,  303,    6,    6,    6,    7,  101,  202,  303,
-      103,  101,  133,  202,  101,  101,    7,  101,  130,  139,
-      133,  103,  101,  103,  101,  101,  101,  103,  130,  133,
-
-      373,  130,  139,  984,    7,  139,  373,  139,  130,  121,
-      121,  121,  121,  121,  121,  121,  121,  121,  126,  136,
-      128,  209,  252,    7,    7,    7,    7,    7,    7,  209,
-      252,    7,  986,    7,    7,    7,    7,    7,  121,    7,
-        7,    7,  142,    7,    7,    7,  142,  412,    7,   25,
-       25,   25,   25,  412,  136,  142,  198,  126,  128,  126,
-      128,  198,  136,  128,  128,  136,  126,  198,  126,  222,
-      126,  128,  126,  222,  264,  128,  128,  145,  128,  243,
-      145,  243,  264,  243,  145,   25,   25,   25,   25,   25,
-       25,   25,   25,   25,  145,   25,   25,   25,   25,   25,
-
-      987,   25,   25,   25,   25,   25,   25,   25,   25,   25,
-       27,  988,   27,   27,   27,   27,   27,   27,   27,   27,
-       27,  131,  135,  131,  131,  194,  131,  292,   27,  257,
-       27,  135,   27,  194,  257,  135,  135,  135,  989,  127,
-      194,   27,  127,   27,  127,  127,  292,  127,  127,   27,
-      137,   27,  137,   27,  137,  278,  144,  144,  144,  278,
-      319,  131,  319,  990,  131,  249,  143,  137,  143,  137,
-       27,  143,  143,  249,  131,  131,  308,  127,  127,  140,
-      249,  140,  127,  168,  308,  287,  290,  168,  140,  140,
-      140,  140,  168,  127,  144,  305,  543,  204,  144,  543,
-
-      127,  140,  141,  305,  141,  168,  204,  141,  141,  290,
-      305,  204,  204,  143,  991,  143,  151,  151,  151,  151,
-      151,  151,  151,  151,  151,  992,  152,  287,  152,  152,
-      152,  152,  152,  152,  152,  152,  152,  141,  346,  993,
-      346,  141,  141,  211,  141,  234,  211,  290,  234,  994,
-      366,  314,  366,  141,  141,  141,  141,  152,  211,  153,
-      141,  153,  153,  153,  153,  153,  153,  153,  153,  153,
-      259,  223,  253,  266,  314,  223,  266,  253,  997,  259,
-      223,  313,  357,  253,  259,  259,  313,  357,  266,  320,
-      153,  179,  179,  223,  179,  179,  179,  320,  179,  179,
-
-      179,  179,  179,  179,  179,  179,  179,  299,  410,  299,
-      322,  299,  410,  322,  234,  315,  179,  179,  179,  179,
-      179,  179,  279,  309,  315,  322,  279,  376,  309,  315,
-      315,  279,  362,  376,  309,  336,  336,  336,  336,  362,
-      179,  179,  404,  376,  279,  384,  413,  384,  431,  413,
-      423,  461,  523,  431,  529,  404,  980,  423,  413,  980,
-      533,  610,  179,  461,  999,  523,  533,  610,  179,  179,
-      179,  203,  203,  529,  203,  203,  203,  486,  203,  203,
-      203,  203,  203,  203,  203,  203,  203,  452,  452,  452,
-      452,  452,  452,  452,  452,  452,  203,  203,  203,  203,
-
-      203,  203,  454,  454,  468,  540,  547,  617,  606,  564,
-      600,  620,  624,  486,  683,  677,  687,  540, 1001,  617,
-      203,  203,  687,  600,  468,  486,  547,  606,  677, 1002,
-     1004,  454,  624,  683,  620,  641, 1000, 1007, 1008, 1013,
-     1000, 1015, 1016, 1018, 1019,  564, 1020, 1022,  203,  203,
-      203,  464,  464, 1017,  464,  464,  464,  564,  464,  464,
-      464,  464,  464,  464,  464,  464,  464, 1017, 1025, 1017,
-     1026,  641, 1027, 1028, 1030, 1031,  464,  464,  464,  464,
-      464,  464, 1032,  641, 1034, 1035, 1036, 1037, 1038, 1043,
-     1044, 1045, 1046, 1048, 1051, 1052, 1053, 1055, 1058, 1059,
-
-      464,  464, 1061, 1062, 1065, 1066, 1067, 1068, 1069, 1071,
-     1074, 1075, 1066, 1077, 1080, 1081, 1083, 1086, 1087, 1088,
-     1089, 1090, 1092, 1089, 1095, 1087, 1096, 1098,  464,  464,
-      464, 1101, 1102, 1104, 1107, 1108, 1109, 1110, 1112, 1113,
-     1114, 1115, 1108, 1116, 1117, 1119, 1120, 1121, 1122, 1124,
-     1125, 1126, 1127, 1129, 1130, 1131, 1132, 1133, 1134, 1136,
-     1138, 1140, 1141, 1142, 1143, 1144, 1145, 1147, 1148, 1149,
-     1150, 1151, 1152, 1153, 1133, 1154, 1155, 1156, 1158, 1159,
-     1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1171, 1172,
-     1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182,
-
-     1183, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193,
-     1194, 1195, 1196, 1197, 1199, 1200, 1201, 1202, 1203, 1204,
-     1205, 1206, 1207, 1207, 1207, 1207, 1207, 1207, 1207, 1207,
-     1207, 1209, 1211, 1213, 1216, 1221, 1223, 1224, 1225, 1225,
-     1225, 1225, 1225, 1225, 1225, 1225, 1225, 1226, 1230, 1231,
-     1232, 1233, 1234, 1235, 1237, 1238, 1239, 1243, 1246, 1247,
-     1250, 1252, 1253, 1254, 1255, 1256, 1257, 1259, 1260, 1261,
-     1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1272,
-     1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282,
-     1283, 1285, 1286, 1287, 1288, 1289, 1290, 1292, 1292, 1292,
-
-     1292, 1292, 1292, 1292, 1292, 1292, 1293, 1294, 1296, 1297,
-     1298, 1299, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300,
-     1300, 1301, 1302, 1303, 1304, 1307, 1308, 1310, 1320, 1322,
-     1323, 1324, 1325, 1326, 1328, 1329, 1330, 1333, 1335, 1330,
-     1336, 1337, 1339, 1340, 1341, 1342, 1300, 1345, 1347, 1348,
-     1349, 1351, 1341, 1352, 1353, 1355, 1356, 1357, 1358, 1359,
-     1360, 1363, 1364, 1365, 1366, 1368, 1372, 1373, 1375, 1376,
-     1378, 1379, 1382, 1384, 1379, 1341, 1386, 1387, 1388, 1390,
-     1392, 1394, 1396, 1397, 1396, 1387, 1398, 1399, 1396, 1400,
-     1401, 1403, 1404, 1405, 1408, 1409, 1410, 1411, 1413, 1414,
-
-     1417, 1418, 1420, 1421, 1422, 1423, 1424, 1425, 1387, 1426,
-     1427, 1428, 1429, 1445, 1446, 1447, 1449, 1421, 1422, 1450,
-     1451, 1454, 1455, 1456, 1462, 1464, 1471, 1471, 1471, 1471,
-     1471, 1471, 1471, 1472, 1472, 1472, 1472, 1472, 1472, 1472,
-     1473,  963, 1473, 1473, 1473, 1473, 1473, 1474,  961, 1474,
-     1474, 1475,  960, 1475, 1475, 1476, 1476,  957, 1476, 1476,
-     1476, 1476, 1477,  951, 1477, 1477,  948, 1477, 1478,  946,
-     1478, 1478, 1478, 1478, 1478, 1479, 1479, 1480, 1480, 1481,
-     1481, 1482,  944, 1482, 1482, 1482, 1482, 1482, 1483,  943,
-     1483, 1483, 1483, 1483, 1483, 1484, 1484, 1484, 1484, 1484,
-
-     1484, 1484, 1485, 1485, 1485, 1485, 1485, 1485, 1485, 1486,
-      942, 1486, 1486, 1486, 1486, 1486, 1487,  939, 1487, 1487,
-     1487, 1487, 1487, 1488, 1488, 1489, 1489, 1490,  938, 1490,
-     1490, 1490, 1490, 1490, 1491, 1491, 1491, 1491, 1491, 1491,
-     1491, 1492,  937, 1492, 1492, 1492, 1492, 1492, 1493, 1493,
-     1494, 1494, 1495, 1495, 1496, 1496, 1497, 1497, 1498, 1498,
-     1499,  932, 1499, 1499, 1499, 1499, 1499, 1500, 1500, 1501,
-     1501, 1502, 1502, 1502, 1502, 1502, 1502, 1502, 1503, 1503,
-     1504, 1504, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1506,
-     1506, 1506, 1506, 1506, 1506, 1506, 1507, 1507, 1508, 1508,
-
-     1509, 1509, 1510, 1510, 1511, 1511, 1512, 1512, 1513, 1513,
-     1514, 1514, 1515, 1515, 1516, 1516,  931,  929,  928,  927,
-      922,  921,  920,  910,  908,  907,  904,  898,  895,  893,
-      891,  890,  889,  886,  885,  884,  879,  878,  876,  875,
-      874,  869,  868,  867,  857,  855,  854,  851,  850,  844,
-      841,  839,  837,  836,  835,  832,  831,  830,  829,  824,
-      823,  821,  820,  819,  818,  813,  812,  806,  805,  804,
-      803,  802,  798,  797,  795,  794,  793,  792,  791,  790,
-      789,  787,  778,  774,  772,  771,  770,  769,  763,  762,
-      761,  760,  758,  754,  752,  750,  749,  747,  746,  742,
-
-      741,  740,  739,  737,  734,  732,  730,  727,  726,  725,
-      724,  723,  722,  721,  720,  715,  714,  713,  709,  708,
-      706,  705,  703,  701,  700,  699,  698,  697,  696,  695,
-      694,  693,  692,  691,  689,  688,  686,  685,  684,  682,
-      681,  679,  675,  674,  673,  670,  669,  665,  664,  663,
-      661,  660,  656,  655,  654,  653,  650,  647,  646,  643,
-      639,  638,  637,  635,  633,  631,  629,  627,  625,  622,
-      619,  618,  615,  614,  612,  611,  609,  608,  607,  605,
-      604,  602,  598,  597,  596,  593,  592,  588,  587,  586,
-      584,  583,  579,  578,  577,  576,  573,  570,  569,  566,
-
-      562,  561,  560,  558,  556,  554,  552,  550,  548,  545,
-      542,  541,  538,  537,  535,  534,  532,  531,  530,  528,
-      527,  525,  521,  520,  519,  518,  515,  514,  510,  509,
-      508,  506,  505,  501,  500,  499,  498,  495,  492,  491,
-      488,  484,  483,  482,  481,  479,  477,  475,  473,  471,
-      469,  466,  463,  462,  459,  458,  457,  450,  448,  447,
-      445,  444,  439,  437,  436,  435,  433,  432,  430,  429,
-      428,  427,  426,  424,  421,  420,  419,  418,  417,  416,
-      415,  414,  411,  409,  408,  406,  405,  402,  401,  400,
-      399,  398,  396,  394,  393,  392,  391,  388,  387,  386,
-
-      385,  383,  382,  381,  380,  379,  378,  377,  375,  374,
-      372,  371,  370,  368,  361,  360,  359,  358,  356,  354,
-      353,  352,  351,  350,  349,  348,  345,  344,  343,  339,
-      338,  337,  335,  334,  333,  332,  331,  330,  329,  328,
-      327,  326,  325,  324,  323,  321,  318,  317,  316,  312,
-      311,  310,  307,  306,  304,  302,  301,  300,  298,  297,
-      296,  295,  294,  293,  291,  289,  288,  286,  285,  284,
-      283,  282,  281,  280,  277,  276,  275,  274,  273,  272,
-      270,  268,  267,  265,  262,  261,  260,  256,  255,  254,
-      251,  250,  248,  246,  245,  244,  242,  241,  240,  239,
-
-      238,  237,  235,  233,  232,  230,  229,  228,  227,  226,
-      225,  224,  221,  220,  219,  218,  217,  216,  215,  213,
-      212,  210,  207,  206,  205,  201,  200,  199,  196,  195,
-      193,  191,  190,  189,  187,  186,  185,  184,  183,  182,
-      180,  178,  177,  175,  174,  173,  172,  171,  170,  169,
-      166,  165,  164,  163,  162,  161,  160,  146,  138,  125,
-      124,  123,  120,  119,  118,  117,  115,  114,  113,  112,
-      108,  106,  105,  104,   98,   94,   93,   92,   87,   86,
-       84,   82,   81,   75,   71,   70,   69,   64,   62,   57,
-       56,   50,   46,   45,   44,   37,   36,   13,   11,   10,
-
-        9, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,
-     1470, 1470
+      182,    3,    3,    3,    3,    3,   28,    3,    3,    3,
+       42,    3,    3,    3,    4,   40,   40,   40,    4,  182,
+       42,   48,   42,   42,   49,   48,   40,  198,   49,   48,
+       42,   51,   40,   51,   48,  198,   51,   49,   68,   52,
+       48,   52,   91,   53,   68,   54,    4,   53,   91,  955,
+       53,   53,  238,   53,   52,  154,  154,   54,   53,  958,
+
+       53,   53,   53,   54,  177,    4,    4,    4,    4,    4,
+        4,  238,  209,    4,  209,    4,    4,    4,    4,    4,
+      168,    4,    4,    4,  168,    4,    4,    4,    5,    5,
+       55,   65,   65,   65,   66,  959,   74,    5,  249,   66,
+       74,   55,   65,   55,   67,  249,  177,   55,   65,   74,
+       66,   77,  350,   77,   67,   73,   67,   67,   80,   73,
+        5,  960,  233,   73,   67,   76,   77,   76,   73,   80,
+       76,   80,  350,   97,   73,   80,   79,   97,  306,    5,
+        5,    5,    5,    5,    5,  306,   97,    5,   79,    5,
+        5,    5,    5,    5,   79,    5,    5,    5,  962,    5,
+
+        5,    5,    6,    6,  233,   78,  100,  260,  100,   78,
+      260,    6,   78,   78,   89,   78,   88,   88,   88,   89,
+       78,  100,   78,   78,   78,   90,  964,   88,   96,  967,
+       89,  133,   96,   88,    6,   90,   96,   90,   90,  133,
+      265,   96,  265,  210,   99,   90,   99,   96,  133,   99,
+      102,  210,  254,    6,    6,    6,    6,    6,    6,  290,
+      254,    6,  102,    6,    6,    6,    6,    6,  102,    6,
+        6,    6,  973,    6,    6,    6,    7,  101,  224,  129,
+      103,  101,  224,  116,  101,  101,    7,  101,  295,  976,
+      139,  103,  101,  103,  101,  101,  101,  103,  116,  116,
+
+      203,  290,  116,  139,    7,  203,  139,  295,  139,  121,
+      121,  121,  121,  121,  121,  121,  121,  121,  126,  409,
+      128,  129,  365,    7,    7,    7,    7,    7,    7,  365,
+      129,    7,  409,    7,    7,    7,    7,    7,  121,    7,
+        7,    7,  142,    7,    7,    7,  142,  132,    7,   25,
+       25,   25,   25,  132,  266,  142,  977,  126,  128,  126,
+      128,  132,  266,  128,  128,  137,  126,  137,  126,  137,
+      126,  128,  126,  979,  376,  128,  128,  189,  128,  189,
+      376,  189,  137,  989,  137,   25,   25,   25,   25,   25,
+       25,   25,   25,   25,  417,   25,   25,   25,   25,   25,
+
+      417,   25,   25,   25,   25,   25,   25,   25,   25,   25,
+       27,  130,   27,   27,   27,   27,   27,   27,   27,   27,
+       27,  130,  259,  136,  130,  136,  311,  259,   27,  136,
+       27,  130,   27,  131,  311,  131,  131,  212,  131,  127,
+      212,   27,  127,   27,  127,  127,  135,  127,  127,   27,
+      281,   27,  212,   27,  281,  135,  428,  195,  136,  135,
+      135,  135,  143,  428,  143,  195,  136,  143,  143,  136,
+       27,  236,  195,  131,  236,  316,  131,  127,  127,  140,
+      316,  140,  127,  144,  144,  144,  131,  131,  140,  140,
+      140,  140,  322,  127,  322,  990,  145,  317,  323,  145,
+
+      127,  140,  141,  145,  141,  199,  323,  141,  141,  143,
+      199,  143,  991,  145,  169,  245,  199,  245,  169,  245,
+      317,  144,  992,  169,  293,  144,  151,  151,  151,  151,
+      151,  151,  151,  151,  151,  268,  169,  141,  268,  539,
+      236,  141,  141,  205,  141,  539,  349,  293,  349,  369,
+      268,  369,  205,  141,  141,  141,  141,  205,  205,  152,
+      141,  152,  152,  152,  152,  152,  152,  152,  152,  152,
+      153,  993,  153,  153,  153,  153,  153,  153,  153,  153,
+      153,  251,  225,  255,  261,  293,  225,  360,  255,  251,
+      152,  225,  360,  261,  255,  387,  251,  387,  261,  261,
+
+      994,  153,  180,  180,  225,  180,  180,  180,  995,  180,
+      180,  180,  180,  180,  180,  180,  180,  180,  302,  415,
+      302,  325,  302,  415,  325,  474,  308,  180,  180,  180,
+      180,  180,  180,  282,  308,  312,  325,  282,  436,  318,
+      312,  308,  282,  436,  379,  474,  312,  535,  318,  467,
+      379,  180,  180,  318,  318,  282,  339,  339,  339,  339,
+      379,  467,  418,  529,  550,  418,  535,  550,  997,  617,
+      998,  695,  547,  180,  418,  617,  529,  695, 1000,  180,
+      180,  180,  204,  204,  547,  204,  204,  204,  492,  204,
+      204,  204,  204,  204,  204,  204,  204,  204,  457,  457,
+
+      457,  457,  457,  457,  457,  457,  457,  204,  204,  204,
+      204,  204,  204,  459,  459,  554,  571,  625,  613,  632,
+      607,  628,  691,  996,  492,  685,  996,  831, 1002,  625,
+      550,  204,  204,  607,  888,  554,  492,  613,  685,  632,
+      831,  691,  459,  943,  628,  649, 1003,  888, 1004, 1005,
+     1006, 1007,  571, 1008, 1009, 1010,  943, 1013, 1015,  204,
+      204,  204,  470,  470,  571,  470,  470,  470, 1017,  470,
+      470,  470,  470,  470,  470,  470,  470,  470, 1016,  628,
+     1018,  649, 1016, 1020, 1023, 1024, 1029,  470,  470,  470,
+      470,  470,  470,  649, 1031, 1032, 1033, 1034, 1036, 1035,
+
+     1037, 1038, 1040, 1043, 1044, 1045, 1046, 1048, 1049, 1050,
+     1052,  470,  470, 1035, 1053, 1035, 1054, 1055, 1056, 1061,
+     1062, 1063, 1064, 1065, 1066, 1068, 1071, 1072, 1073, 1075,
+      470, 1078, 1079, 1081, 1082, 1085, 1086, 1087, 1088,  470,
+      470,  470, 1089, 1086, 1090, 1091, 1093, 1096, 1097, 1099,
+     1102, 1103, 1105, 1108, 1109, 1110, 1111, 1112, 1113, 1111,
+     1114, 1109, 1116, 1119, 1120, 1122, 1125, 1126, 1128, 1131,
+     1132, 1133, 1134, 1136, 1137, 1138, 1139, 1132, 1140, 1141,
+     1143, 1144, 1145, 1146, 1148, 1149, 1150, 1151, 1153, 1154,
+     1155, 1156, 1157, 1158, 1160, 1162, 1164, 1165, 1166, 1167,
+
+     1168, 1169, 1170, 1171, 1173, 1174, 1175, 1176, 1177, 1157,
+     1178, 1179, 1180, 1181, 1182, 1184, 1185, 1188, 1189, 1190,
+     1191, 1192, 1193, 1194, 1195, 1196, 1197, 1199, 1200, 1201,
+     1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211,
+     1212, 1213, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222,
+     1223, 1224, 1225, 1226, 1227, 1228, 1229, 1231, 1232, 1233,
+     1234, 1235, 1236, 1237, 1238, 1239, 1239, 1239, 1239, 1239,
+     1239, 1239, 1239, 1239, 1241, 1243, 1245, 1247, 1248, 1251,
+     1253, 1254, 1255, 1256, 1257, 1257, 1257, 1257, 1257, 1257,
+     1257, 1257, 1257, 1258, 1259, 1262, 1263, 1264, 1265, 1266,
+
+     1267, 1268, 1269, 1271, 1272, 1273, 1277, 1280, 1281, 1284,
+     1286, 1287, 1288, 1290, 1291, 1292, 1293, 1295, 1296, 1297,
+     1298, 1299, 1300, 1301, 1302, 1303, 1304, 1306, 1307, 1308,
+     1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319,
+     1321, 1322, 1323, 1325, 1326, 1327, 1328, 1329, 1330, 1332,
+     1332, 1332, 1332, 1332, 1332, 1332, 1332, 1332, 1333, 1334,
+     1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1343, 1343,
+     1343, 1343, 1343, 1343, 1343, 1343, 1344, 1345, 1346, 1347,
+     1348, 1349, 1350, 1353, 1354, 1356, 1364, 1367, 1369, 1370,
+     1371, 1372, 1373, 1375, 1376, 1377, 1379, 1381, 1377, 1383,
+
+     1384, 1343, 1385, 1387, 1388, 1389, 1390, 1392, 1394, 1396,
+     1397, 1398, 1400, 1389, 1401, 1402, 1404, 1405, 1406, 1407,
+     1408, 1409, 1410, 1411, 1412, 1415, 1416, 1417, 1418, 1419,
+     1420, 1421, 1422, 1423, 1426, 1428, 1389, 1429, 1431, 1432,
+     1434, 1435, 1437, 1439, 1435, 1441, 1443, 1444, 1445, 1446,
+     1448, 1450, 1452, 1455, 1454, 1444, 1454, 1456, 1457, 1458,
+     1454, 1459, 1460, 1461, 1462, 1463, 1464, 1467, 1465, 1466,
+     1468, 1469, 1470, 1474, 1475, 1476, 1477, 1480, 1444, 1465,
+     1466, 1481, 1485, 1486, 1488, 1489, 1490, 1491, 1492, 1493,
+     1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1489,
+
+     1490, 1503, 1504, 1505, 1506, 1507, 1523, 1500, 1524, 1525,
+     1526, 1527, 1528, 1530, 1531, 1532, 1533, 1534, 1535, 1536,
+     1537, 1538, 1539, 1540, 1542, 1544, 1545, 1546, 1547, 1548,
+     1549, 1550, 1551, 1552, 1553, 1554, 1555, 1558, 1560, 1561,
+     1563, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573,
+     1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1589, 1590,
+     1595, 1603, 1603, 1603, 1603, 1603, 1603, 1603, 1604, 1604,
+     1604, 1604, 1604, 1604, 1604, 1605,  948, 1605, 1605, 1605,
+     1605, 1605, 1606,  947, 1606, 1606, 1607,  945, 1607, 1607,
+     1608, 1608,  944, 1608, 1608, 1608, 1608, 1609,  942, 1609,
+
+     1609,  937, 1609, 1610,  936, 1610, 1610, 1610, 1610, 1610,
+     1611, 1611, 1612, 1612, 1613, 1613, 1614,  935, 1614, 1614,
+     1614, 1614, 1614, 1615,  934, 1615, 1615, 1615, 1615, 1615,
+     1616, 1616, 1616, 1616, 1616, 1616, 1616, 1617, 1617, 1617,
+     1617, 1617, 1617, 1617, 1618,  924, 1618, 1618, 1618, 1618,
+     1618, 1619,  922, 1619, 1619, 1619, 1619, 1619, 1620, 1620,
+     1621, 1621, 1622,  921, 1622, 1622, 1622, 1622, 1622, 1623,
+     1623, 1623, 1623, 1623, 1623, 1623, 1624,  918, 1624, 1624,
+     1624, 1624, 1624, 1625, 1625, 1626, 1626, 1627, 1627, 1628,
+     1628, 1629, 1629, 1630, 1630, 1631,  912, 1631, 1631, 1631,
+
+     1631, 1631, 1632, 1632, 1633, 1633, 1634, 1634, 1634, 1634,
+     1634, 1634, 1634, 1635, 1635, 1636, 1636, 1637, 1637, 1637,
+     1637, 1637, 1637, 1637, 1638, 1638, 1638, 1638, 1638, 1638,
+     1638, 1639, 1639, 1640, 1640, 1641, 1641, 1642, 1642, 1643,
+     1643, 1644, 1644, 1645, 1645, 1646, 1646, 1647, 1647, 1648,
+     1648,  909,  907,  905,  904,  903,  900,  899,  898,  893,
+      892,  890,  889,  887,  882,  881,  880,  879,  869,  867,
+      866,  863,  862,  856,  853,  851,  849,  848,  847,  844,
+      843,  842,  841,  836,  835,  833,  832,  830,  829,  824,
+      823,  822,  816,  815,  814,  813,  812,  808,  807,  805,
+
+      804,  803,  802,  801,  800,  799,  797,  788,  784,  782,
+      781,  780,  779,  773,  772,  771,  770,  769,  768,  766,
+      762,  760,  758,  757,  755,  754,  750,  749,  748,  747,
+      745,  742,  740,  738,  735,  734,  733,  732,  731,  730,
+      729,  728,  723,  722,  721,  717,  716,  714,  713,  711,
+      709,  708,  707,  706,  705,  704,  703,  702,  701,  700,
+      699,  697,  696,  694,  693,  692,  690,  689,  687,  683,
+      682,  681,  678,  677,  673,  672,  671,  669,  668,  664,
+      663,  662,  661,  658,  655,  654,  651,  647,  646,  645,
+      643,  641,  639,  637,  635,  633,  630,  627,  626,  624,
+
+      622,  621,  619,  618,  616,  615,  614,  612,  611,  609,
+      605,  604,  603,  600,  599,  595,  594,  593,  591,  590,
+      586,  585,  584,  583,  580,  577,  576,  573,  569,  568,
+      567,  565,  563,  561,  559,  557,  555,  552,  549,  548,
+      546,  544,  543,  541,  540,  538,  537,  536,  534,  533,
+      531,  527,  526,  525,  524,  521,  520,  516,  515,  514,
+      512,  511,  507,  506,  505,  504,  501,  498,  497,  494,
+      490,  489,  488,  487,  485,  483,  481,  479,  477,  475,
+      472,  469,  468,  466,  464,  463,  462,  455,  453,  452,
+      450,  449,  444,  442,  441,  440,  438,  437,  435,  434,
+
+      433,  432,  431,  429,  426,  425,  424,  423,  422,  421,
+      420,  419,  416,  414,  413,  411,  410,  407,  406,  405,
+      404,  403,  402,  401,  399,  397,  396,  395,  394,  391,
+      390,  389,  388,  386,  385,  384,  383,  382,  381,  380,
+      378,  377,  375,  374,  373,  371,  364,  363,  362,  361,
+      359,  357,  356,  355,  354,  353,  352,  351,  348,  347,
+      346,  342,  341,  340,  338,  337,  336,  335,  334,  333,
+      332,  331,  330,  329,  328,  327,  326,  324,  321,  320,
+      319,  315,  314,  313,  310,  309,  307,  305,  304,  303,
+      301,  300,  299,  298,  297,  296,  294,  292,  291,  289,
+
+      288,  287,  286,  285,  284,  283,  280,  279,  278,  277,
+      276,  275,  274,  272,  270,  269,  267,  264,  263,  262,
+      258,  257,  256,  253,  252,  250,  248,  247,  246,  244,
+      243,  242,  241,  240,  239,  237,  235,  234,  232,  231,
+      230,  229,  228,  227,  226,  223,  222,  221,  220,  219,
+      218,  217,  216,  214,  213,  211,  208,  207,  206,  202,
+      201,  200,  197,  196,  194,  192,  191,  190,  188,  187,
+      186,  185,  184,  183,  181,  179,  178,  176,  175,  174,
+      173,  172,  171,  170,  167,  166,  165,  164,  163,  162,
+      161,  160,  146,  138,  125,  124,  123,  120,  119,  118,
+
+      117,  115,  114,  113,  112,  108,  106,  105,  104,   98,
+       94,   93,   92,   87,   86,   84,   82,   81,   75,   71,
+       70,   69,   64,   62,   57,   56,   50,   46,   45,   44,
+       37,   36,   13,   11,   10,    9, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+
+     1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,
+     1602, 1602, 1602, 1602, 1602, 1602, 1602
     } ;
 
 /* Table of booleans, true if rule could match eol. */
-static yyconst flex_int32_t yy_rule_can_match_eol[330] =
+static const flex_int32_t yy_rule_can_match_eol[340] =
     {   0,
 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
@@ -1439,14 +1676,9 @@ static yyconst flex_int32_t yy_rule_can_match_eol[330] =
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
-    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
-    0, 0, 0, 1, 0, 0, 1, 1, 0, 0,     };
-
-static yy_state_type yy_last_accepting_state;
-static char *yy_last_accepting_cpos;
-
-extern int ptx__flex_debug;
-int ptx__flex_debug = 0;
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
+    0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 
+        };
 
 /* The intent behind this definition is that it'll catch
  * any uses of REJECT which flex missed.
@@ -1455,56 +1687,57 @@ int ptx__flex_debug = 0;
 #define yymore() yymore_used_but_not_detected
 #define YY_MORE_ADJ 0
 #define YY_RESTORE_YY_MORE_OFFSET
-char *ptx_text;
 #line 1 "ptx.l"
 /*
-Copyright (c) 2009-2011, Tor M. Aamodt
-The University of British Columbia
+Copyright (c) 2009-2021, Tor M. Aamodt, Vijay Kandiah, Nikos Hardavellas
+The University of British Columbia, Northwestern University
 All rights reserved.
-
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
-Redistributions of source code must retain the above copyright notice, this
-list of conditions and the following disclaimer.
-Redistributions in binary form must reproduce the above copyright notice, this
-list of conditions and the following disclaimer in the documentation and/or
-other materials provided with the distribution.
-Neither the name of The University of British Columbia nor the names of its
-contributors may be used to endorse or promote products derived from this
-software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+1. Redistributions of source code must retain the above copyright notice, this
+   list of conditions and the following disclaimer;
+2. Redistributions in binary form must reproduce the above copyright notice,
+   this list of conditions and the following disclaimer in the documentation
+   and/or other materials provided with the distribution;
+3. Neither the names of The University of British Columbia, Northwestern 
+   University nor the names of their contributors may be used to
+   endorse or promote products derived from this software without specific
+   prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+POSSIBILITY OF SUCH DAMAGE.
 */
-#line 35 "ptx.l"
+#line 39 "ptx.l"
 #include "opcodes.h"
+#include "ptx_parser.h"
 #include "ptx.tab.h"
 #include <string.h>
+#include "../../libcuda_sim/gpgpu_context.h"
+// typedef void* yyscan_t;
 
-#define LINEBUF_SIZE (64*1024)
-char linebuf[LINEBUF_SIZE];
-unsigned col = 0;
-#define TC col+=strlen(ptx_text); 
+#define LINEBUF_SIZE (4*1024)
+#define TC recognizer->col+=strlen(yytext);
 #define CHECK_UNSIGNED \
-	if( ptx_text[strlen(ptx_text)-1]=='U' ) { \
+	if( yytext[strlen(yytext)-1]=='U' ) { \
 		printf("GPGPU-Sim: ERROR ** U modifier not implemented\n"); \
 		abort(); \
 	}
-int ptx_error( const char *s );
-
-
-
+#define YY_DECL int ptx_lex \
+	       (YYSTYPE * yylval_param , yyscan_t yyscanner, ptx_recognizer* recognizer)
+int ptx_error( yyscan_t yyscanner, ptx_recognizer* recognizer, const char *s );
+#line 1739 "lex.ptx_c.c"
 
-#line 1508 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptx_.c"
+#line 1741 "lex.ptx_c.c"
 
 #define INITIAL 0
 #define IN_STRING 1
@@ -1524,36 +1757,88 @@ int ptx_error( const char *s );
 #define YY_EXTRA_TYPE void *
 #endif
 
-static int yy_init_globals (void );
+/* Holds the entire state of the reentrant scanner. */
+struct yyguts_t
+    {
+
+    /* User-defined. Not touched by flex. */
+    YY_EXTRA_TYPE yyextra_r;
+
+    /* The rest are the same as the globals declared in the non-reentrant scanner. */
+    FILE *yyin_r, *yyout_r;
+    size_t yy_buffer_stack_top; /**< index of top of stack. */
+    size_t yy_buffer_stack_max; /**< capacity of stack. */
+    YY_BUFFER_STATE * yy_buffer_stack; /**< Stack as an array. */
+    char yy_hold_char;
+    int yy_n_chars;
+    int yyleng_r;
+    char *yy_c_buf_p;
+    int yy_init;
+    int yy_start;
+    int yy_did_buffer_switch_on_eof;
+    int yy_start_stack_ptr;
+    int yy_start_stack_depth;
+    int *yy_start_stack;
+    yy_state_type yy_last_accepting_state;
+    char* yy_last_accepting_cpos;
+
+    int yylineno_r;
+    int yy_flex_debug_r;
+
+    char *yytext_r;
+    int yy_more_flag;
+    int yy_more_len;
+
+    YYSTYPE * yylval_r;
+
+    }; /* end struct yyguts_t */
+
+static int yy_init_globals ( yyscan_t yyscanner );
+
+    /* This must go here because YYSTYPE and YYLTYPE are included
+     * from bison output in section 1.*/
+    #    define yylval yyg->yylval_r
+    
+int yylex_init (yyscan_t* scanner);
+
+int yylex_init_extra ( YY_EXTRA_TYPE user_defined, yyscan_t* scanner);
 
 /* Accessor methods to globals.
    These are made visible to non-reentrant scanners for convenience. */
 
-int ptx_lex_destroy (void );
+int yylex_destroy ( yyscan_t yyscanner );
+
+int yyget_debug ( yyscan_t yyscanner );
+
+void yyset_debug ( int debug_flag , yyscan_t yyscanner );
 
-int ptx_get_debug (void );
+YY_EXTRA_TYPE yyget_extra ( yyscan_t yyscanner );
 
-void ptx_set_debug (int debug_flag  );
+void yyset_extra ( YY_EXTRA_TYPE user_defined , yyscan_t yyscanner );
 
-YY_EXTRA_TYPE ptx_get_extra (void );
+FILE *yyget_in ( yyscan_t yyscanner );
 
-void ptx_set_extra (YY_EXTRA_TYPE user_defined  );
+void yyset_in  ( FILE * _in_str , yyscan_t yyscanner );
 
-FILE *ptx_get_in (void );
+FILE *yyget_out ( yyscan_t yyscanner );
 
-void ptx_set_in  (FILE * _in_str  );
+void yyset_out  ( FILE * _out_str , yyscan_t yyscanner );
 
-FILE *ptx_get_out (void );
+			int yyget_leng ( yyscan_t yyscanner );
 
-void ptx_set_out  (FILE * _out_str  );
+char *yyget_text ( yyscan_t yyscanner );
 
-yy_size_t ptx_get_leng (void );
+int yyget_lineno ( yyscan_t yyscanner );
 
-char *ptx_get_text (void );
+void yyset_lineno ( int _line_number , yyscan_t yyscanner );
 
-int ptx_get_lineno (void );
+int yyget_column  ( yyscan_t yyscanner );
 
-void ptx_set_lineno (int _line_number  );
+void yyset_column ( int _column_no , yyscan_t yyscanner );
+
+YYSTYPE * yyget_lval ( yyscan_t yyscanner );
+
+void yyset_lval ( YYSTYPE * yylval_param , yyscan_t yyscanner );
 
 /* Macros after this point can all be overridden by user definitions in
  * section 1.
@@ -1561,9 +1846,9 @@ void ptx_set_lineno (int _line_number  );
 
 #ifndef YY_SKIP_YYWRAP
 #ifdef __cplusplus
-extern "C" int ptx_wrap (void );
+extern "C" int yywrap ( yyscan_t yyscanner );
 #else
-extern int ptx_wrap (void );
+extern int yywrap ( yyscan_t yyscanner );
 #endif
 #endif
 
@@ -1572,19 +1857,18 @@ extern int ptx_wrap (void );
 #endif
 
 #ifndef yytext_ptr
-static void yy_flex_strncpy (char *,yyconst char *,int );
+static void yy_flex_strncpy ( char *, const char *, int , yyscan_t yyscanner);
 #endif
 
 #ifdef YY_NEED_STRLEN
-static int yy_flex_strlen (yyconst char * );
+static int yy_flex_strlen ( const char * , yyscan_t yyscanner);
 #endif
 
 #ifndef YY_NO_INPUT
-
 #ifdef __cplusplus
-static int yyinput (void );
+static int yyinput ( yyscan_t yyscanner );
 #else
-static int input (void );
+static int input ( yyscan_t yyscanner );
 #endif
 
 #endif
@@ -1604,7 +1888,7 @@ static int input (void );
 /* This used to be an fputs(), but since the string might contain NUL's,
  * we now use fwrite().
  */
-#define ECHO do { if (fwrite( ptx_text, ptx_leng, 1, ptx_out )) {} } while (0)
+#define ECHO do { if (fwrite( yytext, (size_t) yyleng, 1, yyout )) {} } while (0)
 #endif
 
 /* Gets input and stuffs it into "buf".  number of characters read, or YY_NULL,
@@ -1615,20 +1899,20 @@ static int input (void );
 	if ( YY_CURRENT_BUFFER_LVALUE->yy_is_interactive ) \
 		{ \
 		int c = '*'; \
-		size_t n; \
+		int n; \
 		for ( n = 0; n < max_size && \
-			     (c = getc( ptx_in )) != EOF && c != '\n'; ++n ) \
+			     (c = getc( yyin )) != EOF && c != '\n'; ++n ) \
 			buf[n] = (char) c; \
 		if ( c == '\n' ) \
 			buf[n++] = (char) c; \
-		if ( c == EOF && ferror( ptx_in ) ) \
+		if ( c == EOF && ferror( yyin ) ) \
 			YY_FATAL_ERROR( "input in flex scanner failed" ); \
 		result = n; \
 		} \
 	else \
 		{ \
 		errno=0; \
-		while ( (result = fread(buf, 1, max_size, ptx_in))==0 && ferror(ptx_in)) \
+		while ( (result = (int) fread(buf, 1, (yy_size_t) max_size, yyin)) == 0 && ferror(yyin)) \
 			{ \
 			if( errno != EINTR) \
 				{ \
@@ -1636,7 +1920,7 @@ static int input (void );
 				break; \
 				} \
 			errno=0; \
-			clearerr(ptx_in); \
+			clearerr(yyin); \
 			} \
 		}\
 \
@@ -1658,7 +1942,7 @@ static int input (void );
 
 /* Report a fatal error. */
 #ifndef YY_FATAL_ERROR
-#define YY_FATAL_ERROR(msg) yy_fatal_error( msg )
+#define YY_FATAL_ERROR(msg) yy_fatal_error( msg , yyscanner)
 #endif
 
 /* end tables serialization structures and prototypes */
@@ -1669,12 +1953,14 @@ static int input (void );
 #ifndef YY_DECL
 #define YY_DECL_IS_OURS 1
 
-extern int ptx_lex (void);
+extern int yylex \
+               (YYSTYPE * yylval_param , yyscan_t yyscanner);
 
-#define YY_DECL int ptx_lex (void)
+#define YY_DECL int yylex \
+               (YYSTYPE * yylval_param , yyscan_t yyscanner)
 #endif /* !YY_DECL */
 
-/* Code executed at the beginning of each rule, after ptx_text and ptx_leng
+/* Code executed at the beginning of each rule, after yytext and yyleng
  * have been set up.
  */
 #ifndef YY_USER_ACTION
@@ -1696,78 +1982,81 @@ YY_DECL
 	yy_state_type yy_current_state;
 	char *yy_cp, *yy_bp;
 	int yy_act;
-    
-	if ( !(yy_init) )
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+    yylval = yylval_param;
+
+	if ( !yyg->yy_init )
 		{
-		(yy_init) = 1;
+		yyg->yy_init = 1;
 
 #ifdef YY_USER_INIT
 		YY_USER_INIT;
 #endif
 
-		if ( ! (yy_start) )
-			(yy_start) = 1;	/* first start state */
+		if ( ! yyg->yy_start )
+			yyg->yy_start = 1;	/* first start state */
 
-		if ( ! ptx_in )
-			ptx_in = stdin;
+		if ( ! yyin )
+			yyin = stdin;
 
-		if ( ! ptx_out )
-			ptx_out = stdout;
+		if ( ! yyout )
+			yyout = stdout;
 
 		if ( ! YY_CURRENT_BUFFER ) {
-			ptx_ensure_buffer_stack ();
+			yyensure_buffer_stack (yyscanner);
 			YY_CURRENT_BUFFER_LVALUE =
-				ptx__create_buffer(ptx_in,YY_BUF_SIZE );
+				yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner);
 		}
 
-		ptx__load_buffer_state( );
+		yy_load_buffer_state( yyscanner );
 		}
 
 	{
-#line 55 "ptx.l"
+#line 62 "ptx.l"
 
 
-#line 1731 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptx_.c"
+#line 2020 "lex.ptx_c.c"
 
 	while ( /*CONSTCOND*/1 )		/* loops until end-of-file is reached */
 		{
-		yy_cp = (yy_c_buf_p);
+		yy_cp = yyg->yy_c_buf_p;
 
-		/* Support of ptx_text. */
-		*yy_cp = (yy_hold_char);
+		/* Support of yytext. */
+		*yy_cp = yyg->yy_hold_char;
 
 		/* yy_bp points to the position in yy_ch_buf of the start of
 		 * the current run.
 		 */
 		yy_bp = yy_cp;
 
-		yy_current_state = (yy_start);
+		yy_current_state = yyg->yy_start;
 yy_match:
 		do
 			{
 			YY_CHAR yy_c = yy_ec[YY_SC_TO_UI(*yy_cp)] ;
 			if ( yy_accept[yy_current_state] )
 				{
-				(yy_last_accepting_state) = yy_current_state;
-				(yy_last_accepting_cpos) = yy_cp;
+				yyg->yy_last_accepting_state = yy_current_state;
+				yyg->yy_last_accepting_cpos = yy_cp;
 				}
 			while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
 				{
 				yy_current_state = (int) yy_def[yy_current_state];
-				if ( yy_current_state >= 1471 )
-					yy_c = yy_meta[(unsigned int) yy_c];
+				if ( yy_current_state >= 1603 )
+					yy_c = yy_meta[yy_c];
 				}
-			yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+			yy_current_state = yy_nxt[yy_base[yy_current_state] + yy_c];
 			++yy_cp;
 			}
-		while ( yy_base[yy_current_state] != 1902 );
+		while ( yy_base[yy_current_state] != 2037 );
 
 yy_find_action:
 		yy_act = yy_accept[yy_current_state];
 		if ( yy_act == 0 )
 			{ /* have to back up */
-			yy_cp = (yy_last_accepting_cpos);
-			yy_current_state = (yy_last_accepting_state);
+			yy_cp = yyg->yy_last_accepting_cpos;
+			yy_current_state = yyg->yy_last_accepting_state;
 			yy_act = yy_accept[yy_current_state];
 			}
 
@@ -1775,11 +2064,13 @@ yy_find_action:
 
 		if ( yy_act != YY_END_OF_BUFFER && yy_rule_can_match_eol[yy_act] )
 			{
-			yy_size_t yyl;
-			for ( yyl = 0; yyl < ptx_leng; ++yyl )
-				if ( ptx_text[yyl] == '\n' )
-					   
-    ptx_lineno++;
+			int yyl;
+			for ( yyl = 0; yyl < yyleng; ++yyl )
+				if ( yytext[yyl] == '\n' )
+					
+    do{ yylineno++;
+        yycolumn=0;
+    }while(0)
 ;
 			}
 
@@ -1789,1671 +2080,1721 @@ do_action:	/* This label is used only to access EOF actions. */
 	{ /* beginning of action switch */
 			case 0: /* must back up */
 			/* undo the effects of YY_DO_BEFORE_ACTION */
-			*yy_cp = (yy_hold_char);
-			yy_cp = (yy_last_accepting_cpos);
-			yy_current_state = (yy_last_accepting_state);
+			*yy_cp = yyg->yy_hold_char;
+			yy_cp = yyg->yy_last_accepting_cpos;
+			yy_current_state = yyg->yy_last_accepting_state;
 			goto yy_find_action;
 
 case 1:
 YY_RULE_SETUP
-#line 57 "ptx.l"
-TC; ptx_lval.int_value = ABS_OP; return OPCODE;
+#line 64 "ptx.l"
+TC; yylval->int_value = ABS_OP; return OPCODE;
 	YY_BREAK
 case 2:
 YY_RULE_SETUP
-#line 58 "ptx.l"
-TC; ptx_lval.int_value = ADD_OP; return OPCODE;
+#line 65 "ptx.l"
+TC; yylval->int_value = ADD_OP; return OPCODE;
 	YY_BREAK
 case 3:
 YY_RULE_SETUP
-#line 59 "ptx.l"
-TC; ptx_lval.int_value = ADDP_OP; return OPCODE;
+#line 66 "ptx.l"
+TC; yylval->int_value = ADDP_OP; return OPCODE;
 	YY_BREAK
 case 4:
 YY_RULE_SETUP
-#line 60 "ptx.l"
-TC; ptx_lval.int_value = ADDC_OP; return OPCODE;
+#line 67 "ptx.l"
+TC; yylval->int_value = ADDC_OP; return OPCODE;
 	YY_BREAK
 case 5:
 YY_RULE_SETUP
-#line 61 "ptx.l"
-TC; ptx_lval.int_value = AND_OP; return OPCODE;
+#line 68 "ptx.l"
+TC; yylval->int_value = AND_OP; return OPCODE;
 	YY_BREAK
 case 6:
 YY_RULE_SETUP
-#line 62 "ptx.l"
-TC; ptx_lval.int_value = ANDN_OP; return OPCODE;
+#line 69 "ptx.l"
+TC; yylval->int_value = ANDN_OP; return OPCODE;
 	YY_BREAK
 case 7:
 YY_RULE_SETUP
-#line 63 "ptx.l"
-TC; ptx_lval.int_value = ATOM_OP; return OPCODE;
+#line 70 "ptx.l"
+TC; yylval->int_value = ATOM_OP; return OPCODE;
 	YY_BREAK
 case 8:
 YY_RULE_SETUP
-#line 64 "ptx.l"
-TC; ptx_lval.int_value = NOP_OP; return OPCODE;
+#line 71 "ptx.l"
+TC; yylval->int_value = NOP_OP; return OPCODE;
 	YY_BREAK
 case 9:
 YY_RULE_SETUP
-#line 65 "ptx.l"
-TC; ptx_lval.int_value = BAR_OP; return OPCODE;
+#line 72 "ptx.l"
+TC; yylval->int_value = BAR_OP; return OPCODE;
 	YY_BREAK
 case 10:
 YY_RULE_SETUP
-#line 66 "ptx.l"
-TC; ptx_lval.int_value = BFE_OP; return OPCODE;
+#line 73 "ptx.l"
+TC; yylval->int_value = BAR_OP; return OPCODE;
 	YY_BREAK
 case 11:
 YY_RULE_SETUP
-#line 67 "ptx.l"
-TC; ptx_lval.int_value = BFI_OP; return OPCODE;
+#line 74 "ptx.l"
+TC; yylval->int_value = BFE_OP; return OPCODE;
 	YY_BREAK
 case 12:
 YY_RULE_SETUP
-#line 68 "ptx.l"
-TC; ptx_lval.int_value = BFIND_OP; return OPCODE;
+#line 75 "ptx.l"
+TC; yylval->int_value = BFI_OP; return OPCODE;
 	YY_BREAK
 case 13:
 YY_RULE_SETUP
-#line 69 "ptx.l"
-TC; ptx_lval.int_value = BRA_OP; return OPCODE;
+#line 76 "ptx.l"
+TC; yylval->int_value = BFIND_OP; return OPCODE;
 	YY_BREAK
 case 14:
 YY_RULE_SETUP
-#line 70 "ptx.l"
-TC; ptx_lval.int_value = BRX_OP; return OPCODE;
+#line 77 "ptx.l"
+TC; yylval->int_value = BRA_OP; return OPCODE;
 	YY_BREAK
 case 15:
 YY_RULE_SETUP
-#line 71 "ptx.l"
-TC; ptx_lval.int_value = BREV_OP; return OPCODE;
+#line 78 "ptx.l"
+TC; yylval->int_value = BRX_OP; return OPCODE;
 	YY_BREAK
 case 16:
 YY_RULE_SETUP
-#line 72 "ptx.l"
-TC; ptx_lval.int_value = BRKPT_OP; return OPCODE;
+#line 79 "ptx.l"
+TC; yylval->int_value = BREV_OP; return OPCODE;
 	YY_BREAK
 case 17:
 YY_RULE_SETUP
-#line 74 "ptx.l"
-TC; ptx_lval.int_value = MMA_OP; return OPCODE;
+#line 80 "ptx.l"
+TC; yylval->int_value = BRKPT_OP; return OPCODE;
 	YY_BREAK
 case 18:
 YY_RULE_SETUP
-#line 75 "ptx.l"
-TC; ptx_lval.int_value = MMA_LD_OP; return OPCODE;
+#line 82 "ptx.l"
+TC; yylval->int_value = MMA_OP; return OPCODE;
 	YY_BREAK
 case 19:
 YY_RULE_SETUP
-#line 76 "ptx.l"
-TC; ptx_lval.int_value = MMA_ST_OP; return OPCODE;
+#line 83 "ptx.l"
+TC; yylval->int_value = MMA_LD_OP; return OPCODE;
 	YY_BREAK
 case 20:
 YY_RULE_SETUP
-#line 78 "ptx.l"
-TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
+#line 84 "ptx.l"
+TC; yylval->int_value = MMA_ST_OP; return OPCODE;
 	YY_BREAK
 case 21:
 YY_RULE_SETUP
-#line 79 "ptx.l"
-TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALLP_OP; return OPCODE;
+#line 86 "ptx.l"
+TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
 	YY_BREAK
 case 22:
 YY_RULE_SETUP
-#line 80 "ptx.l"
-TC; ptx_lval.int_value = CLZ_OP; return OPCODE;
+#line 87 "ptx.l"
+TC; BEGIN(NOT_OPCODE); yylval->int_value = CALLP_OP; return OPCODE;
 	YY_BREAK
 case 23:
 YY_RULE_SETUP
-#line 81 "ptx.l"
-TC; ptx_lval.int_value = CNOT_OP; return OPCODE;
+#line 88 "ptx.l"
+TC; yylval->int_value = CLZ_OP; return OPCODE;
 	YY_BREAK
 case 24:
 YY_RULE_SETUP
-#line 82 "ptx.l"
-TC; ptx_lval.int_value = COS_OP; return OPCODE;
+#line 89 "ptx.l"
+TC; yylval->int_value = CNOT_OP; return OPCODE;
 	YY_BREAK
 case 25:
 YY_RULE_SETUP
-#line 83 "ptx.l"
-TC; ptx_lval.int_value = CVT_OP; return OPCODE;
+#line 90 "ptx.l"
+TC; yylval->int_value = COS_OP; return OPCODE;
 	YY_BREAK
 case 26:
 YY_RULE_SETUP
-#line 84 "ptx.l"
-TC; ptx_lval.int_value = CVTA_OP; return OPCODE;
+#line 91 "ptx.l"
+TC; yylval->int_value = CVT_OP; return OPCODE;
 	YY_BREAK
 case 27:
 YY_RULE_SETUP
-#line 85 "ptx.l"
-TC; ptx_lval.int_value = DIV_OP; return OPCODE;
+#line 92 "ptx.l"
+TC; yylval->int_value = CVTA_OP; return OPCODE;
 	YY_BREAK
 case 28:
 YY_RULE_SETUP
-#line 86 "ptx.l"
-TC; ptx_lval.int_value = DP4A_OP; return OPCODE;
+#line 93 "ptx.l"
+TC; yylval->int_value = DIV_OP; return OPCODE;
 	YY_BREAK
 case 29:
 YY_RULE_SETUP
-#line 87 "ptx.l"
-TC; ptx_lval.int_value = EX2_OP; return OPCODE;
+#line 94 "ptx.l"
+TC; yylval->int_value = DP4A_OP; return OPCODE;
 	YY_BREAK
 case 30:
 YY_RULE_SETUP
-#line 88 "ptx.l"
-TC; ptx_lval.int_value = EXIT_OP; return OPCODE;
+#line 95 "ptx.l"
+TC; yylval->int_value = EX2_OP; return OPCODE;
 	YY_BREAK
 case 31:
 YY_RULE_SETUP
-#line 89 "ptx.l"
-TC; ptx_lval.int_value = FMA_OP; return OPCODE;
+#line 96 "ptx.l"
+TC; yylval->int_value = EXIT_OP; return OPCODE;
 	YY_BREAK
 case 32:
 YY_RULE_SETUP
-#line 90 "ptx.l"
-TC; ptx_lval.int_value = ISSPACEP_OP; return OPCODE;
+#line 97 "ptx.l"
+TC; yylval->int_value = FMA_OP; return OPCODE;
 	YY_BREAK
 case 33:
 YY_RULE_SETUP
-#line 91 "ptx.l"
-TC; ptx_lval.int_value = LD_OP; return OPCODE;
+#line 98 "ptx.l"
+TC; yylval->int_value = ISSPACEP_OP; return OPCODE;
 	YY_BREAK
 case 34:
 YY_RULE_SETUP
-#line 92 "ptx.l"
-TC; ptx_lval.int_value = LD_OP; return OPCODE;
+#line 99 "ptx.l"
+TC; yylval->int_value = LD_OP; return OPCODE;
 	YY_BREAK
 case 35:
 YY_RULE_SETUP
-#line 93 "ptx.l"
-TC; ptx_lval.int_value = LDU_OP; return OPCODE;
+#line 100 "ptx.l"
+TC; yylval->int_value = LD_OP; return OPCODE;
 	YY_BREAK
 case 36:
 YY_RULE_SETUP
-#line 94 "ptx.l"
-TC; ptx_lval.int_value = LG2_OP; return OPCODE;
+#line 101 "ptx.l"
+TC; yylval->int_value = LDU_OP; return OPCODE;
 	YY_BREAK
 case 37:
 YY_RULE_SETUP
-#line 95 "ptx.l"
-TC; ptx_lval.int_value = MAD24_OP; return OPCODE;
+#line 102 "ptx.l"
+TC; yylval->int_value = LG2_OP; return OPCODE;
 	YY_BREAK
 case 38:
 YY_RULE_SETUP
-#line 96 "ptx.l"
-TC; ptx_lval.int_value = MAD_OP; return OPCODE;
+#line 103 "ptx.l"
+TC; yylval->int_value = MAD24_OP; return OPCODE;
 	YY_BREAK
 case 39:
 YY_RULE_SETUP
-#line 97 "ptx.l"
-TC; ptx_lval.int_value = MADC_OP; return OPCODE;
+#line 104 "ptx.l"
+TC; yylval->int_value = MAD_OP; return OPCODE;
 	YY_BREAK
 case 40:
 YY_RULE_SETUP
-#line 98 "ptx.l"
-TC; ptx_lval.int_value = MADP_OP; return OPCODE;
+#line 105 "ptx.l"
+TC; yylval->int_value = MADC_OP; return OPCODE;
 	YY_BREAK
 case 41:
 YY_RULE_SETUP
-#line 99 "ptx.l"
-TC; ptx_lval.int_value = MAX_OP; return OPCODE;
+#line 106 "ptx.l"
+TC; yylval->int_value = MADP_OP; return OPCODE;
 	YY_BREAK
 case 42:
 YY_RULE_SETUP
-#line 100 "ptx.l"
-TC; ptx_lval.int_value = MEMBAR_OP; return OPCODE;
+#line 107 "ptx.l"
+TC; yylval->int_value = MAX_OP; return OPCODE;
 	YY_BREAK
 case 43:
 YY_RULE_SETUP
-#line 101 "ptx.l"
-TC; ptx_lval.int_value = MIN_OP; return OPCODE;
+#line 108 "ptx.l"
+TC; yylval->int_value = MEMBAR_OP; return OPCODE;
 	YY_BREAK
 case 44:
 YY_RULE_SETUP
-#line 102 "ptx.l"
-TC; ptx_lval.int_value = MOV_OP; return OPCODE;
+#line 109 "ptx.l"
+TC; yylval->int_value = MIN_OP; return OPCODE;
 	YY_BREAK
 case 45:
 YY_RULE_SETUP
-#line 103 "ptx.l"
-TC; ptx_lval.int_value = MUL24_OP; return OPCODE;
+#line 110 "ptx.l"
+TC; yylval->int_value = MOV_OP; return OPCODE;
 	YY_BREAK
 case 46:
 YY_RULE_SETUP
-#line 104 "ptx.l"
-TC; ptx_lval.int_value = MUL_OP; return OPCODE;
+#line 111 "ptx.l"
+TC; yylval->int_value = MUL24_OP; return OPCODE;
 	YY_BREAK
 case 47:
 YY_RULE_SETUP
-#line 105 "ptx.l"
-TC; ptx_lval.int_value = NEG_OP; return OPCODE;
+#line 112 "ptx.l"
+TC; yylval->int_value = MUL_OP; return OPCODE;
 	YY_BREAK
 case 48:
 YY_RULE_SETUP
-#line 106 "ptx.l"
-TC; ptx_lval.int_value = NANDN_OP; return OPCODE;
+#line 113 "ptx.l"
+TC; yylval->int_value = NEG_OP; return OPCODE;
 	YY_BREAK
 case 49:
 YY_RULE_SETUP
-#line 107 "ptx.l"
-TC; ptx_lval.int_value = NORN_OP; return OPCODE;
+#line 114 "ptx.l"
+TC; yylval->int_value = NANDN_OP; return OPCODE;
 	YY_BREAK
 case 50:
 YY_RULE_SETUP
-#line 108 "ptx.l"
-TC; ptx_lval.int_value = NOT_OP; return OPCODE;
+#line 115 "ptx.l"
+TC; yylval->int_value = NORN_OP; return OPCODE;
 	YY_BREAK
 case 51:
 YY_RULE_SETUP
-#line 109 "ptx.l"
-TC; ptx_lval.int_value = OR_OP; return OPCODE;
+#line 116 "ptx.l"
+TC; yylval->int_value = NOT_OP; return OPCODE;
 	YY_BREAK
 case 52:
 YY_RULE_SETUP
-#line 110 "ptx.l"
-TC; ptx_lval.int_value = ORN_OP; return OPCODE;
+#line 117 "ptx.l"
+TC; yylval->int_value = OR_OP; return OPCODE;
 	YY_BREAK
 case 53:
 YY_RULE_SETUP
-#line 111 "ptx.l"
-TC; ptx_lval.int_value = PMEVENT_OP; return OPCODE;
+#line 118 "ptx.l"
+TC; yylval->int_value = ORN_OP; return OPCODE;
 	YY_BREAK
 case 54:
 YY_RULE_SETUP
-#line 112 "ptx.l"
-TC; ptx_lval.int_value = POPC_OP; return OPCODE;
+#line 119 "ptx.l"
+TC; yylval->int_value = PMEVENT_OP; return OPCODE;
 	YY_BREAK
 case 55:
 YY_RULE_SETUP
-#line 113 "ptx.l"
-TC; ptx_lval.int_value = PREFETCH_OP; return OPCODE;
+#line 120 "ptx.l"
+TC; yylval->int_value = POPC_OP; return OPCODE;
 	YY_BREAK
 case 56:
 YY_RULE_SETUP
-#line 114 "ptx.l"
-TC; ptx_lval.int_value = PREFETCHU_OP; return OPCODE;
+#line 121 "ptx.l"
+TC; yylval->int_value = PREFETCH_OP; return OPCODE;
 	YY_BREAK
 case 57:
 YY_RULE_SETUP
-#line 115 "ptx.l"
-TC; ptx_lval.int_value = PRMT_OP; return OPCODE;
+#line 122 "ptx.l"
+TC; yylval->int_value = PREFETCHU_OP; return OPCODE;
 	YY_BREAK
 case 58:
 YY_RULE_SETUP
-#line 116 "ptx.l"
-TC; ptx_lval.int_value = RCP_OP; return OPCODE;
+#line 123 "ptx.l"
+TC; yylval->int_value = PRMT_OP; return OPCODE;
 	YY_BREAK
 case 59:
 YY_RULE_SETUP
-#line 117 "ptx.l"
-TC; ptx_lval.int_value = RED_OP; return OPCODE;
+#line 124 "ptx.l"
+TC; yylval->int_value = RCP_OP; return OPCODE;
 	YY_BREAK
 case 60:
 YY_RULE_SETUP
-#line 118 "ptx.l"
-TC; ptx_lval.int_value = REM_OP; return OPCODE;
+#line 125 "ptx.l"
+TC; yylval->int_value = RED_OP; return OPCODE;
 	YY_BREAK
 case 61:
 YY_RULE_SETUP
-#line 119 "ptx.l"
-TC; ptx_lval.int_value = RET_OP; return OPCODE;
+#line 126 "ptx.l"
+TC; yylval->int_value = REM_OP; return OPCODE;
 	YY_BREAK
 case 62:
 YY_RULE_SETUP
-#line 120 "ptx.l"
-TC; ptx_lval.int_value = RETP_OP; return OPCODE;
+#line 127 "ptx.l"
+TC; yylval->int_value = RET_OP; return OPCODE;
 	YY_BREAK
 case 63:
 YY_RULE_SETUP
-#line 121 "ptx.l"
-TC; ptx_lval.int_value = RSQRT_OP; return OPCODE;
+#line 128 "ptx.l"
+TC; yylval->int_value = RETP_OP; return OPCODE;
 	YY_BREAK
 case 64:
 YY_RULE_SETUP
-#line 122 "ptx.l"
-TC; ptx_lval.int_value = SAD_OP; return OPCODE;
+#line 129 "ptx.l"
+TC; yylval->int_value = RSQRT_OP; return OPCODE;
 	YY_BREAK
 case 65:
 YY_RULE_SETUP
-#line 123 "ptx.l"
-TC; ptx_lval.int_value = SELP_OP; return OPCODE;
+#line 130 "ptx.l"
+TC; yylval->int_value = SAD_OP; return OPCODE;
 	YY_BREAK
 case 66:
 YY_RULE_SETUP
-#line 124 "ptx.l"
-TC; ptx_lval.int_value = SETP_OP; return OPCODE;
+#line 131 "ptx.l"
+TC; yylval->int_value = SELP_OP; return OPCODE;
 	YY_BREAK
 case 67:
 YY_RULE_SETUP
-#line 125 "ptx.l"
-TC; ptx_lval.int_value = SET_OP; return OPCODE;
+#line 132 "ptx.l"
+TC; yylval->int_value = SETP_OP; return OPCODE;
 	YY_BREAK
 case 68:
 YY_RULE_SETUP
-#line 126 "ptx.l"
-TC; ptx_lval.int_value = SHFL_OP; return OPCODE;
+#line 133 "ptx.l"
+TC; yylval->int_value = SET_OP; return OPCODE;
 	YY_BREAK
 case 69:
 YY_RULE_SETUP
-#line 127 "ptx.l"
-TC; ptx_lval.int_value = SHL_OP; return OPCODE;
+#line 134 "ptx.l"
+TC; yylval->int_value = SHFL_OP; return OPCODE;
 	YY_BREAK
 case 70:
 YY_RULE_SETUP
-#line 128 "ptx.l"
-TC; ptx_lval.int_value = SHR_OP; return OPCODE;
+#line 135 "ptx.l"
+TC; yylval->int_value = SHL_OP; return OPCODE;
 	YY_BREAK
 case 71:
 YY_RULE_SETUP
-#line 129 "ptx.l"
-TC; ptx_lval.int_value = SIN_OP; return OPCODE;
+#line 136 "ptx.l"
+TC; yylval->int_value = SHR_OP; return OPCODE;
 	YY_BREAK
 case 72:
 YY_RULE_SETUP
-#line 130 "ptx.l"
-TC; ptx_lval.int_value = SLCT_OP; return OPCODE;
+#line 137 "ptx.l"
+TC; yylval->int_value = SIN_OP; return OPCODE;
 	YY_BREAK
 case 73:
 YY_RULE_SETUP
-#line 131 "ptx.l"
-TC; ptx_lval.int_value = SQRT_OP; return OPCODE;
+#line 138 "ptx.l"
+TC; yylval->int_value = SLCT_OP; return OPCODE;
 	YY_BREAK
 case 74:
 YY_RULE_SETUP
-#line 132 "ptx.l"
-TC; ptx_lval.int_value = SST_OP; return OPCODE;
+#line 139 "ptx.l"
+TC; yylval->int_value = SQRT_OP; return OPCODE;
 	YY_BREAK
 case 75:
 YY_RULE_SETUP
-#line 133 "ptx.l"
-TC; ptx_lval.int_value = SSY_OP; return OPCODE;
+#line 140 "ptx.l"
+TC; yylval->int_value = SST_OP; return OPCODE;
 	YY_BREAK
 case 76:
 YY_RULE_SETUP
-#line 134 "ptx.l"
-TC; ptx_lval.int_value = ST_OP; return OPCODE;
+#line 141 "ptx.l"
+TC; yylval->int_value = SSY_OP; return OPCODE;
 	YY_BREAK
 case 77:
 YY_RULE_SETUP
-#line 135 "ptx.l"
-TC; ptx_lval.int_value = ST_OP; return OPCODE;
+#line 142 "ptx.l"
+TC; yylval->int_value = ST_OP; return OPCODE;
 	YY_BREAK
 case 78:
 YY_RULE_SETUP
-#line 136 "ptx.l"
-TC; ptx_lval.int_value = SUB_OP; return OPCODE;
+#line 143 "ptx.l"
+TC; yylval->int_value = ST_OP; return OPCODE;
 	YY_BREAK
 case 79:
 YY_RULE_SETUP
-#line 137 "ptx.l"
-TC; ptx_lval.int_value = SUBC_OP; return OPCODE;
+#line 144 "ptx.l"
+TC; yylval->int_value = SUB_OP; return OPCODE;
 	YY_BREAK
 case 80:
 YY_RULE_SETUP
-#line 138 "ptx.l"
-TC; ptx_lval.int_value = SULD_OP; return OPCODE;
+#line 145 "ptx.l"
+TC; yylval->int_value = SUBC_OP; return OPCODE;
 	YY_BREAK
 case 81:
 YY_RULE_SETUP
-#line 139 "ptx.l"
-TC; ptx_lval.int_value = SURED_OP; return OPCODE;
+#line 146 "ptx.l"
+TC; yylval->int_value = SULD_OP; return OPCODE;
 	YY_BREAK
 case 82:
 YY_RULE_SETUP
-#line 140 "ptx.l"
-TC; ptx_lval.int_value = SUST_OP; return OPCODE;
+#line 147 "ptx.l"
+TC; yylval->int_value = SURED_OP; return OPCODE;
 	YY_BREAK
 case 83:
 YY_RULE_SETUP
-#line 141 "ptx.l"
-TC; ptx_lval.int_value = SUQ_OP; return OPCODE;
+#line 148 "ptx.l"
+TC; yylval->int_value = SUST_OP; return OPCODE;
 	YY_BREAK
 case 84:
 YY_RULE_SETUP
-#line 142 "ptx.l"
-TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = TEX_OP; return OPCODE;
+#line 149 "ptx.l"
+TC; yylval->int_value = SUQ_OP; return OPCODE;
 	YY_BREAK
 case 85:
 YY_RULE_SETUP
-#line 143 "ptx.l"
-TC; ptx_lval.int_value = TEX_OP; return OPCODE;
+#line 150 "ptx.l"
+TC; BEGIN(NOT_OPCODE); yylval->int_value = TEX_OP; return OPCODE;
 	YY_BREAK
 case 86:
 YY_RULE_SETUP
-#line 144 "ptx.l"
-TC; ptx_lval.int_value = TRAP_OP; return OPCODE;
+#line 151 "ptx.l"
+TC; yylval->int_value = TEX_OP; return OPCODE;
 	YY_BREAK
 case 87:
 YY_RULE_SETUP
-#line 145 "ptx.l"
-TC; ptx_lval.int_value = VABSDIFF_OP; return OPCODE;
+#line 152 "ptx.l"
+TC; yylval->int_value = TRAP_OP; return OPCODE;
 	YY_BREAK
 case 88:
 YY_RULE_SETUP
-#line 146 "ptx.l"
-TC; ptx_lval.int_value = VADD_OP; return OPCODE;
+#line 153 "ptx.l"
+TC; yylval->int_value = VABSDIFF_OP; return OPCODE;
 	YY_BREAK
 case 89:
 YY_RULE_SETUP
-#line 147 "ptx.l"
-TC; ptx_lval.int_value = VMAD_OP; return OPCODE;
+#line 154 "ptx.l"
+TC; yylval->int_value = VADD_OP; return OPCODE;
 	YY_BREAK
 case 90:
 YY_RULE_SETUP
-#line 148 "ptx.l"
-TC; ptx_lval.int_value = VMAX_OP; return OPCODE;
+#line 155 "ptx.l"
+TC; yylval->int_value = VMAD_OP; return OPCODE;
 	YY_BREAK
 case 91:
 YY_RULE_SETUP
-#line 149 "ptx.l"
-TC; ptx_lval.int_value = VMIN_OP; return OPCODE;
+#line 156 "ptx.l"
+TC; yylval->int_value = VMAX_OP; return OPCODE;
 	YY_BREAK
 case 92:
 YY_RULE_SETUP
-#line 150 "ptx.l"
-TC; ptx_lval.int_value = VSET_OP; return OPCODE;
+#line 157 "ptx.l"
+TC; yylval->int_value = VMIN_OP; return OPCODE;
 	YY_BREAK
 case 93:
 YY_RULE_SETUP
-#line 151 "ptx.l"
-TC; ptx_lval.int_value = VSHL_OP; return OPCODE;
+#line 158 "ptx.l"
+TC; yylval->int_value = VSET_OP; return OPCODE;
 	YY_BREAK
 case 94:
 YY_RULE_SETUP
-#line 152 "ptx.l"
-TC; ptx_lval.int_value = VSHR_OP; return OPCODE;
+#line 159 "ptx.l"
+TC; yylval->int_value = VSHL_OP; return OPCODE;
 	YY_BREAK
 case 95:
 YY_RULE_SETUP
-#line 153 "ptx.l"
-TC; ptx_lval.int_value = VSUB_OP; return OPCODE;
+#line 160 "ptx.l"
+TC; yylval->int_value = VSHR_OP; return OPCODE;
 	YY_BREAK
 case 96:
 YY_RULE_SETUP
-#line 154 "ptx.l"
-TC; ptx_lval.int_value = VOTE_OP; return OPCODE;
+#line 161 "ptx.l"
+TC; yylval->int_value = VSUB_OP; return OPCODE;
 	YY_BREAK
 case 97:
 YY_RULE_SETUP
-#line 155 "ptx.l"
-TC; ptx_lval.int_value = XOR_OP; return OPCODE;
+#line 162 "ptx.l"
+TC; yylval->int_value = VOTE_OP; return OPCODE;
 	YY_BREAK
 case 98:
 YY_RULE_SETUP
-#line 156 "ptx.l"
-TC; ptx_lval.int_value = NOP_OP; return OPCODE;
+#line 163 "ptx.l"
+TC; yylval->int_value = ACTIVEMASK_OP; return OPCODE;
 	YY_BREAK
 case 99:
 YY_RULE_SETUP
-#line 157 "ptx.l"
-TC; ptx_lval.int_value = BREAK_OP; return OPCODE;
+#line 164 "ptx.l"
+TC; yylval->int_value = XOR_OP; return OPCODE;
 	YY_BREAK
 case 100:
 YY_RULE_SETUP
-#line 158 "ptx.l"
-TC; ptx_lval.int_value = BREAKADDR_OP; return OPCODE;
+#line 165 "ptx.l"
+TC; yylval->int_value = NOP_OP; return OPCODE;
 	YY_BREAK
 case 101:
 YY_RULE_SETUP
-#line 160 "ptx.l"
-printf("ENDING CUSTOM PTX.\n"); BEGIN(IN_COMMENT);
+#line 166 "ptx.l"
+TC; yylval->int_value = BREAK_OP; return OPCODE;
 	YY_BREAK
-
 case 102:
 YY_RULE_SETUP
-#line 163 "ptx.l"
-TC; ptx_lval.int_value = LOAD_A; return WMMA_DIRECTIVE;
+#line 167 "ptx.l"
+TC; yylval->int_value = BREAKADDR_OP; return OPCODE;
 	YY_BREAK
 case 103:
 YY_RULE_SETUP
-#line 164 "ptx.l"
-TC; ptx_lval.int_value = LOAD_B; return WMMA_DIRECTIVE;
+#line 169 "ptx.l"
+printf("ENDING CUSTOM PTX.\n"); BEGIN(IN_COMMENT);
 	YY_BREAK
+
 case 104:
 YY_RULE_SETUP
-#line 165 "ptx.l"
-TC; ptx_lval.int_value = LOAD_C; return WMMA_DIRECTIVE;
+#line 172 "ptx.l"
+TC; yylval->int_value = LOAD_A; return WMMA_DIRECTIVE;
 	YY_BREAK
 case 105:
 YY_RULE_SETUP
-#line 166 "ptx.l"
-TC; ptx_lval.int_value = STORE_D; return WMMA_DIRECTIVE;
+#line 173 "ptx.l"
+TC; yylval->int_value = LOAD_B; return WMMA_DIRECTIVE;
 	YY_BREAK
 case 106:
 YY_RULE_SETUP
-#line 167 "ptx.l"
-TC;ptx_lval.int_value=MMA; return WMMA_DIRECTIVE;
+#line 174 "ptx.l"
+TC; yylval->int_value = LOAD_C; return WMMA_DIRECTIVE;
 	YY_BREAK
 case 107:
 YY_RULE_SETUP
-#line 169 "ptx.l"
-TC; ptx_lval.int_value = ROW; return LAYOUT;
+#line 175 "ptx.l"
+TC; yylval->int_value = STORE_D; return WMMA_DIRECTIVE;
 	YY_BREAK
 case 108:
 YY_RULE_SETUP
-#line 170 "ptx.l"
-TC; ptx_lval.int_value = COL; return LAYOUT;
+#line 176 "ptx.l"
+TC;yylval->int_value=MMA; return WMMA_DIRECTIVE;
 	YY_BREAK
 case 109:
 YY_RULE_SETUP
-#line 171 "ptx.l"
-TC; ptx_lval.int_value = M16N16K16; return CONFIGURATION;
+#line 178 "ptx.l"
+TC; yylval->int_value = ROW; return LAYOUT;
 	YY_BREAK
 case 110:
 YY_RULE_SETUP
-#line 172 "ptx.l"
-TC;  return PRMT_F4E_MODE;
+#line 179 "ptx.l"
+TC; yylval->int_value = COL; return LAYOUT;
 	YY_BREAK
 case 111:
 YY_RULE_SETUP
-#line 173 "ptx.l"
-TC;  return PRMT_B4E_MODE;
+#line 180 "ptx.l"
+TC; yylval->int_value = M16N16K16; return CONFIGURATION;
 	YY_BREAK
 case 112:
 YY_RULE_SETUP
-#line 174 "ptx.l"
-TC;  return PRMT_RC8_MODE;
+#line 181 "ptx.l"
+TC; yylval->int_value = M32N8K16; return CONFIGURATION;
 	YY_BREAK
 case 113:
 YY_RULE_SETUP
-#line 175 "ptx.l"
-TC;  return PRMT_ECL_MODE;
+#line 182 "ptx.l"
+TC;  yylval->int_value = M8N32K16; return CONFIGURATION;
 	YY_BREAK
 case 114:
 YY_RULE_SETUP
-#line 176 "ptx.l"
-TC;  return PRMT_ECR_MODE;
+#line 184 "ptx.l"
+TC; yylval->int_value = M16N16K16; return CONFIGURATION;
 	YY_BREAK
 case 115:
 YY_RULE_SETUP
-#line 177 "ptx.l"
-TC; return PRMT_RC16_MODE;
+#line 185 "ptx.l"
+TC; yylval->int_value = M32N8K16; return CONFIGURATION;
 	YY_BREAK
 case 116:
 YY_RULE_SETUP
-#line 179 "ptx.l"
-TC; return ALIGN_DIRECTIVE;
+#line 186 "ptx.l"
+TC;  yylval->int_value = M8N32K16; return CONFIGURATION;
 	YY_BREAK
 case 117:
 YY_RULE_SETUP
-#line 180 "ptx.l"
-TC; return BRANCHTARGETS_DIRECTIVE;
+#line 188 "ptx.l"
+TC; yylval->int_value = M16N16K16; return CONFIGURATION;
 	YY_BREAK
 case 118:
 YY_RULE_SETUP
-#line 181 "ptx.l"
-TC; return BYTE_DIRECTIVE; /* not in PTX 2.1 */
+#line 189 "ptx.l"
+TC; yylval->int_value = M32N8K16; return CONFIGURATION;
 	YY_BREAK
 case 119:
 YY_RULE_SETUP
-#line 182 "ptx.l"
-TC; return CALLPROTOTYPE_DIRECTIVE;
+#line 190 "ptx.l"
+TC;  yylval->int_value = M8N32K16; return CONFIGURATION;
 	YY_BREAK
 case 120:
 YY_RULE_SETUP
-#line 183 "ptx.l"
-TC; return CALLTARGETS_DIRECTIVE;
+#line 192 "ptx.l"
+TC;  return PRMT_F4E_MODE;
 	YY_BREAK
 case 121:
 YY_RULE_SETUP
-#line 184 "ptx.l"
-TC; ptx_lval.int_value = atoi(ptx_text+7); return CONST_DIRECTIVE;
+#line 193 "ptx.l"
+TC;  return PRMT_B4E_MODE;
 	YY_BREAK
 case 122:
 YY_RULE_SETUP
-#line 185 "ptx.l"
-TC; ptx_lval.int_value = 0; return CONST_DIRECTIVE;
+#line 194 "ptx.l"
+TC;  return PRMT_RC8_MODE;
 	YY_BREAK
 case 123:
 YY_RULE_SETUP
-#line 186 "ptx.l"
-TC; return ENTRY_DIRECTIVE;
+#line 195 "ptx.l"
+TC;  return PRMT_ECL_MODE;
 	YY_BREAK
 case 124:
 YY_RULE_SETUP
-#line 187 "ptx.l"
-TC; return EXTERN_DIRECTIVE;
+#line 196 "ptx.l"
+TC;  return PRMT_ECR_MODE;
 	YY_BREAK
 case 125:
 YY_RULE_SETUP
-#line 188 "ptx.l"
-TC; BEGIN(INITIAL); return FILE_DIRECTIVE;
+#line 197 "ptx.l"
+TC; return PRMT_RC16_MODE;
 	YY_BREAK
 case 126:
 YY_RULE_SETUP
-#line 189 "ptx.l"
-TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
+#line 199 "ptx.l"
+TC; return ALIGN_DIRECTIVE;
 	YY_BREAK
 case 127:
 YY_RULE_SETUP
-#line 190 "ptx.l"
-TC; return GLOBAL_DIRECTIVE;
+#line 200 "ptx.l"
+TC; return BRANCHTARGETS_DIRECTIVE;
 	YY_BREAK
 case 128:
 YY_RULE_SETUP
-#line 191 "ptx.l"
-TC; return GLOBAL_DIRECTIVE; //TODO: fix this!
+#line 201 "ptx.l"
+TC; return BYTE_DIRECTIVE; /* not in PTX 2.1 */
 	YY_BREAK
 case 129:
 YY_RULE_SETUP
-#line 192 "ptx.l"
-TC; return LOCAL_DIRECTIVE;
+#line 202 "ptx.l"
+TC; return CALLPROTOTYPE_DIRECTIVE;
 	YY_BREAK
 case 130:
 YY_RULE_SETUP
-#line 193 "ptx.l"
-TC; return LOC_DIRECTIVE;
+#line 203 "ptx.l"
+TC; return CALLTARGETS_DIRECTIVE;
 	YY_BREAK
 case 131:
 YY_RULE_SETUP
-#line 194 "ptx.l"
-TC; return MAXNCTAPERSM_DIRECTIVE;
+#line 204 "ptx.l"
+TC; yylval->int_value = atoi(yytext+7); return CONST_DIRECTIVE;
 	YY_BREAK
 case 132:
 YY_RULE_SETUP
-#line 195 "ptx.l"
-TC; return MAXNNREG_DIRECTIVE;
+#line 205 "ptx.l"
+TC; yylval->int_value = 0; return CONST_DIRECTIVE;
 	YY_BREAK
 case 133:
 YY_RULE_SETUP
-#line 196 "ptx.l"
-TC; return MAXNTID_DIRECTIVE;
+#line 206 "ptx.l"
+TC; return ENTRY_DIRECTIVE;
 	YY_BREAK
 case 134:
 YY_RULE_SETUP
-#line 197 "ptx.l"
-TC; return MINNCTAPERSM_DIRECTIVE;
+#line 207 "ptx.l"
+TC; return EXTERN_DIRECTIVE;
 	YY_BREAK
 case 135:
 YY_RULE_SETUP
-#line 198 "ptx.l"
-TC; return PARAM_DIRECTIVE;
+#line 208 "ptx.l"
+TC; BEGIN(INITIAL); return FILE_DIRECTIVE;
 	YY_BREAK
 case 136:
 YY_RULE_SETUP
-#line 199 "ptx.l"
-TC; return PRAGMA_DIRECTIVE;
+#line 209 "ptx.l"
+TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
 	YY_BREAK
 case 137:
 YY_RULE_SETUP
-#line 200 "ptx.l"
-TC; return REG_DIRECTIVE;
+#line 210 "ptx.l"
+TC; return GLOBAL_DIRECTIVE;
 	YY_BREAK
 case 138:
 YY_RULE_SETUP
-#line 201 "ptx.l"
-TC; return REQNTID_DIRECTIVE;
+#line 211 "ptx.l"
+TC; return GLOBAL_DIRECTIVE; //TODO: fix this!
 	YY_BREAK
 case 139:
 YY_RULE_SETUP
-#line 202 "ptx.l"
-TC; return SECTION_DIRECTIVE;
+#line 212 "ptx.l"
+TC; return LOCAL_DIRECTIVE;
 	YY_BREAK
 case 140:
 YY_RULE_SETUP
-#line 203 "ptx.l"
-TC; return SHARED_DIRECTIVE;
+#line 213 "ptx.l"
+TC; return LOC_DIRECTIVE;
 	YY_BREAK
 case 141:
 YY_RULE_SETUP
-#line 204 "ptx.l"
-TC; return SREG_DIRECTIVE;
+#line 214 "ptx.l"
+TC; return MAXNCTAPERSM_DIRECTIVE;
 	YY_BREAK
 case 142:
 YY_RULE_SETUP
-#line 205 "ptx.l"
-TC; return SSTARR_DIRECTIVE;
+#line 215 "ptx.l"
+TC; return MAXNNREG_DIRECTIVE;
 	YY_BREAK
 case 143:
 YY_RULE_SETUP
-#line 206 "ptx.l"
-TC; return STRUCT_DIRECTIVE;
+#line 216 "ptx.l"
+TC; return MAXNTID_DIRECTIVE;
 	YY_BREAK
 case 144:
 YY_RULE_SETUP
-#line 207 "ptx.l"
-TC; return SURF_DIRECTIVE;   /* not in PTX 2.1 */
+#line 217 "ptx.l"
+TC; return MINNCTAPERSM_DIRECTIVE;
 	YY_BREAK
 case 145:
 YY_RULE_SETUP
-#line 208 "ptx.l"
-TC; return TARGET_DIRECTIVE;
+#line 218 "ptx.l"
+TC; return PARAM_DIRECTIVE;
 	YY_BREAK
 case 146:
 YY_RULE_SETUP
-#line 209 "ptx.l"
-TC; BEGIN(NOT_OPCODE); return TEX_DIRECTIVE;
+#line 219 "ptx.l"
+TC; return PRAGMA_DIRECTIVE;
 	YY_BREAK
 case 147:
 YY_RULE_SETUP
-#line 210 "ptx.l"
-TC; return UNION_DIRECTIVE; /* not in PTX 2.1 */
+#line 220 "ptx.l"
+TC; return REG_DIRECTIVE;
 	YY_BREAK
 case 148:
 YY_RULE_SETUP
-#line 211 "ptx.l"
-TC; return VERSION_DIRECTIVE;
+#line 221 "ptx.l"
+TC; return REQNTID_DIRECTIVE;
 	YY_BREAK
 case 149:
 YY_RULE_SETUP
-#line 212 "ptx.l"
-TC; return VISIBLE_DIRECTIVE;
+#line 222 "ptx.l"
+TC; return SECTION_DIRECTIVE;
 	YY_BREAK
 case 150:
 YY_RULE_SETUP
-#line 213 "ptx.l"
-TC; return WEAK_DIRECTIVE;
+#line 223 "ptx.l"
+TC; return SHARED_DIRECTIVE;
 	YY_BREAK
 case 151:
 YY_RULE_SETUP
-#line 214 "ptx.l"
-TC; return ADDRESS_SIZE_DIRECTIVE;
+#line 224 "ptx.l"
+TC; return SREG_DIRECTIVE;
 	YY_BREAK
 case 152:
 YY_RULE_SETUP
-#line 215 "ptx.l"
-TC; return WEAK_DIRECTIVE;
+#line 225 "ptx.l"
+TC; return SSTARR_DIRECTIVE;
 	YY_BREAK
 case 153:
 YY_RULE_SETUP
-#line 217 "ptx.l"
-TC; return CONSTPTR_DIRECTIVE; /* Ptx plus directive for pointer to constant memory */
+#line 226 "ptx.l"
+TC; return STRUCT_DIRECTIVE;
 	YY_BREAK
 case 154:
 YY_RULE_SETUP
-#line 218 "ptx.l"
-TC; return PTR_DIRECTIVE; /* Added for new OpenCL genrated code */
+#line 227 "ptx.l"
+TC; return SURF_DIRECTIVE;   /* not in PTX 2.1 */
 	YY_BREAK
 case 155:
 YY_RULE_SETUP
-#line 220 "ptx.l"
-TC; ptx_lval.int_value = CLOCK_REG; return SPECIAL_REGISTER;
+#line 228 "ptx.l"
+TC; return TARGET_DIRECTIVE;
 	YY_BREAK
 case 156:
 YY_RULE_SETUP
-#line 221 "ptx.l"
-TC; ptx_lval.int_value = HALFCLOCK_ID; return SPECIAL_REGISTER;
+#line 229 "ptx.l"
+TC; BEGIN(NOT_OPCODE); return TEX_DIRECTIVE;
 	YY_BREAK
 case 157:
 YY_RULE_SETUP
-#line 222 "ptx.l"
-TC; ptx_lval.int_value = CLOCK64_REG; return SPECIAL_REGISTER;
+#line 230 "ptx.l"
+TC; return UNION_DIRECTIVE; /* not in PTX 2.1 */
 	YY_BREAK
 case 158:
 YY_RULE_SETUP
-#line 223 "ptx.l"
-TC; ptx_lval.int_value = CTAID_REG; return SPECIAL_REGISTER;
+#line 231 "ptx.l"
+TC; return VERSION_DIRECTIVE;
 	YY_BREAK
 case 159:
 YY_RULE_SETUP
-#line 224 "ptx.l"
-TC; sscanf(ptx_text+7,"%u",&ptx_lval.int_value); ptx_lval.int_value<<=16; ptx_lval.int_value += ENVREG_REG; return SPECIAL_REGISTER;
+#line 232 "ptx.l"
+TC; return VISIBLE_DIRECTIVE;
 	YY_BREAK
 case 160:
 YY_RULE_SETUP
-#line 225 "ptx.l"
-TC; ptx_lval.int_value = GRIDID_REG; return SPECIAL_REGISTER;
+#line 233 "ptx.l"
+TC; return WEAK_DIRECTIVE;
 	YY_BREAK
 case 161:
 YY_RULE_SETUP
-#line 226 "ptx.l"
-TC; ptx_lval.int_value = LANEID_REG; return SPECIAL_REGISTER;
+#line 234 "ptx.l"
+TC; return ADDRESS_SIZE_DIRECTIVE;
 	YY_BREAK
 case 162:
 YY_RULE_SETUP
-#line 227 "ptx.l"
-TC; ptx_lval.int_value = LANEMASK_EQ_REG; return SPECIAL_REGISTER;
+#line 235 "ptx.l"
+TC; return WEAK_DIRECTIVE;
 	YY_BREAK
 case 163:
 YY_RULE_SETUP
-#line 228 "ptx.l"
-TC; ptx_lval.int_value = LANEMASK_LE_REG; return SPECIAL_REGISTER;
+#line 237 "ptx.l"
+TC; return CONSTPTR_DIRECTIVE; /* Ptx plus directive for pointer to constant memory */
 	YY_BREAK
 case 164:
 YY_RULE_SETUP
-#line 229 "ptx.l"
-TC; ptx_lval.int_value = LANEMASK_LT_REG; return SPECIAL_REGISTER;
+#line 238 "ptx.l"
+TC; return PTR_DIRECTIVE; /* Added for new OpenCL genrated code */
 	YY_BREAK
 case 165:
 YY_RULE_SETUP
-#line 230 "ptx.l"
-TC; ptx_lval.int_value = LANEMASK_GE_REG; return SPECIAL_REGISTER;
+#line 240 "ptx.l"
+TC; yylval->int_value = CLOCK_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 166:
 YY_RULE_SETUP
-#line 231 "ptx.l"
-TC; ptx_lval.int_value = LANEMASK_GT_REG; return SPECIAL_REGISTER;
+#line 241 "ptx.l"
+TC; yylval->int_value = HALFCLOCK_ID; return SPECIAL_REGISTER;
 	YY_BREAK
 case 167:
 YY_RULE_SETUP
-#line 232 "ptx.l"
-TC; ptx_lval.int_value = NCTAID_REG; return SPECIAL_REGISTER;
+#line 242 "ptx.l"
+TC; yylval->int_value = CLOCK64_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 168:
 YY_RULE_SETUP
-#line 233 "ptx.l"
-TC; ptx_lval.int_value = NTID_REG; return SPECIAL_REGISTER;
+#line 243 "ptx.l"
+TC; yylval->int_value = CTAID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 169:
 YY_RULE_SETUP
-#line 234 "ptx.l"
-TC; ptx_lval.int_value = NSMID_REG; return SPECIAL_REGISTER;
+#line 244 "ptx.l"
+TC; sscanf(yytext+7,"%u",&yylval->int_value); yylval->int_value<<=16; yylval->int_value += ENVREG_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 170:
 YY_RULE_SETUP
-#line 235 "ptx.l"
-TC; ptx_lval.int_value = NWARPID_REG; return SPECIAL_REGISTER;
+#line 245 "ptx.l"
+TC; yylval->int_value = GRIDID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 171:
 YY_RULE_SETUP
-#line 236 "ptx.l"
-TC; sscanf(ptx_text+3,"%u",&ptx_lval.int_value); ptx_lval.int_value<<=16; ptx_lval.int_value += PM_REG; return SPECIAL_REGISTER;
+#line 246 "ptx.l"
+TC; yylval->int_value = LANEID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 172:
 YY_RULE_SETUP
-#line 237 "ptx.l"
-TC; ptx_lval.int_value = SMID_REG; return SPECIAL_REGISTER;
+#line 247 "ptx.l"
+TC; yylval->int_value = LANEMASK_EQ_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 173:
 YY_RULE_SETUP
-#line 238 "ptx.l"
-TC; ptx_lval.int_value = TID_REG; return SPECIAL_REGISTER;
+#line 248 "ptx.l"
+TC; yylval->int_value = LANEMASK_LE_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 174:
 YY_RULE_SETUP
-#line 239 "ptx.l"
-TC; ptx_lval.int_value = WARPID_REG; return SPECIAL_REGISTER;
+#line 249 "ptx.l"
+TC; yylval->int_value = LANEMASK_LT_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 175:
 YY_RULE_SETUP
-#line 240 "ptx.l"
-TC; ptx_lval.int_value = WARPSZ_REG; return SPECIAL_REGISTER;
+#line 250 "ptx.l"
+TC; yylval->int_value = LANEMASK_GE_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 176:
 YY_RULE_SETUP
-#line 242 "ptx.l"
-TC; ptx_lval.string_value = strdup(ptx_text); return IDENTIFIER;
+#line 251 "ptx.l"
+TC; yylval->int_value = LANEMASK_GT_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 177:
 YY_RULE_SETUP
-#line 243 "ptx.l"
-TC; ptx_lval.string_value = strdup(ptx_text); return IDENTIFIER;
+#line 252 "ptx.l"
+TC; yylval->int_value = NCTAID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 178:
 YY_RULE_SETUP
-#line 245 "ptx.l"
-TC; sscanf(ptx_text,"%lf", &ptx_lval.double_value); return DOUBLE_OPERAND;
+#line 253 "ptx.l"
+TC; yylval->int_value = NTID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 179:
 YY_RULE_SETUP
-#line 247 "ptx.l"
-TC; CHECK_UNSIGNED; sscanf(ptx_text,"%x", &ptx_lval.int_value); return INT_OPERAND;
+#line 254 "ptx.l"
+TC; yylval->int_value = NSMID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 180:
 YY_RULE_SETUP
-#line 248 "ptx.l"
-TC; printf("GPGPU-Sim: ERROR ** parsing octal not (yet) implemented\n"); abort(); return INT_OPERAND;
+#line 255 "ptx.l"
+TC; yylval->int_value = NWARPID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 181:
 YY_RULE_SETUP
-#line 249 "ptx.l"
-TC; printf("GPGPU-Sim: ERROR ** parsing binary not (yet) implemented\n"); abort(); return INT_OPERAND;
+#line 256 "ptx.l"
+TC; sscanf(yytext+3,"%u",&yylval->int_value); yylval->int_value<<=16; yylval->int_value += PM_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 182:
 YY_RULE_SETUP
-#line 250 "ptx.l"
-TC; CHECK_UNSIGNED; ptx_lval.int_value =  atoi(ptx_text); return INT_OPERAND;
+#line 257 "ptx.l"
+TC; yylval->int_value = SMID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 183:
 YY_RULE_SETUP
-#line 252 "ptx.l"
-TC; sscanf(ptx_text+2,"%x", (unsigned*)(void*)&ptx_lval.float_value); return FLOAT_OPERAND;
+#line 258 "ptx.l"
+TC; yylval->int_value = TID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 184:
 YY_RULE_SETUP
-#line 253 "ptx.l"
-TC; sscanf(ptx_text+2,"%Lx", (unsigned long long*)(void*)&ptx_lval.double_value); return DOUBLE_OPERAND;
+#line 259 "ptx.l"
+TC; yylval->int_value = WARPID_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 185:
 YY_RULE_SETUP
-#line 255 "ptx.l"
-TC;  return S8_TYPE;
+#line 260 "ptx.l"
+TC; yylval->int_value = WARPSZ_REG; return SPECIAL_REGISTER;
 	YY_BREAK
 case 186:
 YY_RULE_SETUP
-#line 256 "ptx.l"
-TC;  return S16_TYPE;
+#line 262 "ptx.l"
+TC; yylval->string_value = strdup(yytext); return IDENTIFIER;
 	YY_BREAK
 case 187:
 YY_RULE_SETUP
-#line 257 "ptx.l"
-TC;  return S32_TYPE;
+#line 263 "ptx.l"
+TC; yylval->string_value = strdup(yytext); return IDENTIFIER;
 	YY_BREAK
 case 188:
 YY_RULE_SETUP
-#line 258 "ptx.l"
-TC;  return S64_TYPE;
+#line 265 "ptx.l"
+TC; sscanf(yytext,"%lf", &yylval->double_value); return DOUBLE_OPERAND;
 	YY_BREAK
 case 189:
 YY_RULE_SETUP
-#line 259 "ptx.l"
-TC;  return U8_TYPE;
+#line 267 "ptx.l"
+TC; CHECK_UNSIGNED; sscanf(yytext,"%x", &yylval->int_value); return INT_OPERAND;
 	YY_BREAK
 case 190:
 YY_RULE_SETUP
-#line 260 "ptx.l"
-TC;  return U16_TYPE;
+#line 268 "ptx.l"
+TC; printf("GPGPU-Sim: ERROR ** parsing octal not (yet) implemented\n"); abort(); return INT_OPERAND;
 	YY_BREAK
 case 191:
 YY_RULE_SETUP
-#line 261 "ptx.l"
-TC;  return U32_TYPE;
+#line 269 "ptx.l"
+TC; printf("GPGPU-Sim: ERROR ** parsing binary not (yet) implemented\n"); abort(); return INT_OPERAND;
 	YY_BREAK
 case 192:
 YY_RULE_SETUP
-#line 262 "ptx.l"
-TC;  return U64_TYPE;
+#line 270 "ptx.l"
+TC; CHECK_UNSIGNED; yylval->int_value =  atoi(yytext); return INT_OPERAND;
 	YY_BREAK
 case 193:
 YY_RULE_SETUP
-#line 263 "ptx.l"
-TC;  return F16_TYPE;
+#line 272 "ptx.l"
+TC; sscanf(yytext+2,"%x", (unsigned*)(void*)&yylval->float_value); return FLOAT_OPERAND;
 	YY_BREAK
 case 194:
 YY_RULE_SETUP
-#line 264 "ptx.l"
-TC;  return F16_TYPE; /* TODO: figure out what this should really be */
+#line 273 "ptx.l"
+TC; sscanf(yytext+2,"%Lx", (unsigned long long*)(void*)&yylval->double_value); return DOUBLE_OPERAND;
 	YY_BREAK
 case 195:
 YY_RULE_SETUP
-#line 265 "ptx.l"
-TC;  return F32_TYPE;
+#line 275 "ptx.l"
+TC;  return S8_TYPE;
 	YY_BREAK
 case 196:
 YY_RULE_SETUP
-#line 266 "ptx.l"
-TC;  return F64_TYPE;
+#line 276 "ptx.l"
+TC;  return S16_TYPE;
 	YY_BREAK
 case 197:
 YY_RULE_SETUP
-#line 267 "ptx.l"
-TC;  return FF64_TYPE;
+#line 277 "ptx.l"
+TC;  return S32_TYPE;
 	YY_BREAK
 case 198:
 YY_RULE_SETUP
-#line 268 "ptx.l"
-TC;  return B8_TYPE;
+#line 278 "ptx.l"
+TC;  return S64_TYPE;
 	YY_BREAK
 case 199:
 YY_RULE_SETUP
-#line 269 "ptx.l"
-TC;  return B16_TYPE;
+#line 279 "ptx.l"
+TC;  return U8_TYPE;
 	YY_BREAK
 case 200:
 YY_RULE_SETUP
-#line 270 "ptx.l"
-TC;  return B32_TYPE;
+#line 280 "ptx.l"
+TC;  return U16_TYPE;
 	YY_BREAK
 case 201:
 YY_RULE_SETUP
-#line 271 "ptx.l"
-TC;  return B64_TYPE;
+#line 281 "ptx.l"
+TC;  return U32_TYPE;
 	YY_BREAK
 case 202:
 YY_RULE_SETUP
-#line 272 "ptx.l"
-TC;  return BB64_TYPE;
+#line 282 "ptx.l"
+TC;  return U64_TYPE;
 	YY_BREAK
 case 203:
 YY_RULE_SETUP
-#line 273 "ptx.l"
-TC;  return BB128_TYPE;
+#line 283 "ptx.l"
+TC;  return F16_TYPE;
 	YY_BREAK
 case 204:
 YY_RULE_SETUP
-#line 274 "ptx.l"
-TC;  return PRED_TYPE;
+#line 284 "ptx.l"
+TC;  return F16_TYPE; /* TODO: figure out what this should really be */
 	YY_BREAK
 case 205:
 YY_RULE_SETUP
-#line 276 "ptx.l"
-TC; BEGIN(NOT_OPCODE); return TEXREF_TYPE;
+#line 285 "ptx.l"
+TC;  return F32_TYPE;
 	YY_BREAK
 case 206:
 YY_RULE_SETUP
-#line 277 "ptx.l"
-TC;  return SAMPLERREF_TYPE;
+#line 286 "ptx.l"
+TC;  return F64_TYPE;
 	YY_BREAK
 case 207:
 YY_RULE_SETUP
-#line 278 "ptx.l"
-TC;  return SURFREF_TYPE;
+#line 287 "ptx.l"
+TC;  return FF64_TYPE;
 	YY_BREAK
 case 208:
 YY_RULE_SETUP
-#line 280 "ptx.l"
-TC; return V2_TYPE;
+#line 288 "ptx.l"
+TC;  return B8_TYPE;
 	YY_BREAK
 case 209:
 YY_RULE_SETUP
-#line 281 "ptx.l"
-TC; return V3_TYPE;
+#line 289 "ptx.l"
+TC;  return B16_TYPE;
 	YY_BREAK
 case 210:
 YY_RULE_SETUP
-#line 282 "ptx.l"
-TC; return V4_TYPE;
+#line 290 "ptx.l"
+TC;  return B32_TYPE;
 	YY_BREAK
 case 211:
 YY_RULE_SETUP
-#line 284 "ptx.l"
-TC; return HALF_OPTION; /* ptxplus */
+#line 291 "ptx.l"
+TC;  return B64_TYPE;
 	YY_BREAK
 case 212:
 YY_RULE_SETUP
-#line 285 "ptx.l"
-TC; return EXTP_OPTION; /* extended precision option */
+#line 292 "ptx.l"
+TC;  return BB64_TYPE;
 	YY_BREAK
 case 213:
 YY_RULE_SETUP
-#line 287 "ptx.l"
-TC; return EQU_OPTION;
+#line 293 "ptx.l"
+TC;  return BB128_TYPE;
 	YY_BREAK
 case 214:
 YY_RULE_SETUP
-#line 288 "ptx.l"
-TC; return NEU_OPTION;
+#line 294 "ptx.l"
+TC;  return PRED_TYPE;
 	YY_BREAK
 case 215:
 YY_RULE_SETUP
-#line 289 "ptx.l"
-TC; return LTU_OPTION;
+#line 296 "ptx.l"
+TC; BEGIN(NOT_OPCODE); return TEXREF_TYPE;
 	YY_BREAK
 case 216:
 YY_RULE_SETUP
-#line 290 "ptx.l"
-TC; return LEU_OPTION;
+#line 297 "ptx.l"
+TC;  return SAMPLERREF_TYPE;
 	YY_BREAK
 case 217:
 YY_RULE_SETUP
-#line 291 "ptx.l"
-TC; return GTU_OPTION;
+#line 298 "ptx.l"
+TC;  return SURFREF_TYPE;
 	YY_BREAK
 case 218:
 YY_RULE_SETUP
-#line 292 "ptx.l"
-TC; return GEU_OPTION;
+#line 300 "ptx.l"
+TC; return V2_TYPE;
 	YY_BREAK
 case 219:
 YY_RULE_SETUP
-#line 293 "ptx.l"
-TC; return NUM_OPTION;
+#line 301 "ptx.l"
+TC; return V3_TYPE;
 	YY_BREAK
 case 220:
 YY_RULE_SETUP
-#line 294 "ptx.l"
-TC; return NAN_OPTION;
+#line 302 "ptx.l"
+TC; return V4_TYPE;
 	YY_BREAK
 case 221:
 YY_RULE_SETUP
-#line 296 "ptx.l"
-TC; return SAT_OPTION;
+#line 304 "ptx.l"
+TC; return HALF_OPTION; /* ptxplus */
 	YY_BREAK
 case 222:
 YY_RULE_SETUP
-#line 298 "ptx.l"
-TC; return EQ_OPTION;
+#line 305 "ptx.l"
+TC; return EXTP_OPTION; /* extended precision option */
 	YY_BREAK
 case 223:
 YY_RULE_SETUP
-#line 299 "ptx.l"
-TC; return NE_OPTION;
+#line 307 "ptx.l"
+TC; return EQU_OPTION;
 	YY_BREAK
 case 224:
 YY_RULE_SETUP
-#line 300 "ptx.l"
-TC; return LT_OPTION;
+#line 308 "ptx.l"
+TC; return NEU_OPTION;
 	YY_BREAK
 case 225:
 YY_RULE_SETUP
-#line 301 "ptx.l"
-TC; return LE_OPTION;
+#line 309 "ptx.l"
+TC; return LTU_OPTION;
 	YY_BREAK
 case 226:
 YY_RULE_SETUP
-#line 302 "ptx.l"
-TC; return GT_OPTION;
+#line 310 "ptx.l"
+TC; return LEU_OPTION;
 	YY_BREAK
 case 227:
 YY_RULE_SETUP
-#line 303 "ptx.l"
-TC; return GE_OPTION;
+#line 311 "ptx.l"
+TC; return GTU_OPTION;
 	YY_BREAK
 case 228:
 YY_RULE_SETUP
-#line 304 "ptx.l"
-TC; return CF_OPTION;
+#line 312 "ptx.l"
+TC; return GEU_OPTION;
 	YY_BREAK
 case 229:
 YY_RULE_SETUP
-#line 305 "ptx.l"
-TC; return SF_OPTION;
+#line 313 "ptx.l"
+TC; return NUM_OPTION;
 	YY_BREAK
 case 230:
 YY_RULE_SETUP
-#line 306 "ptx.l"
-TC; return NSF_OPTION;
+#line 314 "ptx.l"
+TC; return NAN_OPTION;
 	YY_BREAK
 case 231:
 YY_RULE_SETUP
-#line 308 "ptx.l"
-TC; return LO_OPTION;
+#line 316 "ptx.l"
+TC; return SAT_OPTION;
 	YY_BREAK
 case 232:
 YY_RULE_SETUP
-#line 309 "ptx.l"
-TC; return LS_OPTION;
+#line 318 "ptx.l"
+TC; return EQ_OPTION;
 	YY_BREAK
 case 233:
 YY_RULE_SETUP
-#line 310 "ptx.l"
-TC; return HI_OPTION;
+#line 319 "ptx.l"
+TC; return NE_OPTION;
 	YY_BREAK
 case 234:
 YY_RULE_SETUP
-#line 311 "ptx.l"
-TC; return HS_OPTION;
+#line 320 "ptx.l"
+TC; return LT_OPTION;
 	YY_BREAK
 case 235:
 YY_RULE_SETUP
-#line 314 "ptx.l"
-TC; return RNI_OPTION;
+#line 321 "ptx.l"
+TC; return LE_OPTION;
 	YY_BREAK
 case 236:
 YY_RULE_SETUP
-#line 315 "ptx.l"
-TC; return RZI_OPTION;
+#line 322 "ptx.l"
+TC; return GT_OPTION;
 	YY_BREAK
 case 237:
 YY_RULE_SETUP
-#line 316 "ptx.l"
-TC; return RMI_OPTION;
+#line 323 "ptx.l"
+TC; return GE_OPTION;
 	YY_BREAK
 case 238:
 YY_RULE_SETUP
-#line 317 "ptx.l"
-TC; return RPI_OPTION;
+#line 324 "ptx.l"
+TC; return CF_OPTION;
 	YY_BREAK
 case 239:
 YY_RULE_SETUP
-#line 319 "ptx.l"
-TC; return RN_OPTION;
+#line 325 "ptx.l"
+TC; return SF_OPTION;
 	YY_BREAK
 case 240:
 YY_RULE_SETUP
-#line 320 "ptx.l"
-TC; return RZ_OPTION;
+#line 326 "ptx.l"
+TC; return NSF_OPTION;
 	YY_BREAK
 case 241:
 YY_RULE_SETUP
-#line 321 "ptx.l"
-TC; return RM_OPTION;
+#line 328 "ptx.l"
+TC; return LO_OPTION;
 	YY_BREAK
 case 242:
 YY_RULE_SETUP
-#line 322 "ptx.l"
-TC; return RP_OPTION;
+#line 329 "ptx.l"
+TC; return LS_OPTION;
 	YY_BREAK
 case 243:
 YY_RULE_SETUP
-#line 324 "ptx.l"
-TC; return FTZ_OPTION;
+#line 330 "ptx.l"
+TC; return HI_OPTION;
 	YY_BREAK
 case 244:
 YY_RULE_SETUP
-#line 326 "ptx.l"
-TC; return NEG_OPTION;
+#line 331 "ptx.l"
+TC; return HS_OPTION;
 	YY_BREAK
 case 245:
 YY_RULE_SETUP
-#line 328 "ptx.l"
-TC; return WIDE_OPTION;
+#line 334 "ptx.l"
+TC; return RNI_OPTION;
 	YY_BREAK
 case 246:
 YY_RULE_SETUP
-#line 329 "ptx.l"
-TC; return UNI_OPTION;
+#line 335 "ptx.l"
+TC; return RZI_OPTION;
 	YY_BREAK
 case 247:
 YY_RULE_SETUP
-#line 331 "ptx.l"
-TC; return SYNC_OPTION;
+#line 336 "ptx.l"
+TC; return RMI_OPTION;
 	YY_BREAK
 case 248:
 YY_RULE_SETUP
-#line 332 "ptx.l"
-TC; return ARRIVE_OPTION;
+#line 337 "ptx.l"
+TC; return RPI_OPTION;
 	YY_BREAK
 case 249:
 YY_RULE_SETUP
-#line 333 "ptx.l"
-TC; return RED_OPTION;
+#line 339 "ptx.l"
+TC; return RN_OPTION;
 	YY_BREAK
 case 250:
 YY_RULE_SETUP
-#line 336 "ptx.l"
-TC; return APPROX_OPTION;
+#line 340 "ptx.l"
+TC; return RZ_OPTION;
 	YY_BREAK
 case 251:
 YY_RULE_SETUP
-#line 337 "ptx.l"
-TC; return FULL_OPTION;
+#line 341 "ptx.l"
+TC; return RM_OPTION;
 	YY_BREAK
 case 252:
 YY_RULE_SETUP
-#line 339 "ptx.l"
-TC; return ANY_OPTION;
+#line 342 "ptx.l"
+TC; return RP_OPTION;
 	YY_BREAK
 case 253:
 YY_RULE_SETUP
-#line 340 "ptx.l"
-TC; return ALL_OPTION;
+#line 344 "ptx.l"
+TC; return FTZ_OPTION;
 	YY_BREAK
 case 254:
 YY_RULE_SETUP
-#line 341 "ptx.l"
-TC; return BALLOT_OPTION;
+#line 346 "ptx.l"
+TC; return NEG_OPTION;
 	YY_BREAK
 case 255:
 YY_RULE_SETUP
-#line 342 "ptx.l"
-TC; return GLOBAL_OPTION;
+#line 348 "ptx.l"
+TC; return WIDE_OPTION;
 	YY_BREAK
 case 256:
 YY_RULE_SETUP
-#line 343 "ptx.l"
-TC; return CTA_OPTION;
+#line 349 "ptx.l"
+TC; return UNI_OPTION;
 	YY_BREAK
 case 257:
 YY_RULE_SETUP
-#line 344 "ptx.l"
-TC; return SYS_OPTION;
+#line 351 "ptx.l"
+TC; return SYNC_OPTION;
 	YY_BREAK
 case 258:
 YY_RULE_SETUP
-#line 346 "ptx.l"
-TC; return EXIT_OPTION;
+#line 352 "ptx.l"
+TC; return ARRIVE_OPTION;
 	YY_BREAK
 case 259:
 YY_RULE_SETUP
-#line 348 "ptx.l"
-TC; return ABS_OPTION;
+#line 353 "ptx.l"
+TC; return RED_OPTION;
 	YY_BREAK
 case 260:
 YY_RULE_SETUP
-#line 350 "ptx.l"
-TC; return TO_OPTION;
+#line 356 "ptx.l"
+TC; return APPROX_OPTION;
 	YY_BREAK
 case 261:
 YY_RULE_SETUP
-#line 352 "ptx.l"
-TC; return CA_OPTION;
+#line 357 "ptx.l"
+TC; return FULL_OPTION;
 	YY_BREAK
 case 262:
 YY_RULE_SETUP
-#line 353 "ptx.l"
-TC; return CG_OPTION;
+#line 359 "ptx.l"
+TC; return ANY_OPTION;
 	YY_BREAK
 case 263:
 YY_RULE_SETUP
-#line 354 "ptx.l"
-TC; return CS_OPTION;
+#line 360 "ptx.l"
+TC; return ALL_OPTION;
 	YY_BREAK
 case 264:
 YY_RULE_SETUP
-#line 355 "ptx.l"
-TC; return LU_OPTION;
+#line 361 "ptx.l"
+TC; return BALLOT_OPTION;
 	YY_BREAK
 case 265:
 YY_RULE_SETUP
-#line 356 "ptx.l"
-TC; return CV_OPTION;
+#line 362 "ptx.l"
+TC; return GLOBAL_OPTION;
 	YY_BREAK
 case 266:
 YY_RULE_SETUP
-#line 358 "ptx.l"
-TC; return WB_OPTION;
+#line 363 "ptx.l"
+TC; return CTA_OPTION;
 	YY_BREAK
 case 267:
 YY_RULE_SETUP
-#line 359 "ptx.l"
-TC; return WT_OPTION;
+#line 364 "ptx.l"
+TC; return SYS_OPTION;
 	YY_BREAK
 case 268:
 YY_RULE_SETUP
-#line 361 "ptx.l"
-TC; return NC_OPTION;
+#line 366 "ptx.l"
+TC; return EXIT_OPTION;
 	YY_BREAK
 case 269:
 YY_RULE_SETUP
-#line 363 "ptx.l"
-TC; return UP_OPTION;
+#line 368 "ptx.l"
+TC; return ABS_OPTION;
 	YY_BREAK
 case 270:
 YY_RULE_SETUP
-#line 364 "ptx.l"
-TC; return DOWN_OPTION;
+#line 370 "ptx.l"
+TC; return TO_OPTION;
 	YY_BREAK
 case 271:
 YY_RULE_SETUP
-#line 365 "ptx.l"
-TC; return BFLY_OPTION;
+#line 372 "ptx.l"
+TC; return CA_OPTION;
 	YY_BREAK
 case 272:
 YY_RULE_SETUP
-#line 366 "ptx.l"
-TC; return IDX_OPTION;
+#line 373 "ptx.l"
+TC; return CG_OPTION;
 	YY_BREAK
 case 273:
 YY_RULE_SETUP
-#line 368 "ptx.l"
+#line 374 "ptx.l"
+TC; return CS_OPTION;
+	YY_BREAK
+case 274:
+YY_RULE_SETUP
+#line 375 "ptx.l"
+TC; return LU_OPTION;
+	YY_BREAK
+case 275:
+YY_RULE_SETUP
+#line 376 "ptx.l"
+TC; return CV_OPTION;
+	YY_BREAK
+case 276:
+YY_RULE_SETUP
+#line 378 "ptx.l"
+TC; return WB_OPTION;
+	YY_BREAK
+case 277:
+YY_RULE_SETUP
+#line 379 "ptx.l"
+TC; return WT_OPTION;
+	YY_BREAK
+case 278:
+YY_RULE_SETUP
+#line 381 "ptx.l"
+TC; return NC_OPTION;
+	YY_BREAK
+case 279:
+YY_RULE_SETUP
+#line 383 "ptx.l"
+TC; return UP_OPTION;
+	YY_BREAK
+case 280:
+YY_RULE_SETUP
+#line 384 "ptx.l"
+TC; return DOWN_OPTION;
+	YY_BREAK
+case 281:
+YY_RULE_SETUP
+#line 385 "ptx.l"
+TC; return BFLY_OPTION;
+	YY_BREAK
+case 282:
+YY_RULE_SETUP
+#line 386 "ptx.l"
+TC; return IDX_OPTION;
+	YY_BREAK
+case 283:
+YY_RULE_SETUP
+#line 388 "ptx.l"
 TC; return ATOMIC_POPC;
 	YY_BREAK
-case 274:
+case 284:
 YY_RULE_SETUP
-#line 369 "ptx.l"
+#line 389 "ptx.l"
 TC; return ATOMIC_AND;
 	YY_BREAK
-case 275:
+case 285:
 YY_RULE_SETUP
-#line 370 "ptx.l"
+#line 390 "ptx.l"
 TC; return ATOMIC_OR;
 	YY_BREAK
-case 276:
+case 286:
 YY_RULE_SETUP
-#line 371 "ptx.l"
+#line 391 "ptx.l"
 TC; return ATOMIC_XOR;
 	YY_BREAK
-case 277:
+case 287:
 YY_RULE_SETUP
-#line 372 "ptx.l"
+#line 392 "ptx.l"
 TC; return ATOMIC_CAS;
 	YY_BREAK
-case 278:
+case 288:
 YY_RULE_SETUP
-#line 373 "ptx.l"
+#line 393 "ptx.l"
 TC; return ATOMIC_EXCH;
 	YY_BREAK
-case 279:
+case 289:
 YY_RULE_SETUP
-#line 374 "ptx.l"
+#line 394 "ptx.l"
 TC; return ATOMIC_ADD;
 	YY_BREAK
-case 280:
+case 290:
 YY_RULE_SETUP
-#line 375 "ptx.l"
+#line 395 "ptx.l"
 TC; return ATOMIC_INC;
 	YY_BREAK
-case 281:
+case 291:
 YY_RULE_SETUP
-#line 376 "ptx.l"
+#line 396 "ptx.l"
 TC; return ATOMIC_DEC;
 	YY_BREAK
-case 282:
+case 292:
 YY_RULE_SETUP
-#line 377 "ptx.l"
+#line 397 "ptx.l"
 TC; return ATOMIC_MIN;
 	YY_BREAK
-case 283:
+case 293:
 YY_RULE_SETUP
-#line 378 "ptx.l"
+#line 398 "ptx.l"
 TC; return ATOMIC_MAX;
 	YY_BREAK
-case 284:
+case 294:
 YY_RULE_SETUP
-#line 382 "ptx.l"
+#line 402 "ptx.l"
 TC; return GEOM_MODIFIER_1D;
 	YY_BREAK
-case 285:
+case 295:
 YY_RULE_SETUP
-#line 383 "ptx.l"
+#line 403 "ptx.l"
 TC; return GEOM_MODIFIER_2D;
 	YY_BREAK
-case 286:
+case 296:
 YY_RULE_SETUP
-#line 384 "ptx.l"
+#line 404 "ptx.l"
 TC; return GEOM_MODIFIER_3D;
 	YY_BREAK
-case 287:
+case 297:
 YY_RULE_SETUP
-#line 386 "ptx.l"
-TC; ptx_lval.int_value = 0; return DIMENSION_MODIFIER;
+#line 406 "ptx.l"
+TC; yylval->int_value = 0; return DIMENSION_MODIFIER;
 	YY_BREAK
-case 288:
+case 298:
 YY_RULE_SETUP
-#line 387 "ptx.l"
-TC; ptx_lval.int_value = 1; return DIMENSION_MODIFIER;
+#line 407 "ptx.l"
+TC; yylval->int_value = 1; return DIMENSION_MODIFIER;
 	YY_BREAK
-case 289:
+case 299:
 YY_RULE_SETUP
-#line 388 "ptx.l"
-TC; ptx_lval.int_value = 2; return DIMENSION_MODIFIER;
+#line 408 "ptx.l"
+TC; yylval->int_value = 2; return DIMENSION_MODIFIER;
 	YY_BREAK
-case 290:
+case 300:
 YY_RULE_SETUP
-#line 389 "ptx.l"
-TC; ptx_lval.int_value = 0; return DIMENSION_MODIFIER;
+#line 409 "ptx.l"
+TC; yylval->int_value = 0; return DIMENSION_MODIFIER;
 	YY_BREAK
-case 291:
+case 301:
 YY_RULE_SETUP
-#line 390 "ptx.l"
-TC; ptx_lval.int_value = 1; return DIMENSION_MODIFIER;
+#line 410 "ptx.l"
+TC; yylval->int_value = 1; return DIMENSION_MODIFIER;
 	YY_BREAK
-case 292:
+case 302:
 YY_RULE_SETUP
-#line 391 "ptx.l"
-TC; ptx_lval.int_value = 2; return DIMENSION_MODIFIER;
+#line 411 "ptx.l"
+TC; yylval->int_value = 2; return DIMENSION_MODIFIER;
 	YY_BREAK
-case 293:
+case 303:
 YY_RULE_SETUP
-#line 393 "ptx.l"
+#line 413 "ptx.l"
 TC; return MINUS;
 	YY_BREAK
-case 294:
+case 304:
 YY_RULE_SETUP
-#line 394 "ptx.l"
+#line 414 "ptx.l"
 TC; return PLUS;
 	YY_BREAK
-case 295:
+case 305:
 YY_RULE_SETUP
-#line 395 "ptx.l"
+#line 415 "ptx.l"
 TC; return COMMA;
 	YY_BREAK
-case 296:
+case 306:
 YY_RULE_SETUP
-#line 396 "ptx.l"
+#line 416 "ptx.l"
 TC; return PRED;
 	YY_BREAK
-case 297:
+case 307:
 YY_RULE_SETUP
-#line 397 "ptx.l"
+#line 417 "ptx.l"
 TC; return PIPE;
 	YY_BREAK
-case 298:
+case 308:
 YY_RULE_SETUP
-#line 398 "ptx.l"
+#line 418 "ptx.l"
 TC; return LEFT_SQUARE_BRACKET;
 	YY_BREAK
-case 299:
+case 309:
 YY_RULE_SETUP
-#line 399 "ptx.l"
+#line 419 "ptx.l"
 TC; return RIGHT_SQUARE_BRACKET;
 	YY_BREAK
-case 300:
+case 310:
 YY_RULE_SETUP
-#line 400 "ptx.l"
+#line 420 "ptx.l"
 TC; return LEFT_ANGLE_BRACKET;
 	YY_BREAK
-case 301:
+case 311:
 YY_RULE_SETUP
-#line 401 "ptx.l"
+#line 421 "ptx.l"
 TC; return RIGHT_ANGLE_BRACKET;
 	YY_BREAK
-case 302:
+case 312:
 YY_RULE_SETUP
-#line 402 "ptx.l"
+#line 422 "ptx.l"
 TC; return LEFT_PAREN;
 	YY_BREAK
-case 303:
+case 313:
 YY_RULE_SETUP
-#line 403 "ptx.l"
+#line 423 "ptx.l"
 TC; return RIGHT_PAREN;
 	YY_BREAK
-case 304:
+case 314:
 YY_RULE_SETUP
-#line 404 "ptx.l"
+#line 424 "ptx.l"
 TC; BEGIN(INITIAL); return COLON;
 	YY_BREAK
-case 305:
+case 315:
 YY_RULE_SETUP
-#line 405 "ptx.l"
+#line 425 "ptx.l"
 TC; BEGIN(INITIAL); return SEMI_COLON;
 	YY_BREAK
-case 306:
+case 316:
 YY_RULE_SETUP
-#line 406 "ptx.l"
+#line 426 "ptx.l"
 TC; return EXCLAMATION;
 	YY_BREAK
-case 307:
+case 317:
 YY_RULE_SETUP
-#line 407 "ptx.l"
+#line 427 "ptx.l"
 TC; return EQUALS;
 	YY_BREAK
-case 308:
+case 318:
 YY_RULE_SETUP
-#line 408 "ptx.l"
+#line 428 "ptx.l"
 TC; return RIGHT_BRACE;
 	YY_BREAK
-case 309:
+case 319:
 YY_RULE_SETUP
-#line 409 "ptx.l"
+#line 429 "ptx.l"
 TC; return PERIOD;
 	YY_BREAK
-case 310:
+case 320:
 YY_RULE_SETUP
-#line 410 "ptx.l"
+#line 430 "ptx.l"
 TC; return BACKSLASH;
 	YY_BREAK
-case 311:
+case 321:
 YY_RULE_SETUP
-#line 412 "ptx.l"
+#line 432 "ptx.l"
 TC;	// eat single
 	YY_BREAK
-case 312:
-/* rule 312 can match eol */
+case 322:
+/* rule 322 can match eol */
 YY_RULE_SETUP
-#line 414 "ptx.l"
-col=0; strncpy(linebuf, ptx_text + 1, LINEBUF_SIZE); yyless( 1 );
+#line 434 "ptx.l"
+recognizer->col=0; strncpy(recognizer->linebuf, yytext + 1, LINEBUF_SIZE); yyless( 1 );
 	YY_BREAK
-case 313:
+case 323:
 YY_RULE_SETUP
-#line 416 "ptx.l"
+#line 436 "ptx.l"
 TC;
 	YY_BREAK
-case 314:
+case 324:
 YY_RULE_SETUP
-#line 417 "ptx.l"
+#line 437 "ptx.l"
 TC;
 	YY_BREAK
 
-case 315:
+case 325:
 YY_RULE_SETUP
-#line 422 "ptx.l"
+#line 442 "ptx.l"
 TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
 	YY_BREAK
-case 316:
+case 326:
 YY_RULE_SETUP
-#line 423 "ptx.l"
+#line 443 "ptx.l"
 TC; return LEFT_BRACE; // starting a vector operand (next token cannot be opcode)
 	YY_BREAK
 
-case 317:
+case 327:
 YY_RULE_SETUP
-#line 426 "ptx.l"
+#line 446 "ptx.l"
 BEGIN(IN_COMMENT);
 	YY_BREAK
 
 
-case 318:
+case 328:
 YY_RULE_SETUP
-#line 429 "ptx.l"
+#line 449 "ptx.l"
 BEGIN(INITIAL);
 	YY_BREAK
-case 319:
+case 329:
 YY_RULE_SETUP
-#line 430 "ptx.l"
+#line 450 "ptx.l"
 printf("BEGINNING CUSTOM PTX.\n"); BEGIN(INITIAL);
 	YY_BREAK
-case 320:
+case 330:
 YY_RULE_SETUP
-#line 431 "ptx.l"
+#line 451 "ptx.l"
 // eat comment in chunks
 	YY_BREAK
-case 321:
+case 331:
 YY_RULE_SETUP
-#line 432 "ptx.l"
+#line 452 "ptx.l"
 // eat the lone C 
 	YY_BREAK
-case 322:
+case 332:
 YY_RULE_SETUP
-#line 433 "ptx.l"
+#line 453 "ptx.l"
 // eat the lone star
 	YY_BREAK
-case 323:
-/* rule 323 can match eol */
+case 333:
+/* rule 333 can match eol */
 YY_RULE_SETUP
-#line 434 "ptx.l"
+#line 454 "ptx.l"
 TC; 
 	YY_BREAK
 
 
-case 324:
+case 334:
 YY_RULE_SETUP
-#line 438 "ptx.l"
+#line 458 "ptx.l"
 BEGIN(IN_STRING);
 	YY_BREAK
 
 
-case 325:
+case 335:
 YY_RULE_SETUP
-#line 441 "ptx.l"
+#line 461 "ptx.l"
 TC; BEGIN(INITIAL); return STRING;
 	YY_BREAK
-case 326:
-/* rule 326 can match eol */
+case 336:
+/* rule 336 can match eol */
 YY_RULE_SETUP
-#line 442 "ptx.l"
-TC; ptx_lval.string_value = strdup(ptx_text); 
+#line 462 "ptx.l"
+TC; yylval->string_value = strdup(yytext);
 	YY_BREAK
 
-case 327:
-/* rule 327 can match eol */
+case 337:
+/* rule 337 can match eol */
 YY_RULE_SETUP
-#line 445 "ptx.l"
+#line 465 "ptx.l"
 
 	YY_BREAK
-case 328:
+case 338:
 YY_RULE_SETUP
-#line 447 "ptx.l"
-TC; ptx_error((const char*)NULL);
+#line 467 "ptx.l"
+TC; ptx_error(yyscanner, recognizer, (const char*)NULL);
 	YY_BREAK
-case 329:
+case 339:
 YY_RULE_SETUP
-#line 448 "ptx.l"
+#line 468 "ptx.l"
 ECHO;
 	YY_BREAK
-#line 3457 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptx_.c"
+#line 3798 "lex.ptx_c.c"
 case YY_STATE_EOF(INITIAL):
 case YY_STATE_EOF(IN_STRING):
 case YY_STATE_EOF(IN_COMMENT):
@@ -3464,25 +3805,25 @@ case YY_STATE_EOF(NOT_OPCODE):
 	case YY_END_OF_BUFFER:
 		{
 		/* Amount of text matched not including the EOB char. */
-		int yy_amount_of_matched_text = (int) (yy_cp - (yytext_ptr)) - 1;
+		int yy_amount_of_matched_text = (int) (yy_cp - yyg->yytext_ptr) - 1;
 
 		/* Undo the effects of YY_DO_BEFORE_ACTION. */
-		*yy_cp = (yy_hold_char);
+		*yy_cp = yyg->yy_hold_char;
 		YY_RESTORE_YY_MORE_OFFSET
 
 		if ( YY_CURRENT_BUFFER_LVALUE->yy_buffer_status == YY_BUFFER_NEW )
 			{
 			/* We're scanning a new file or input source.  It's
 			 * possible that this happened because the user
-			 * just pointed ptx_in at a new source and called
-			 * ptx_lex().  If so, then we have to assure
+			 * just pointed yyin at a new source and called
+			 * yylex().  If so, then we have to assure
 			 * consistency between YY_CURRENT_BUFFER and our
 			 * globals.  Here is the right place to do so, because
 			 * this is the first action (other than possibly a
 			 * back-up) that will match for the new input source.
 			 */
-			(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
-			YY_CURRENT_BUFFER_LVALUE->yy_input_file = ptx_in;
+			yyg->yy_n_chars = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
+			YY_CURRENT_BUFFER_LVALUE->yy_input_file = yyin;
 			YY_CURRENT_BUFFER_LVALUE->yy_buffer_status = YY_BUFFER_NORMAL;
 			}
 
@@ -3493,13 +3834,13 @@ case YY_STATE_EOF(NOT_OPCODE):
 		 * end-of-buffer state).  Contrast this with the test
 		 * in input().
 		 */
-		if ( (yy_c_buf_p) <= &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] )
+		if ( yyg->yy_c_buf_p <= &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars] )
 			{ /* This was really a NUL. */
 			yy_state_type yy_next_state;
 
-			(yy_c_buf_p) = (yytext_ptr) + yy_amount_of_matched_text;
+			yyg->yy_c_buf_p = yyg->yytext_ptr + yy_amount_of_matched_text;
 
-			yy_current_state = yy_get_previous_state(  );
+			yy_current_state = yy_get_previous_state( yyscanner );
 
 			/* Okay, we're now positioned to make the NUL
 			 * transition.  We couldn't have
@@ -3510,43 +3851,43 @@ case YY_STATE_EOF(NOT_OPCODE):
 			 * will run more slowly).
 			 */
 
-			yy_next_state = yy_try_NUL_trans( yy_current_state );
+			yy_next_state = yy_try_NUL_trans( yy_current_state , yyscanner);
 
-			yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+			yy_bp = yyg->yytext_ptr + YY_MORE_ADJ;
 
 			if ( yy_next_state )
 				{
 				/* Consume the NUL. */
-				yy_cp = ++(yy_c_buf_p);
+				yy_cp = ++yyg->yy_c_buf_p;
 				yy_current_state = yy_next_state;
 				goto yy_match;
 				}
 
 			else
 				{
-				yy_cp = (yy_c_buf_p);
+				yy_cp = yyg->yy_c_buf_p;
 				goto yy_find_action;
 				}
 			}
 
-		else switch ( yy_get_next_buffer(  ) )
+		else switch ( yy_get_next_buffer( yyscanner ) )
 			{
 			case EOB_ACT_END_OF_FILE:
 				{
-				(yy_did_buffer_switch_on_eof) = 0;
+				yyg->yy_did_buffer_switch_on_eof = 0;
 
-				if ( ptx_wrap( ) )
+				if ( yywrap( yyscanner ) )
 					{
 					/* Note: because we've taken care in
 					 * yy_get_next_buffer() to have set up
-					 * ptx_text, we can now set up
+					 * yytext, we can now set up
 					 * yy_c_buf_p so that if some total
 					 * hoser (like flex itself) wants to
 					 * call the scanner after we return the
 					 * YY_NULL, it'll still work - another
 					 * YY_NULL will get returned.
 					 */
-					(yy_c_buf_p) = (yytext_ptr) + YY_MORE_ADJ;
+					yyg->yy_c_buf_p = yyg->yytext_ptr + YY_MORE_ADJ;
 
 					yy_act = YY_STATE_EOF(YY_START);
 					goto do_action;
@@ -3554,30 +3895,30 @@ case YY_STATE_EOF(NOT_OPCODE):
 
 				else
 					{
-					if ( ! (yy_did_buffer_switch_on_eof) )
+					if ( ! yyg->yy_did_buffer_switch_on_eof )
 						YY_NEW_FILE;
 					}
 				break;
 				}
 
 			case EOB_ACT_CONTINUE_SCAN:
-				(yy_c_buf_p) =
-					(yytext_ptr) + yy_amount_of_matched_text;
+				yyg->yy_c_buf_p =
+					yyg->yytext_ptr + yy_amount_of_matched_text;
 
-				yy_current_state = yy_get_previous_state(  );
+				yy_current_state = yy_get_previous_state( yyscanner );
 
-				yy_cp = (yy_c_buf_p);
-				yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+				yy_cp = yyg->yy_c_buf_p;
+				yy_bp = yyg->yytext_ptr + YY_MORE_ADJ;
 				goto yy_match;
 
 			case EOB_ACT_LAST_MATCH:
-				(yy_c_buf_p) =
-				&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)];
+				yyg->yy_c_buf_p =
+				&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars];
 
-				yy_current_state = yy_get_previous_state(  );
+				yy_current_state = yy_get_previous_state( yyscanner );
 
-				yy_cp = (yy_c_buf_p);
-				yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+				yy_cp = yyg->yy_c_buf_p;
+				yy_bp = yyg->yytext_ptr + YY_MORE_ADJ;
 				goto yy_find_action;
 			}
 		break;
@@ -3589,7 +3930,7 @@ case YY_STATE_EOF(NOT_OPCODE):
 	} /* end of action switch */
 		} /* end of scanning one token */
 	} /* end of user's declarations */
-} /* end of ptx_lex */
+} /* end of yylex */
 
 /* yy_get_next_buffer - try to read in a new buffer
  *
@@ -3598,20 +3939,21 @@ case YY_STATE_EOF(NOT_OPCODE):
  *	EOB_ACT_CONTINUE_SCAN - continue scanning from current position
  *	EOB_ACT_END_OF_FILE - end of file
  */
-static int yy_get_next_buffer (void)
+static int yy_get_next_buffer (yyscan_t yyscanner)
 {
-    	char *dest = YY_CURRENT_BUFFER_LVALUE->yy_ch_buf;
-	char *source = (yytext_ptr);
-	yy_size_t number_to_move, i;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	char *dest = YY_CURRENT_BUFFER_LVALUE->yy_ch_buf;
+	char *source = yyg->yytext_ptr;
+	int number_to_move, i;
 	int ret_val;
 
-	if ( (yy_c_buf_p) > &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars) + 1] )
+	if ( yyg->yy_c_buf_p > &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars + 1] )
 		YY_FATAL_ERROR(
 		"fatal flex scanner internal error--end of buffer missed" );
 
 	if ( YY_CURRENT_BUFFER_LVALUE->yy_fill_buffer == 0 )
 		{ /* Don't try to fill the buffer, so this is an EOF. */
-		if ( (yy_c_buf_p) - (yytext_ptr) - YY_MORE_ADJ == 1 )
+		if ( yyg->yy_c_buf_p - yyg->yytext_ptr - YY_MORE_ADJ == 1 )
 			{
 			/* We matched a single character, the EOB, so
 			 * treat this as a final EOF.
@@ -3631,7 +3973,7 @@ static int yy_get_next_buffer (void)
 	/* Try to read more data. */
 
 	/* First move last chars to start of buffer. */
-	number_to_move = (yy_size_t) ((yy_c_buf_p) - (yytext_ptr)) - 1;
+	number_to_move = (int) (yyg->yy_c_buf_p - yyg->yytext_ptr - 1);
 
 	for ( i = 0; i < number_to_move; ++i )
 		*(dest++) = *(source++);
@@ -3640,11 +3982,11 @@ static int yy_get_next_buffer (void)
 		/* don't do the read, it's not guaranteed to return an EOF,
 		 * just force an EOF
 		 */
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars) = 0;
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars = 0;
 
 	else
 		{
-			yy_size_t num_to_read =
+			int num_to_read =
 			YY_CURRENT_BUFFER_LVALUE->yy_buf_size - number_to_move - 1;
 
 		while ( num_to_read <= 0 )
@@ -3654,11 +3996,11 @@ static int yy_get_next_buffer (void)
 			YY_BUFFER_STATE b = YY_CURRENT_BUFFER_LVALUE;
 
 			int yy_c_buf_p_offset =
-				(int) ((yy_c_buf_p) - b->yy_ch_buf);
+				(int) (yyg->yy_c_buf_p - b->yy_ch_buf);
 
 			if ( b->yy_is_our_buffer )
 				{
-				yy_size_t new_size = b->yy_buf_size * 2;
+				int new_size = b->yy_buf_size * 2;
 
 				if ( new_size <= 0 )
 					b->yy_buf_size += b->yy_buf_size / 8;
@@ -3667,17 +4009,18 @@ static int yy_get_next_buffer (void)
 
 				b->yy_ch_buf = (char *)
 					/* Include room in for 2 EOB chars. */
-					ptx_realloc((void *) b->yy_ch_buf,b->yy_buf_size + 2  );
+					yyrealloc( (void *) b->yy_ch_buf,
+							 (yy_size_t) (b->yy_buf_size + 2) , yyscanner );
 				}
 			else
 				/* Can't grow it, we don't own it. */
-				b->yy_ch_buf = 0;
+				b->yy_ch_buf = NULL;
 
 			if ( ! b->yy_ch_buf )
 				YY_FATAL_ERROR(
 				"fatal error - scanner input buffer overflow" );
 
-			(yy_c_buf_p) = &b->yy_ch_buf[yy_c_buf_p_offset];
+			yyg->yy_c_buf_p = &b->yy_ch_buf[yy_c_buf_p_offset];
 
 			num_to_read = YY_CURRENT_BUFFER_LVALUE->yy_buf_size -
 						number_to_move - 1;
@@ -3689,17 +4032,17 @@ static int yy_get_next_buffer (void)
 
 		/* Read in more data. */
 		YY_INPUT( (&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[number_to_move]),
-			(yy_n_chars), num_to_read );
+			yyg->yy_n_chars, num_to_read );
 
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars;
 		}
 
-	if ( (yy_n_chars) == 0 )
+	if ( yyg->yy_n_chars == 0 )
 		{
 		if ( number_to_move == YY_MORE_ADJ )
 			{
 			ret_val = EOB_ACT_END_OF_FILE;
-			ptx_restart(ptx_in  );
+			yyrestart( yyin  , yyscanner);
 			}
 
 		else
@@ -3713,47 +4056,51 @@ static int yy_get_next_buffer (void)
 	else
 		ret_val = EOB_ACT_CONTINUE_SCAN;
 
-	if ((int) ((yy_n_chars) + number_to_move) > YY_CURRENT_BUFFER_LVALUE->yy_buf_size) {
+	if ((yyg->yy_n_chars + number_to_move) > YY_CURRENT_BUFFER_LVALUE->yy_buf_size) {
 		/* Extend the array by 50%, plus the number we really need. */
-		int new_size = (yy_n_chars) + number_to_move + ((yy_n_chars) >> 1);
-		YY_CURRENT_BUFFER_LVALUE->yy_ch_buf = (char *) ptx_realloc((void *) YY_CURRENT_BUFFER_LVALUE->yy_ch_buf,new_size  );
+		int new_size = yyg->yy_n_chars + number_to_move + (yyg->yy_n_chars >> 1);
+		YY_CURRENT_BUFFER_LVALUE->yy_ch_buf = (char *) yyrealloc(
+			(void *) YY_CURRENT_BUFFER_LVALUE->yy_ch_buf, (yy_size_t) new_size , yyscanner );
 		if ( ! YY_CURRENT_BUFFER_LVALUE->yy_ch_buf )
 			YY_FATAL_ERROR( "out of dynamic memory in yy_get_next_buffer()" );
+		/* "- 2" to take care of EOB's */
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_size = (int) (new_size - 2);
 	}
 
-	(yy_n_chars) += number_to_move;
-	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] = YY_END_OF_BUFFER_CHAR;
-	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars) + 1] = YY_END_OF_BUFFER_CHAR;
+	yyg->yy_n_chars += number_to_move;
+	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars] = YY_END_OF_BUFFER_CHAR;
+	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars + 1] = YY_END_OF_BUFFER_CHAR;
 
-	(yytext_ptr) = &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[0];
+	yyg->yytext_ptr = &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[0];
 
 	return ret_val;
 }
 
 /* yy_get_previous_state - get the state just before the EOB char was reached */
 
-    static yy_state_type yy_get_previous_state (void)
+    static yy_state_type yy_get_previous_state (yyscan_t yyscanner)
 {
 	yy_state_type yy_current_state;
 	char *yy_cp;
-    
-	yy_current_state = (yy_start);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
 
-	for ( yy_cp = (yytext_ptr) + YY_MORE_ADJ; yy_cp < (yy_c_buf_p); ++yy_cp )
+	yy_current_state = yyg->yy_start;
+
+	for ( yy_cp = yyg->yytext_ptr + YY_MORE_ADJ; yy_cp < yyg->yy_c_buf_p; ++yy_cp )
 		{
 		YY_CHAR yy_c = (*yy_cp ? yy_ec[YY_SC_TO_UI(*yy_cp)] : 1);
 		if ( yy_accept[yy_current_state] )
 			{
-			(yy_last_accepting_state) = yy_current_state;
-			(yy_last_accepting_cpos) = yy_cp;
+			yyg->yy_last_accepting_state = yy_current_state;
+			yyg->yy_last_accepting_cpos = yy_cp;
 			}
 		while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
 			{
 			yy_current_state = (int) yy_def[yy_current_state];
-			if ( yy_current_state >= 1471 )
-				yy_c = yy_meta[(unsigned int) yy_c];
+			if ( yy_current_state >= 1603 )
+				yy_c = yy_meta[yy_c];
 			}
-		yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+		yy_current_state = yy_nxt[yy_base[yy_current_state] + yy_c];
 		}
 
 	return yy_current_state;
@@ -3764,27 +4111,29 @@ static int yy_get_next_buffer (void)
  * synopsis
  *	next_state = yy_try_NUL_trans( current_state );
  */
-    static yy_state_type yy_try_NUL_trans  (yy_state_type yy_current_state )
+    static yy_state_type yy_try_NUL_trans  (yy_state_type yy_current_state , yyscan_t yyscanner)
 {
 	int yy_is_jam;
-    	char *yy_cp = (yy_c_buf_p);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner; /* This var may be unused depending upon options. */
+	char *yy_cp = yyg->yy_c_buf_p;
 
 	YY_CHAR yy_c = 1;
 	if ( yy_accept[yy_current_state] )
 		{
-		(yy_last_accepting_state) = yy_current_state;
-		(yy_last_accepting_cpos) = yy_cp;
+		yyg->yy_last_accepting_state = yy_current_state;
+		yyg->yy_last_accepting_cpos = yy_cp;
 		}
 	while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
 		{
 		yy_current_state = (int) yy_def[yy_current_state];
-		if ( yy_current_state >= 1471 )
-			yy_c = yy_meta[(unsigned int) yy_c];
+		if ( yy_current_state >= 1603 )
+			yy_c = yy_meta[yy_c];
 		}
-	yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
-	yy_is_jam = (yy_current_state == 1470);
+	yy_current_state = yy_nxt[yy_base[yy_current_state] + yy_c];
+	yy_is_jam = (yy_current_state == 1602);
 
-		return yy_is_jam ? 0 : yy_current_state;
+	(void)yyg;
+	return yy_is_jam ? 0 : yy_current_state;
 }
 
 #ifndef YY_NO_UNPUT
@@ -3793,32 +4142,33 @@ static int yy_get_next_buffer (void)
 
 #ifndef YY_NO_INPUT
 #ifdef __cplusplus
-    static int yyinput (void)
+    static int yyinput (yyscan_t yyscanner)
 #else
-    static int input  (void)
+    static int input  (yyscan_t yyscanner)
 #endif
 
 {
 	int c;
-    
-	*(yy_c_buf_p) = (yy_hold_char);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
 
-	if ( *(yy_c_buf_p) == YY_END_OF_BUFFER_CHAR )
+	*yyg->yy_c_buf_p = yyg->yy_hold_char;
+
+	if ( *yyg->yy_c_buf_p == YY_END_OF_BUFFER_CHAR )
 		{
 		/* yy_c_buf_p now points to the character we want to return.
 		 * If this occurs *before* the EOB characters, then it's a
 		 * valid NUL; if not, then we've hit the end of the buffer.
 		 */
-		if ( (yy_c_buf_p) < &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] )
+		if ( yyg->yy_c_buf_p < &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars] )
 			/* This was really a NUL. */
-			*(yy_c_buf_p) = '\0';
+			*yyg->yy_c_buf_p = '\0';
 
 		else
 			{ /* need more input */
-			yy_size_t offset = (yy_c_buf_p) - (yytext_ptr);
-			++(yy_c_buf_p);
+			int offset = (int) (yyg->yy_c_buf_p - yyg->yytext_ptr);
+			++yyg->yy_c_buf_p;
 
-			switch ( yy_get_next_buffer(  ) )
+			switch ( yy_get_next_buffer( yyscanner ) )
 				{
 				case EOB_ACT_LAST_MATCH:
 					/* This happens because yy_g_n_b()
@@ -3832,38 +4182,40 @@ static int yy_get_next_buffer (void)
 					 */
 
 					/* Reset buffer status. */
-					ptx_restart(ptx_in );
+					yyrestart( yyin , yyscanner);
 
 					/*FALLTHROUGH*/
 
 				case EOB_ACT_END_OF_FILE:
 					{
-					if ( ptx_wrap( ) )
-						return EOF;
+					if ( yywrap( yyscanner ) )
+						return 0;
 
-					if ( ! (yy_did_buffer_switch_on_eof) )
+					if ( ! yyg->yy_did_buffer_switch_on_eof )
 						YY_NEW_FILE;
 #ifdef __cplusplus
-					return yyinput();
+					return yyinput(yyscanner);
 #else
-					return input();
+					return input(yyscanner);
 #endif
 					}
 
 				case EOB_ACT_CONTINUE_SCAN:
-					(yy_c_buf_p) = (yytext_ptr) + offset;
+					yyg->yy_c_buf_p = yyg->yytext_ptr + offset;
 					break;
 				}
 			}
 		}
 
-	c = *(unsigned char *) (yy_c_buf_p);	/* cast for 8-bit char's */
-	*(yy_c_buf_p) = '\0';	/* preserve ptx_text */
-	(yy_hold_char) = *++(yy_c_buf_p);
+	c = *(unsigned char *) yyg->yy_c_buf_p;	/* cast for 8-bit char's */
+	*yyg->yy_c_buf_p = '\0';	/* preserve yytext */
+	yyg->yy_hold_char = *++yyg->yy_c_buf_p;
 
 	if ( c == '\n' )
-		   
-    ptx_lineno++;
+		
+    do{ yylineno++;
+        yycolumn=0;
+    }while(0)
 ;
 
 	return c;
@@ -3872,102 +4224,106 @@ static int yy_get_next_buffer (void)
 
 /** Immediately switch to a different input stream.
  * @param input_file A readable stream.
- * 
+ * @param yyscanner The scanner object.
  * @note This function does not reset the start condition to @c INITIAL .
  */
-    void ptx_restart  (FILE * input_file )
+    void yyrestart  (FILE * input_file , yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
 	if ( ! YY_CURRENT_BUFFER ){
-        ptx_ensure_buffer_stack ();
+        yyensure_buffer_stack (yyscanner);
 		YY_CURRENT_BUFFER_LVALUE =
-            ptx__create_buffer(ptx_in,YY_BUF_SIZE );
+            yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner);
 	}
 
-	ptx__init_buffer(YY_CURRENT_BUFFER,input_file );
-	ptx__load_buffer_state( );
+	yy_init_buffer( YY_CURRENT_BUFFER, input_file , yyscanner);
+	yy_load_buffer_state( yyscanner );
 }
 
 /** Switch to a different input buffer.
  * @param new_buffer The new input buffer.
- * 
+ * @param yyscanner The scanner object.
  */
-    void ptx__switch_to_buffer  (YY_BUFFER_STATE  new_buffer )
+    void yy_switch_to_buffer  (YY_BUFFER_STATE  new_buffer , yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
 	/* TODO. We should be able to replace this entire function body
 	 * with
-	 *		ptx_pop_buffer_state();
-	 *		ptx_push_buffer_state(new_buffer);
+	 *		yypop_buffer_state();
+	 *		yypush_buffer_state(new_buffer);
      */
-	ptx_ensure_buffer_stack ();
+	yyensure_buffer_stack (yyscanner);
 	if ( YY_CURRENT_BUFFER == new_buffer )
 		return;
 
 	if ( YY_CURRENT_BUFFER )
 		{
 		/* Flush out information for old buffer. */
-		*(yy_c_buf_p) = (yy_hold_char);
-		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = (yy_c_buf_p);
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		*yyg->yy_c_buf_p = yyg->yy_hold_char;
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = yyg->yy_c_buf_p;
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars;
 		}
 
 	YY_CURRENT_BUFFER_LVALUE = new_buffer;
-	ptx__load_buffer_state( );
+	yy_load_buffer_state( yyscanner );
 
 	/* We don't actually know whether we did this switch during
-	 * EOF (ptx_wrap()) processing, but the only time this flag
-	 * is looked at is after ptx_wrap() is called, so it's safe
+	 * EOF (yywrap()) processing, but the only time this flag
+	 * is looked at is after yywrap() is called, so it's safe
 	 * to go ahead and always set it.
 	 */
-	(yy_did_buffer_switch_on_eof) = 1;
+	yyg->yy_did_buffer_switch_on_eof = 1;
 }
 
-static void ptx__load_buffer_state  (void)
+static void yy_load_buffer_state  (yyscan_t yyscanner)
 {
-    	(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
-	(yytext_ptr) = (yy_c_buf_p) = YY_CURRENT_BUFFER_LVALUE->yy_buf_pos;
-	ptx_in = YY_CURRENT_BUFFER_LVALUE->yy_input_file;
-	(yy_hold_char) = *(yy_c_buf_p);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	yyg->yy_n_chars = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
+	yyg->yytext_ptr = yyg->yy_c_buf_p = YY_CURRENT_BUFFER_LVALUE->yy_buf_pos;
+	yyin = YY_CURRENT_BUFFER_LVALUE->yy_input_file;
+	yyg->yy_hold_char = *yyg->yy_c_buf_p;
 }
 
 /** Allocate and initialize an input buffer state.
  * @param file A readable stream.
  * @param size The character buffer size in bytes. When in doubt, use @c YY_BUF_SIZE.
- * 
+ * @param yyscanner The scanner object.
  * @return the allocated buffer state.
  */
-    YY_BUFFER_STATE ptx__create_buffer  (FILE * file, int  size )
+    YY_BUFFER_STATE yy_create_buffer  (FILE * file, int  size , yyscan_t yyscanner)
 {
 	YY_BUFFER_STATE b;
     
-	b = (YY_BUFFER_STATE) ptx_alloc(sizeof( struct yy_buffer_state )  );
+	b = (YY_BUFFER_STATE) yyalloc( sizeof( struct yy_buffer_state ) , yyscanner );
 	if ( ! b )
-		YY_FATAL_ERROR( "out of dynamic memory in ptx__create_buffer()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
 
-	b->yy_buf_size = (yy_size_t)size;
+	b->yy_buf_size = size;
 
 	/* yy_ch_buf has to be 2 characters longer than the size given because
 	 * we need to put in 2 end-of-buffer characters.
 	 */
-	b->yy_ch_buf = (char *) ptx_alloc(b->yy_buf_size + 2  );
+	b->yy_ch_buf = (char *) yyalloc( (yy_size_t) (b->yy_buf_size + 2) , yyscanner );
 	if ( ! b->yy_ch_buf )
-		YY_FATAL_ERROR( "out of dynamic memory in ptx__create_buffer()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
 
 	b->yy_is_our_buffer = 1;
 
-	ptx__init_buffer(b,file );
+	yy_init_buffer( b, file , yyscanner);
 
 	return b;
 }
 
 /** Destroy the buffer.
- * @param b a buffer created with ptx__create_buffer()
- * 
+ * @param b a buffer created with yy_create_buffer()
+ * @param yyscanner The scanner object.
  */
-    void ptx__delete_buffer (YY_BUFFER_STATE  b )
+    void yy_delete_buffer (YY_BUFFER_STATE  b , yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
 	if ( ! b )
 		return;
 
@@ -3975,27 +4331,28 @@ static void ptx__load_buffer_state  (void)
 		YY_CURRENT_BUFFER_LVALUE = (YY_BUFFER_STATE) 0;
 
 	if ( b->yy_is_our_buffer )
-		ptx_free((void *) b->yy_ch_buf  );
+		yyfree( (void *) b->yy_ch_buf , yyscanner );
 
-	ptx_free((void *) b  );
+	yyfree( (void *) b , yyscanner );
 }
 
 /* Initializes or reinitializes a buffer.
  * This function is sometimes called more than once on the same buffer,
- * such as during a ptx_restart() or at EOF.
+ * such as during a yyrestart() or at EOF.
  */
-    static void ptx__init_buffer  (YY_BUFFER_STATE  b, FILE * file )
+    static void yy_init_buffer  (YY_BUFFER_STATE  b, FILE * file , yyscan_t yyscanner)
 
 {
 	int oerrno = errno;
-    
-	ptx__flush_buffer(b );
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+	yy_flush_buffer( b , yyscanner);
 
 	b->yy_input_file = file;
 	b->yy_fill_buffer = 1;
 
-    /* If b is the current buffer, then ptx__init_buffer was _probably_
-     * called from ptx_restart() or through yy_get_next_buffer.
+    /* If b is the current buffer, then yy_init_buffer was _probably_
+     * called from yyrestart() or through yy_get_next_buffer.
      * In that case, we don't want to reset the lineno or column.
      */
     if (b != YY_CURRENT_BUFFER){
@@ -4010,11 +4367,12 @@ static void ptx__load_buffer_state  (void)
 
 /** Discard all buffered characters. On the next scan, YY_INPUT will be called.
  * @param b the buffer state to be flushed, usually @c YY_CURRENT_BUFFER.
- * 
+ * @param yyscanner The scanner object.
  */
-    void ptx__flush_buffer (YY_BUFFER_STATE  b )
+    void yy_flush_buffer (YY_BUFFER_STATE  b , yyscan_t yyscanner)
 {
-    	if ( ! b )
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	if ( ! b )
 		return;
 
 	b->yy_n_chars = 0;
@@ -4032,114 +4390,117 @@ static void ptx__load_buffer_state  (void)
 	b->yy_buffer_status = YY_BUFFER_NEW;
 
 	if ( b == YY_CURRENT_BUFFER )
-		ptx__load_buffer_state( );
+		yy_load_buffer_state( yyscanner );
 }
 
 /** Pushes the new state onto the stack. The new state becomes
  *  the current state. This function will allocate the stack
  *  if necessary.
  *  @param new_buffer The new state.
- *  
+ *  @param yyscanner The scanner object.
  */
-void ptx_push_buffer_state (YY_BUFFER_STATE new_buffer )
+void yypush_buffer_state (YY_BUFFER_STATE new_buffer , yyscan_t yyscanner)
 {
-    	if (new_buffer == NULL)
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	if (new_buffer == NULL)
 		return;
 
-	ptx_ensure_buffer_stack();
+	yyensure_buffer_stack(yyscanner);
 
-	/* This block is copied from ptx__switch_to_buffer. */
+	/* This block is copied from yy_switch_to_buffer. */
 	if ( YY_CURRENT_BUFFER )
 		{
 		/* Flush out information for old buffer. */
-		*(yy_c_buf_p) = (yy_hold_char);
-		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = (yy_c_buf_p);
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		*yyg->yy_c_buf_p = yyg->yy_hold_char;
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = yyg->yy_c_buf_p;
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars;
 		}
 
 	/* Only push if top exists. Otherwise, replace top. */
 	if (YY_CURRENT_BUFFER)
-		(yy_buffer_stack_top)++;
+		yyg->yy_buffer_stack_top++;
 	YY_CURRENT_BUFFER_LVALUE = new_buffer;
 
-	/* copied from ptx__switch_to_buffer. */
-	ptx__load_buffer_state( );
-	(yy_did_buffer_switch_on_eof) = 1;
+	/* copied from yy_switch_to_buffer. */
+	yy_load_buffer_state( yyscanner );
+	yyg->yy_did_buffer_switch_on_eof = 1;
 }
 
 /** Removes and deletes the top of the stack, if present.
  *  The next element becomes the new top.
- *  
+ *  @param yyscanner The scanner object.
  */
-void ptx_pop_buffer_state (void)
+void yypop_buffer_state (yyscan_t yyscanner)
 {
-    	if (!YY_CURRENT_BUFFER)
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	if (!YY_CURRENT_BUFFER)
 		return;
 
-	ptx__delete_buffer(YY_CURRENT_BUFFER );
+	yy_delete_buffer(YY_CURRENT_BUFFER , yyscanner);
 	YY_CURRENT_BUFFER_LVALUE = NULL;
-	if ((yy_buffer_stack_top) > 0)
-		--(yy_buffer_stack_top);
+	if (yyg->yy_buffer_stack_top > 0)
+		--yyg->yy_buffer_stack_top;
 
 	if (YY_CURRENT_BUFFER) {
-		ptx__load_buffer_state( );
-		(yy_did_buffer_switch_on_eof) = 1;
+		yy_load_buffer_state( yyscanner );
+		yyg->yy_did_buffer_switch_on_eof = 1;
 	}
 }
 
 /* Allocates the stack if it does not exist.
  *  Guarantees space for at least one push.
  */
-static void ptx_ensure_buffer_stack (void)
+static void yyensure_buffer_stack (yyscan_t yyscanner)
 {
 	yy_size_t num_to_alloc;
-    
-	if (!(yy_buffer_stack)) {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+	if (!yyg->yy_buffer_stack) {
 
 		/* First allocation is just for 2 elements, since we don't know if this
 		 * scanner will even need a stack. We use 2 instead of 1 to avoid an
 		 * immediate realloc on the next call.
          */
-		num_to_alloc = 1; /* After all that talk, this was set to 1 anyways... */
-		(yy_buffer_stack) = (struct yy_buffer_state**)ptx_alloc
+      num_to_alloc = 1; /* After all that talk, this was set to 1 anyways... */
+		yyg->yy_buffer_stack = (struct yy_buffer_state**)yyalloc
 								(num_to_alloc * sizeof(struct yy_buffer_state*)
-								);
-		if ( ! (yy_buffer_stack) )
-			YY_FATAL_ERROR( "out of dynamic memory in ptx_ensure_buffer_stack()" );
-								  
-		memset((yy_buffer_stack), 0, num_to_alloc * sizeof(struct yy_buffer_state*));
-				
-		(yy_buffer_stack_max) = num_to_alloc;
-		(yy_buffer_stack_top) = 0;
+								, yyscanner);
+		if ( ! yyg->yy_buffer_stack )
+			YY_FATAL_ERROR( "out of dynamic memory in yyensure_buffer_stack()" );
+
+		memset(yyg->yy_buffer_stack, 0, num_to_alloc * sizeof(struct yy_buffer_state*));
+
+		yyg->yy_buffer_stack_max = num_to_alloc;
+		yyg->yy_buffer_stack_top = 0;
 		return;
 	}
 
-	if ((yy_buffer_stack_top) >= ((yy_buffer_stack_max)) - 1){
+	if (yyg->yy_buffer_stack_top >= (yyg->yy_buffer_stack_max) - 1){
 
 		/* Increase the buffer to prepare for a possible push. */
 		yy_size_t grow_size = 8 /* arbitrary grow size */;
 
-		num_to_alloc = (yy_buffer_stack_max) + grow_size;
-		(yy_buffer_stack) = (struct yy_buffer_state**)ptx_realloc
-								((yy_buffer_stack),
+		num_to_alloc = yyg->yy_buffer_stack_max + grow_size;
+		yyg->yy_buffer_stack = (struct yy_buffer_state**)yyrealloc
+								(yyg->yy_buffer_stack,
 								num_to_alloc * sizeof(struct yy_buffer_state*)
-								);
-		if ( ! (yy_buffer_stack) )
-			YY_FATAL_ERROR( "out of dynamic memory in ptx_ensure_buffer_stack()" );
+								, yyscanner);
+		if ( ! yyg->yy_buffer_stack )
+			YY_FATAL_ERROR( "out of dynamic memory in yyensure_buffer_stack()" );
 
 		/* zero only the new slots.*/
-		memset((yy_buffer_stack) + (yy_buffer_stack_max), 0, grow_size * sizeof(struct yy_buffer_state*));
-		(yy_buffer_stack_max) = num_to_alloc;
+		memset(yyg->yy_buffer_stack + yyg->yy_buffer_stack_max, 0, grow_size * sizeof(struct yy_buffer_state*));
+		yyg->yy_buffer_stack_max = num_to_alloc;
 	}
 }
 
 /** Setup the input buffer state to scan directly from a user-specified character buffer.
  * @param base the character buffer
  * @param size the size in bytes of the character buffer
- * 
- * @return the newly allocated buffer state object. 
+ * @param yyscanner The scanner object.
+ * @return the newly allocated buffer state object.
  */
-YY_BUFFER_STATE ptx__scan_buffer  (char * base, yy_size_t  size )
+YY_BUFFER_STATE yy_scan_buffer  (char * base, yy_size_t  size , yyscan_t yyscanner)
 {
 	YY_BUFFER_STATE b;
     
@@ -4147,69 +4508,69 @@ YY_BUFFER_STATE ptx__scan_buffer  (char * base, yy_size_t  size )
 	     base[size-2] != YY_END_OF_BUFFER_CHAR ||
 	     base[size-1] != YY_END_OF_BUFFER_CHAR )
 		/* They forgot to leave room for the EOB's. */
-		return 0;
+		return NULL;
 
-	b = (YY_BUFFER_STATE) ptx_alloc(sizeof( struct yy_buffer_state )  );
+	b = (YY_BUFFER_STATE) yyalloc( sizeof( struct yy_buffer_state ) , yyscanner );
 	if ( ! b )
-		YY_FATAL_ERROR( "out of dynamic memory in ptx__scan_buffer()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_buffer()" );
 
-	b->yy_buf_size = size - 2;	/* "- 2" to take care of EOB's */
+	b->yy_buf_size = (int) (size - 2);	/* "- 2" to take care of EOB's */
 	b->yy_buf_pos = b->yy_ch_buf = base;
 	b->yy_is_our_buffer = 0;
-	b->yy_input_file = 0;
+	b->yy_input_file = NULL;
 	b->yy_n_chars = b->yy_buf_size;
 	b->yy_is_interactive = 0;
 	b->yy_at_bol = 1;
 	b->yy_fill_buffer = 0;
 	b->yy_buffer_status = YY_BUFFER_NEW;
 
-	ptx__switch_to_buffer(b  );
+	yy_switch_to_buffer( b , yyscanner );
 
 	return b;
 }
 
-/** Setup the input buffer state to scan a string. The next call to ptx_lex() will
+/** Setup the input buffer state to scan a string. The next call to yylex() will
  * scan from a @e copy of @a str.
  * @param yystr a NUL-terminated string to scan
- * 
+ * @param yyscanner The scanner object.
  * @return the newly allocated buffer state object.
  * @note If you want to scan bytes that may contain NUL values, then use
- *       ptx__scan_bytes() instead.
+ *       yy_scan_bytes() instead.
  */
-YY_BUFFER_STATE ptx__scan_string (yyconst char * yystr )
+YY_BUFFER_STATE yy_scan_string (const char * yystr , yyscan_t yyscanner)
 {
     
-	return ptx__scan_bytes(yystr,strlen(yystr) );
+	return yy_scan_bytes( yystr, (int) strlen(yystr) , yyscanner);
 }
 
-/** Setup the input buffer state to scan the given bytes. The next call to ptx_lex() will
+/** Setup the input buffer state to scan the given bytes. The next call to yylex() will
  * scan from a @e copy of @a bytes.
  * @param yybytes the byte buffer to scan
  * @param _yybytes_len the number of bytes in the buffer pointed to by @a bytes.
- * 
+ * @param yyscanner The scanner object.
  * @return the newly allocated buffer state object.
  */
-YY_BUFFER_STATE ptx__scan_bytes  (yyconst char * yybytes, yy_size_t  _yybytes_len )
+YY_BUFFER_STATE yy_scan_bytes  (const char * yybytes, int  _yybytes_len , yyscan_t yyscanner)
 {
 	YY_BUFFER_STATE b;
 	char *buf;
 	yy_size_t n;
-	yy_size_t i;
+	int i;
     
 	/* Get memory for full buffer, including space for trailing EOB's. */
-	n = _yybytes_len + 2;
-	buf = (char *) ptx_alloc(n  );
+	n = (yy_size_t) (_yybytes_len + 2);
+	buf = (char *) yyalloc( n , yyscanner );
 	if ( ! buf )
-		YY_FATAL_ERROR( "out of dynamic memory in ptx__scan_bytes()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_bytes()" );
 
 	for ( i = 0; i < _yybytes_len; ++i )
 		buf[i] = yybytes[i];
 
 	buf[_yybytes_len] = buf[_yybytes_len+1] = YY_END_OF_BUFFER_CHAR;
 
-	b = ptx__scan_buffer(buf,n );
+	b = yy_scan_buffer( buf, n , yyscanner);
 	if ( ! b )
-		YY_FATAL_ERROR( "bad buffer in ptx__scan_bytes()" );
+		YY_FATAL_ERROR( "bad buffer in yy_scan_bytes()" );
 
 	/* It's okay to grow etc. this buffer, and we should throw it
 	 * away when we're done.
@@ -4223,9 +4584,11 @@ YY_BUFFER_STATE ptx__scan_bytes  (yyconst char * yybytes, yy_size_t  _yybytes_le
 #define YY_EXIT_FAILURE 2
 #endif
 
-static void yy_fatal_error (yyconst char* msg )
+static void yynoreturn yy_fatal_error (const char* msg , yyscan_t yyscanner)
 {
-			(void) fprintf( stderr, "%s\n", msg );
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+	fprintf( stderr, "%s\n", msg );
 	exit( YY_EXIT_FAILURE );
 }
 
@@ -4235,147 +4598,295 @@ static void yy_fatal_error (yyconst char* msg )
 #define yyless(n) \
 	do \
 		{ \
-		/* Undo effects of setting up ptx_text. */ \
+		/* Undo effects of setting up yytext. */ \
         int yyless_macro_arg = (n); \
         YY_LESS_LINENO(yyless_macro_arg);\
-		ptx_text[ptx_leng] = (yy_hold_char); \
-		(yy_c_buf_p) = ptx_text + yyless_macro_arg; \
-		(yy_hold_char) = *(yy_c_buf_p); \
-		*(yy_c_buf_p) = '\0'; \
-		ptx_leng = yyless_macro_arg; \
+		yytext[yyleng] = yyg->yy_hold_char; \
+		yyg->yy_c_buf_p = yytext + yyless_macro_arg; \
+		yyg->yy_hold_char = *yyg->yy_c_buf_p; \
+		*yyg->yy_c_buf_p = '\0'; \
+		yyleng = yyless_macro_arg; \
 		} \
 	while ( 0 )
 
 /* Accessor  methods (get/set functions) to struct members. */
 
+/** Get the user-defined data for this scanner.
+ * @param yyscanner The scanner object.
+ */
+YY_EXTRA_TYPE yyget_extra  (yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyextra;
+}
+
 /** Get the current line number.
- * 
+ * @param yyscanner The scanner object.
+ */
+int yyget_lineno  (yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        if (! YY_CURRENT_BUFFER)
+            return 0;
+    
+    return yylineno;
+}
+
+/** Get the current column number.
+ * @param yyscanner The scanner object.
  */
-int ptx_get_lineno  (void)
+int yyget_column  (yyscan_t yyscanner)
 {
-        
-    return ptx_lineno;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        if (! YY_CURRENT_BUFFER)
+            return 0;
+    
+    return yycolumn;
 }
 
 /** Get the input stream.
- * 
+ * @param yyscanner The scanner object.
  */
-FILE *ptx_get_in  (void)
+FILE *yyget_in  (yyscan_t yyscanner)
 {
-        return ptx_in;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyin;
 }
 
 /** Get the output stream.
- * 
+ * @param yyscanner The scanner object.
  */
-FILE *ptx_get_out  (void)
+FILE *yyget_out  (yyscan_t yyscanner)
 {
-        return ptx_out;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyout;
 }
 
 /** Get the length of the current token.
- * 
+ * @param yyscanner The scanner object.
  */
-yy_size_t ptx_get_leng  (void)
+int yyget_leng  (yyscan_t yyscanner)
 {
-        return ptx_leng;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyleng;
 }
 
 /** Get the current token.
- * 
+ * @param yyscanner The scanner object.
  */
 
-char *ptx_get_text  (void)
+char *yyget_text  (yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yytext;
+}
+
+/** Set the user-defined data. This data is never touched by the scanner.
+ * @param user_defined The data to be associated with this scanner.
+ * @param yyscanner The scanner object.
+ */
+void yyset_extra (YY_EXTRA_TYPE  user_defined , yyscan_t yyscanner)
 {
-        return ptx_text;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yyextra = user_defined ;
 }
 
 /** Set the current line number.
  * @param _line_number line number
- * 
+ * @param yyscanner The scanner object.
+ */
+void yyset_lineno (int  _line_number , yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        /* lineno is only valid if an input buffer exists. */
+        if (! YY_CURRENT_BUFFER )
+           YY_FATAL_ERROR( "yyset_lineno called with no buffer" );
+    
+    yylineno = _line_number;
+}
+
+/** Set the current column.
+ * @param _column_no column number
+ * @param yyscanner The scanner object.
  */
-void ptx_set_lineno (int  _line_number )
+void yyset_column (int  _column_no , yyscan_t yyscanner)
 {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        /* column is only valid if an input buffer exists. */
+        if (! YY_CURRENT_BUFFER )
+           YY_FATAL_ERROR( "yyset_column called with no buffer" );
     
-    ptx_lineno = _line_number;
+    yycolumn = _column_no;
 }
 
 /** Set the input stream. This does not discard the current
  * input buffer.
  * @param _in_str A readable stream.
- * 
- * @see ptx__switch_to_buffer
+ * @param yyscanner The scanner object.
+ * @see yy_switch_to_buffer
  */
-void ptx_set_in (FILE *  _in_str )
+void yyset_in (FILE *  _in_str , yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yyin = _in_str ;
+}
+
+void yyset_out (FILE *  _out_str , yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yyout = _out_str ;
+}
+
+int yyget_debug  (yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yy_flex_debug;
+}
+
+void yyset_debug (int  _bdebug , yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yy_flex_debug = _bdebug ;
+}
+
+/* Accessor methods for yylval and yylloc */
+
+YYSTYPE * yyget_lval  (yyscan_t yyscanner)
 {
-        ptx_in = _in_str ;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yylval;
 }
 
-void ptx_set_out (FILE *  _out_str )
+void yyset_lval (YYSTYPE *  yylval_param , yyscan_t yyscanner)
 {
-        ptx_out = _out_str ;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yylval = yylval_param;
 }
 
-int ptx_get_debug  (void)
+/* User-visible API */
+
+/* yylex_init is special because it creates the scanner itself, so it is
+ * the ONLY reentrant function that doesn't take the scanner as the last argument.
+ * That's why we explicitly handle the declaration, instead of using our macros.
+ */
+int yylex_init(yyscan_t* ptr_yy_globals)
 {
-        return ptx__flex_debug;
+    if (ptr_yy_globals == NULL){
+        errno = EINVAL;
+        return 1;
+    }
+
+    *ptr_yy_globals = (yyscan_t) yyalloc ( sizeof( struct yyguts_t ), NULL );
+
+    if (*ptr_yy_globals == NULL){
+        errno = ENOMEM;
+        return 1;
+    }
+
+    /* By setting to 0xAA, we expose bugs in yy_init_globals. Leave at 0x00 for releases. */
+    memset(*ptr_yy_globals,0x00,sizeof(struct yyguts_t));
+
+    return yy_init_globals ( *ptr_yy_globals );
 }
 
-void ptx_set_debug (int  _bdebug )
+/* yylex_init_extra has the same functionality as yylex_init, but follows the
+ * convention of taking the scanner as the last argument. Note however, that
+ * this is a *pointer* to a scanner, as it will be allocated by this call (and
+ * is the reason, too, why this function also must handle its own declaration).
+ * The user defined value in the first argument will be available to yyalloc in
+ * the yyextra field.
+ */
+int yylex_init_extra( YY_EXTRA_TYPE yy_user_defined, yyscan_t* ptr_yy_globals )
 {
-        ptx__flex_debug = _bdebug ;
+    struct yyguts_t dummy_yyguts;
+
+    yyset_extra (yy_user_defined, &dummy_yyguts);
+
+    if (ptr_yy_globals == NULL){
+        errno = EINVAL;
+        return 1;
+    }
+
+    *ptr_yy_globals = (yyscan_t) yyalloc ( sizeof( struct yyguts_t ), &dummy_yyguts );
+
+    if (*ptr_yy_globals == NULL){
+        errno = ENOMEM;
+        return 1;
+    }
+
+    /* By setting to 0xAA, we expose bugs in
+    yy_init_globals. Leave at 0x00 for releases. */
+    memset(*ptr_yy_globals,0x00,sizeof(struct yyguts_t));
+
+    yyset_extra (yy_user_defined, *ptr_yy_globals);
+
+    return yy_init_globals ( *ptr_yy_globals );
 }
 
-static int yy_init_globals (void)
+static int yy_init_globals (yyscan_t yyscanner)
 {
-        /* Initialization is the same as for the non-reentrant scanner.
-     * This function is called from ptx_lex_destroy(), so don't allocate here.
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    /* Initialization is the same as for the non-reentrant scanner.
+     * This function is called from yylex_destroy(), so don't allocate here.
      */
 
-    /* We do not touch ptx_lineno unless the option is enabled. */
-    ptx_lineno =  1;
-    
-    (yy_buffer_stack) = 0;
-    (yy_buffer_stack_top) = 0;
-    (yy_buffer_stack_max) = 0;
-    (yy_c_buf_p) = (char *) 0;
-    (yy_init) = 0;
-    (yy_start) = 0;
+    yyg->yy_buffer_stack = NULL;
+    yyg->yy_buffer_stack_top = 0;
+    yyg->yy_buffer_stack_max = 0;
+    yyg->yy_c_buf_p = NULL;
+    yyg->yy_init = 0;
+    yyg->yy_start = 0;
+
+    yyg->yy_start_stack_ptr = 0;
+    yyg->yy_start_stack_depth = 0;
+    yyg->yy_start_stack =  NULL;
 
 /* Defined in main.c */
 #ifdef YY_STDINIT
-    ptx_in = stdin;
-    ptx_out = stdout;
+    yyin = stdin;
+    yyout = stdout;
 #else
-    ptx_in = (FILE *) 0;
-    ptx_out = (FILE *) 0;
+    yyin = NULL;
+    yyout = NULL;
 #endif
 
     /* For future reference: Set errno on error, since we are called by
-     * ptx_lex_init()
+     * yylex_init()
      */
     return 0;
 }
 
-/* ptx_lex_destroy is for both reentrant and non-reentrant scanners. */
-int ptx_lex_destroy  (void)
+/* yylex_destroy is for both reentrant and non-reentrant scanners. */
+int yylex_destroy  (yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
     /* Pop the buffer stack, destroying each element. */
 	while(YY_CURRENT_BUFFER){
-		ptx__delete_buffer(YY_CURRENT_BUFFER  );
+		yy_delete_buffer( YY_CURRENT_BUFFER , yyscanner );
 		YY_CURRENT_BUFFER_LVALUE = NULL;
-		ptx_pop_buffer_state();
+		yypop_buffer_state(yyscanner);
 	}
 
 	/* Destroy the stack itself. */
-	ptx_free((yy_buffer_stack) );
-	(yy_buffer_stack) = NULL;
+	yyfree(yyg->yy_buffer_stack , yyscanner);
+	yyg->yy_buffer_stack = NULL;
+
+    /* Destroy the start condition stack. */
+        yyfree( yyg->yy_start_stack , yyscanner );
+        yyg->yy_start_stack = NULL;
 
     /* Reset the globals. This is important in a non-reentrant scanner so the next time
-     * ptx_lex() is called, initialization will occur. */
-    yy_init_globals( );
+     * yylex() is called, initialization will occur. */
+    yy_init_globals( yyscanner);
 
+    /* Destroy the main struct (reentrant only). */
+    yyfree ( yyscanner , yyscanner );
+    yyscanner = NULL;
     return 0;
 }
 
@@ -4384,9 +4895,11 @@ int ptx_lex_destroy  (void)
  */
 
 #ifndef yytext_ptr
-static void yy_flex_strncpy (char* s1, yyconst char * s2, int n )
+static void yy_flex_strncpy (char* s1, const char * s2, int n , yyscan_t yyscanner)
 {
-		
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+
 	int i;
 	for ( i = 0; i < n; ++i )
 		s1[i] = s2[i];
@@ -4394,7 +4907,7 @@ static void yy_flex_strncpy (char* s1, yyconst char * s2, int n )
 #endif
 
 #ifdef YY_NEED_STRLEN
-static int yy_flex_strlen (yyconst char * s )
+static int yy_flex_strlen (const char * s , yyscan_t yyscanner)
 {
 	int n;
 	for ( n = 0; s[n]; ++n )
@@ -4404,14 +4917,18 @@ static int yy_flex_strlen (yyconst char * s )
 }
 #endif
 
-void *ptx_alloc (yy_size_t  size )
+void *yyalloc (yy_size_t  size , yyscan_t yyscanner)
 {
-			return (void *) malloc( size );
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+	return malloc(size);
 }
 
-void *ptx_realloc  (void * ptr, yy_size_t  size )
+void *yyrealloc  (void * ptr, yy_size_t  size , yyscan_t yyscanner)
 {
-		
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+
 	/* The cast to (char *) in the following accommodates both
 	 * implementations that use char* generic pointers, and those
 	 * that use void* generic pointers.  It works with the latter
@@ -4419,34 +4936,33 @@ void *ptx_realloc  (void * ptr, yy_size_t  size )
 	 * any pointer type to void*, and deal with argument conversions
 	 * as though doing an assignment.
 	 */
-	return (void *) realloc( (char *) ptr, size );
+	return realloc(ptr, size);
 }
 
-void ptx_free (void * ptr )
+void yyfree (void * ptr , yyscan_t yyscanner)
 {
-			free( (char *) ptr );	/* see ptx_realloc() for (char *) cast */
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+	free( (char *) ptr );	/* see yyrealloc() for (char *) cast */
 }
 
 #define YYTABLES_NAME "yytables"
 
-#line 448 "ptx.l"
-
-
+#line 468 "ptx.l"
 
-extern int g_error_detected;
-extern const char *g_filename;
 
-int ptx_error( const char *s )
+int ptx_error( yyscan_t yyscanner, ptx_recognizer* recognizer, const char *s )
 {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
 	int i;
-	g_error_detected = 1;
+	recognizer->g_error_detected = 1;
 	fflush(stdout);
 	if( s != NULL )
-		printf("%s:%u: Syntax error:\n\n", g_filename, ptx_lineno );
-	printf("   %s\n", linebuf );
+		printf("%s:%u Syntax error:\n\n", recognizer->gpgpu_ctx->g_filename, yylineno );
+	printf("   %s\n", recognizer->linebuf );
 	printf("   ");
-	for( i=0; i < col-1; i++ ) {
-		if( linebuf[i] == '\t' ) printf("\t");
+	for( i=0; i < recognizer->col-1; i++ ) {
+		if( recognizer->linebuf[i] == '\t' ) printf("\t");
 		else printf(" ");
 	}
 			
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptxinfo_.c b/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptxinfo_.c
index a6ec59dd13..259dae152b 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptxinfo_.c
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/lex.ptxinfo_.c
@@ -1,36 +1,239 @@
-#line 2 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptxinfo_.c"
+#line 2 "lex.ptxinfo_c.c"
 
-#line 4 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptxinfo_.c"
+#line 4 "lex.ptxinfo_c.c"
 
 #define  YY_INT_ALIGNED short int
 
 /* A lexical scanner generated by flex */
 
+#define FLEX_SCANNER
+#define YY_FLEX_MAJOR_VERSION 2
+#define YY_FLEX_MINOR_VERSION 6
+#define YY_FLEX_SUBMINOR_VERSION 4
+#if YY_FLEX_SUBMINOR_VERSION > 0
+#define FLEX_BETA
+#endif
+
+#ifdef yy_create_buffer
+#define ptxinfo__create_buffer_ALREADY_DEFINED
+#else
 #define yy_create_buffer ptxinfo__create_buffer
+#endif
+
+#ifdef yy_delete_buffer
+#define ptxinfo__delete_buffer_ALREADY_DEFINED
+#else
 #define yy_delete_buffer ptxinfo__delete_buffer
-#define yy_flex_debug ptxinfo__flex_debug
+#endif
+
+#ifdef yy_scan_buffer
+#define ptxinfo__scan_buffer_ALREADY_DEFINED
+#else
+#define yy_scan_buffer ptxinfo__scan_buffer
+#endif
+
+#ifdef yy_scan_string
+#define ptxinfo__scan_string_ALREADY_DEFINED
+#else
+#define yy_scan_string ptxinfo__scan_string
+#endif
+
+#ifdef yy_scan_bytes
+#define ptxinfo__scan_bytes_ALREADY_DEFINED
+#else
+#define yy_scan_bytes ptxinfo__scan_bytes
+#endif
+
+#ifdef yy_init_buffer
+#define ptxinfo__init_buffer_ALREADY_DEFINED
+#else
 #define yy_init_buffer ptxinfo__init_buffer
+#endif
+
+#ifdef yy_flush_buffer
+#define ptxinfo__flush_buffer_ALREADY_DEFINED
+#else
 #define yy_flush_buffer ptxinfo__flush_buffer
+#endif
+
+#ifdef yy_load_buffer_state
+#define ptxinfo__load_buffer_state_ALREADY_DEFINED
+#else
 #define yy_load_buffer_state ptxinfo__load_buffer_state
+#endif
+
+#ifdef yy_switch_to_buffer
+#define ptxinfo__switch_to_buffer_ALREADY_DEFINED
+#else
 #define yy_switch_to_buffer ptxinfo__switch_to_buffer
-#define yyin ptxinfo_in
-#define yyleng ptxinfo_leng
+#endif
+
+#ifdef yypush_buffer_state
+#define ptxinfo_push_buffer_state_ALREADY_DEFINED
+#else
+#define yypush_buffer_state ptxinfo_push_buffer_state
+#endif
+
+#ifdef yypop_buffer_state
+#define ptxinfo_pop_buffer_state_ALREADY_DEFINED
+#else
+#define yypop_buffer_state ptxinfo_pop_buffer_state
+#endif
+
+#ifdef yyensure_buffer_stack
+#define ptxinfo_ensure_buffer_stack_ALREADY_DEFINED
+#else
+#define yyensure_buffer_stack ptxinfo_ensure_buffer_stack
+#endif
+
+#ifdef yylex
+#define ptxinfo_lex_ALREADY_DEFINED
+#else
 #define yylex ptxinfo_lex
-#define yylineno ptxinfo_lineno
-#define yyout ptxinfo_out
+#endif
+
+#ifdef yyrestart
+#define ptxinfo_restart_ALREADY_DEFINED
+#else
 #define yyrestart ptxinfo_restart
-#define yytext ptxinfo_text
+#endif
+
+#ifdef yylex_init
+#define ptxinfo_lex_init_ALREADY_DEFINED
+#else
+#define yylex_init ptxinfo_lex_init
+#endif
+
+#ifdef yylex_init_extra
+#define ptxinfo_lex_init_extra_ALREADY_DEFINED
+#else
+#define yylex_init_extra ptxinfo_lex_init_extra
+#endif
+
+#ifdef yylex_destroy
+#define ptxinfo_lex_destroy_ALREADY_DEFINED
+#else
+#define yylex_destroy ptxinfo_lex_destroy
+#endif
+
+#ifdef yyget_debug
+#define ptxinfo_get_debug_ALREADY_DEFINED
+#else
+#define yyget_debug ptxinfo_get_debug
+#endif
+
+#ifdef yyset_debug
+#define ptxinfo_set_debug_ALREADY_DEFINED
+#else
+#define yyset_debug ptxinfo_set_debug
+#endif
+
+#ifdef yyget_extra
+#define ptxinfo_get_extra_ALREADY_DEFINED
+#else
+#define yyget_extra ptxinfo_get_extra
+#endif
+
+#ifdef yyset_extra
+#define ptxinfo_set_extra_ALREADY_DEFINED
+#else
+#define yyset_extra ptxinfo_set_extra
+#endif
+
+#ifdef yyget_in
+#define ptxinfo_get_in_ALREADY_DEFINED
+#else
+#define yyget_in ptxinfo_get_in
+#endif
+
+#ifdef yyset_in
+#define ptxinfo_set_in_ALREADY_DEFINED
+#else
+#define yyset_in ptxinfo_set_in
+#endif
+
+#ifdef yyget_out
+#define ptxinfo_get_out_ALREADY_DEFINED
+#else
+#define yyget_out ptxinfo_get_out
+#endif
+
+#ifdef yyset_out
+#define ptxinfo_set_out_ALREADY_DEFINED
+#else
+#define yyset_out ptxinfo_set_out
+#endif
+
+#ifdef yyget_leng
+#define ptxinfo_get_leng_ALREADY_DEFINED
+#else
+#define yyget_leng ptxinfo_get_leng
+#endif
+
+#ifdef yyget_text
+#define ptxinfo_get_text_ALREADY_DEFINED
+#else
+#define yyget_text ptxinfo_get_text
+#endif
+
+#ifdef yyget_lineno
+#define ptxinfo_get_lineno_ALREADY_DEFINED
+#else
+#define yyget_lineno ptxinfo_get_lineno
+#endif
+
+#ifdef yyset_lineno
+#define ptxinfo_set_lineno_ALREADY_DEFINED
+#else
+#define yyset_lineno ptxinfo_set_lineno
+#endif
+
+#ifdef yyget_column
+#define ptxinfo_get_column_ALREADY_DEFINED
+#else
+#define yyget_column ptxinfo_get_column
+#endif
+
+#ifdef yyset_column
+#define ptxinfo_set_column_ALREADY_DEFINED
+#else
+#define yyset_column ptxinfo_set_column
+#endif
+
+#ifdef yywrap
+#define ptxinfo_wrap_ALREADY_DEFINED
+#else
 #define yywrap ptxinfo_wrap
+#endif
+
+#ifdef yyget_lval
+#define ptxinfo_get_lval_ALREADY_DEFINED
+#else
+#define yyget_lval ptxinfo_get_lval
+#endif
+
+#ifdef yyset_lval
+#define ptxinfo_set_lval_ALREADY_DEFINED
+#else
+#define yyset_lval ptxinfo_set_lval
+#endif
+
+#ifdef yyalloc
+#define ptxinfo_alloc_ALREADY_DEFINED
+#else
 #define yyalloc ptxinfo_alloc
+#endif
+
+#ifdef yyrealloc
+#define ptxinfo_realloc_ALREADY_DEFINED
+#else
 #define yyrealloc ptxinfo_realloc
-#define yyfree ptxinfo_free
+#endif
 
-#define FLEX_SCANNER
-#define YY_FLEX_MAJOR_VERSION 2
-#define YY_FLEX_MINOR_VERSION 6
-#define YY_FLEX_SUBMINOR_VERSION 0
-#if YY_FLEX_SUBMINOR_VERSION > 0
-#define FLEX_BETA
+#ifdef yyfree
+#define ptxinfo_free_ALREADY_DEFINED
+#else
+#define yyfree ptxinfo_free
 #endif
 
 /* First, we deal with  platform-specific or compiler-specific issues. */
@@ -103,60 +306,65 @@ typedef unsigned int flex_uint32_t;
 #define UINT32_MAX             (4294967295U)
 #endif
 
+#ifndef SIZE_MAX
+#define SIZE_MAX               (~(size_t)0)
+#endif
+
 #endif /* ! C99 */
 
 #endif /* ! FLEXINT_H */
 
-#ifdef __cplusplus
-
-/* The "const" storage-class-modifier is valid. */
-#define YY_USE_CONST
-
-#else	/* ! __cplusplus */
-
-/* C99 requires __STDC__ to be defined as 1. */
-#if defined (__STDC__)
-
-#define YY_USE_CONST
-
-#endif	/* defined (__STDC__) */
-#endif	/* ! __cplusplus */
+/* begin standard C++ headers. */
 
-#ifdef YY_USE_CONST
+/* TODO: this is always defined, so inline it */
 #define yyconst const
+
+#if defined(__GNUC__) && __GNUC__ >= 3
+#define yynoreturn __attribute__((__noreturn__))
 #else
-#define yyconst
+#define yynoreturn
 #endif
 
 /* Returned upon end-of-file. */
 #define YY_NULL 0
 
-/* Promotes a possibly negative, possibly signed char to an unsigned
- * integer for use as an array index.  If the signed char is negative,
- * we want to instead treat it as an 8-bit unsigned char, hence the
- * double cast.
+/* Promotes a possibly negative, possibly signed char to an
+ *   integer in range [0..255] for use as an array index.
  */
-#define YY_SC_TO_UI(c) ((unsigned int) (unsigned char) c)
+#define YY_SC_TO_UI(c) ((YY_CHAR) (c))
+
+/* An opaque pointer. */
+#ifndef YY_TYPEDEF_YY_SCANNER_T
+#define YY_TYPEDEF_YY_SCANNER_T
+typedef void* yyscan_t;
+#endif
+
+/* For convenience, these vars (plus the bison vars far below)
+   are macros in the reentrant scanner. */
+#define yyin yyg->yyin_r
+#define yyout yyg->yyout_r
+#define yyextra yyg->yyextra_r
+#define yyleng yyg->yyleng_r
+#define yytext yyg->yytext_r
+#define yylineno (YY_CURRENT_BUFFER_LVALUE->yy_bs_lineno)
+#define yycolumn (YY_CURRENT_BUFFER_LVALUE->yy_bs_column)
+#define yy_flex_debug yyg->yy_flex_debug_r
 
 /* Enter a start condition.  This macro really ought to take a parameter,
  * but we do it the disgusting crufty way forced on us by the ()-less
  * definition of BEGIN.
  */
-#define BEGIN (yy_start) = 1 + 2 *
-
+#define BEGIN yyg->yy_start = 1 + 2 *
 /* Translate the current start state into a value that can be later handed
  * to BEGIN to return to the state.  The YYSTATE alias is for lex
  * compatibility.
  */
-#define YY_START (((yy_start) - 1) / 2)
+#define YY_START ((yyg->yy_start - 1) / 2)
 #define YYSTATE YY_START
-
 /* Action number for EOF rule of a given start state. */
 #define YY_STATE_EOF(state) (YY_END_OF_BUFFER + state + 1)
-
 /* Special action meaning "start processing a new file". */
-#define YY_NEW_FILE ptxinfo_restart(ptxinfo_in  )
-
+#define YY_NEW_FILE yyrestart( yyin , yyscanner )
 #define YY_END_OF_BUFFER_CHAR 0
 
 /* Size of default input buffer. */
@@ -186,51 +394,46 @@ typedef struct yy_buffer_state *YY_BUFFER_STATE;
 typedef size_t yy_size_t;
 #endif
 
-extern yy_size_t ptxinfo_leng;
-
-extern FILE *ptxinfo_in, *ptxinfo_out;
-
 #define EOB_ACT_CONTINUE_SCAN 0
 #define EOB_ACT_END_OF_FILE 1
 #define EOB_ACT_LAST_MATCH 2
-
+    
     /* Note: We specifically omit the test for yy_rule_can_match_eol because it requires
      *       access to the local variable yy_act. Since yyless() is a macro, it would break
-     *       existing scanners that call yyless() from OUTSIDE ptxinfo_lex. 
+     *       existing scanners that call yyless() from OUTSIDE yylex.
      *       One obvious solution it to make yy_act a global. I tried that, and saw
-     *       a 5% performance hit in a non-ptxinfo_lineno scanner, because yy_act is
+     *       a 5% performance hit in a non-yylineno scanner, because yy_act is
      *       normally declared as a register variable-- so it is not worth it.
      */
     #define  YY_LESS_LINENO(n) \
             do { \
                 int yyl;\
-                for ( yyl = n; yyl < ptxinfo_leng; ++yyl )\
-                    if ( ptxinfo_text[yyl] == '\n' )\
-                        --ptxinfo_lineno;\
+                for ( yyl = n; yyl < yyleng; ++yyl )\
+                    if ( yytext[yyl] == '\n' )\
+                        --yylineno;\
             }while(0)
     #define YY_LINENO_REWIND_TO(dst) \
             do {\
                 const char *p;\
                 for ( p = yy_cp-1; p >= (dst); --p)\
                     if ( *p == '\n' )\
-                        --ptxinfo_lineno;\
+                        --yylineno;\
             }while(0)
     
 /* Return all but the first "n" matched characters back to the input stream. */
 #define yyless(n) \
 	do \
 		{ \
-		/* Undo effects of setting up ptxinfo_text. */ \
+		/* Undo effects of setting up yytext. */ \
         int yyless_macro_arg = (n); \
         YY_LESS_LINENO(yyless_macro_arg);\
-		*yy_cp = (yy_hold_char); \
+		*yy_cp = yyg->yy_hold_char; \
 		YY_RESTORE_YY_MORE_OFFSET \
-		(yy_c_buf_p) = yy_cp = yy_bp + yyless_macro_arg - YY_MORE_ADJ; \
-		YY_DO_BEFORE_ACTION; /* set up ptxinfo_text again */ \
+		yyg->yy_c_buf_p = yy_cp = yy_bp + yyless_macro_arg - YY_MORE_ADJ; \
+		YY_DO_BEFORE_ACTION; /* set up yytext again */ \
 		} \
 	while ( 0 )
-
-#define unput(c) yyunput( c, (yytext_ptr)  )
+#define unput(c) yyunput( c, yyg->yytext_ptr , yyscanner )
 
 #ifndef YY_STRUCT_YY_BUFFER_STATE
 #define YY_STRUCT_YY_BUFFER_STATE
@@ -244,7 +447,7 @@ struct yy_buffer_state
 	/* Size of input buffer in bytes, not including room for EOB
 	 * characters.
 	 */
-	yy_size_t yy_buf_size;
+	int yy_buf_size;
 
 	/* Number of characters read into yy_ch_buf, not including EOB
 	 * characters.
@@ -272,7 +475,7 @@ struct yy_buffer_state
 
     int yy_bs_lineno; /**< The line count. */
     int yy_bs_column; /**< The column count. */
-    
+
 	/* Whether to try to fill the input buffer when we reach the
 	 * end of it.
 	 */
@@ -289,134 +492,94 @@ struct yy_buffer_state
 	 * possible backing-up.
 	 *
 	 * When we actually see the EOF, we change the status to "new"
-	 * (via ptxinfo_restart()), so that the user can continue scanning by
-	 * just pointing ptxinfo_in at a new input file.
+	 * (via yyrestart()), so that the user can continue scanning by
+	 * just pointing yyin at a new input file.
 	 */
 #define YY_BUFFER_EOF_PENDING 2
 
 	};
 #endif /* !YY_STRUCT_YY_BUFFER_STATE */
 
-/* Stack of input buffers. */
-static size_t yy_buffer_stack_top = 0; /**< index of top of stack. */
-static size_t yy_buffer_stack_max = 0; /**< capacity of stack. */
-static YY_BUFFER_STATE * yy_buffer_stack = 0; /**< Stack as an array. */
-
 /* We provide macros for accessing buffer states in case in the
  * future we want to put the buffer states in a more general
  * "scanner state".
  *
  * Returns the top of the stack, or NULL.
  */
-#define YY_CURRENT_BUFFER ( (yy_buffer_stack) \
-                          ? (yy_buffer_stack)[(yy_buffer_stack_top)] \
+#define YY_CURRENT_BUFFER ( yyg->yy_buffer_stack \
+                          ? yyg->yy_buffer_stack[yyg->yy_buffer_stack_top] \
                           : NULL)
-
 /* Same as previous macro, but useful when we know that the buffer stack is not
  * NULL or when we need an lvalue. For internal use only.
  */
-#define YY_CURRENT_BUFFER_LVALUE (yy_buffer_stack)[(yy_buffer_stack_top)]
-
-/* yy_hold_char holds the character lost when ptxinfo_text is formed. */
-static char yy_hold_char;
-static int yy_n_chars;		/* number of characters read into yy_ch_buf */
-yy_size_t ptxinfo_leng;
-
-/* Points to current character in buffer. */
-static char *yy_c_buf_p = (char *) 0;
-static int yy_init = 0;		/* whether we need to initialize */
-static int yy_start = 0;	/* start state number */
-
-/* Flag which is used to allow ptxinfo_wrap()'s to do buffer switches
- * instead of setting up a fresh ptxinfo_in.  A bit of a hack ...
- */
-static int yy_did_buffer_switch_on_eof;
-
-void ptxinfo_restart (FILE *input_file  );
-void ptxinfo__switch_to_buffer (YY_BUFFER_STATE new_buffer  );
-YY_BUFFER_STATE ptxinfo__create_buffer (FILE *file,int size  );
-void ptxinfo__delete_buffer (YY_BUFFER_STATE b  );
-void ptxinfo__flush_buffer (YY_BUFFER_STATE b  );
-void ptxinfo_push_buffer_state (YY_BUFFER_STATE new_buffer  );
-void ptxinfo_pop_buffer_state (void );
-
-static void ptxinfo_ensure_buffer_stack (void );
-static void ptxinfo__load_buffer_state (void );
-static void ptxinfo__init_buffer (YY_BUFFER_STATE b,FILE *file  );
-
-#define YY_FLUSH_BUFFER ptxinfo__flush_buffer(YY_CURRENT_BUFFER )
-
-YY_BUFFER_STATE ptxinfo__scan_buffer (char *base,yy_size_t size  );
-YY_BUFFER_STATE ptxinfo__scan_string (yyconst char *yy_str  );
-YY_BUFFER_STATE ptxinfo__scan_bytes (yyconst char *bytes,yy_size_t len  );
-
-void *ptxinfo_alloc (yy_size_t  );
-void *ptxinfo_realloc (void *,yy_size_t  );
-void ptxinfo_free (void *  );
-
-#define yy_new_buffer ptxinfo__create_buffer
-
+#define YY_CURRENT_BUFFER_LVALUE yyg->yy_buffer_stack[yyg->yy_buffer_stack_top]
+
+void yyrestart ( FILE *input_file , yyscan_t yyscanner );
+void yy_switch_to_buffer ( YY_BUFFER_STATE new_buffer , yyscan_t yyscanner );
+YY_BUFFER_STATE yy_create_buffer ( FILE *file, int size , yyscan_t yyscanner );
+void yy_delete_buffer ( YY_BUFFER_STATE b , yyscan_t yyscanner );
+void yy_flush_buffer ( YY_BUFFER_STATE b , yyscan_t yyscanner );
+void yypush_buffer_state ( YY_BUFFER_STATE new_buffer , yyscan_t yyscanner );
+void yypop_buffer_state ( yyscan_t yyscanner );
+
+static void yyensure_buffer_stack ( yyscan_t yyscanner );
+static void yy_load_buffer_state ( yyscan_t yyscanner );
+static void yy_init_buffer ( YY_BUFFER_STATE b, FILE *file , yyscan_t yyscanner );
+#define YY_FLUSH_BUFFER yy_flush_buffer( YY_CURRENT_BUFFER , yyscanner)
+
+YY_BUFFER_STATE yy_scan_buffer ( char *base, yy_size_t size , yyscan_t yyscanner );
+YY_BUFFER_STATE yy_scan_string ( const char *yy_str , yyscan_t yyscanner );
+YY_BUFFER_STATE yy_scan_bytes ( const char *bytes, int len , yyscan_t yyscanner );
+
+void *yyalloc ( yy_size_t , yyscan_t yyscanner );
+void *yyrealloc ( void *, yy_size_t , yyscan_t yyscanner );
+void yyfree ( void * , yyscan_t yyscanner );
+
+#define yy_new_buffer yy_create_buffer
 #define yy_set_interactive(is_interactive) \
 	{ \
 	if ( ! YY_CURRENT_BUFFER ){ \
-        ptxinfo_ensure_buffer_stack (); \
+        yyensure_buffer_stack (yyscanner); \
 		YY_CURRENT_BUFFER_LVALUE =    \
-            ptxinfo__create_buffer(ptxinfo_in,YY_BUF_SIZE ); \
+            yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner); \
 	} \
 	YY_CURRENT_BUFFER_LVALUE->yy_is_interactive = is_interactive; \
 	}
-
 #define yy_set_bol(at_bol) \
 	{ \
 	if ( ! YY_CURRENT_BUFFER ){\
-        ptxinfo_ensure_buffer_stack (); \
+        yyensure_buffer_stack (yyscanner); \
 		YY_CURRENT_BUFFER_LVALUE =    \
-            ptxinfo__create_buffer(ptxinfo_in,YY_BUF_SIZE ); \
+            yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner); \
 	} \
 	YY_CURRENT_BUFFER_LVALUE->yy_at_bol = at_bol; \
 	}
-
 #define YY_AT_BOL() (YY_CURRENT_BUFFER_LVALUE->yy_at_bol)
 
 /* Begin user sect3 */
 
-#define ptxinfo_wrap() (/*CONSTCOND*/1)
+#define ptxinfo_wrap(yyscanner) (/*CONSTCOND*/1)
 #define YY_SKIP_YYWRAP
-
-typedef unsigned char YY_CHAR;
-
-FILE *ptxinfo_in = (FILE *) 0, *ptxinfo_out = (FILE *) 0;
+typedef flex_uint8_t YY_CHAR;
 
 typedef int yy_state_type;
 
-extern int ptxinfo_lineno;
-
-int ptxinfo_lineno = 1;
-
-extern char *ptxinfo_text;
-#ifdef yytext_ptr
-#undef yytext_ptr
-#endif
-#define yytext_ptr ptxinfo_text
+#define yytext_ptr yytext_r
 
-static yy_state_type yy_get_previous_state (void );
-static yy_state_type yy_try_NUL_trans (yy_state_type current_state  );
-static int yy_get_next_buffer (void );
-#if defined(__GNUC__) && __GNUC__ >= 3
-__attribute__((__noreturn__))
-#endif
-static void yy_fatal_error (yyconst char msg[]  );
+static yy_state_type yy_get_previous_state ( yyscan_t yyscanner );
+static yy_state_type yy_try_NUL_trans ( yy_state_type current_state  , yyscan_t yyscanner);
+static int yy_get_next_buffer ( yyscan_t yyscanner );
+static void yynoreturn yy_fatal_error ( const char* msg , yyscan_t yyscanner );
 
 /* Done after the current pattern has been matched and before the
- * corresponding action - sets up ptxinfo_text.
+ * corresponding action - sets up yytext.
  */
 #define YY_DO_BEFORE_ACTION \
-	(yytext_ptr) = yy_bp; \
-	ptxinfo_leng = (size_t) (yy_cp - yy_bp); \
-	(yy_hold_char) = *yy_cp; \
+	yyg->yytext_ptr = yy_bp; \
+	yyleng = (int) (yy_cp - yy_bp); \
+	yyg->yy_hold_char = *yy_cp; \
 	*yy_cp = '\0'; \
-	(yy_c_buf_p) = yy_cp;
-
+	yyg->yy_c_buf_p = yy_cp;
 #define YY_NUM_RULES 32
 #define YY_END_OF_BUFFER 33
 /* This struct is not used in this scanner,
@@ -426,7 +589,7 @@ struct yy_trans_info
 	flex_int32_t yy_verify;
 	flex_int32_t yy_nxt;
 	};
-static yyconst flex_int16_t yy_accept[240] =
+static const flex_int16_t yy_accept[240] =
     {   0,
         0,    0,   33,   32,   30,   31,   29,   20,   28,   22,
        23,   32,   21,   26,   27,   20,   20,   24,   25,   20,
@@ -456,7 +619,7 @@ static yyconst flex_int16_t yy_accept[240] =
         0,    0,    1,    0,    1,    0,    0,   19,    0
     } ;
 
-static yyconst YY_CHAR yy_ec[256] =
+static const YY_CHAR yy_ec[256] =
     {   0,
         1,    1,    1,    1,    1,    1,    1,    1,    2,    3,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
@@ -488,7 +651,7 @@ static yyconst YY_CHAR yy_ec[256] =
         1,    1,    1,    1,    1
     } ;
 
-static yyconst YY_CHAR yy_meta[42] =
+static const YY_CHAR yy_meta[42] =
     {   0,
         1,    1,    2,    1,    3,    1,    1,    1,    1,    1,
         3,    1,    1,    3,    3,    3,    3,    3,    1,    1,
@@ -497,7 +660,7 @@ static yyconst YY_CHAR yy_meta[42] =
         3
     } ;
 
-static yyconst flex_uint16_t yy_base[246] =
+static const flex_int16_t yy_base[246] =
     {   0,
         0,    0,  281,  282,  282,    0,  282,    0,  282,  282,
       282,  269,  268,  282,  282,  246,  242,  282,  282,  235,
@@ -528,7 +691,7 @@ static yyconst flex_uint16_t yy_base[246] =
        41,  101,  104,  107,  110
     } ;
 
-static yyconst flex_int16_t yy_def[246] =
+static const flex_int16_t yy_def[246] =
     {   0,
       239,    1,  239,  239,  239,  240,  239,  241,  239,  239,
       239,  239,  239,  239,  239,  241,  241,  239,  239,  241,
@@ -559,7 +722,7 @@ static yyconst flex_int16_t yy_def[246] =
       239,  239,  239,  239,  239
     } ;
 
-static yyconst flex_uint16_t yy_nxt[324] =
+static const flex_int16_t yy_nxt[324] =
     {   0,
         4,    5,    6,    7,    8,    8,    9,   10,   11,   12,
        13,   14,   15,   16,    8,    8,    8,   17,   18,   19,
@@ -599,7 +762,7 @@ static yyconst flex_uint16_t yy_nxt[324] =
       239,  239,  239
     } ;
 
-static yyconst flex_int16_t yy_chk[324] =
+static const flex_int16_t yy_chk[324] =
     {   0,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
@@ -640,17 +803,11 @@ static yyconst flex_int16_t yy_chk[324] =
     } ;
 
 /* Table of booleans, true if rule could match eol. */
-static yyconst flex_int32_t yy_rule_can_match_eol[33] =
+static const flex_int32_t yy_rule_can_match_eol[33] =
     {   0,
 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,     };
 
-static yy_state_type yy_last_accepting_state;
-static char *yy_last_accepting_cpos;
-
-extern int ptxinfo__flex_debug;
-int ptxinfo__flex_debug = 0;
-
 /* The intent behind this definition is that it'll catch
  * any uses of REJECT which flex missed.
  */
@@ -658,7 +815,6 @@ int ptxinfo__flex_debug = 0;
 #define yymore() yymore_used_but_not_detected
 #define YY_MORE_ADJ 0
 #define YY_RESTORE_YY_MORE_OFFSET
-char *ptxinfo_text;
 #line 1 "ptxinfo.l"
 /*
 Copyright (c) 2009-2011, Tor M. Aamodt
@@ -688,18 +844,21 @@ CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
-#line 35 "ptxinfo.l"
+#line 39 "ptxinfo.l"
+#include "ptx_loader.h"
 #include "ptxinfo.tab.h"
 #include <string.h>
+#include "../../libcuda_sim/gpgpu_context.h"
 
 #define LINEBUF_SIZE 1024
-char ptxinfo_linebuf[LINEBUF_SIZE];
-unsigned ptxinfo_col = 0;
-#define TC if( (ptxinfo_lineno == 1) && ((ptxinfo_col + strlen(ptxinfo_text)) < LINEBUF_SIZE) ) { \
-		strncpy(ptxinfo_linebuf+ptxinfo_col,ptxinfo_text,strlen(ptxinfo_text)); \
+#define TC if( (yylineno == 1) && (ptxinfo->col + strlen(yytext) < LINEBUF_SIZE) ) { \
+		strncpy(ptxinfo->linebuf+ptxinfo->col,yytext,strlen(yytext)); \
 	   } \
-	   ptxinfo_col+=strlen(ptxinfo_text); 
-#line 703 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptxinfo_.c"
+	   ptxinfo->col+=strlen(yytext);
+#define YY_DECL int ptxinfo_lex \
+	       (YYSTYPE * yylval_param , yyscan_t yyscanner, ptxinfo_data* ptxinfo)
+#line 861 "lex.ptxinfo_c.c"
+#line 862 "lex.ptxinfo_c.c"
 
 #define INITIAL 0
 
@@ -715,36 +874,88 @@ unsigned ptxinfo_col = 0;
 #define YY_EXTRA_TYPE void *
 #endif
 
-static int yy_init_globals (void );
+/* Holds the entire state of the reentrant scanner. */
+struct yyguts_t
+    {
+
+    /* User-defined. Not touched by flex. */
+    YY_EXTRA_TYPE yyextra_r;
+
+    /* The rest are the same as the globals declared in the non-reentrant scanner. */
+    FILE *yyin_r, *yyout_r;
+    size_t yy_buffer_stack_top; /**< index of top of stack. */
+    size_t yy_buffer_stack_max; /**< capacity of stack. */
+    YY_BUFFER_STATE * yy_buffer_stack; /**< Stack as an array. */
+    char yy_hold_char;
+    int yy_n_chars;
+    int yyleng_r;
+    char *yy_c_buf_p;
+    int yy_init;
+    int yy_start;
+    int yy_did_buffer_switch_on_eof;
+    int yy_start_stack_ptr;
+    int yy_start_stack_depth;
+    int *yy_start_stack;
+    yy_state_type yy_last_accepting_state;
+    char* yy_last_accepting_cpos;
+
+    int yylineno_r;
+    int yy_flex_debug_r;
+
+    char *yytext_r;
+    int yy_more_flag;
+    int yy_more_len;
+
+    YYSTYPE * yylval_r;
+
+    }; /* end struct yyguts_t */
+
+static int yy_init_globals ( yyscan_t yyscanner );
+
+    /* This must go here because YYSTYPE and YYLTYPE are included
+     * from bison output in section 1.*/
+    #    define yylval yyg->yylval_r
+    
+int yylex_init (yyscan_t* scanner);
+
+int yylex_init_extra ( YY_EXTRA_TYPE user_defined, yyscan_t* scanner);
 
 /* Accessor methods to globals.
    These are made visible to non-reentrant scanners for convenience. */
 
-int ptxinfo_lex_destroy (void );
+int yylex_destroy ( yyscan_t yyscanner );
+
+int yyget_debug ( yyscan_t yyscanner );
+
+void yyset_debug ( int debug_flag , yyscan_t yyscanner );
 
-int ptxinfo_get_debug (void );
+YY_EXTRA_TYPE yyget_extra ( yyscan_t yyscanner );
 
-void ptxinfo_set_debug (int debug_flag  );
+void yyset_extra ( YY_EXTRA_TYPE user_defined , yyscan_t yyscanner );
 
-YY_EXTRA_TYPE ptxinfo_get_extra (void );
+FILE *yyget_in ( yyscan_t yyscanner );
 
-void ptxinfo_set_extra (YY_EXTRA_TYPE user_defined  );
+void yyset_in  ( FILE * _in_str , yyscan_t yyscanner );
 
-FILE *ptxinfo_get_in (void );
+FILE *yyget_out ( yyscan_t yyscanner );
 
-void ptxinfo_set_in  (FILE * _in_str  );
+void yyset_out  ( FILE * _out_str , yyscan_t yyscanner );
 
-FILE *ptxinfo_get_out (void );
+			int yyget_leng ( yyscan_t yyscanner );
 
-void ptxinfo_set_out  (FILE * _out_str  );
+char *yyget_text ( yyscan_t yyscanner );
 
-yy_size_t ptxinfo_get_leng (void );
+int yyget_lineno ( yyscan_t yyscanner );
 
-char *ptxinfo_get_text (void );
+void yyset_lineno ( int _line_number , yyscan_t yyscanner );
 
-int ptxinfo_get_lineno (void );
+int yyget_column  ( yyscan_t yyscanner );
 
-void ptxinfo_set_lineno (int _line_number  );
+void yyset_column ( int _column_no , yyscan_t yyscanner );
+
+YYSTYPE * yyget_lval ( yyscan_t yyscanner );
+
+void yyset_lval ( YYSTYPE * yylval_param , yyscan_t yyscanner );
 
 /* Macros after this point can all be overridden by user definitions in
  * section 1.
@@ -752,32 +963,31 @@ void ptxinfo_set_lineno (int _line_number  );
 
 #ifndef YY_SKIP_YYWRAP
 #ifdef __cplusplus
-extern "C" int ptxinfo_wrap (void );
+extern "C" int yywrap ( yyscan_t yyscanner );
 #else
-extern int ptxinfo_wrap (void );
+extern int yywrap ( yyscan_t yyscanner );
 #endif
 #endif
 
 #ifndef YY_NO_UNPUT
     
-    static void yyunput (int c,char *buf_ptr  );
+    static void yyunput ( int c, char *buf_ptr  , yyscan_t yyscanner);
     
 #endif
 
 #ifndef yytext_ptr
-static void yy_flex_strncpy (char *,yyconst char *,int );
+static void yy_flex_strncpy ( char *, const char *, int , yyscan_t yyscanner);
 #endif
 
 #ifdef YY_NEED_STRLEN
-static int yy_flex_strlen (yyconst char * );
+static int yy_flex_strlen ( const char * , yyscan_t yyscanner);
 #endif
 
 #ifndef YY_NO_INPUT
-
 #ifdef __cplusplus
-static int yyinput (void );
+static int yyinput ( yyscan_t yyscanner );
 #else
-static int input (void );
+static int input ( yyscan_t yyscanner );
 #endif
 
 #endif
@@ -797,7 +1007,7 @@ static int input (void );
 /* This used to be an fputs(), but since the string might contain NUL's,
  * we now use fwrite().
  */
-#define ECHO do { if (fwrite( ptxinfo_text, ptxinfo_leng, 1, ptxinfo_out )) {} } while (0)
+#define ECHO do { if (fwrite( yytext, (size_t) yyleng, 1, yyout )) {} } while (0)
 #endif
 
 /* Gets input and stuffs it into "buf".  number of characters read, or YY_NULL,
@@ -808,20 +1018,20 @@ static int input (void );
 	if ( YY_CURRENT_BUFFER_LVALUE->yy_is_interactive ) \
 		{ \
 		int c = '*'; \
-		size_t n; \
+		int n; \
 		for ( n = 0; n < max_size && \
-			     (c = getc( ptxinfo_in )) != EOF && c != '\n'; ++n ) \
+			     (c = getc( yyin )) != EOF && c != '\n'; ++n ) \
 			buf[n] = (char) c; \
 		if ( c == '\n' ) \
 			buf[n++] = (char) c; \
-		if ( c == EOF && ferror( ptxinfo_in ) ) \
+		if ( c == EOF && ferror( yyin ) ) \
 			YY_FATAL_ERROR( "input in flex scanner failed" ); \
 		result = n; \
 		} \
 	else \
 		{ \
 		errno=0; \
-		while ( (result = fread(buf, 1, max_size, ptxinfo_in))==0 && ferror(ptxinfo_in)) \
+		while ( (result = (int) fread(buf, 1, (yy_size_t) max_size, yyin)) == 0 && ferror(yyin)) \
 			{ \
 			if( errno != EINTR) \
 				{ \
@@ -829,7 +1039,7 @@ static int input (void );
 				break; \
 				} \
 			errno=0; \
-			clearerr(ptxinfo_in); \
+			clearerr(yyin); \
 			} \
 		}\
 \
@@ -851,7 +1061,7 @@ static int input (void );
 
 /* Report a fatal error. */
 #ifndef YY_FATAL_ERROR
-#define YY_FATAL_ERROR(msg) yy_fatal_error( msg )
+#define YY_FATAL_ERROR(msg) yy_fatal_error( msg , yyscanner)
 #endif
 
 /* end tables serialization structures and prototypes */
@@ -862,12 +1072,14 @@ static int input (void );
 #ifndef YY_DECL
 #define YY_DECL_IS_OURS 1
 
-extern int ptxinfo_lex (void);
+extern int yylex \
+               (YYSTYPE * yylval_param , yyscan_t yyscanner);
 
-#define YY_DECL int ptxinfo_lex (void)
+#define YY_DECL int yylex \
+               (YYSTYPE * yylval_param , yyscan_t yyscanner)
 #endif /* !YY_DECL */
 
-/* Code executed at the beginning of each rule, after ptxinfo_text and ptxinfo_leng
+/* Code executed at the beginning of each rule, after yytext and yyleng
  * have been set up.
  */
 #ifndef YY_USER_ACTION
@@ -889,67 +1101,70 @@ YY_DECL
 	yy_state_type yy_current_state;
 	char *yy_cp, *yy_bp;
 	int yy_act;
-    
-	if ( !(yy_init) )
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+    yylval = yylval_param;
+
+	if ( !yyg->yy_init )
 		{
-		(yy_init) = 1;
+		yyg->yy_init = 1;
 
 #ifdef YY_USER_INIT
 		YY_USER_INIT;
 #endif
 
-		if ( ! (yy_start) )
-			(yy_start) = 1;	/* first start state */
+		if ( ! yyg->yy_start )
+			yyg->yy_start = 1;	/* first start state */
 
-		if ( ! ptxinfo_in )
-			ptxinfo_in = stdin;
+		if ( ! yyin )
+			yyin = stdin;
 
-		if ( ! ptxinfo_out )
-			ptxinfo_out = stdout;
+		if ( ! yyout )
+			yyout = stdout;
 
 		if ( ! YY_CURRENT_BUFFER ) {
-			ptxinfo_ensure_buffer_stack ();
+			yyensure_buffer_stack (yyscanner);
 			YY_CURRENT_BUFFER_LVALUE =
-				ptxinfo__create_buffer(ptxinfo_in,YY_BUF_SIZE );
+				yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner);
 		}
 
-		ptxinfo__load_buffer_state( );
+		yy_load_buffer_state( yyscanner );
 		}
 
 	{
-#line 47 "ptxinfo.l"
+#line 53 "ptxinfo.l"
 
-#line 923 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptxinfo_.c"
+#line 1138 "lex.ptxinfo_c.c"
 
 	while ( /*CONSTCOND*/1 )		/* loops until end-of-file is reached */
 		{
-		yy_cp = (yy_c_buf_p);
+		yy_cp = yyg->yy_c_buf_p;
 
-		/* Support of ptxinfo_text. */
-		*yy_cp = (yy_hold_char);
+		/* Support of yytext. */
+		*yy_cp = yyg->yy_hold_char;
 
 		/* yy_bp points to the position in yy_ch_buf of the start of
 		 * the current run.
 		 */
 		yy_bp = yy_cp;
 
-		yy_current_state = (yy_start);
+		yy_current_state = yyg->yy_start;
 yy_match:
 		do
 			{
 			YY_CHAR yy_c = yy_ec[YY_SC_TO_UI(*yy_cp)] ;
 			if ( yy_accept[yy_current_state] )
 				{
-				(yy_last_accepting_state) = yy_current_state;
-				(yy_last_accepting_cpos) = yy_cp;
+				yyg->yy_last_accepting_state = yy_current_state;
+				yyg->yy_last_accepting_cpos = yy_cp;
 				}
 			while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
 				{
 				yy_current_state = (int) yy_def[yy_current_state];
 				if ( yy_current_state >= 240 )
-					yy_c = yy_meta[(unsigned int) yy_c];
+					yy_c = yy_meta[yy_c];
 				}
-			yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+			yy_current_state = yy_nxt[yy_base[yy_current_state] + yy_c];
 			++yy_cp;
 			}
 		while ( yy_base[yy_current_state] != 282 );
@@ -958,8 +1173,8 @@ yy_find_action:
 		yy_act = yy_accept[yy_current_state];
 		if ( yy_act == 0 )
 			{ /* have to back up */
-			yy_cp = (yy_last_accepting_cpos);
-			yy_current_state = (yy_last_accepting_state);
+			yy_cp = yyg->yy_last_accepting_cpos;
+			yy_current_state = yyg->yy_last_accepting_state;
 			yy_act = yy_accept[yy_current_state];
 			}
 
@@ -967,11 +1182,13 @@ yy_find_action:
 
 		if ( yy_act != YY_END_OF_BUFFER && yy_rule_can_match_eol[yy_act] )
 			{
-			yy_size_t yyl;
-			for ( yyl = 0; yyl < ptxinfo_leng; ++yyl )
-				if ( ptxinfo_text[yyl] == '\n' )
-					   
-    ptxinfo_lineno++;
+			int yyl;
+			for ( yyl = 0; yyl < yyleng; ++yyl )
+				if ( yytext[yyl] == '\n' )
+					
+    do{ yylineno++;
+        yycolumn=0;
+    }while(0)
 ;
 			}
 
@@ -981,199 +1198,199 @@ do_action:	/* This label is used only to access EOF actions. */
 	{ /* beginning of action switch */
 			case 0: /* must back up */
 			/* undo the effects of YY_DO_BEFORE_ACTION */
-			*yy_cp = (yy_hold_char);
-			yy_cp = (yy_last_accepting_cpos);
-			yy_current_state = (yy_last_accepting_state);
+			*yy_cp = yyg->yy_hold_char;
+			yy_cp = yyg->yy_last_accepting_cpos;
+			yy_current_state = yyg->yy_last_accepting_state;
 			goto yy_find_action;
 
 case 1:
 /* rule 1 can match eol */
 YY_RULE_SETUP
-#line 48 "ptxinfo.l"
+#line 54 "ptxinfo.l"
 
 	YY_BREAK
 case 2:
 YY_RULE_SETUP
-#line 49 "ptxinfo.l"
+#line 55 "ptxinfo.l"
 TC; return WARNING;
 	YY_BREAK
 case 3:
 YY_RULE_SETUP
-#line 50 "ptxinfo.l"
+#line 56 "ptxinfo.l"
 TC; 	return HEADER;
 	YY_BREAK
 case 4:
 YY_RULE_SETUP
-#line 51 "ptxinfo.l"
-TC; 	return INFO;
+#line 57 "ptxinfo.l"
+TC; 	return INFO_GPGPU;
 	YY_BREAK
 case 5:
 YY_RULE_SETUP
-#line 52 "ptxinfo.l"
+#line 58 "ptxinfo.l"
 TC; return FUNC;
 	YY_BREAK
 case 6:
 YY_RULE_SETUP
-#line 53 "ptxinfo.l"
+#line 59 "ptxinfo.l"
 TC; return USED;
 	YY_BREAK
 case 7:
 YY_RULE_SETUP
-#line 54 "ptxinfo.l"
+#line 60 "ptxinfo.l"
 TC; return REGS;
 	YY_BREAK
 case 8:
 YY_RULE_SETUP
-#line 55 "ptxinfo.l"
+#line 61 "ptxinfo.l"
 TC; return BYTES;
 	YY_BREAK
 case 9:
 YY_RULE_SETUP
-#line 56 "ptxinfo.l"
+#line 62 "ptxinfo.l"
 TC; return LMEM;
 	YY_BREAK
 case 10:
 YY_RULE_SETUP
-#line 57 "ptxinfo.l"
+#line 63 "ptxinfo.l"
 TC; return SMEM;
 	YY_BREAK
 case 11:
 YY_RULE_SETUP
-#line 58 "ptxinfo.l"
+#line 64 "ptxinfo.l"
 TC; return CMEM;
 	YY_BREAK
 case 12:
 YY_RULE_SETUP
-#line 59 "ptxinfo.l"
+#line 65 "ptxinfo.l"
 TC; return GMEM;
 	YY_BREAK
 case 13:
 YY_RULE_SETUP
-#line 60 "ptxinfo.l"
+#line 66 "ptxinfo.l"
 TC; return LINE;
 	YY_BREAK
 case 14:
 YY_RULE_SETUP
-#line 61 "ptxinfo.l"
+#line 67 "ptxinfo.l"
 TC; return FOR;
 	YY_BREAK
 case 15:
 YY_RULE_SETUP
-#line 62 "ptxinfo.l"
+#line 68 "ptxinfo.l"
 TC; return TEXTURES;
 	YY_BREAK
 case 16:
 YY_RULE_SETUP
-#line 63 "ptxinfo.l"
+#line 69 "ptxinfo.l"
 TC; return DUPLICATE;
 	YY_BREAK
 case 17:
 YY_RULE_SETUP
-#line 64 "ptxinfo.l"
-TC; ptxinfo_lval.string_value = strdup(ptxinfo_text); return FUNCTION;
+#line 70 "ptxinfo.l"
+TC; yylval->string_value = strdup(yytext); return FUNCTION;
 	YY_BREAK
 case 18:
 YY_RULE_SETUP
-#line 65 "ptxinfo.l"
-TC; ptxinfo_lval.string_value = strdup(ptxinfo_text); return VARIABLE;
+#line 71 "ptxinfo.l"
+TC; yylval->string_value = strdup(yytext); return VARIABLE;
 	YY_BREAK
 case 19:
 YY_RULE_SETUP
-#line 66 "ptxinfo.l"
-TC; return FATAL;
+#line 72 "ptxinfo.l"
+TC; return FATAL_GPGPU;
 	YY_BREAK
 case 20:
 YY_RULE_SETUP
-#line 68 "ptxinfo.l"
-TC; ptxinfo_lval.string_value = strdup(ptxinfo_text); return IDENTIFIER;
+#line 74 "ptxinfo.l"
+TC; yylval->string_value = strdup(yytext); return IDENTIFIER;
 	YY_BREAK
 case 21:
 YY_RULE_SETUP
-#line 69 "ptxinfo.l"
-TC; ptxinfo_lval.int_value =  atoi(ptxinfo_text); return INT_OPERAND;
+#line 75 "ptxinfo.l"
+TC; yylval->int_value =  atoi(yytext); return INT_OPERAND;
 	YY_BREAK
 case 22:
 YY_RULE_SETUP
-#line 71 "ptxinfo.l"
+#line 77 "ptxinfo.l"
 TC; return PLUS;
 	YY_BREAK
 case 23:
 YY_RULE_SETUP
-#line 72 "ptxinfo.l"
+#line 78 "ptxinfo.l"
 TC; return COMMA;
 	YY_BREAK
 case 24:
 YY_RULE_SETUP
-#line 73 "ptxinfo.l"
+#line 79 "ptxinfo.l"
 TC; return LEFT_SQUARE_BRACKET;
 	YY_BREAK
 case 25:
 YY_RULE_SETUP
-#line 74 "ptxinfo.l"
+#line 80 "ptxinfo.l"
 TC; return RIGHT_SQUARE_BRACKET;
 	YY_BREAK
 case 26:
 YY_RULE_SETUP
-#line 75 "ptxinfo.l"
+#line 81 "ptxinfo.l"
 TC; return COLON;
 	YY_BREAK
 case 27:
 YY_RULE_SETUP
-#line 76 "ptxinfo.l"
+#line 82 "ptxinfo.l"
 TC; return SEMICOLON;
 	YY_BREAK
 case 28:
 YY_RULE_SETUP
-#line 77 "ptxinfo.l"
+#line 83 "ptxinfo.l"
 TC; return QUOTE;
 	YY_BREAK
 case 29:
 YY_RULE_SETUP
-#line 78 "ptxinfo.l"
+#line 84 "ptxinfo.l"
 TC;
 	YY_BREAK
 case 30:
 YY_RULE_SETUP
-#line 79 "ptxinfo.l"
+#line 85 "ptxinfo.l"
 TC;
 	YY_BREAK
 case 31:
 /* rule 31 can match eol */
 YY_RULE_SETUP
-#line 81 "ptxinfo.l"
-ptxinfo_col=0; strncpy(ptxinfo_linebuf, ptxinfo_text + 1, 1024); yyless( 1 );
+#line 87 "ptxinfo.l"
+ptxinfo->col=0; strncpy(ptxinfo->linebuf, yytext + 1, 1024); yyless( 1 );
 	YY_BREAK
 case 32:
 YY_RULE_SETUP
-#line 83 "ptxinfo.l"
+#line 89 "ptxinfo.l"
 ECHO;
 	YY_BREAK
-#line 1152 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/lex.ptxinfo_.c"
+#line 1369 "lex.ptxinfo_c.c"
 case YY_STATE_EOF(INITIAL):
 	yyterminate();
 
 	case YY_END_OF_BUFFER:
 		{
 		/* Amount of text matched not including the EOB char. */
-		int yy_amount_of_matched_text = (int) (yy_cp - (yytext_ptr)) - 1;
+		int yy_amount_of_matched_text = (int) (yy_cp - yyg->yytext_ptr) - 1;
 
 		/* Undo the effects of YY_DO_BEFORE_ACTION. */
-		*yy_cp = (yy_hold_char);
+		*yy_cp = yyg->yy_hold_char;
 		YY_RESTORE_YY_MORE_OFFSET
 
 		if ( YY_CURRENT_BUFFER_LVALUE->yy_buffer_status == YY_BUFFER_NEW )
 			{
 			/* We're scanning a new file or input source.  It's
 			 * possible that this happened because the user
-			 * just pointed ptxinfo_in at a new source and called
-			 * ptxinfo_lex().  If so, then we have to assure
+			 * just pointed yyin at a new source and called
+			 * yylex().  If so, then we have to assure
 			 * consistency between YY_CURRENT_BUFFER and our
 			 * globals.  Here is the right place to do so, because
 			 * this is the first action (other than possibly a
 			 * back-up) that will match for the new input source.
 			 */
-			(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
-			YY_CURRENT_BUFFER_LVALUE->yy_input_file = ptxinfo_in;
+			yyg->yy_n_chars = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
+			YY_CURRENT_BUFFER_LVALUE->yy_input_file = yyin;
 			YY_CURRENT_BUFFER_LVALUE->yy_buffer_status = YY_BUFFER_NORMAL;
 			}
 
@@ -1184,13 +1401,13 @@ case YY_STATE_EOF(INITIAL):
 		 * end-of-buffer state).  Contrast this with the test
 		 * in input().
 		 */
-		if ( (yy_c_buf_p) <= &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] )
+		if ( yyg->yy_c_buf_p <= &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars] )
 			{ /* This was really a NUL. */
 			yy_state_type yy_next_state;
 
-			(yy_c_buf_p) = (yytext_ptr) + yy_amount_of_matched_text;
+			yyg->yy_c_buf_p = yyg->yytext_ptr + yy_amount_of_matched_text;
 
-			yy_current_state = yy_get_previous_state(  );
+			yy_current_state = yy_get_previous_state( yyscanner );
 
 			/* Okay, we're now positioned to make the NUL
 			 * transition.  We couldn't have
@@ -1201,43 +1418,43 @@ case YY_STATE_EOF(INITIAL):
 			 * will run more slowly).
 			 */
 
-			yy_next_state = yy_try_NUL_trans( yy_current_state );
+			yy_next_state = yy_try_NUL_trans( yy_current_state , yyscanner);
 
-			yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+			yy_bp = yyg->yytext_ptr + YY_MORE_ADJ;
 
 			if ( yy_next_state )
 				{
 				/* Consume the NUL. */
-				yy_cp = ++(yy_c_buf_p);
+				yy_cp = ++yyg->yy_c_buf_p;
 				yy_current_state = yy_next_state;
 				goto yy_match;
 				}
 
 			else
 				{
-				yy_cp = (yy_c_buf_p);
+				yy_cp = yyg->yy_c_buf_p;
 				goto yy_find_action;
 				}
 			}
 
-		else switch ( yy_get_next_buffer(  ) )
+		else switch ( yy_get_next_buffer( yyscanner ) )
 			{
 			case EOB_ACT_END_OF_FILE:
 				{
-				(yy_did_buffer_switch_on_eof) = 0;
+				yyg->yy_did_buffer_switch_on_eof = 0;
 
-				if ( ptxinfo_wrap( ) )
+				if ( yywrap( yyscanner ) )
 					{
 					/* Note: because we've taken care in
 					 * yy_get_next_buffer() to have set up
-					 * ptxinfo_text, we can now set up
+					 * yytext, we can now set up
 					 * yy_c_buf_p so that if some total
 					 * hoser (like flex itself) wants to
 					 * call the scanner after we return the
 					 * YY_NULL, it'll still work - another
 					 * YY_NULL will get returned.
 					 */
-					(yy_c_buf_p) = (yytext_ptr) + YY_MORE_ADJ;
+					yyg->yy_c_buf_p = yyg->yytext_ptr + YY_MORE_ADJ;
 
 					yy_act = YY_STATE_EOF(YY_START);
 					goto do_action;
@@ -1245,30 +1462,30 @@ case YY_STATE_EOF(INITIAL):
 
 				else
 					{
-					if ( ! (yy_did_buffer_switch_on_eof) )
+					if ( ! yyg->yy_did_buffer_switch_on_eof )
 						YY_NEW_FILE;
 					}
 				break;
 				}
 
 			case EOB_ACT_CONTINUE_SCAN:
-				(yy_c_buf_p) =
-					(yytext_ptr) + yy_amount_of_matched_text;
+				yyg->yy_c_buf_p =
+					yyg->yytext_ptr + yy_amount_of_matched_text;
 
-				yy_current_state = yy_get_previous_state(  );
+				yy_current_state = yy_get_previous_state( yyscanner );
 
-				yy_cp = (yy_c_buf_p);
-				yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+				yy_cp = yyg->yy_c_buf_p;
+				yy_bp = yyg->yytext_ptr + YY_MORE_ADJ;
 				goto yy_match;
 
 			case EOB_ACT_LAST_MATCH:
-				(yy_c_buf_p) =
-				&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)];
+				yyg->yy_c_buf_p =
+				&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars];
 
-				yy_current_state = yy_get_previous_state(  );
+				yy_current_state = yy_get_previous_state( yyscanner );
 
-				yy_cp = (yy_c_buf_p);
-				yy_bp = (yytext_ptr) + YY_MORE_ADJ;
+				yy_cp = yyg->yy_c_buf_p;
+				yy_bp = yyg->yytext_ptr + YY_MORE_ADJ;
 				goto yy_find_action;
 			}
 		break;
@@ -1280,7 +1497,7 @@ case YY_STATE_EOF(INITIAL):
 	} /* end of action switch */
 		} /* end of scanning one token */
 	} /* end of user's declarations */
-} /* end of ptxinfo_lex */
+} /* end of yylex */
 
 /* yy_get_next_buffer - try to read in a new buffer
  *
@@ -1289,20 +1506,21 @@ case YY_STATE_EOF(INITIAL):
  *	EOB_ACT_CONTINUE_SCAN - continue scanning from current position
  *	EOB_ACT_END_OF_FILE - end of file
  */
-static int yy_get_next_buffer (void)
+static int yy_get_next_buffer (yyscan_t yyscanner)
 {
-    	char *dest = YY_CURRENT_BUFFER_LVALUE->yy_ch_buf;
-	char *source = (yytext_ptr);
-	yy_size_t number_to_move, i;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	char *dest = YY_CURRENT_BUFFER_LVALUE->yy_ch_buf;
+	char *source = yyg->yytext_ptr;
+	int number_to_move, i;
 	int ret_val;
 
-	if ( (yy_c_buf_p) > &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars) + 1] )
+	if ( yyg->yy_c_buf_p > &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars + 1] )
 		YY_FATAL_ERROR(
 		"fatal flex scanner internal error--end of buffer missed" );
 
 	if ( YY_CURRENT_BUFFER_LVALUE->yy_fill_buffer == 0 )
 		{ /* Don't try to fill the buffer, so this is an EOF. */
-		if ( (yy_c_buf_p) - (yytext_ptr) - YY_MORE_ADJ == 1 )
+		if ( yyg->yy_c_buf_p - yyg->yytext_ptr - YY_MORE_ADJ == 1 )
 			{
 			/* We matched a single character, the EOB, so
 			 * treat this as a final EOF.
@@ -1322,7 +1540,7 @@ static int yy_get_next_buffer (void)
 	/* Try to read more data. */
 
 	/* First move last chars to start of buffer. */
-	number_to_move = (yy_size_t) ((yy_c_buf_p) - (yytext_ptr)) - 1;
+	number_to_move = (int) (yyg->yy_c_buf_p - yyg->yytext_ptr - 1);
 
 	for ( i = 0; i < number_to_move; ++i )
 		*(dest++) = *(source++);
@@ -1331,11 +1549,11 @@ static int yy_get_next_buffer (void)
 		/* don't do the read, it's not guaranteed to return an EOF,
 		 * just force an EOF
 		 */
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars) = 0;
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars = 0;
 
 	else
 		{
-			yy_size_t num_to_read =
+			int num_to_read =
 			YY_CURRENT_BUFFER_LVALUE->yy_buf_size - number_to_move - 1;
 
 		while ( num_to_read <= 0 )
@@ -1345,11 +1563,11 @@ static int yy_get_next_buffer (void)
 			YY_BUFFER_STATE b = YY_CURRENT_BUFFER_LVALUE;
 
 			int yy_c_buf_p_offset =
-				(int) ((yy_c_buf_p) - b->yy_ch_buf);
+				(int) (yyg->yy_c_buf_p - b->yy_ch_buf);
 
 			if ( b->yy_is_our_buffer )
 				{
-				yy_size_t new_size = b->yy_buf_size * 2;
+				int new_size = b->yy_buf_size * 2;
 
 				if ( new_size <= 0 )
 					b->yy_buf_size += b->yy_buf_size / 8;
@@ -1358,17 +1576,18 @@ static int yy_get_next_buffer (void)
 
 				b->yy_ch_buf = (char *)
 					/* Include room in for 2 EOB chars. */
-					ptxinfo_realloc((void *) b->yy_ch_buf,b->yy_buf_size + 2  );
+					yyrealloc( (void *) b->yy_ch_buf,
+							 (yy_size_t) (b->yy_buf_size + 2) , yyscanner );
 				}
 			else
 				/* Can't grow it, we don't own it. */
-				b->yy_ch_buf = 0;
+				b->yy_ch_buf = NULL;
 
 			if ( ! b->yy_ch_buf )
 				YY_FATAL_ERROR(
 				"fatal error - scanner input buffer overflow" );
 
-			(yy_c_buf_p) = &b->yy_ch_buf[yy_c_buf_p_offset];
+			yyg->yy_c_buf_p = &b->yy_ch_buf[yy_c_buf_p_offset];
 
 			num_to_read = YY_CURRENT_BUFFER_LVALUE->yy_buf_size -
 						number_to_move - 1;
@@ -1380,17 +1599,17 @@ static int yy_get_next_buffer (void)
 
 		/* Read in more data. */
 		YY_INPUT( (&YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[number_to_move]),
-			(yy_n_chars), num_to_read );
+			yyg->yy_n_chars, num_to_read );
 
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars;
 		}
 
-	if ( (yy_n_chars) == 0 )
+	if ( yyg->yy_n_chars == 0 )
 		{
 		if ( number_to_move == YY_MORE_ADJ )
 			{
 			ret_val = EOB_ACT_END_OF_FILE;
-			ptxinfo_restart(ptxinfo_in  );
+			yyrestart( yyin  , yyscanner);
 			}
 
 		else
@@ -1404,47 +1623,51 @@ static int yy_get_next_buffer (void)
 	else
 		ret_val = EOB_ACT_CONTINUE_SCAN;
 
-	if ((int) ((yy_n_chars) + number_to_move) > YY_CURRENT_BUFFER_LVALUE->yy_buf_size) {
+	if ((yyg->yy_n_chars + number_to_move) > YY_CURRENT_BUFFER_LVALUE->yy_buf_size) {
 		/* Extend the array by 50%, plus the number we really need. */
-		int new_size = (yy_n_chars) + number_to_move + ((yy_n_chars) >> 1);
-		YY_CURRENT_BUFFER_LVALUE->yy_ch_buf = (char *) ptxinfo_realloc((void *) YY_CURRENT_BUFFER_LVALUE->yy_ch_buf,new_size  );
+		int new_size = yyg->yy_n_chars + number_to_move + (yyg->yy_n_chars >> 1);
+		YY_CURRENT_BUFFER_LVALUE->yy_ch_buf = (char *) yyrealloc(
+			(void *) YY_CURRENT_BUFFER_LVALUE->yy_ch_buf, (yy_size_t) new_size , yyscanner );
 		if ( ! YY_CURRENT_BUFFER_LVALUE->yy_ch_buf )
 			YY_FATAL_ERROR( "out of dynamic memory in yy_get_next_buffer()" );
+		/* "- 2" to take care of EOB's */
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_size = (int) (new_size - 2);
 	}
 
-	(yy_n_chars) += number_to_move;
-	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] = YY_END_OF_BUFFER_CHAR;
-	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars) + 1] = YY_END_OF_BUFFER_CHAR;
+	yyg->yy_n_chars += number_to_move;
+	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars] = YY_END_OF_BUFFER_CHAR;
+	YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars + 1] = YY_END_OF_BUFFER_CHAR;
 
-	(yytext_ptr) = &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[0];
+	yyg->yytext_ptr = &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[0];
 
 	return ret_val;
 }
 
 /* yy_get_previous_state - get the state just before the EOB char was reached */
 
-    static yy_state_type yy_get_previous_state (void)
+    static yy_state_type yy_get_previous_state (yyscan_t yyscanner)
 {
 	yy_state_type yy_current_state;
 	char *yy_cp;
-    
-	yy_current_state = (yy_start);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+	yy_current_state = yyg->yy_start;
 
-	for ( yy_cp = (yytext_ptr) + YY_MORE_ADJ; yy_cp < (yy_c_buf_p); ++yy_cp )
+	for ( yy_cp = yyg->yytext_ptr + YY_MORE_ADJ; yy_cp < yyg->yy_c_buf_p; ++yy_cp )
 		{
 		YY_CHAR yy_c = (*yy_cp ? yy_ec[YY_SC_TO_UI(*yy_cp)] : 1);
 		if ( yy_accept[yy_current_state] )
 			{
-			(yy_last_accepting_state) = yy_current_state;
-			(yy_last_accepting_cpos) = yy_cp;
+			yyg->yy_last_accepting_state = yy_current_state;
+			yyg->yy_last_accepting_cpos = yy_cp;
 			}
 		while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
 			{
 			yy_current_state = (int) yy_def[yy_current_state];
 			if ( yy_current_state >= 240 )
-				yy_c = yy_meta[(unsigned int) yy_c];
+				yy_c = yy_meta[yy_c];
 			}
-		yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+		yy_current_state = yy_nxt[yy_base[yy_current_state] + yy_c];
 		}
 
 	return yy_current_state;
@@ -1455,44 +1678,47 @@ static int yy_get_next_buffer (void)
  * synopsis
  *	next_state = yy_try_NUL_trans( current_state );
  */
-    static yy_state_type yy_try_NUL_trans  (yy_state_type yy_current_state )
+    static yy_state_type yy_try_NUL_trans  (yy_state_type yy_current_state , yyscan_t yyscanner)
 {
 	int yy_is_jam;
-    	char *yy_cp = (yy_c_buf_p);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner; /* This var may be unused depending upon options. */
+	char *yy_cp = yyg->yy_c_buf_p;
 
 	YY_CHAR yy_c = 1;
 	if ( yy_accept[yy_current_state] )
 		{
-		(yy_last_accepting_state) = yy_current_state;
-		(yy_last_accepting_cpos) = yy_cp;
+		yyg->yy_last_accepting_state = yy_current_state;
+		yyg->yy_last_accepting_cpos = yy_cp;
 		}
 	while ( yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state )
 		{
 		yy_current_state = (int) yy_def[yy_current_state];
 		if ( yy_current_state >= 240 )
-			yy_c = yy_meta[(unsigned int) yy_c];
+			yy_c = yy_meta[yy_c];
 		}
-	yy_current_state = yy_nxt[yy_base[yy_current_state] + (unsigned int) yy_c];
+	yy_current_state = yy_nxt[yy_base[yy_current_state] + yy_c];
 	yy_is_jam = (yy_current_state == 239);
 
-		return yy_is_jam ? 0 : yy_current_state;
+	(void)yyg;
+	return yy_is_jam ? 0 : yy_current_state;
 }
 
 #ifndef YY_NO_UNPUT
 
-    static void yyunput (int c, char * yy_bp )
+    static void yyunput (int c, char * yy_bp , yyscan_t yyscanner)
 {
 	char *yy_cp;
-    
-    yy_cp = (yy_c_buf_p);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+    yy_cp = yyg->yy_c_buf_p;
 
-	/* undo effects of setting up ptxinfo_text */
-	*yy_cp = (yy_hold_char);
+	/* undo effects of setting up yytext */
+	*yy_cp = yyg->yy_hold_char;
 
 	if ( yy_cp < YY_CURRENT_BUFFER_LVALUE->yy_ch_buf + 2 )
 		{ /* need to shift things up to make room */
 		/* +2 for EOB chars. */
-		yy_size_t number_to_move = (yy_n_chars) + 2;
+		int number_to_move = yyg->yy_n_chars + 2;
 		char *dest = &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[
 					YY_CURRENT_BUFFER_LVALUE->yy_buf_size + 2];
 		char *source =
@@ -1504,7 +1730,7 @@ static int yy_get_next_buffer (void)
 		yy_cp += (int) (dest - source);
 		yy_bp += (int) (dest - source);
 		YY_CURRENT_BUFFER_LVALUE->yy_n_chars =
-			(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_buf_size;
+			yyg->yy_n_chars = (int) YY_CURRENT_BUFFER_LVALUE->yy_buf_size;
 
 		if ( yy_cp < YY_CURRENT_BUFFER_LVALUE->yy_ch_buf + 2 )
 			YY_FATAL_ERROR( "flex scanner push-back overflow" );
@@ -1513,44 +1739,45 @@ static int yy_get_next_buffer (void)
 	*--yy_cp = (char) c;
 
     if ( c == '\n' ){
-        --ptxinfo_lineno;
+        --yylineno;
     }
 
-	(yytext_ptr) = yy_bp;
-	(yy_hold_char) = *yy_cp;
-	(yy_c_buf_p) = yy_cp;
+	yyg->yytext_ptr = yy_bp;
+	yyg->yy_hold_char = *yy_cp;
+	yyg->yy_c_buf_p = yy_cp;
 }
 
 #endif
 
 #ifndef YY_NO_INPUT
 #ifdef __cplusplus
-    static int yyinput (void)
+    static int yyinput (yyscan_t yyscanner)
 #else
-    static int input  (void)
+    static int input  (yyscan_t yyscanner)
 #endif
 
 {
 	int c;
-    
-	*(yy_c_buf_p) = (yy_hold_char);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+	*yyg->yy_c_buf_p = yyg->yy_hold_char;
 
-	if ( *(yy_c_buf_p) == YY_END_OF_BUFFER_CHAR )
+	if ( *yyg->yy_c_buf_p == YY_END_OF_BUFFER_CHAR )
 		{
 		/* yy_c_buf_p now points to the character we want to return.
 		 * If this occurs *before* the EOB characters, then it's a
 		 * valid NUL; if not, then we've hit the end of the buffer.
 		 */
-		if ( (yy_c_buf_p) < &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[(yy_n_chars)] )
+		if ( yyg->yy_c_buf_p < &YY_CURRENT_BUFFER_LVALUE->yy_ch_buf[yyg->yy_n_chars] )
 			/* This was really a NUL. */
-			*(yy_c_buf_p) = '\0';
+			*yyg->yy_c_buf_p = '\0';
 
 		else
 			{ /* need more input */
-			yy_size_t offset = (yy_c_buf_p) - (yytext_ptr);
-			++(yy_c_buf_p);
+			int offset = (int) (yyg->yy_c_buf_p - yyg->yytext_ptr);
+			++yyg->yy_c_buf_p;
 
-			switch ( yy_get_next_buffer(  ) )
+			switch ( yy_get_next_buffer( yyscanner ) )
 				{
 				case EOB_ACT_LAST_MATCH:
 					/* This happens because yy_g_n_b()
@@ -1564,38 +1791,40 @@ static int yy_get_next_buffer (void)
 					 */
 
 					/* Reset buffer status. */
-					ptxinfo_restart(ptxinfo_in );
+					yyrestart( yyin , yyscanner);
 
 					/*FALLTHROUGH*/
 
 				case EOB_ACT_END_OF_FILE:
 					{
-					if ( ptxinfo_wrap( ) )
-						return EOF;
+					if ( yywrap( yyscanner ) )
+						return 0;
 
-					if ( ! (yy_did_buffer_switch_on_eof) )
+					if ( ! yyg->yy_did_buffer_switch_on_eof )
 						YY_NEW_FILE;
 #ifdef __cplusplus
-					return yyinput();
+					return yyinput(yyscanner);
 #else
-					return input();
+					return input(yyscanner);
 #endif
 					}
 
 				case EOB_ACT_CONTINUE_SCAN:
-					(yy_c_buf_p) = (yytext_ptr) + offset;
+					yyg->yy_c_buf_p = yyg->yytext_ptr + offset;
 					break;
 				}
 			}
 		}
 
-	c = *(unsigned char *) (yy_c_buf_p);	/* cast for 8-bit char's */
-	*(yy_c_buf_p) = '\0';	/* preserve ptxinfo_text */
-	(yy_hold_char) = *++(yy_c_buf_p);
+	c = *(unsigned char *) yyg->yy_c_buf_p;	/* cast for 8-bit char's */
+	*yyg->yy_c_buf_p = '\0';	/* preserve yytext */
+	yyg->yy_hold_char = *++yyg->yy_c_buf_p;
 
 	if ( c == '\n' )
-		   
-    ptxinfo_lineno++;
+		
+    do{ yylineno++;
+        yycolumn=0;
+    }while(0)
 ;
 
 	return c;
@@ -1604,102 +1833,106 @@ static int yy_get_next_buffer (void)
 
 /** Immediately switch to a different input stream.
  * @param input_file A readable stream.
- * 
+ * @param yyscanner The scanner object.
  * @note This function does not reset the start condition to @c INITIAL .
  */
-    void ptxinfo_restart  (FILE * input_file )
+    void yyrestart  (FILE * input_file , yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
 	if ( ! YY_CURRENT_BUFFER ){
-        ptxinfo_ensure_buffer_stack ();
+        yyensure_buffer_stack (yyscanner);
 		YY_CURRENT_BUFFER_LVALUE =
-            ptxinfo__create_buffer(ptxinfo_in,YY_BUF_SIZE );
+            yy_create_buffer( yyin, YY_BUF_SIZE , yyscanner);
 	}
 
-	ptxinfo__init_buffer(YY_CURRENT_BUFFER,input_file );
-	ptxinfo__load_buffer_state( );
+	yy_init_buffer( YY_CURRENT_BUFFER, input_file , yyscanner);
+	yy_load_buffer_state( yyscanner );
 }
 
 /** Switch to a different input buffer.
  * @param new_buffer The new input buffer.
- * 
+ * @param yyscanner The scanner object.
  */
-    void ptxinfo__switch_to_buffer  (YY_BUFFER_STATE  new_buffer )
+    void yy_switch_to_buffer  (YY_BUFFER_STATE  new_buffer , yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
 	/* TODO. We should be able to replace this entire function body
 	 * with
-	 *		ptxinfo_pop_buffer_state();
-	 *		ptxinfo_push_buffer_state(new_buffer);
+	 *		yypop_buffer_state();
+	 *		yypush_buffer_state(new_buffer);
      */
-	ptxinfo_ensure_buffer_stack ();
+	yyensure_buffer_stack (yyscanner);
 	if ( YY_CURRENT_BUFFER == new_buffer )
 		return;
 
 	if ( YY_CURRENT_BUFFER )
 		{
 		/* Flush out information for old buffer. */
-		*(yy_c_buf_p) = (yy_hold_char);
-		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = (yy_c_buf_p);
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		*yyg->yy_c_buf_p = yyg->yy_hold_char;
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = yyg->yy_c_buf_p;
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars;
 		}
 
 	YY_CURRENT_BUFFER_LVALUE = new_buffer;
-	ptxinfo__load_buffer_state( );
+	yy_load_buffer_state( yyscanner );
 
 	/* We don't actually know whether we did this switch during
-	 * EOF (ptxinfo_wrap()) processing, but the only time this flag
-	 * is looked at is after ptxinfo_wrap() is called, so it's safe
+	 * EOF (yywrap()) processing, but the only time this flag
+	 * is looked at is after yywrap() is called, so it's safe
 	 * to go ahead and always set it.
 	 */
-	(yy_did_buffer_switch_on_eof) = 1;
+	yyg->yy_did_buffer_switch_on_eof = 1;
 }
 
-static void ptxinfo__load_buffer_state  (void)
+static void yy_load_buffer_state  (yyscan_t yyscanner)
 {
-    	(yy_n_chars) = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
-	(yytext_ptr) = (yy_c_buf_p) = YY_CURRENT_BUFFER_LVALUE->yy_buf_pos;
-	ptxinfo_in = YY_CURRENT_BUFFER_LVALUE->yy_input_file;
-	(yy_hold_char) = *(yy_c_buf_p);
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	yyg->yy_n_chars = YY_CURRENT_BUFFER_LVALUE->yy_n_chars;
+	yyg->yytext_ptr = yyg->yy_c_buf_p = YY_CURRENT_BUFFER_LVALUE->yy_buf_pos;
+	yyin = YY_CURRENT_BUFFER_LVALUE->yy_input_file;
+	yyg->yy_hold_char = *yyg->yy_c_buf_p;
 }
 
 /** Allocate and initialize an input buffer state.
  * @param file A readable stream.
  * @param size The character buffer size in bytes. When in doubt, use @c YY_BUF_SIZE.
- * 
+ * @param yyscanner The scanner object.
  * @return the allocated buffer state.
  */
-    YY_BUFFER_STATE ptxinfo__create_buffer  (FILE * file, int  size )
+    YY_BUFFER_STATE yy_create_buffer  (FILE * file, int  size , yyscan_t yyscanner)
 {
 	YY_BUFFER_STATE b;
     
-	b = (YY_BUFFER_STATE) ptxinfo_alloc(sizeof( struct yy_buffer_state )  );
+	b = (YY_BUFFER_STATE) yyalloc( sizeof( struct yy_buffer_state ) , yyscanner );
 	if ( ! b )
-		YY_FATAL_ERROR( "out of dynamic memory in ptxinfo__create_buffer()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
 
-	b->yy_buf_size = (yy_size_t)size;
+	b->yy_buf_size = size;
 
 	/* yy_ch_buf has to be 2 characters longer than the size given because
 	 * we need to put in 2 end-of-buffer characters.
 	 */
-	b->yy_ch_buf = (char *) ptxinfo_alloc(b->yy_buf_size + 2  );
+	b->yy_ch_buf = (char *) yyalloc( (yy_size_t) (b->yy_buf_size + 2) , yyscanner );
 	if ( ! b->yy_ch_buf )
-		YY_FATAL_ERROR( "out of dynamic memory in ptxinfo__create_buffer()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_create_buffer()" );
 
 	b->yy_is_our_buffer = 1;
 
-	ptxinfo__init_buffer(b,file );
+	yy_init_buffer( b, file , yyscanner);
 
 	return b;
 }
 
 /** Destroy the buffer.
- * @param b a buffer created with ptxinfo__create_buffer()
- * 
+ * @param b a buffer created with yy_create_buffer()
+ * @param yyscanner The scanner object.
  */
-    void ptxinfo__delete_buffer (YY_BUFFER_STATE  b )
+    void yy_delete_buffer (YY_BUFFER_STATE  b , yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
 	if ( ! b )
 		return;
 
@@ -1707,27 +1940,28 @@ static void ptxinfo__load_buffer_state  (void)
 		YY_CURRENT_BUFFER_LVALUE = (YY_BUFFER_STATE) 0;
 
 	if ( b->yy_is_our_buffer )
-		ptxinfo_free((void *) b->yy_ch_buf  );
+		yyfree( (void *) b->yy_ch_buf , yyscanner );
 
-	ptxinfo_free((void *) b  );
+	yyfree( (void *) b , yyscanner );
 }
 
 /* Initializes or reinitializes a buffer.
  * This function is sometimes called more than once on the same buffer,
- * such as during a ptxinfo_restart() or at EOF.
+ * such as during a yyrestart() or at EOF.
  */
-    static void ptxinfo__init_buffer  (YY_BUFFER_STATE  b, FILE * file )
+    static void yy_init_buffer  (YY_BUFFER_STATE  b, FILE * file , yyscan_t yyscanner)
 
 {
 	int oerrno = errno;
-    
-	ptxinfo__flush_buffer(b );
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+	yy_flush_buffer( b , yyscanner);
 
 	b->yy_input_file = file;
 	b->yy_fill_buffer = 1;
 
-    /* If b is the current buffer, then ptxinfo__init_buffer was _probably_
-     * called from ptxinfo_restart() or through yy_get_next_buffer.
+    /* If b is the current buffer, then yy_init_buffer was _probably_
+     * called from yyrestart() or through yy_get_next_buffer.
      * In that case, we don't want to reset the lineno or column.
      */
     if (b != YY_CURRENT_BUFFER){
@@ -1742,11 +1976,12 @@ static void ptxinfo__load_buffer_state  (void)
 
 /** Discard all buffered characters. On the next scan, YY_INPUT will be called.
  * @param b the buffer state to be flushed, usually @c YY_CURRENT_BUFFER.
- * 
+ * @param yyscanner The scanner object.
  */
-    void ptxinfo__flush_buffer (YY_BUFFER_STATE  b )
+    void yy_flush_buffer (YY_BUFFER_STATE  b , yyscan_t yyscanner)
 {
-    	if ( ! b )
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	if ( ! b )
 		return;
 
 	b->yy_n_chars = 0;
@@ -1764,114 +1999,117 @@ static void ptxinfo__load_buffer_state  (void)
 	b->yy_buffer_status = YY_BUFFER_NEW;
 
 	if ( b == YY_CURRENT_BUFFER )
-		ptxinfo__load_buffer_state( );
+		yy_load_buffer_state( yyscanner );
 }
 
 /** Pushes the new state onto the stack. The new state becomes
  *  the current state. This function will allocate the stack
  *  if necessary.
  *  @param new_buffer The new state.
- *  
+ *  @param yyscanner The scanner object.
  */
-void ptxinfo_push_buffer_state (YY_BUFFER_STATE new_buffer )
+void yypush_buffer_state (YY_BUFFER_STATE new_buffer , yyscan_t yyscanner)
 {
-    	if (new_buffer == NULL)
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	if (new_buffer == NULL)
 		return;
 
-	ptxinfo_ensure_buffer_stack();
+	yyensure_buffer_stack(yyscanner);
 
-	/* This block is copied from ptxinfo__switch_to_buffer. */
+	/* This block is copied from yy_switch_to_buffer. */
 	if ( YY_CURRENT_BUFFER )
 		{
 		/* Flush out information for old buffer. */
-		*(yy_c_buf_p) = (yy_hold_char);
-		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = (yy_c_buf_p);
-		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = (yy_n_chars);
+		*yyg->yy_c_buf_p = yyg->yy_hold_char;
+		YY_CURRENT_BUFFER_LVALUE->yy_buf_pos = yyg->yy_c_buf_p;
+		YY_CURRENT_BUFFER_LVALUE->yy_n_chars = yyg->yy_n_chars;
 		}
 
 	/* Only push if top exists. Otherwise, replace top. */
 	if (YY_CURRENT_BUFFER)
-		(yy_buffer_stack_top)++;
+		yyg->yy_buffer_stack_top++;
 	YY_CURRENT_BUFFER_LVALUE = new_buffer;
 
-	/* copied from ptxinfo__switch_to_buffer. */
-	ptxinfo__load_buffer_state( );
-	(yy_did_buffer_switch_on_eof) = 1;
+	/* copied from yy_switch_to_buffer. */
+	yy_load_buffer_state( yyscanner );
+	yyg->yy_did_buffer_switch_on_eof = 1;
 }
 
 /** Removes and deletes the top of the stack, if present.
  *  The next element becomes the new top.
- *  
+ *  @param yyscanner The scanner object.
  */
-void ptxinfo_pop_buffer_state (void)
+void yypop_buffer_state (yyscan_t yyscanner)
 {
-    	if (!YY_CURRENT_BUFFER)
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	if (!YY_CURRENT_BUFFER)
 		return;
 
-	ptxinfo__delete_buffer(YY_CURRENT_BUFFER );
+	yy_delete_buffer(YY_CURRENT_BUFFER , yyscanner);
 	YY_CURRENT_BUFFER_LVALUE = NULL;
-	if ((yy_buffer_stack_top) > 0)
-		--(yy_buffer_stack_top);
+	if (yyg->yy_buffer_stack_top > 0)
+		--yyg->yy_buffer_stack_top;
 
 	if (YY_CURRENT_BUFFER) {
-		ptxinfo__load_buffer_state( );
-		(yy_did_buffer_switch_on_eof) = 1;
+		yy_load_buffer_state( yyscanner );
+		yyg->yy_did_buffer_switch_on_eof = 1;
 	}
 }
 
 /* Allocates the stack if it does not exist.
  *  Guarantees space for at least one push.
  */
-static void ptxinfo_ensure_buffer_stack (void)
+static void yyensure_buffer_stack (yyscan_t yyscanner)
 {
 	yy_size_t num_to_alloc;
-    
-	if (!(yy_buffer_stack)) {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+	if (!yyg->yy_buffer_stack) {
 
 		/* First allocation is just for 2 elements, since we don't know if this
 		 * scanner will even need a stack. We use 2 instead of 1 to avoid an
 		 * immediate realloc on the next call.
          */
-		num_to_alloc = 1; /* After all that talk, this was set to 1 anyways... */
-		(yy_buffer_stack) = (struct yy_buffer_state**)ptxinfo_alloc
+      num_to_alloc = 1; /* After all that talk, this was set to 1 anyways... */
+		yyg->yy_buffer_stack = (struct yy_buffer_state**)yyalloc
 								(num_to_alloc * sizeof(struct yy_buffer_state*)
-								);
-		if ( ! (yy_buffer_stack) )
-			YY_FATAL_ERROR( "out of dynamic memory in ptxinfo_ensure_buffer_stack()" );
-								  
-		memset((yy_buffer_stack), 0, num_to_alloc * sizeof(struct yy_buffer_state*));
-				
-		(yy_buffer_stack_max) = num_to_alloc;
-		(yy_buffer_stack_top) = 0;
+								, yyscanner);
+		if ( ! yyg->yy_buffer_stack )
+			YY_FATAL_ERROR( "out of dynamic memory in yyensure_buffer_stack()" );
+
+		memset(yyg->yy_buffer_stack, 0, num_to_alloc * sizeof(struct yy_buffer_state*));
+
+		yyg->yy_buffer_stack_max = num_to_alloc;
+		yyg->yy_buffer_stack_top = 0;
 		return;
 	}
 
-	if ((yy_buffer_stack_top) >= ((yy_buffer_stack_max)) - 1){
+	if (yyg->yy_buffer_stack_top >= (yyg->yy_buffer_stack_max) - 1){
 
 		/* Increase the buffer to prepare for a possible push. */
 		yy_size_t grow_size = 8 /* arbitrary grow size */;
 
-		num_to_alloc = (yy_buffer_stack_max) + grow_size;
-		(yy_buffer_stack) = (struct yy_buffer_state**)ptxinfo_realloc
-								((yy_buffer_stack),
+		num_to_alloc = yyg->yy_buffer_stack_max + grow_size;
+		yyg->yy_buffer_stack = (struct yy_buffer_state**)yyrealloc
+								(yyg->yy_buffer_stack,
 								num_to_alloc * sizeof(struct yy_buffer_state*)
-								);
-		if ( ! (yy_buffer_stack) )
-			YY_FATAL_ERROR( "out of dynamic memory in ptxinfo_ensure_buffer_stack()" );
+								, yyscanner);
+		if ( ! yyg->yy_buffer_stack )
+			YY_FATAL_ERROR( "out of dynamic memory in yyensure_buffer_stack()" );
 
 		/* zero only the new slots.*/
-		memset((yy_buffer_stack) + (yy_buffer_stack_max), 0, grow_size * sizeof(struct yy_buffer_state*));
-		(yy_buffer_stack_max) = num_to_alloc;
+		memset(yyg->yy_buffer_stack + yyg->yy_buffer_stack_max, 0, grow_size * sizeof(struct yy_buffer_state*));
+		yyg->yy_buffer_stack_max = num_to_alloc;
 	}
 }
 
 /** Setup the input buffer state to scan directly from a user-specified character buffer.
  * @param base the character buffer
  * @param size the size in bytes of the character buffer
- * 
- * @return the newly allocated buffer state object. 
+ * @param yyscanner The scanner object.
+ * @return the newly allocated buffer state object.
  */
-YY_BUFFER_STATE ptxinfo__scan_buffer  (char * base, yy_size_t  size )
+YY_BUFFER_STATE yy_scan_buffer  (char * base, yy_size_t  size , yyscan_t yyscanner)
 {
 	YY_BUFFER_STATE b;
     
@@ -1879,69 +2117,69 @@ YY_BUFFER_STATE ptxinfo__scan_buffer  (char * base, yy_size_t  size )
 	     base[size-2] != YY_END_OF_BUFFER_CHAR ||
 	     base[size-1] != YY_END_OF_BUFFER_CHAR )
 		/* They forgot to leave room for the EOB's. */
-		return 0;
+		return NULL;
 
-	b = (YY_BUFFER_STATE) ptxinfo_alloc(sizeof( struct yy_buffer_state )  );
+	b = (YY_BUFFER_STATE) yyalloc( sizeof( struct yy_buffer_state ) , yyscanner );
 	if ( ! b )
-		YY_FATAL_ERROR( "out of dynamic memory in ptxinfo__scan_buffer()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_buffer()" );
 
-	b->yy_buf_size = size - 2;	/* "- 2" to take care of EOB's */
+	b->yy_buf_size = (int) (size - 2);	/* "- 2" to take care of EOB's */
 	b->yy_buf_pos = b->yy_ch_buf = base;
 	b->yy_is_our_buffer = 0;
-	b->yy_input_file = 0;
+	b->yy_input_file = NULL;
 	b->yy_n_chars = b->yy_buf_size;
 	b->yy_is_interactive = 0;
 	b->yy_at_bol = 1;
 	b->yy_fill_buffer = 0;
 	b->yy_buffer_status = YY_BUFFER_NEW;
 
-	ptxinfo__switch_to_buffer(b  );
+	yy_switch_to_buffer( b , yyscanner );
 
 	return b;
 }
 
-/** Setup the input buffer state to scan a string. The next call to ptxinfo_lex() will
+/** Setup the input buffer state to scan a string. The next call to yylex() will
  * scan from a @e copy of @a str.
  * @param yystr a NUL-terminated string to scan
- * 
+ * @param yyscanner The scanner object.
  * @return the newly allocated buffer state object.
  * @note If you want to scan bytes that may contain NUL values, then use
- *       ptxinfo__scan_bytes() instead.
+ *       yy_scan_bytes() instead.
  */
-YY_BUFFER_STATE ptxinfo__scan_string (yyconst char * yystr )
+YY_BUFFER_STATE yy_scan_string (const char * yystr , yyscan_t yyscanner)
 {
     
-	return ptxinfo__scan_bytes(yystr,strlen(yystr) );
+	return yy_scan_bytes( yystr, (int) strlen(yystr) , yyscanner);
 }
 
-/** Setup the input buffer state to scan the given bytes. The next call to ptxinfo_lex() will
+/** Setup the input buffer state to scan the given bytes. The next call to yylex() will
  * scan from a @e copy of @a bytes.
  * @param yybytes the byte buffer to scan
  * @param _yybytes_len the number of bytes in the buffer pointed to by @a bytes.
- * 
+ * @param yyscanner The scanner object.
  * @return the newly allocated buffer state object.
  */
-YY_BUFFER_STATE ptxinfo__scan_bytes  (yyconst char * yybytes, yy_size_t  _yybytes_len )
+YY_BUFFER_STATE yy_scan_bytes  (const char * yybytes, int  _yybytes_len , yyscan_t yyscanner)
 {
 	YY_BUFFER_STATE b;
 	char *buf;
 	yy_size_t n;
-	yy_size_t i;
+	int i;
     
 	/* Get memory for full buffer, including space for trailing EOB's. */
-	n = _yybytes_len + 2;
-	buf = (char *) ptxinfo_alloc(n  );
+	n = (yy_size_t) (_yybytes_len + 2);
+	buf = (char *) yyalloc( n , yyscanner );
 	if ( ! buf )
-		YY_FATAL_ERROR( "out of dynamic memory in ptxinfo__scan_bytes()" );
+		YY_FATAL_ERROR( "out of dynamic memory in yy_scan_bytes()" );
 
 	for ( i = 0; i < _yybytes_len; ++i )
 		buf[i] = yybytes[i];
 
 	buf[_yybytes_len] = buf[_yybytes_len+1] = YY_END_OF_BUFFER_CHAR;
 
-	b = ptxinfo__scan_buffer(buf,n );
+	b = yy_scan_buffer( buf, n , yyscanner);
 	if ( ! b )
-		YY_FATAL_ERROR( "bad buffer in ptxinfo__scan_bytes()" );
+		YY_FATAL_ERROR( "bad buffer in yy_scan_bytes()" );
 
 	/* It's okay to grow etc. this buffer, and we should throw it
 	 * away when we're done.
@@ -1955,9 +2193,11 @@ YY_BUFFER_STATE ptxinfo__scan_bytes  (yyconst char * yybytes, yy_size_t  _yybyte
 #define YY_EXIT_FAILURE 2
 #endif
 
-static void yy_fatal_error (yyconst char* msg )
+static void yynoreturn yy_fatal_error (const char* msg , yyscan_t yyscanner)
 {
-			(void) fprintf( stderr, "%s\n", msg );
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+	fprintf( stderr, "%s\n", msg );
 	exit( YY_EXIT_FAILURE );
 }
 
@@ -1967,147 +2207,295 @@ static void yy_fatal_error (yyconst char* msg )
 #define yyless(n) \
 	do \
 		{ \
-		/* Undo effects of setting up ptxinfo_text. */ \
+		/* Undo effects of setting up yytext. */ \
         int yyless_macro_arg = (n); \
         YY_LESS_LINENO(yyless_macro_arg);\
-		ptxinfo_text[ptxinfo_leng] = (yy_hold_char); \
-		(yy_c_buf_p) = ptxinfo_text + yyless_macro_arg; \
-		(yy_hold_char) = *(yy_c_buf_p); \
-		*(yy_c_buf_p) = '\0'; \
-		ptxinfo_leng = yyless_macro_arg; \
+		yytext[yyleng] = yyg->yy_hold_char; \
+		yyg->yy_c_buf_p = yytext + yyless_macro_arg; \
+		yyg->yy_hold_char = *yyg->yy_c_buf_p; \
+		*yyg->yy_c_buf_p = '\0'; \
+		yyleng = yyless_macro_arg; \
 		} \
 	while ( 0 )
 
 /* Accessor  methods (get/set functions) to struct members. */
 
+/** Get the user-defined data for this scanner.
+ * @param yyscanner The scanner object.
+ */
+YY_EXTRA_TYPE yyget_extra  (yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyextra;
+}
+
 /** Get the current line number.
- * 
+ * @param yyscanner The scanner object.
+ */
+int yyget_lineno  (yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        if (! YY_CURRENT_BUFFER)
+            return 0;
+    
+    return yylineno;
+}
+
+/** Get the current column number.
+ * @param yyscanner The scanner object.
  */
-int ptxinfo_get_lineno  (void)
+int yyget_column  (yyscan_t yyscanner)
 {
-        
-    return ptxinfo_lineno;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        if (! YY_CURRENT_BUFFER)
+            return 0;
+    
+    return yycolumn;
 }
 
 /** Get the input stream.
- * 
+ * @param yyscanner The scanner object.
  */
-FILE *ptxinfo_get_in  (void)
+FILE *yyget_in  (yyscan_t yyscanner)
 {
-        return ptxinfo_in;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyin;
 }
 
 /** Get the output stream.
- * 
+ * @param yyscanner The scanner object.
  */
-FILE *ptxinfo_get_out  (void)
+FILE *yyget_out  (yyscan_t yyscanner)
 {
-        return ptxinfo_out;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyout;
 }
 
 /** Get the length of the current token.
- * 
+ * @param yyscanner The scanner object.
  */
-yy_size_t ptxinfo_get_leng  (void)
+int yyget_leng  (yyscan_t yyscanner)
 {
-        return ptxinfo_leng;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yyleng;
 }
 
 /** Get the current token.
- * 
+ * @param yyscanner The scanner object.
  */
 
-char *ptxinfo_get_text  (void)
+char *yyget_text  (yyscan_t yyscanner)
 {
-        return ptxinfo_text;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yytext;
+}
+
+/** Set the user-defined data. This data is never touched by the scanner.
+ * @param user_defined The data to be associated with this scanner.
+ * @param yyscanner The scanner object.
+ */
+void yyset_extra (YY_EXTRA_TYPE  user_defined , yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yyextra = user_defined ;
 }
 
 /** Set the current line number.
  * @param _line_number line number
- * 
+ * @param yyscanner The scanner object.
+ */
+void yyset_lineno (int  _line_number , yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        /* lineno is only valid if an input buffer exists. */
+        if (! YY_CURRENT_BUFFER )
+           YY_FATAL_ERROR( "yyset_lineno called with no buffer" );
+    
+    yylineno = _line_number;
+}
+
+/** Set the current column.
+ * @param _column_no column number
+ * @param yyscanner The scanner object.
  */
-void ptxinfo_set_lineno (int  _line_number )
+void yyset_column (int  _column_no , yyscan_t yyscanner)
 {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
+        /* column is only valid if an input buffer exists. */
+        if (! YY_CURRENT_BUFFER )
+           YY_FATAL_ERROR( "yyset_column called with no buffer" );
     
-    ptxinfo_lineno = _line_number;
+    yycolumn = _column_no;
 }
 
 /** Set the input stream. This does not discard the current
  * input buffer.
  * @param _in_str A readable stream.
- * 
- * @see ptxinfo__switch_to_buffer
+ * @param yyscanner The scanner object.
+ * @see yy_switch_to_buffer
  */
-void ptxinfo_set_in (FILE *  _in_str )
+void yyset_in (FILE *  _in_str , yyscan_t yyscanner)
 {
-        ptxinfo_in = _in_str ;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yyin = _in_str ;
 }
 
-void ptxinfo_set_out (FILE *  _out_str )
+void yyset_out (FILE *  _out_str , yyscan_t yyscanner)
 {
-        ptxinfo_out = _out_str ;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yyout = _out_str ;
 }
 
-int ptxinfo_get_debug  (void)
+int yyget_debug  (yyscan_t yyscanner)
 {
-        return ptxinfo__flex_debug;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yy_flex_debug;
 }
 
-void ptxinfo_set_debug (int  _bdebug )
+void yyset_debug (int  _bdebug , yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yy_flex_debug = _bdebug ;
+}
+
+/* Accessor methods for yylval and yylloc */
+
+YYSTYPE * yyget_lval  (yyscan_t yyscanner)
 {
-        ptxinfo__flex_debug = _bdebug ;
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    return yylval;
 }
 
-static int yy_init_globals (void)
+void yyset_lval (YYSTYPE *  yylval_param , yyscan_t yyscanner)
 {
-        /* Initialization is the same as for the non-reentrant scanner.
-     * This function is called from ptxinfo_lex_destroy(), so don't allocate here.
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    yylval = yylval_param;
+}
+
+/* User-visible API */
+
+/* yylex_init is special because it creates the scanner itself, so it is
+ * the ONLY reentrant function that doesn't take the scanner as the last argument.
+ * That's why we explicitly handle the declaration, instead of using our macros.
+ */
+int yylex_init(yyscan_t* ptr_yy_globals)
+{
+    if (ptr_yy_globals == NULL){
+        errno = EINVAL;
+        return 1;
+    }
+
+    *ptr_yy_globals = (yyscan_t) yyalloc ( sizeof( struct yyguts_t ), NULL );
+
+    if (*ptr_yy_globals == NULL){
+        errno = ENOMEM;
+        return 1;
+    }
+
+    /* By setting to 0xAA, we expose bugs in yy_init_globals. Leave at 0x00 for releases. */
+    memset(*ptr_yy_globals,0x00,sizeof(struct yyguts_t));
+
+    return yy_init_globals ( *ptr_yy_globals );
+}
+
+/* yylex_init_extra has the same functionality as yylex_init, but follows the
+ * convention of taking the scanner as the last argument. Note however, that
+ * this is a *pointer* to a scanner, as it will be allocated by this call (and
+ * is the reason, too, why this function also must handle its own declaration).
+ * The user defined value in the first argument will be available to yyalloc in
+ * the yyextra field.
+ */
+int yylex_init_extra( YY_EXTRA_TYPE yy_user_defined, yyscan_t* ptr_yy_globals )
+{
+    struct yyguts_t dummy_yyguts;
+
+    yyset_extra (yy_user_defined, &dummy_yyguts);
+
+    if (ptr_yy_globals == NULL){
+        errno = EINVAL;
+        return 1;
+    }
+
+    *ptr_yy_globals = (yyscan_t) yyalloc ( sizeof( struct yyguts_t ), &dummy_yyguts );
+
+    if (*ptr_yy_globals == NULL){
+        errno = ENOMEM;
+        return 1;
+    }
+
+    /* By setting to 0xAA, we expose bugs in
+    yy_init_globals. Leave at 0x00 for releases. */
+    memset(*ptr_yy_globals,0x00,sizeof(struct yyguts_t));
+
+    yyset_extra (yy_user_defined, *ptr_yy_globals);
+
+    return yy_init_globals ( *ptr_yy_globals );
+}
+
+static int yy_init_globals (yyscan_t yyscanner)
+{
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+    /* Initialization is the same as for the non-reentrant scanner.
+     * This function is called from yylex_destroy(), so don't allocate here.
      */
 
-    /* We do not touch ptxinfo_lineno unless the option is enabled. */
-    ptxinfo_lineno =  1;
-    
-    (yy_buffer_stack) = 0;
-    (yy_buffer_stack_top) = 0;
-    (yy_buffer_stack_max) = 0;
-    (yy_c_buf_p) = (char *) 0;
-    (yy_init) = 0;
-    (yy_start) = 0;
+    yyg->yy_buffer_stack = NULL;
+    yyg->yy_buffer_stack_top = 0;
+    yyg->yy_buffer_stack_max = 0;
+    yyg->yy_c_buf_p = NULL;
+    yyg->yy_init = 0;
+    yyg->yy_start = 0;
+
+    yyg->yy_start_stack_ptr = 0;
+    yyg->yy_start_stack_depth = 0;
+    yyg->yy_start_stack =  NULL;
 
 /* Defined in main.c */
 #ifdef YY_STDINIT
-    ptxinfo_in = stdin;
-    ptxinfo_out = stdout;
+    yyin = stdin;
+    yyout = stdout;
 #else
-    ptxinfo_in = (FILE *) 0;
-    ptxinfo_out = (FILE *) 0;
+    yyin = NULL;
+    yyout = NULL;
 #endif
 
     /* For future reference: Set errno on error, since we are called by
-     * ptxinfo_lex_init()
+     * yylex_init()
      */
     return 0;
 }
 
-/* ptxinfo_lex_destroy is for both reentrant and non-reentrant scanners. */
-int ptxinfo_lex_destroy  (void)
+/* yylex_destroy is for both reentrant and non-reentrant scanners. */
+int yylex_destroy  (yyscan_t yyscanner)
 {
-    
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+
     /* Pop the buffer stack, destroying each element. */
 	while(YY_CURRENT_BUFFER){
-		ptxinfo__delete_buffer(YY_CURRENT_BUFFER  );
+		yy_delete_buffer( YY_CURRENT_BUFFER , yyscanner );
 		YY_CURRENT_BUFFER_LVALUE = NULL;
-		ptxinfo_pop_buffer_state();
+		yypop_buffer_state(yyscanner);
 	}
 
 	/* Destroy the stack itself. */
-	ptxinfo_free((yy_buffer_stack) );
-	(yy_buffer_stack) = NULL;
+	yyfree(yyg->yy_buffer_stack , yyscanner);
+	yyg->yy_buffer_stack = NULL;
+
+    /* Destroy the start condition stack. */
+        yyfree( yyg->yy_start_stack , yyscanner );
+        yyg->yy_start_stack = NULL;
 
     /* Reset the globals. This is important in a non-reentrant scanner so the next time
-     * ptxinfo_lex() is called, initialization will occur. */
-    yy_init_globals( );
+     * yylex() is called, initialization will occur. */
+    yy_init_globals( yyscanner);
 
+    /* Destroy the main struct (reentrant only). */
+    yyfree ( yyscanner , yyscanner );
+    yyscanner = NULL;
     return 0;
 }
 
@@ -2116,9 +2504,11 @@ int ptxinfo_lex_destroy  (void)
  */
 
 #ifndef yytext_ptr
-static void yy_flex_strncpy (char* s1, yyconst char * s2, int n )
+static void yy_flex_strncpy (char* s1, const char * s2, int n , yyscan_t yyscanner)
 {
-		
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+
 	int i;
 	for ( i = 0; i < n; ++i )
 		s1[i] = s2[i];
@@ -2126,7 +2516,7 @@ static void yy_flex_strncpy (char* s1, yyconst char * s2, int n )
 #endif
 
 #ifdef YY_NEED_STRLEN
-static int yy_flex_strlen (yyconst char * s )
+static int yy_flex_strlen (const char * s , yyscan_t yyscanner)
 {
 	int n;
 	for ( n = 0; s[n]; ++n )
@@ -2136,14 +2526,18 @@ static int yy_flex_strlen (yyconst char * s )
 }
 #endif
 
-void *ptxinfo_alloc (yy_size_t  size )
+void *yyalloc (yy_size_t  size , yyscan_t yyscanner)
 {
-			return (void *) malloc( size );
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+	return malloc(size);
 }
 
-void *ptxinfo_realloc  (void * ptr, yy_size_t  size )
+void *yyrealloc  (void * ptr, yy_size_t  size , yyscan_t yyscanner)
 {
-		
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+
 	/* The cast to (char *) in the following accommodates both
 	 * implementations that use char* generic pointers, and those
 	 * that use void* generic pointers.  It works with the latter
@@ -2151,36 +2545,34 @@ void *ptxinfo_realloc  (void * ptr, yy_size_t  size )
 	 * any pointer type to void*, and deal with argument conversions
 	 * as though doing an assignment.
 	 */
-	return (void *) realloc( (char *) ptr, size );
+	return realloc(ptr, size);
 }
 
-void ptxinfo_free (void * ptr )
+void yyfree (void * ptr , yyscan_t yyscanner)
 {
-			free( (char *) ptr );	/* see ptxinfo_realloc() for (char *) cast */
+	struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
+	(void)yyg;
+	free( (char *) ptr );	/* see yyrealloc() for (char *) cast */
 }
 
 #define YYTABLES_NAME "yytables"
 
-#line 83 "ptxinfo.l"
-
-
+#line 89 "ptxinfo.l"
 
-extern int g_ptxinfo_error_detected;
-extern const char *g_filename;
-extern const char *g_ptxinfo_filename;
 
-int ptxinfo_error( const char *s )
+int ptxinfo_error(yyscan_t yyscanner, ptxinfo_data* ptxinfo, const char* msg)
 {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
 	int i;
-	g_ptxinfo_error_detected = 1;
+	ptxinfo->gpgpu_ctx->func_sim->g_ptxinfo_error_detected = 1;
 	fflush(stdout);
 	printf("GPGPU-Sim: ERROR while parsing output of ptxas (used to capture resource usage information)\n");
-	if( s != NULL )
-		printf("GPGPU-Sim:     %s (%s:%u) Syntax error:\n\n", g_filename, g_ptxinfo_filename, ptxinfo_lineno );
-	printf("   %s\n", ptxinfo_linebuf );
+	if( msg != NULL )
+		printf("GPGPU-Sim:     %s (%s:%u) Syntax error:\n\n", ptxinfo->gpgpu_ctx->g_filename, ptxinfo->g_ptxinfo_filename, yylineno );
+	printf("   %s\n", ptxinfo->linebuf );
 	printf("   ");
-	for( i=0; i < ptxinfo_col-1; i++ ) {
-		if( ptxinfo_linebuf[i] == '\t' ) printf("\t");
+	for( i=0; i < ptxinfo->col-1; i++ ) {
+		if( ptxinfo->linebuf[i] == '\t' ) printf("\t");
 		else printf(" ");
 	}
 			
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.cc
index 9554f55a73..be4b43d2f3 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.cc
@@ -27,138 +27,155 @@
 
 #include "memory.h"
 #include <stdlib.h>
+#include "../../libcuda_sim/gpgpu_context.h"
 #include "../debug.h"
 
-template<unsigned BSIZE> memory_space_impl<BSIZE>::memory_space_impl( std::string name, unsigned hash_size )
-{
-   m_name = name;
-   MEM_MAP_RESIZE(hash_size);
-
-   m_log2_block_size = -1;
-   for( unsigned n=0, mask=1; mask != 0; mask <<= 1, n++ ) {
-      if( BSIZE & mask ) {
-         assert( m_log2_block_size == (unsigned)-1 );
-         m_log2_block_size = n; 
-      }
-   }
-   assert( m_log2_block_size != (unsigned)-1 );
+template <unsigned BSIZE>
+memory_space_impl<BSIZE>::memory_space_impl(std::string name,
+                                            unsigned hash_size) {
+  m_name = name;
+  MEM_MAP_RESIZE(hash_size);
+
+  m_log2_block_size = -1;
+  for (unsigned n = 0, mask = 1; mask != 0; mask <<= 1, n++) {
+    if (BSIZE & mask) {
+      assert(m_log2_block_size == (unsigned)-1);
+      m_log2_block_size = n;
+    }
+  }
+  assert(m_log2_block_size != (unsigned)-1);
 }
 
-template<unsigned BSIZE> void memory_space_impl<BSIZE>::write_only( mem_addr_t offset, mem_addr_t index, size_t length, const void *data)
-{
-   m_data[index].write(offset,length,(const unsigned char*)data);
+template <unsigned BSIZE>
+void memory_space_impl<BSIZE>::write_only(mem_addr_t offset, mem_addr_t index,
+                                          size_t length, const void *data) {
+  m_data[index].write(offset, length, (const unsigned char *)data);
 }
 
-template<unsigned BSIZE> void memory_space_impl<BSIZE>::write( mem_addr_t addr, size_t length, const void *data, class ptx_thread_info *thd, const ptx_instruction *pI)
-{
-
-   mem_addr_t index = addr >> m_log2_block_size;
-
-   if ( (addr+length) <= (index+1)*BSIZE ) {
-      // fast route for intra-block access 
-      unsigned offset = addr & (BSIZE-1);
-      unsigned nbytes = length;
-      m_data[index].write(offset,nbytes,(const unsigned char*)data);
-   } else {
-      // slow route for inter-block access
-      unsigned nbytes_remain = length;
-      unsigned src_offset = 0; 
-      mem_addr_t current_addr = addr; 
-
-      while (nbytes_remain > 0) {
-         unsigned offset = current_addr & (BSIZE-1);
-         mem_addr_t page = current_addr >> m_log2_block_size; 
-         mem_addr_t access_limit = offset + nbytes_remain; 
-         if (access_limit > BSIZE) {
-            access_limit = BSIZE;
-         } 
-         
-         size_t tx_bytes = access_limit - offset; 
-         m_data[page].write(offset, tx_bytes, &((const unsigned char*)data)[src_offset]);
-
-         // advance pointers 
-         src_offset += tx_bytes; 
-         current_addr += tx_bytes; 
-         nbytes_remain -= tx_bytes; 
-      }
-      assert(nbytes_remain == 0); 
-   }
-   if( !m_watchpoints.empty() ) {
-      std::map<unsigned,mem_addr_t>::iterator i;
-      for( i=m_watchpoints.begin(); i!=m_watchpoints.end(); i++ ) {
-         mem_addr_t wa = i->second;
-         if( ((addr<=wa) && ((addr+length)>wa)) || ((addr>wa) && (addr < (wa+4))) ) 
-            hit_watchpoint(i->first,thd,pI);
+template <unsigned BSIZE>
+void memory_space_impl<BSIZE>::write(mem_addr_t addr, size_t length,
+                                     const void *data,
+                                     class ptx_thread_info *thd,
+                                     const ptx_instruction *pI) {
+  mem_addr_t index = addr >> m_log2_block_size;
+
+  if ((addr + length) <= (index + 1) * BSIZE) {
+    // fast route for intra-block access
+    unsigned offset = addr & (BSIZE - 1);
+    unsigned nbytes = length;
+    m_data[index].write(offset, nbytes, (const unsigned char *)data);
+  } else {
+    // slow route for inter-block access
+    unsigned nbytes_remain = length;
+    unsigned src_offset = 0;
+    mem_addr_t current_addr = addr;
+
+    while (nbytes_remain > 0) {
+      unsigned offset = current_addr & (BSIZE - 1);
+      mem_addr_t page = current_addr >> m_log2_block_size;
+      mem_addr_t access_limit = offset + nbytes_remain;
+      if (access_limit > BSIZE) {
+        access_limit = BSIZE;
       }
-   }
+
+      size_t tx_bytes = access_limit - offset;
+      m_data[page].write(offset, tx_bytes,
+                         &((const unsigned char *)data)[src_offset]);
+
+      // advance pointers
+      src_offset += tx_bytes;
+      current_addr += tx_bytes;
+      nbytes_remain -= tx_bytes;
+    }
+    assert(nbytes_remain == 0);
+  }
+  if (!m_watchpoints.empty()) {
+    std::map<unsigned, mem_addr_t>::iterator i;
+    for (i = m_watchpoints.begin(); i != m_watchpoints.end(); i++) {
+      mem_addr_t wa = i->second;
+      if (((addr <= wa) && ((addr + length) > wa)) ||
+          ((addr > wa) && (addr < (wa + 4))))
+        thd->get_gpu()->gpgpu_ctx->the_gpgpusim->g_the_gpu->hit_watchpoint(
+            i->first, thd, pI);
+    }
+  }
 }
 
-template<unsigned BSIZE> void memory_space_impl<BSIZE>::read_single_block( mem_addr_t blk_idx, mem_addr_t addr, size_t length, void *data) const
-{
-   if ((addr + length) > (blk_idx + 1) * BSIZE) {
-      printf("GPGPU-Sim PTX: ERROR * access to memory \'%s\' is unaligned : addr=0x%x, length=%zu\n",
-             m_name.c_str(), addr, length);
-      printf("GPGPU-Sim PTX: (addr+length)=0x%lx > 0x%x=(index+1)*BSIZE, index=0x%x, BSIZE=0x%x\n",
-             (addr+length),(blk_idx+1)*BSIZE, blk_idx, BSIZE);
-      throw 1;
-   }
-   typename map_t::const_iterator i = m_data.find(blk_idx);
-   if( i == m_data.end() ) {
-      for( size_t n=0; n < length; n++ ) 
-         ((unsigned char*)data)[n] = (unsigned char) 0;
-      //printf("GPGPU-Sim PTX:  WARNING reading %zu bytes from unititialized memory at address 0x%x in space %s\n", length, addr, m_name.c_str() );
-   } else {
-      unsigned offset = addr & (BSIZE-1);
-      unsigned nbytes = length;
-      i->second.read(offset,nbytes,(unsigned char*)data);
-   }
+template <unsigned BSIZE>
+void memory_space_impl<BSIZE>::read_single_block(mem_addr_t blk_idx,
+                                                 mem_addr_t addr, size_t length,
+                                                 void *data) const {
+  if ((addr + length) > (blk_idx + 1) * BSIZE) {
+    printf(
+        "GPGPU-Sim PTX: ERROR * access to memory \'%s\' is unaligned : "
+        "addr=0x%x, length=%zu\n",
+        m_name.c_str(), addr, length);
+    printf(
+        "GPGPU-Sim PTX: (addr+length)=0x%lx > 0x%x=(index+1)*BSIZE, "
+        "index=0x%x, BSIZE=0x%x\n",
+        (addr + length), (blk_idx + 1) * BSIZE, blk_idx, BSIZE);
+    throw 1;
+  }
+  typename map_t::const_iterator i = m_data.find(blk_idx);
+  if (i == m_data.end()) {
+    for (size_t n = 0; n < length; n++)
+      ((unsigned char *)data)[n] = (unsigned char)0;
+    // printf("GPGPU-Sim PTX:  WARNING reading %zu bytes from unititialized
+    // memory at address 0x%x in space %s\n", length, addr, m_name.c_str() );
+  } else {
+    unsigned offset = addr & (BSIZE - 1);
+    unsigned nbytes = length;
+    i->second.read(offset, nbytes, (unsigned char *)data);
+  }
 }
 
-template<unsigned BSIZE> void memory_space_impl<BSIZE>::read( mem_addr_t addr, size_t length, void *data ) const
-{
-   mem_addr_t index = addr >> m_log2_block_size;
-   if ((addr+length) <= (index+1)*BSIZE ) {
-      // fast route for intra-block access 
-      read_single_block(index, addr, length, data); 
-   } else {
-      // slow route for inter-block access 
-      unsigned nbytes_remain = length;
-      unsigned dst_offset = 0; 
-      mem_addr_t current_addr = addr; 
-
-      while (nbytes_remain > 0) {
-         unsigned offset = current_addr & (BSIZE-1);
-         mem_addr_t page = current_addr >> m_log2_block_size; 
-         mem_addr_t access_limit = offset + nbytes_remain; 
-         if (access_limit > BSIZE) {
-            access_limit = BSIZE;
-         } 
-         
-         size_t tx_bytes = access_limit - offset; 
-         read_single_block(page, current_addr, tx_bytes, &((unsigned char*)data)[dst_offset]); 
-
-         // advance pointers 
-         dst_offset += tx_bytes; 
-         current_addr += tx_bytes; 
-         nbytes_remain -= tx_bytes; 
+template <unsigned BSIZE>
+void memory_space_impl<BSIZE>::read(mem_addr_t addr, size_t length,
+                                    void *data) const {
+  mem_addr_t index = addr >> m_log2_block_size;
+  if ((addr + length) <= (index + 1) * BSIZE) {
+    // fast route for intra-block access
+    read_single_block(index, addr, length, data);
+  } else {
+    // slow route for inter-block access
+    unsigned nbytes_remain = length;
+    unsigned dst_offset = 0;
+    mem_addr_t current_addr = addr;
+
+    while (nbytes_remain > 0) {
+      unsigned offset = current_addr & (BSIZE - 1);
+      mem_addr_t page = current_addr >> m_log2_block_size;
+      mem_addr_t access_limit = offset + nbytes_remain;
+      if (access_limit > BSIZE) {
+        access_limit = BSIZE;
       }
-      assert(nbytes_remain == 0); 
-   }
+
+      size_t tx_bytes = access_limit - offset;
+      read_single_block(page, current_addr, tx_bytes,
+                        &((unsigned char *)data)[dst_offset]);
+
+      // advance pointers
+      dst_offset += tx_bytes;
+      current_addr += tx_bytes;
+      nbytes_remain -= tx_bytes;
+    }
+    assert(nbytes_remain == 0);
+  }
 }
 
-template<unsigned BSIZE> void memory_space_impl<BSIZE>::print( const char *format, FILE *fout ) const
-{
-   typename map_t::const_iterator i_page;
+template <unsigned BSIZE>
+void memory_space_impl<BSIZE>::print(const char *format, FILE *fout) const {
+  typename map_t::const_iterator i_page;
 
-   for ( i_page = m_data.begin(); i_page != m_data.end(); ++i_page) {
-      fprintf(fout, "%s %08x:", m_name.c_str(), i_page->first);
-      i_page->second.print(format, fout);
-   }
+  for (i_page = m_data.begin(); i_page != m_data.end(); ++i_page) {
+    fprintf(fout, "%s %08x:", m_name.c_str(), i_page->first);
+    i_page->second.print(format, fout);
+  }
 }
 
-template<unsigned BSIZE> void memory_space_impl<BSIZE>::set_watch( addr_t addr, unsigned watchpoint ) 
-{
-   m_watchpoints[watchpoint]=addr;
+template <unsigned BSIZE>
+void memory_space_impl<BSIZE>::set_watch(addr_t addr, unsigned watchpoint) {
+  m_watchpoints[watchpoint] = addr;
 }
 
 template class memory_space_impl<32>;
@@ -166,50 +183,49 @@ template class memory_space_impl<64>;
 template class memory_space_impl<8192>;
 template class memory_space_impl<16*1024>;
 
-void g_print_memory_space(memory_space *mem, const char *format = "%08x", FILE *fout = stdout) 
-{
-    mem->print(format,fout);
+void g_print_memory_space(memory_space *mem, const char *format = "%08x",
+                          FILE *fout = stdout) {
+  mem->print(format, fout);
 }
 
 #ifdef UNIT_TEST
 
-int main(int argc, char *argv[] )
-{
-   int errors_found=0;
-   memory_space *mem = new memory_space_impl<32>("test",4);
-   // write address to [address]
-   for( mem_addr_t addr=0; addr < 16*1024; addr+=4) 
-      mem->write(addr,4,&addr,NULL,NULL);
-
-   for( mem_addr_t addr=0; addr < 16*1024; addr+=4) {
-      unsigned tmp=0;
-      mem->read(addr,4,&tmp);
-      if( tmp != addr ) {
-         errors_found=1;
-         printf("ERROR ** mem[0x%x] = 0x%x, expected 0x%x\n", addr, tmp, addr );
-      }
-   }
-
-   for( mem_addr_t addr=0; addr < 16*1024; addr+=1) {
-      unsigned char val = (addr + 128) % 256;
-      mem->write(addr,1,&val,NULL,NULL);
-   }
-
-   for( mem_addr_t addr=0; addr < 16*1024; addr+=1) {
-      unsigned tmp=0;
-      mem->read(addr,1,&tmp);
-      unsigned char val = (addr + 128) % 256;
-      if( tmp != val ) {
-         errors_found=1;
-         printf("ERROR ** mem[0x%x] = 0x%x, expected 0x%x\n", addr, tmp, (unsigned)val );
-      }
-   }
-
-   if( errors_found ) {
-      printf("SUMMARY:  ERRORS FOUND\n");
-   } else {
-      printf("SUMMARY: UNIT TEST PASSED\n");
-   }
+int main(int argc, char *argv[]) {
+  int errors_found = 0;
+  memory_space *mem = new memory_space_impl<32>("test", 4);
+  // write address to [address]
+  for (mem_addr_t addr = 0; addr < 16 * 1024; addr += 4)
+    mem->write(addr, 4, &addr, NULL, NULL);
+
+  for (mem_addr_t addr = 0; addr < 16 * 1024; addr += 4) {
+    unsigned tmp = 0;
+    mem->read(addr, 4, &tmp);
+    if (tmp != addr) {
+      errors_found = 1;
+      printf("ERROR ** mem[0x%x] = 0x%x, expected 0x%x\n", addr, tmp, addr);
+    }
+  }
+
+  for (mem_addr_t addr = 0; addr < 16 * 1024; addr += 1) {
+    unsigned char val = (addr + 128) % 256;
+    mem->write(addr, 1, &val, NULL, NULL);
+  }
+
+  for (mem_addr_t addr = 0; addr < 16 * 1024; addr += 1) {
+    unsigned tmp = 0;
+    mem->read(addr, 1, &tmp);
+    unsigned char val = (addr + 128) % 256;
+    if (tmp != val) {
+      errors_found = 1;
+      printf("ERROR ** mem[0x%x] = 0x%x, expected 0x%x\n", addr, tmp,
+             (unsigned)val);
+    }
+  }
+
+  if (errors_found) {
+    printf("SUMMARY:  ERRORS FOUND\n");
+  } else {
+    printf("SUMMARY: UNIT TEST PASSED\n");
+  }
 }
-
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.h
index a97e197e75..7902998a56 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/memory.h
@@ -28,110 +28,102 @@
 #ifndef memory_h_INCLUDED
 #define memory_h_INCLUDED
 
-#ifndef LIBCUDA
 #include "../abstract_hardware_model.h"
-#else
-#include "../libcuda/abstract_hardware_model.h"
-#endif
 
 #include "../tr1_hash_map.h"
 #define mem_map tr1_hash_map
 #if tr1_hash_map_ismap == 1
-   #define MEM_MAP_RESIZE(hash_size)
+#define MEM_MAP_RESIZE(hash_size)
 #else
-   #define MEM_MAP_RESIZE(hash_size) (m_data.rehash(hash_size))
+#define MEM_MAP_RESIZE(hash_size) (m_data.rehash(hash_size))
 #endif
 
 #include <assert.h>
-#include <string.h>
 #include <stdio.h>
+#include <stdlib.h>
 #include <string>
 #include <map>
 #include <stdlib.h>
 
 typedef address_type mem_addr_t;
 
-#define MEM_BLOCK_SIZE (4*1024)
-
-template<unsigned BSIZE> class mem_storage {
-public:
-   mem_storage( const mem_storage &another )
-   {
-      m_data = (unsigned char*)calloc(1,BSIZE);
-      memcpy(m_data,another.m_data,BSIZE);
-   }
-   mem_storage()
-   {
-      m_data = (unsigned char*)calloc(1,BSIZE);
-   }
-   ~mem_storage()
-   {
-      free(m_data);
-   }
-
-   void write( unsigned offset, size_t length, const unsigned char *data )
-   {
-      assert( offset + length <= BSIZE );
-      memcpy(m_data+offset,data,length);
-   }
-
-   void read( unsigned offset, size_t length, unsigned char *data ) const
-   {
-      assert( offset + length <= BSIZE );
-      memcpy(data,m_data+offset,length);
-   }
-
-   void print( const char *format, FILE *fout ) const
-   {
-      unsigned int *i_data = (unsigned int*)m_data;
-      for (int d = 0; d < (BSIZE / sizeof(unsigned int)); d++) {
-         if (d % 1 == 0) {
-            fprintf(fout, "\n");
-         }
-         fprintf(fout, format, i_data[d]);
-         fprintf(fout, " ");
+#define MEM_BLOCK_SIZE (4 * 1024)
+
+template <unsigned BSIZE>
+class mem_storage {
+ public:
+  mem_storage(const mem_storage &another) {
+    m_data = (unsigned char *)calloc(1, BSIZE);
+    memcpy(m_data, another.m_data, BSIZE);
+  }
+  mem_storage() { m_data = (unsigned char *)calloc(1, BSIZE); }
+  ~mem_storage() { free(m_data); }
+
+  void write(unsigned offset, size_t length, const unsigned char *data) {
+    assert(offset + length <= BSIZE);
+    memcpy(m_data + offset, data, length);
+  }
+
+  void read(unsigned offset, size_t length, unsigned char *data) const {
+    assert(offset + length <= BSIZE);
+    memcpy(data, m_data + offset, length);
+  }
+
+  void print(const char *format, FILE *fout) const {
+    unsigned int *i_data = (unsigned int *)m_data;
+    for (int d = 0; d < (BSIZE / sizeof(unsigned int)); d++) {
+      if (d % 1 == 0) {
+        fprintf(fout, "\n");
       }
-      fprintf(fout, "\n");
-      fflush(fout);
-   }
-
-private:
-   unsigned m_nbytes;
-   unsigned char *m_data;
+      fprintf(fout, format, i_data[d]);
+      fprintf(fout, " ");
+    }
+    fprintf(fout, "\n");
+    fflush(fout);
+  }
+
+ private:
+  unsigned m_nbytes;
+  unsigned char *m_data;
 };
 
 class ptx_thread_info;
 class ptx_instruction;
 
-class memory_space
-{
-public:
-   virtual ~memory_space() {}
-   virtual void write( mem_addr_t addr, size_t length, const void *data, ptx_thread_info *thd, const ptx_instruction *pI ) = 0;
-   virtual void write_only( mem_addr_t index, mem_addr_t offset,  size_t length, const void *data ) = 0;
-   virtual void read( mem_addr_t addr, size_t length, void *data ) const = 0;
-   virtual void print( const char *format, FILE *fout ) const = 0;
-   virtual void set_watch( addr_t addr, unsigned watchpoint ) = 0;
+class memory_space {
+ public:
+  virtual ~memory_space() {}
+  virtual void write(mem_addr_t addr, size_t length, const void *data,
+                     ptx_thread_info *thd, const ptx_instruction *pI) = 0;
+  virtual void write_only(mem_addr_t index, mem_addr_t offset, size_t length,
+                          const void *data) = 0;
+  virtual void read(mem_addr_t addr, size_t length, void *data) const = 0;
+  virtual void print(const char *format, FILE *fout) const = 0;
+  virtual void set_watch(addr_t addr, unsigned watchpoint) = 0;
 };
 
-template<unsigned BSIZE> class memory_space_impl : public memory_space {
-public:
-   memory_space_impl( std::string name, unsigned hash_size );
-
-   virtual void write( mem_addr_t addr, size_t length, const void *data, ptx_thread_info *thd, const ptx_instruction *pI );
-   virtual void write_only( mem_addr_t index, mem_addr_t offset, size_t length, const void *data);
-   virtual void read( mem_addr_t addr, size_t length, void *data ) const;
-   virtual void print( const char *format, FILE *fout ) const;
-
-   virtual void set_watch( addr_t addr, unsigned watchpoint );
-
-private:
-   void read_single_block( mem_addr_t blk_idx, mem_addr_t addr, size_t length, void *data) const;
-   std::string m_name;
-   unsigned m_log2_block_size;
-   typedef mem_map<mem_addr_t,mem_storage<BSIZE> > map_t;
-   map_t m_data;
-   std::map<unsigned,mem_addr_t> m_watchpoints;
+template <unsigned BSIZE>
+class memory_space_impl : public memory_space {
+ public:
+  memory_space_impl(std::string name, unsigned hash_size);
+
+  virtual void write(mem_addr_t addr, size_t length, const void *data,
+                     ptx_thread_info *thd, const ptx_instruction *pI);
+  virtual void write_only(mem_addr_t index, mem_addr_t offset, size_t length,
+                          const void *data);
+  virtual void read(mem_addr_t addr, size_t length, void *data) const;
+  virtual void print(const char *format, FILE *fout) const;
+
+  virtual void set_watch(addr_t addr, unsigned watchpoint);
+
+ private:
+  void read_single_block(mem_addr_t blk_idx, mem_addr_t addr, size_t length,
+                         void *data) const;
+  std::string m_name;
+  unsigned m_log2_block_size;
+  typedef mem_map<mem_addr_t, mem_storage<BSIZE> > map_t;
+  map_t m_data;
+  std::map<unsigned, mem_addr_t> m_watchpoints;
 };
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.def b/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.def
index c4d6a8302d..f5bf156e25 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.def
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.def
@@ -129,6 +129,7 @@ OP_DEF(VSHL_OP,vshl_impl,"vshl",0,11)
 OP_DEF(VSHR_OP,vshr_impl,"vshr",0,11)
 OP_DEF(VSUB_OP,vsub_impl,"vsub",0,11)
 OP_DEF(VOTE_OP,vote_impl,"vote",0,3)
+OP_DEF(ACTIVEMASK_OP,activemask_impl,"activemask",1,3)
 OP_DEF(XOR_OP,xor_impl,"xor",1,1)
 OP_DEF(NOP_OP,nop_impl,"nop",0,7)
 OP_DEF(BREAK_OP,break_impl,"break",0,3)
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.h
index b91d92f6d9..fa40a44edd 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/opcodes.h
@@ -29,45 +29,48 @@
 #define opcodes_h_included
 
 enum opcode_t {
-#define OP_DEF(OP,FUNC,STR,DST,CLASSIFICATION) OP,
-#define OP_W_DEF(OP,FUNC,STR,DST,CLASSIFICATION) OP,
+#define OP_DEF(OP, FUNC, STR, DST, CLASSIFICATION) OP,
+#define OP_W_DEF(OP, FUNC, STR, DST, CLASSIFICATION) OP,
 #include "opcodes.def"
-	NUM_OPCODES
+  NUM_OPCODES
 #undef OP_DEF
 #undef OP_W_DEF
 };
 
 enum special_regs {
-   CLOCK_REG,
-   HALFCLOCK_ID,
-   CLOCK64_REG,
-   CTAID_REG,
-   ENVREG_REG,
-   GRIDID_REG,
-   LANEID_REG,
-   LANEMASK_EQ_REG,
-   LANEMASK_LE_REG,
-   LANEMASK_LT_REG,
-   LANEMASK_GE_REG,
-   LANEMASK_GT_REG,
-   NCTAID_REG,
-   NTID_REG,
-   NSMID_REG,
-   NWARPID_REG,
-   PM_REG,
-   SMID_REG,
-   TID_REG,
-   WARPID_REG,
-   WARPSZ_REG
+  CLOCK_REG,
+  HALFCLOCK_ID,
+  CLOCK64_REG,
+  CTAID_REG,
+  ENVREG_REG,
+  GRIDID_REG,
+  LANEID_REG,
+  LANEMASK_EQ_REG,
+  LANEMASK_LE_REG,
+  LANEMASK_LT_REG,
+  LANEMASK_GE_REG,
+  LANEMASK_GT_REG,
+  NCTAID_REG,
+  NTID_REG,
+  NSMID_REG,
+  NWARPID_REG,
+  PM_REG,
+  SMID_REG,
+  TID_REG,
+  WARPID_REG,
+  WARPSZ_REG
 };
-enum wmma_type{
-	LOAD_A,
-	LOAD_B,
-	LOAD_C,
-	STORE_D,
-	MMA,
-	ROW,
-	COL,
-	M16N16K16
+enum wmma_type {
+  LOAD_A,
+  LOAD_B,
+  LOAD_C,
+  STORE_D,
+  MMA,
+  ROW,
+  COL,
+  M16N16K16,
+  M32N8K16,
+  M8N32K16
+
 };
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.cc
index 0a9f08d9e6..3f136f1bca 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.cc
@@ -25,251 +25,272 @@
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-#include "ptx_ir.h"
-#include "ptx_sim.h"
 #include "ptx-stats.h"
-#include "../option_parser.h"
 #include <stdio.h>
 #include <map>
+#include "../../libcuda_sim/gpgpu_context.h"
+#include "../option_parser.h"
 #include "../tr1_hash_map.h"
+#include "ptx_ir.h"
+#include "ptx_sim.h"
 
-// options
-bool enable_ptx_file_line_stats;
-char * ptx_line_stats_filename = NULL;
-
-void ptx_file_line_stats_options(option_parser_t opp)
-{
-    option_parser_register(opp, "-enable_ptx_file_line_stats", OPT_BOOL, 
-                           &enable_ptx_file_line_stats, 
-                           "Turn on PTX source line statistic profiling. (1 = On)", "1");
-    option_parser_register(opp, "-ptx_line_stats_filename", OPT_CSTR, 
-                           &ptx_line_stats_filename, 
-                           "Output file for PTX source line statistics.", "gpgpu_inst_stats.txt");
+void ptx_stats::ptx_file_line_stats_options(option_parser_t opp) {
+  option_parser_register(
+      opp, "-enable_ptx_file_line_stats", OPT_BOOL, &enable_ptx_file_line_stats,
+      "Turn on PTX source line statistic profiling. (1 = On)", "1");
+  option_parser_register(
+      opp, "-ptx_line_stats_filename", OPT_CSTR, &ptx_line_stats_filename,
+      "Output file for PTX source line statistics.", "gpgpu_inst_stats.txt");
 }
 
 // implementations
 
 // defining a PTX source line = filename + line number
-class ptx_file_line 
-{
-public:
-    ptx_file_line(const char* s, int l) {
-        if( s == NULL ) 
-            st = "NULL_NAME";
-        else 
-            st = s;
-        line = l;
-    }
-
-    bool operator<(const ptx_file_line &other) const {
-        if( st == other.st ) {
-            if( line < other.line ) 
-                return true;
-            else
-                return false; 
-        } else {
-            return st < other.st;
-        }
+class ptx_file_line {
+ public:
+  ptx_file_line(const char *s, int l) {
+    if (s == NULL)
+      st = "NULL_NAME";
+    else
+      st = s;
+    line = l;
+  }
+
+  bool operator<(const ptx_file_line &other) const {
+    if (st == other.st) {
+      if (line < other.line)
+        return true;
+      else
+        return false;
+    } else {
+      return st < other.st;
     }
+  }
 
-    bool operator==(const ptx_file_line &other) const {
-        return (line==other.line) && (st==other.st);
-    }
+  bool operator==(const ptx_file_line &other) const {
+    return (line == other.line) && (st == other.st);
+  }
 
-    std::string st;
-    unsigned line;
+  std::string st;
+  unsigned line;
 };
 
 // holds all statistics collected for a singe PTX source line
-class ptx_file_line_stats
-{
-public:
-    ptx_file_line_stats() 
-        : exec_count(0), latency(0), dram_traffic(0), 
-          smem_n_way_bank_conflict_total(0), smem_warp_count(0),
-          gmem_n_access_total(0), gmem_warp_count(0), exposed_latency(0),
-          warp_divergence(0)
-    { }
-    
-    unsigned long exec_count;
-    unsigned long long latency;
-    unsigned long long dram_traffic;
-    unsigned long long smem_n_way_bank_conflict_total;  // total number of banks accessed by this instruction
-    unsigned long smem_warp_count;                      // number of warps accessing shared memory
-    unsigned long long gmem_n_access_total; // number of uncoalesced access in total from this instruction
-    unsigned long gmem_warp_count;          // number of warps causing these uncoalesced access
-    unsigned long long exposed_latency; // latency exposed as pipeline bubbles (attributed to this instruction)
-    unsigned long long warp_divergence; // number of warp divergence occured at this instruction
+class ptx_file_line_stats {
+ public:
+  ptx_file_line_stats()
+      : exec_count(0),
+        latency(0),
+        dram_traffic(0),
+        smem_n_way_bank_conflict_total(0),
+        smem_warp_count(0),
+        gmem_n_access_total(0),
+        gmem_warp_count(0),
+        exposed_latency(0),
+        warp_divergence(0) {}
+
+  unsigned long exec_count;
+  unsigned long long latency;
+  unsigned long long dram_traffic;
+  unsigned long long
+      smem_n_way_bank_conflict_total;  // total number of banks accessed by this
+                                       // instruction
+  unsigned long smem_warp_count;  // number of warps accessing shared memory
+  unsigned long long gmem_n_access_total;  // number of uncoalesced access in
+                                           // total from this instruction
+  unsigned long
+      gmem_warp_count;  // number of warps causing these uncoalesced access
+  unsigned long long exposed_latency;  // latency exposed as pipeline bubbles
+                                       // (attributed to this instruction)
+  unsigned long long
+      warp_divergence;  // number of warp divergence occured at this instruction
 };
 
 #if (tr1_hash_map_ismap == 1)
-typedef tr1_hash_map<ptx_file_line, ptx_file_line_stats> ptx_file_line_stats_map_t;
+typedef tr1_hash_map<ptx_file_line, ptx_file_line_stats>
+    ptx_file_line_stats_map_t;
 #else
-struct hash_ptx_file_line
-{
-    std::size_t operator()(const ptx_file_line & pfline) const {
-        std::hash<unsigned> hash_line;
-        return hash_line(pfline.line);
-    }
+struct hash_ptx_file_line {
+  std::size_t operator()(const ptx_file_line &pfline) const {
+    std::hash<unsigned> hash_line;
+    return hash_line(pfline.line);
+  }
 };
-typedef tr1_hash_map<ptx_file_line, ptx_file_line_stats, hash_ptx_file_line> ptx_file_line_stats_map_t;
+typedef tr1_hash_map<ptx_file_line, ptx_file_line_stats, hash_ptx_file_line>
+    ptx_file_line_stats_map_t;
 #endif
 
 static ptx_file_line_stats_map_t ptx_file_line_stats_tracker;
 
 // output statistics to a file
-void ptx_file_line_stats_write_file()
-{
-    // check if stat collection is turned on
-    if (enable_ptx_file_line_stats == 0) return;
-
-    ptx_file_line_stats_map_t::iterator it;
-    FILE * pfile;
-
-    pfile = fopen(ptx_line_stats_filename, "w");
-    fprintf(pfile,"kernel line : count latency dram_traffic smem_bk_conflicts smem_warp gmem_access_generated gmem_warp exposed_latency warp_divergence\n");
-    for( it=ptx_file_line_stats_tracker.begin(); it != ptx_file_line_stats_tracker.end(); it++ ) {
-        fprintf(pfile, "%s %i : ", it->first.st.c_str(), it->first.line);
-        fprintf(pfile, "%lu ", it->second.exec_count);
-        fprintf(pfile, "%llu ", it->second.latency);
-        fprintf(pfile, "%llu ", it->second.dram_traffic);
-        fprintf(pfile, "%llu ", it->second.smem_n_way_bank_conflict_total);
-        fprintf(pfile, "%lu ", it->second.smem_warp_count);
-        fprintf(pfile, "%llu ", it->second.gmem_n_access_total);
-        fprintf(pfile, "%lu ", it->second.gmem_warp_count);
-        fprintf(pfile, "%llu ", it->second.exposed_latency);
-        fprintf(pfile, "%llu ", it->second.warp_divergence);
-        fprintf(pfile, "\n");
-    }
-    fflush(pfile);
-    fclose(pfile);
+void ptx_stats::ptx_file_line_stats_write_file() {
+  // check if stat collection is turned on
+  if (enable_ptx_file_line_stats == 0) return;
+
+  ptx_file_line_stats_map_t::iterator it;
+  FILE *pfile;
+
+  pfile = fopen(ptx_line_stats_filename, "w");
+  fprintf(
+      pfile,
+      "kernel line : count latency dram_traffic smem_bk_conflicts smem_warp "
+      "gmem_access_generated gmem_warp exposed_latency warp_divergence\n");
+  for (it = ptx_file_line_stats_tracker.begin();
+       it != ptx_file_line_stats_tracker.end(); it++) {
+    fprintf(pfile, "%s %i : ", it->first.st.c_str(), it->first.line);
+    fprintf(pfile, "%lu ", it->second.exec_count);
+    fprintf(pfile, "%llu ", it->second.latency);
+    fprintf(pfile, "%llu ", it->second.dram_traffic);
+    fprintf(pfile, "%llu ", it->second.smem_n_way_bank_conflict_total);
+    fprintf(pfile, "%lu ", it->second.smem_warp_count);
+    fprintf(pfile, "%llu ", it->second.gmem_n_access_total);
+    fprintf(pfile, "%lu ", it->second.gmem_warp_count);
+    fprintf(pfile, "%llu ", it->second.exposed_latency);
+    fprintf(pfile, "%llu ", it->second.warp_divergence);
+    fprintf(pfile, "\n");
+  }
+  fflush(pfile);
+  fclose(pfile);
 }
 
 // attribute one more execution count to this ptx instruction
 // counting the number of threads (not warps) executing this instruction
-void ptx_file_line_stats_add_exec_count(const ptx_instruction *pInsn)
-{
-    ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(), pInsn->source_line())].exec_count += 1;
+void ptx_file_line_stats_add_exec_count(const ptx_instruction *pInsn) {
+  ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(),
+                                            pInsn->source_line())]
+      .exec_count += 1;
 }
 
 // attribute pipeline latency to this ptx instruction (specified by the pc)
-// pipeline latency is the number of cycles a warp with this instruction spent in the pipeline
-void ptx_file_line_stats_add_latency(unsigned pc, unsigned latency)
-{
-    const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
-    
-    ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(), pInsn->source_line())].latency += latency;
+// pipeline latency is the number of cycles a warp with this instruction spent
+// in the pipeline
+void ptx_stats::ptx_file_line_stats_add_latency(unsigned pc, unsigned latency) {
+  const ptx_instruction *pInsn = gpgpu_ctx->pc_to_instruction(pc);
+
+  if (pInsn != NULL)
+    ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(),
+                                              pInsn->source_line())]
+        .latency += latency;
 }
 
 // attribute dram traffic to this ptx instruction (specified by the pc)
-// dram traffic is counted in number of requests 
-void ptx_file_line_stats_add_dram_traffic(unsigned pc, unsigned dram_traffic)
-{
-    const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
-    
-    ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(), pInsn->source_line())].dram_traffic += dram_traffic;
+// dram traffic is counted in number of requests
+void ptx_stats::ptx_file_line_stats_add_dram_traffic(unsigned pc,
+                                                     unsigned dram_traffic) {
+  const ptx_instruction *pInsn = gpgpu_ctx->pc_to_instruction(pc);
+
+  if (pInsn != NULL)
+    ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(),
+                                              pInsn->source_line())]
+        .dram_traffic += dram_traffic;
 }
 
 // attribute the number of shared memory access cycles to a ptx instruction
-// counts both the number of warps doing shared memory access and the number of cycles involved
-void ptx_file_line_stats_add_smem_bank_conflict(unsigned pc, unsigned n_way_bkconflict)
-{
-    const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
-    
-    ptx_file_line_stats& line_stats = ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(), pInsn->source_line())];
+// counts both the number of warps doing shared memory access and the number of
+// cycles involved
+void ptx_stats::ptx_file_line_stats_add_smem_bank_conflict(
+    unsigned pc, unsigned n_way_bkconflict) {
+  const ptx_instruction *pInsn = gpgpu_ctx->pc_to_instruction(pc);
+
+  if (pInsn != NULL) {
+    ptx_file_line_stats &line_stats = ptx_file_line_stats_tracker[ptx_file_line(
+        pInsn->source_file(), pInsn->source_line())];
     line_stats.smem_n_way_bank_conflict_total += n_way_bkconflict;
     line_stats.smem_warp_count += 1;
+  }
 }
 
-// attribute a non-coalesced mem access to a ptx instruction 
-// counts both the number of warps causing this and the number of memory requests generated
-void ptx_file_line_stats_add_uncoalesced_gmem(unsigned pc, unsigned n_access)
-{
-    const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
-    
-    ptx_file_line_stats& line_stats = ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(), pInsn->source_line())];
+// attribute a non-coalesced mem access to a ptx instruction
+// counts both the number of warps causing this and the number of memory
+// requests generated
+void ptx_stats::ptx_file_line_stats_add_uncoalesced_gmem(unsigned pc,
+                                                         unsigned n_access) {
+  const ptx_instruction *pInsn = gpgpu_ctx->pc_to_instruction(pc);
+
+  if (pInsn != NULL) {
+    ptx_file_line_stats &line_stats = ptx_file_line_stats_tracker[ptx_file_line(
+        pInsn->source_file(), pInsn->source_line())];
     line_stats.gmem_n_access_total += n_access;
     line_stats.gmem_warp_count += 1;
+  }
 }
 
-// a class that tracks the inflight memory instructions of a shader core 
-// and attributes exposed latency to those instructions when signaled to do so 
-class ptx_inflight_memory_insn_tracker
-{
-public:
-    typedef std::map<const ptx_instruction *, int> insn_count_map;
+// a class that tracks the inflight memory instructions of a shader core
+// and attributes exposed latency to those instructions when signaled to do so
+class ptx_inflight_memory_insn_tracker {
+ public:
+  typedef std::map<const ptx_instruction *, int> insn_count_map;
 
-    void add_count(const ptx_instruction * pInsn, int count = 1)
-    {
-        ptx_inflight_memory_insns[pInsn] += count;
-    }
+  void add_count(const ptx_instruction *pInsn, int count = 1) {
+    ptx_inflight_memory_insns[pInsn] += count;
+  }
 
-    void sub_count(const ptx_instruction * pInsn, int count = 1)
-    {
-        insn_count_map::iterator i_insncount; 
-        i_insncount = ptx_inflight_memory_insns.find(pInsn);
+  void sub_count(const ptx_instruction *pInsn, int count = 1) {
+    insn_count_map::iterator i_insncount;
+    i_insncount = ptx_inflight_memory_insns.find(pInsn);
 
-        assert(i_insncount != ptx_inflight_memory_insns.end());
+    assert(i_insncount != ptx_inflight_memory_insns.end());
 
-        i_insncount->second -= count;
+    i_insncount->second -= count;
 
-        if (i_insncount->second <= 0) {
-            ptx_inflight_memory_insns.erase(i_insncount);
-        }
+    if (i_insncount->second <= 0) {
+      ptx_inflight_memory_insns.erase(i_insncount);
     }
-
-    void attribute_exposed_latency(int count = 1)
-    {
-        insn_count_map &exlat_insnmap = ptx_inflight_memory_insns;
-        insn_count_map::const_iterator i_exlatinsn;
-
-        i_exlatinsn = exlat_insnmap.begin();
-        for (; i_exlatinsn != exlat_insnmap.end(); ++i_exlatinsn) {
-            const ptx_instruction *pInsn = i_exlatinsn->first;
-            ptx_file_line_stats& line_stats = ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(), pInsn->source_line())];
-            line_stats.exposed_latency += count;
-        }
+  }
+
+  void attribute_exposed_latency(int count = 1) {
+    insn_count_map &exlat_insnmap = ptx_inflight_memory_insns;
+    insn_count_map::const_iterator i_exlatinsn;
+
+    i_exlatinsn = exlat_insnmap.begin();
+    for (; i_exlatinsn != exlat_insnmap.end(); ++i_exlatinsn) {
+      const ptx_instruction *pInsn = i_exlatinsn->first;
+      ptx_file_line_stats &line_stats =
+          ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(),
+                                                    pInsn->source_line())];
+      line_stats.exposed_latency += count;
     }
+  }
 
-    insn_count_map ptx_inflight_memory_insns;
+  insn_count_map ptx_inflight_memory_insns;
 };
 
 static ptx_inflight_memory_insn_tracker *inflight_mem_tracker = NULL;
 
-void ptx_file_line_stats_create_exposed_latency_tracker(int n_shader_cores)
-{
-    inflight_mem_tracker = new ptx_inflight_memory_insn_tracker[n_shader_cores];
+void ptx_file_line_stats_create_exposed_latency_tracker(int n_shader_cores) {
+  inflight_mem_tracker = new ptx_inflight_memory_insn_tracker[n_shader_cores];
 }
 
 // add an inflight memory instruction
-void ptx_file_line_stats_add_inflight_memory_insn(int sc_id, unsigned pc)
-{
-    const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
+void ptx_stats::ptx_file_line_stats_add_inflight_memory_insn(int sc_id,
+                                                             unsigned pc) {
+  const ptx_instruction *pInsn = gpgpu_ctx->pc_to_instruction(pc);
 
-    inflight_mem_tracker[sc_id].add_count(pInsn);
+  inflight_mem_tracker[sc_id].add_count(pInsn);
 }
 
 // remove an inflight memory instruction
-void ptx_file_line_stats_sub_inflight_memory_insn(int sc_id, unsigned pc)
-{
-    const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
+void ptx_stats::ptx_file_line_stats_sub_inflight_memory_insn(int sc_id,
+                                                             unsigned pc) {
+  const ptx_instruction *pInsn = gpgpu_ctx->pc_to_instruction(pc);
 
-    inflight_mem_tracker[sc_id].sub_count(pInsn);
+  inflight_mem_tracker[sc_id].sub_count(pInsn);
 }
 
-// attribute an empty cycle in the pipeline (exposed latency) to the ptx memory instructions in flight
-void ptx_file_line_stats_commit_exposed_latency(int sc_id, int exposed_latency)
-{
-    assert(exposed_latency > 0);
-    inflight_mem_tracker[sc_id].attribute_exposed_latency(exposed_latency);
+// attribute an empty cycle in the pipeline (exposed latency) to the ptx memory
+// instructions in flight
+void ptx_file_line_stats_commit_exposed_latency(int sc_id,
+                                                int exposed_latency) {
+  assert(exposed_latency > 0);
+  inflight_mem_tracker[sc_id].attribute_exposed_latency(exposed_latency);
 }
 
 // attribute the number of warp divergence to a ptx instruction
-void ptx_file_line_stats_add_warp_divergence(unsigned pc, unsigned n_way_divergence)
-{
-    const ptx_instruction *pInsn = function_info::pc_to_instruction(pc);
-    
-    ptx_file_line_stats& line_stats = ptx_file_line_stats_tracker[ptx_file_line(pInsn->source_file(), pInsn->source_line())];
-    line_stats.warp_divergence += n_way_divergence;
-}
+void ptx_stats::ptx_file_line_stats_add_warp_divergence(
+    unsigned pc, unsigned n_way_divergence) {
+  const ptx_instruction *pInsn = gpgpu_ctx->pc_to_instruction(pc);
 
+  ptx_file_line_stats &line_stats = ptx_file_line_stats_tracker[ptx_file_line(
+      pInsn->source_file(), pInsn->source_line())];
+  line_stats.warp_divergence += n_way_divergence;
+}
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.h
index 4fc6599363..07bf13b042 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx-stats.h
@@ -25,34 +25,44 @@
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-#pragma once 
+#pragma once
 
 #include "../option_parser.h"
 
-extern bool enable_ptx_file_line_stats;
-
-// set options
-void ptx_file_line_stats_options(option_parser_t opp);
-
-// output stats to a file
-void ptx_file_line_stats_write_file();
-
 #ifdef __cplusplus
 // stat collection interface to cuda-sim
 class ptx_instruction;
-void ptx_file_line_stats_add_exec_count(const ptx_instruction *pInsn);
+void ptx_file_line_stats_add_exec_count(const ptx_instruction* pInsn);
 #endif
 
 // stat collection interface to gpgpu-sim
-void ptx_file_line_stats_add_latency(unsigned pc, unsigned latency);
-void ptx_file_line_stats_add_dram_traffic(unsigned pc, unsigned dram_traffic);
-void ptx_file_line_stats_add_smem_bank_conflict(unsigned pc, unsigned n_way_bkconflict);
-void ptx_file_line_stats_add_uncoalesced_gmem(unsigned pc, unsigned n_access);
 
 void ptx_file_line_stats_create_exposed_latency_tracker(int n_shader_cores);
-void ptx_file_line_stats_add_inflight_memory_insn(int sc_id, unsigned pc);
-void ptx_file_line_stats_sub_inflight_memory_insn(int sc_id, unsigned pc);
 void ptx_file_line_stats_commit_exposed_latency(int sc_id, int exposed_latency);
 
-void ptx_file_line_stats_add_warp_divergence(unsigned pc, unsigned n_way_divergence);
+class gpgpu_context;
+class ptx_stats {
+ public:
+  ptx_stats(gpgpu_context* ctx) {
+    ptx_line_stats_filename = NULL;
+    gpgpu_ctx = ctx;
+  }
+  char* ptx_line_stats_filename;
+  bool enable_ptx_file_line_stats;
+  gpgpu_context* gpgpu_ctx;
+  // set options
+  void ptx_file_line_stats_options(option_parser_t opp);
 
+  // output stats to a file
+  void ptx_file_line_stats_write_file();
+  // stat collection interface to gpgpu-sim
+  void ptx_file_line_stats_add_latency(unsigned pc, unsigned latency);
+  void ptx_file_line_stats_add_dram_traffic(unsigned pc, unsigned dram_traffic);
+  void ptx_file_line_stats_add_smem_bank_conflict(unsigned pc,
+                                                  unsigned n_way_bkconflict);
+  void ptx_file_line_stats_add_uncoalesced_gmem(unsigned pc, unsigned n_access);
+  void ptx_file_line_stats_add_inflight_memory_insn(int sc_id, unsigned pc);
+  void ptx_file_line_stats_sub_inflight_memory_insn(int sc_id, unsigned pc);
+  void ptx_file_line_stats_add_warp_divergence(unsigned pc,
+                                               unsigned n_way_divergence);
+};
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.l b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.l
index 32323617b8..9cfab70fd0 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.l
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.l
@@ -1,51 +1,58 @@
 /*
-Copyright (c) 2009-2011, Tor M. Aamodt
-The University of British Columbia
+Copyright (c) 2009-2021, Tor M. Aamodt, Vijay Kandiah, Nikos Hardavellas
+The University of British Columbia, Northwestern University
 All rights reserved.
-
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
-Redistributions of source code must retain the above copyright notice, this
-list of conditions and the following disclaimer.
-Redistributions in binary form must reproduce the above copyright notice, this
-list of conditions and the following disclaimer in the documentation and/or
-other materials provided with the distribution.
-Neither the name of The University of British Columbia nor the names of its
-contributors may be used to endorse or promote products derived from this
-software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+1. Redistributions of source code must retain the above copyright notice, this
+   list of conditions and the following disclaimer;
+2. Redistributions in binary form must reproduce the above copyright notice,
+   this list of conditions and the following disclaimer in the documentation
+   and/or other materials provided with the distribution;
+3. Neither the names of The University of British Columbia, Northwestern 
+   University nor the names of their contributors may be used to
+   endorse or promote products derived from this software without specific
+   prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+POSSIBILITY OF SUCH DAMAGE.
 */
 
 %option nounput
 %option noyywrap
 %option yylineno
 %option prefix="ptx_"
+%option bison-bridge
+%option reentrant
+
 %{
 #include "opcodes.h"
+#include "ptx_parser.h"
 #include "ptx.tab.h"
 #include <string.h>
+#include "../../libcuda_sim/gpgpu_context.h"
+// typedef void* yyscan_t;
 
-#define LINEBUF_SIZE (64*1024)
-char linebuf[LINEBUF_SIZE];
-unsigned col = 0;
-#define TC col+=strlen(ptx_text); 
+#define LINEBUF_SIZE (4*1024)
+#define TC recognizer->col+=strlen(yytext);
 #define CHECK_UNSIGNED \
 	if( yytext[strlen(yytext)-1]=='U' ) { \
 		printf("GPGPU-Sim: ERROR ** U modifier not implemented\n"); \
 		abort(); \
 	}
-int ptx_error( const char *s );
+#define YY_DECL int ptx_lex \
+	       (YYSTYPE * yylval_param , yyscan_t yyscanner, ptx_recognizer* recognizer)
+int ptx_error( yyscan_t yyscanner, ptx_recognizer* recognizer, const char *s );
 %}
 
 %s IN_STRING
@@ -54,121 +61,134 @@ int ptx_error( const char *s );
 %x NOT_OPCODE
 %%
 
-abs	TC; ptx_lval.int_value = ABS_OP; return OPCODE;
-add	TC; ptx_lval.int_value = ADD_OP; return OPCODE;
-addp	TC; ptx_lval.int_value = ADDP_OP; return OPCODE;
-addc    TC; ptx_lval.int_value = ADDC_OP; return OPCODE;
-and	TC; ptx_lval.int_value = AND_OP; return OPCODE;
-andn	TC; ptx_lval.int_value = ANDN_OP; return OPCODE;
-atom	TC; ptx_lval.int_value = ATOM_OP; return OPCODE;
-bar.warp 	TC; ptx_lval.int_value = NOP_OP; return OPCODE;
-bar 	TC; ptx_lval.int_value = BAR_OP; return OPCODE;
-bfe     TC; ptx_lval.int_value = BFE_OP; return OPCODE;
-bfi     TC; ptx_lval.int_value = BFI_OP; return OPCODE;
-bfind   TC; ptx_lval.int_value = BFIND_OP; return OPCODE;
-bra     TC; ptx_lval.int_value = BRA_OP; return OPCODE;
-brx     TC; ptx_lval.int_value = BRX_OP; return OPCODE;
-brev    TC; ptx_lval.int_value = BREV_OP; return OPCODE;
-brkpt   TC; ptx_lval.int_value = BRKPT_OP; return OPCODE;
-
-wmma	TC; ptx_lval.int_value = MMA_OP; return OPCODE;
-wmma\.load	TC; ptx_lval.int_value = MMA_LD_OP; return OPCODE;
-wmma\.store 	TC; ptx_lval.int_value = MMA_ST_OP; return OPCODE;
-
-call	TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
-callp   TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALLP_OP; return OPCODE;
-clz	TC; ptx_lval.int_value = CLZ_OP; return OPCODE;
-cnot	TC; ptx_lval.int_value = CNOT_OP; return OPCODE;
-cos	TC; ptx_lval.int_value = COS_OP; return OPCODE;
-cvt	TC; ptx_lval.int_value = CVT_OP; return OPCODE;
-cvta	TC; ptx_lval.int_value = CVTA_OP; return OPCODE;
-div	TC; ptx_lval.int_value = DIV_OP; return OPCODE;
-dp4a	TC; ptx_lval.int_value = DP4A_OP; return OPCODE;
-ex2	TC; ptx_lval.int_value = EX2_OP; return OPCODE;
-exit	TC; ptx_lval.int_value = EXIT_OP; return OPCODE;
-fma     TC; ptx_lval.int_value = FMA_OP; return OPCODE;
-isspacep TC; ptx_lval.int_value = ISSPACEP_OP; return OPCODE;
-ld      TC; ptx_lval.int_value = LD_OP; return OPCODE;
-ld.volatile TC; ptx_lval.int_value = LD_OP; return OPCODE;
-ldu     TC; ptx_lval.int_value = LDU_OP; return OPCODE;
-lg2	TC; ptx_lval.int_value = LG2_OP; return OPCODE;
-mad24   TC; ptx_lval.int_value = MAD24_OP; return OPCODE;
-mad     TC; ptx_lval.int_value = MAD_OP; return OPCODE;
-madc	TC; ptx_lval.int_value = MADC_OP; return OPCODE;
-madp	TC; ptx_lval.int_value = MADP_OP; return OPCODE;
-max     TC; ptx_lval.int_value = MAX_OP; return OPCODE;
-membar  TC; ptx_lval.int_value = MEMBAR_OP; return OPCODE;
-min     TC; ptx_lval.int_value = MIN_OP; return OPCODE;
-mov     TC; ptx_lval.int_value = MOV_OP; return OPCODE;
-mul24   TC; ptx_lval.int_value = MUL24_OP; return OPCODE;
-mul     TC; ptx_lval.int_value = MUL_OP; return OPCODE;
-neg     TC; ptx_lval.int_value = NEG_OP; return OPCODE;
-nandn   TC; ptx_lval.int_value = NANDN_OP; return OPCODE;
-norn    TC; ptx_lval.int_value = NORN_OP; return OPCODE;
-not     TC; ptx_lval.int_value = NOT_OP; return OPCODE;
-or      TC; ptx_lval.int_value = OR_OP; return OPCODE;
-orn     TC; ptx_lval.int_value = ORN_OP; return OPCODE;
-pmevent TC; ptx_lval.int_value = PMEVENT_OP; return OPCODE;
-popc    TC; ptx_lval.int_value = POPC_OP; return OPCODE;
-prefetch TC; ptx_lval.int_value = PREFETCH_OP; return OPCODE;
-prefetchu TC; ptx_lval.int_value = PREFETCHU_OP; return OPCODE;
-prmt    TC; ptx_lval.int_value = PRMT_OP; return OPCODE;
-rcp	TC; ptx_lval.int_value = RCP_OP; return OPCODE;
-red     TC; ptx_lval.int_value = RED_OP; return OPCODE;
-rem	TC; ptx_lval.int_value = REM_OP; return OPCODE;
-ret	TC; ptx_lval.int_value = RET_OP; return OPCODE;
-retp     TC; ptx_lval.int_value = RETP_OP; return OPCODE;
-rsqrt	TC; ptx_lval.int_value = RSQRT_OP; return OPCODE;
-sad     TC; ptx_lval.int_value = SAD_OP; return OPCODE;
-selp	TC; ptx_lval.int_value = SELP_OP; return OPCODE;
-setp    TC; ptx_lval.int_value = SETP_OP; return OPCODE;
-set	TC; ptx_lval.int_value = SET_OP; return OPCODE;
-shfl	TC; ptx_lval.int_value = SHFL_OP; return OPCODE;
-shl     TC; ptx_lval.int_value = SHL_OP; return OPCODE;
-shr     TC; ptx_lval.int_value = SHR_OP; return OPCODE;
-sin	TC; ptx_lval.int_value = SIN_OP; return OPCODE;
-slct	TC; ptx_lval.int_value = SLCT_OP; return OPCODE;
-sqrt	TC; ptx_lval.int_value = SQRT_OP; return OPCODE;
-sst	TC; ptx_lval.int_value = SST_OP; return OPCODE;
-ssy     TC; ptx_lval.int_value = SSY_OP; return OPCODE;
-st      TC; ptx_lval.int_value = ST_OP; return OPCODE;
-st.volatile TC; ptx_lval.int_value = ST_OP; return OPCODE;
-sub	TC; ptx_lval.int_value = SUB_OP; return OPCODE;
-subc	TC; ptx_lval.int_value = SUBC_OP; return OPCODE;
-suld	TC; ptx_lval.int_value = SULD_OP; return OPCODE;
-sured	TC; ptx_lval.int_value = SURED_OP; return OPCODE;
-surst	TC; ptx_lval.int_value = SUST_OP; return OPCODE;
-suq	TC; ptx_lval.int_value = SUQ_OP; return OPCODE;
-tex	TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = TEX_OP; return OPCODE;
-txq	TC; ptx_lval.int_value = TEX_OP; return OPCODE;
-trap	TC; ptx_lval.int_value = TRAP_OP; return OPCODE;
-vabsdiff TC; ptx_lval.int_value = VABSDIFF_OP; return OPCODE;
-vadd    TC; ptx_lval.int_value = VADD_OP; return OPCODE;
-vmad    TC; ptx_lval.int_value = VMAD_OP; return OPCODE;
-vmax    TC; ptx_lval.int_value = VMAX_OP; return OPCODE;
-vmin    TC; ptx_lval.int_value = VMIN_OP; return OPCODE;
-vset    TC; ptx_lval.int_value = VSET_OP; return OPCODE;
-vshl    TC; ptx_lval.int_value = VSHL_OP; return OPCODE;
-vshr    TC; ptx_lval.int_value = VSHR_OP; return OPCODE;
-vsub    TC; ptx_lval.int_value = VSUB_OP; return OPCODE;
-vote	TC; ptx_lval.int_value = VOTE_OP; return OPCODE;
-xor     TC; ptx_lval.int_value = XOR_OP; return OPCODE;
-nop     TC; ptx_lval.int_value = NOP_OP; return OPCODE;
-break  TC; ptx_lval.int_value = BREAK_OP; return OPCODE;
-breakaddr  TC; ptx_lval.int_value = BREAKADDR_OP; return OPCODE;
+abs	TC; yylval->int_value = ABS_OP; return OPCODE;
+add	TC; yylval->int_value = ADD_OP; return OPCODE;
+addp	TC; yylval->int_value = ADDP_OP; return OPCODE;
+addc    TC; yylval->int_value = ADDC_OP; return OPCODE;
+and	TC; yylval->int_value = AND_OP; return OPCODE;
+andn	TC; yylval->int_value = ANDN_OP; return OPCODE;
+atom	TC; yylval->int_value = ATOM_OP; return OPCODE;
+bar.warp 	TC; yylval->int_value = NOP_OP; return OPCODE;
+bar 	TC; yylval->int_value = BAR_OP; return OPCODE;
+barrier	TC; yylval->int_value = BAR_OP; return OPCODE;
+bfe     TC; yylval->int_value = BFE_OP; return OPCODE;
+bfi     TC; yylval->int_value = BFI_OP; return OPCODE;
+bfind   TC; yylval->int_value = BFIND_OP; return OPCODE;
+bra     TC; yylval->int_value = BRA_OP; return OPCODE;
+brx     TC; yylval->int_value = BRX_OP; return OPCODE;
+brev    TC; yylval->int_value = BREV_OP; return OPCODE;
+brkpt   TC; yylval->int_value = BRKPT_OP; return OPCODE;
+
+wmma	TC; yylval->int_value = MMA_OP; return OPCODE;
+wmma\.load	TC; yylval->int_value = MMA_LD_OP; return OPCODE;
+wmma\.store 	TC; yylval->int_value = MMA_ST_OP; return OPCODE;
+
+call	TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
+callp   TC; BEGIN(NOT_OPCODE); yylval->int_value = CALLP_OP; return OPCODE;
+clz	TC; yylval->int_value = CLZ_OP; return OPCODE;
+cnot	TC; yylval->int_value = CNOT_OP; return OPCODE;
+cos	TC; yylval->int_value = COS_OP; return OPCODE;
+cvt	TC; yylval->int_value = CVT_OP; return OPCODE;
+cvta	TC; yylval->int_value = CVTA_OP; return OPCODE;
+div	TC; yylval->int_value = DIV_OP; return OPCODE;
+dp4a	TC; yylval->int_value = DP4A_OP; return OPCODE;
+ex2	TC; yylval->int_value = EX2_OP; return OPCODE;
+exit	TC; yylval->int_value = EXIT_OP; return OPCODE;
+fma     TC; yylval->int_value = FMA_OP; return OPCODE;
+isspacep TC; yylval->int_value = ISSPACEP_OP; return OPCODE;
+ld      TC; yylval->int_value = LD_OP; return OPCODE;
+ld.volatile TC; yylval->int_value = LD_OP; return OPCODE;
+ldu     TC; yylval->int_value = LDU_OP; return OPCODE;
+lg2	TC; yylval->int_value = LG2_OP; return OPCODE;
+mad24   TC; yylval->int_value = MAD24_OP; return OPCODE;
+mad     TC; yylval->int_value = MAD_OP; return OPCODE;
+madc	TC; yylval->int_value = MADC_OP; return OPCODE;
+madp	TC; yylval->int_value = MADP_OP; return OPCODE;
+max     TC; yylval->int_value = MAX_OP; return OPCODE;
+membar  TC; yylval->int_value = MEMBAR_OP; return OPCODE;
+min     TC; yylval->int_value = MIN_OP; return OPCODE;
+mov     TC; yylval->int_value = MOV_OP; return OPCODE;
+mul24   TC; yylval->int_value = MUL24_OP; return OPCODE;
+mul     TC; yylval->int_value = MUL_OP; return OPCODE;
+neg     TC; yylval->int_value = NEG_OP; return OPCODE;
+nandn   TC; yylval->int_value = NANDN_OP; return OPCODE;
+norn    TC; yylval->int_value = NORN_OP; return OPCODE;
+not     TC; yylval->int_value = NOT_OP; return OPCODE;
+or      TC; yylval->int_value = OR_OP; return OPCODE;
+orn     TC; yylval->int_value = ORN_OP; return OPCODE;
+pmevent TC; yylval->int_value = PMEVENT_OP; return OPCODE;
+popc    TC; yylval->int_value = POPC_OP; return OPCODE;
+prefetch TC; yylval->int_value = PREFETCH_OP; return OPCODE;
+prefetchu TC; yylval->int_value = PREFETCHU_OP; return OPCODE;
+prmt    TC; yylval->int_value = PRMT_OP; return OPCODE;
+rcp	TC; yylval->int_value = RCP_OP; return OPCODE;
+red     TC; yylval->int_value = RED_OP; return OPCODE;
+rem	TC; yylval->int_value = REM_OP; return OPCODE;
+ret	TC; yylval->int_value = RET_OP; return OPCODE;
+retp     TC; yylval->int_value = RETP_OP; return OPCODE;
+rsqrt	TC; yylval->int_value = RSQRT_OP; return OPCODE;
+sad     TC; yylval->int_value = SAD_OP; return OPCODE;
+selp	TC; yylval->int_value = SELP_OP; return OPCODE;
+setp    TC; yylval->int_value = SETP_OP; return OPCODE;
+set	TC; yylval->int_value = SET_OP; return OPCODE;
+shfl	TC; yylval->int_value = SHFL_OP; return OPCODE;
+shl     TC; yylval->int_value = SHL_OP; return OPCODE;
+shr     TC; yylval->int_value = SHR_OP; return OPCODE;
+sin	TC; yylval->int_value = SIN_OP; return OPCODE;
+slct	TC; yylval->int_value = SLCT_OP; return OPCODE;
+sqrt	TC; yylval->int_value = SQRT_OP; return OPCODE;
+sst	TC; yylval->int_value = SST_OP; return OPCODE;
+ssy     TC; yylval->int_value = SSY_OP; return OPCODE;
+st      TC; yylval->int_value = ST_OP; return OPCODE;
+st.volatile TC; yylval->int_value = ST_OP; return OPCODE;
+sub	TC; yylval->int_value = SUB_OP; return OPCODE;
+subc	TC; yylval->int_value = SUBC_OP; return OPCODE;
+suld	TC; yylval->int_value = SULD_OP; return OPCODE;
+sured	TC; yylval->int_value = SURED_OP; return OPCODE;
+surst	TC; yylval->int_value = SUST_OP; return OPCODE;
+suq	TC; yylval->int_value = SUQ_OP; return OPCODE;
+tex	TC; BEGIN(NOT_OPCODE); yylval->int_value = TEX_OP; return OPCODE;
+txq	TC; yylval->int_value = TEX_OP; return OPCODE;
+trap	TC; yylval->int_value = TRAP_OP; return OPCODE;
+vabsdiff TC; yylval->int_value = VABSDIFF_OP; return OPCODE;
+vadd    TC; yylval->int_value = VADD_OP; return OPCODE;
+vmad    TC; yylval->int_value = VMAD_OP; return OPCODE;
+vmax    TC; yylval->int_value = VMAX_OP; return OPCODE;
+vmin    TC; yylval->int_value = VMIN_OP; return OPCODE;
+vset    TC; yylval->int_value = VSET_OP; return OPCODE;
+vshl    TC; yylval->int_value = VSHL_OP; return OPCODE;
+vshr    TC; yylval->int_value = VSHR_OP; return OPCODE;
+vsub    TC; yylval->int_value = VSUB_OP; return OPCODE;
+vote	TC; yylval->int_value = VOTE_OP; return OPCODE;
+activemask TC; yylval->int_value = ACTIVEMASK_OP; return OPCODE;
+xor     TC; yylval->int_value = XOR_OP; return OPCODE;
+nop     TC; yylval->int_value = NOP_OP; return OPCODE;
+break  TC; yylval->int_value = BREAK_OP; return OPCODE;
+breakaddr  TC; yylval->int_value = BREAKADDR_OP; return OPCODE;
 
 "CPTX_END"	printf("ENDING CUSTOM PTX.\n"); BEGIN(IN_COMMENT);
 
 <INITIAL,NOT_OPCODE,IN_INST,IN_FUNC_DECL>{
-\.a\.sync TC; ptx_lval.int_value = LOAD_A; return WMMA_DIRECTIVE;
-\.b\.sync TC; ptx_lval.int_value = LOAD_B; return WMMA_DIRECTIVE;
-\.c\.sync TC; ptx_lval.int_value = LOAD_C; return WMMA_DIRECTIVE;
-\.d\.sync TC; ptx_lval.int_value = STORE_D; return WMMA_DIRECTIVE;
-\.mma\.sync TC;ptx_lval.int_value=MMA; return WMMA_DIRECTIVE;
-
-\.row TC; ptx_lval.int_value = ROW; return LAYOUT;
-\.col TC; ptx_lval.int_value = COL; return LAYOUT;
-\.m16n16k16 TC; ptx_lval.int_value = M16N16K16; return CONFIGURATION;
+\.a\.sync\.aligned TC; yylval->int_value = LOAD_A; return WMMA_DIRECTIVE;
+\.b\.sync\.aligned TC; yylval->int_value = LOAD_B; return WMMA_DIRECTIVE;
+\.c\.sync\.aligned TC; yylval->int_value = LOAD_C; return WMMA_DIRECTIVE;
+\.d\.sync\.aligned TC; yylval->int_value = STORE_D; return WMMA_DIRECTIVE;
+\.mma\.sync\.aligned TC;yylval->int_value=MMA; return WMMA_DIRECTIVE;
+
+\.row TC; yylval->int_value = ROW; return LAYOUT;
+\.col TC; yylval->int_value = COL; return LAYOUT;
+\.m16n16k16\.global TC; yylval->int_value = M16N16K16; return CONFIGURATION;
+\.m32n8k16\.global TC; yylval->int_value = M32N8K16; return CONFIGURATION;
+\.m8n32k16\.global TC;  yylval->int_value = M8N32K16; return CONFIGURATION;
+
+\.m16n16k16\.shared TC; yylval->int_value = M16N16K16; return CONFIGURATION;
+\.m32n8k16\.shared TC; yylval->int_value = M32N8K16; return CONFIGURATION;
+\.m8n32k16\.shared TC;  yylval->int_value = M8N32K16; return CONFIGURATION;
+
+\.m16n16k16 TC; yylval->int_value = M16N16K16; return CONFIGURATION;
+\.m32n8k16 TC; yylval->int_value = M32N8K16; return CONFIGURATION;
+\.m8n32k16 TC;  yylval->int_value = M8N32K16; return CONFIGURATION;
+
 \.f4e TC;  return PRMT_F4E_MODE;
 \.b4e TC;  return PRMT_B4E_MODE;
 \.rc8 TC;  return PRMT_RC8_MODE;
@@ -181,8 +201,8 @@ breakaddr  TC; ptx_lval.int_value = BREAKADDR_OP; return OPCODE;
 \.byte	TC; return BYTE_DIRECTIVE; /* not in PTX 2.1 */
 \.callprototype TC; return CALLPROTOTYPE_DIRECTIVE;
 \.calltargets TC; return CALLTARGETS_DIRECTIVE;
-\.const\[[0-9]+\] TC; ptx_lval.int_value = atoi(yytext+7); return CONST_DIRECTIVE;
-\.const TC; ptx_lval.int_value = 0; return CONST_DIRECTIVE;
+\.const\[[0-9]+\] TC; yylval->int_value = atoi(yytext+7); return CONST_DIRECTIVE;
+\.const TC; yylval->int_value = 0; return CONST_DIRECTIVE;
 \.entry TC; return ENTRY_DIRECTIVE;
 \.extern TC; return EXTERN_DIRECTIVE;
 \.file	 TC; BEGIN(INITIAL); return FILE_DIRECTIVE;
@@ -217,40 +237,40 @@ breakaddr  TC; ptx_lval.int_value = BREAKADDR_OP; return OPCODE;
 \.constptr TC; return CONSTPTR_DIRECTIVE; /* Ptx plus directive for pointer to constant memory */
 \.ptr TC; return PTR_DIRECTIVE; /* Added for new OpenCL genrated code */
 
-"%clock" TC; ptx_lval.int_value = CLOCK_REG; return SPECIAL_REGISTER;
-"%halfclock" TC; ptx_lval.int_value = HALFCLOCK_ID; return SPECIAL_REGISTER;
-"%clock64" TC; ptx_lval.int_value = CLOCK64_REG; return SPECIAL_REGISTER;
-"%ctaid" TC; ptx_lval.int_value = CTAID_REG; return SPECIAL_REGISTER;
-"%envreg"[0-9]+ TC; sscanf(yytext+7,"%u",&ptx_lval.int_value); ptx_lval.int_value<<=16; ptx_lval.int_value += ENVREG_REG; return SPECIAL_REGISTER;
-"%gridid" TC; ptx_lval.int_value = GRIDID_REG; return SPECIAL_REGISTER;
-"%laneid"  TC; ptx_lval.int_value = LANEID_REG; return SPECIAL_REGISTER;
-"%lanemask_eq"  TC; ptx_lval.int_value = LANEMASK_EQ_REG; return SPECIAL_REGISTER;
-"%lanemask_le"  TC; ptx_lval.int_value = LANEMASK_LE_REG; return SPECIAL_REGISTER;
-"%lanemask_lt"  TC; ptx_lval.int_value = LANEMASK_LT_REG; return SPECIAL_REGISTER;
-"%lanemask_ge"  TC; ptx_lval.int_value = LANEMASK_GE_REG; return SPECIAL_REGISTER;
-"%lanemask_gt"  TC; ptx_lval.int_value = LANEMASK_GT_REG; return SPECIAL_REGISTER;
-"%nctaid" TC; ptx_lval.int_value = NCTAID_REG; return SPECIAL_REGISTER;
-"%ntid"  TC; ptx_lval.int_value = NTID_REG; return SPECIAL_REGISTER;
-"%nsmid"  TC; ptx_lval.int_value = NSMID_REG; return SPECIAL_REGISTER;
-"%nwarpid"  TC; ptx_lval.int_value = NWARPID_REG; return SPECIAL_REGISTER;
-"%pm"[0-3]  TC; sscanf(yytext+3,"%u",&ptx_lval.int_value); ptx_lval.int_value<<=16; ptx_lval.int_value += PM_REG; return SPECIAL_REGISTER;
-"%smid"  TC; ptx_lval.int_value = SMID_REG; return SPECIAL_REGISTER;
-"%tid"  TC; ptx_lval.int_value = TID_REG; return SPECIAL_REGISTER;
-"%warpid"  TC; ptx_lval.int_value = WARPID_REG; return SPECIAL_REGISTER;
-"WARP_SZ"  TC; ptx_lval.int_value = WARPSZ_REG; return SPECIAL_REGISTER;
-
-[a-zA-Z_][a-zA-Z0-9_$]*  TC; ptx_lval.string_value = strdup(yytext); return IDENTIFIER;
-[$%][a-zA-Z0-9_$]+  TC; ptx_lval.string_value = strdup(yytext); return IDENTIFIER;
-
-[0-9]+\.[0-9]+ 	 TC; sscanf(yytext,"%lf", &ptx_lval.double_value); return DOUBLE_OPERAND;
+"%clock" TC; yylval->int_value = CLOCK_REG; return SPECIAL_REGISTER;
+"%halfclock" TC; yylval->int_value = HALFCLOCK_ID; return SPECIAL_REGISTER;
+"%clock64" TC; yylval->int_value = CLOCK64_REG; return SPECIAL_REGISTER;
+"%ctaid" TC; yylval->int_value = CTAID_REG; return SPECIAL_REGISTER;
+"%envreg"[0-9]+ TC; sscanf(yytext+7,"%u",&yylval->int_value); yylval->int_value<<=16; yylval->int_value += ENVREG_REG; return SPECIAL_REGISTER;
+"%gridid" TC; yylval->int_value = GRIDID_REG; return SPECIAL_REGISTER;
+"%laneid"  TC; yylval->int_value = LANEID_REG; return SPECIAL_REGISTER;
+"%lanemask_eq"  TC; yylval->int_value = LANEMASK_EQ_REG; return SPECIAL_REGISTER;
+"%lanemask_le"  TC; yylval->int_value = LANEMASK_LE_REG; return SPECIAL_REGISTER;
+"%lanemask_lt"  TC; yylval->int_value = LANEMASK_LT_REG; return SPECIAL_REGISTER;
+"%lanemask_ge"  TC; yylval->int_value = LANEMASK_GE_REG; return SPECIAL_REGISTER;
+"%lanemask_gt"  TC; yylval->int_value = LANEMASK_GT_REG; return SPECIAL_REGISTER;
+"%nctaid" TC; yylval->int_value = NCTAID_REG; return SPECIAL_REGISTER;
+"%ntid"  TC; yylval->int_value = NTID_REG; return SPECIAL_REGISTER;
+"%nsmid"  TC; yylval->int_value = NSMID_REG; return SPECIAL_REGISTER;
+"%nwarpid"  TC; yylval->int_value = NWARPID_REG; return SPECIAL_REGISTER;
+"%pm"[0-3]  TC; sscanf(yytext+3,"%u",&yylval->int_value); yylval->int_value<<=16; yylval->int_value += PM_REG; return SPECIAL_REGISTER;
+"%smid"  TC; yylval->int_value = SMID_REG; return SPECIAL_REGISTER;
+"%tid"  TC; yylval->int_value = TID_REG; return SPECIAL_REGISTER;
+"%warpid"  TC; yylval->int_value = WARPID_REG; return SPECIAL_REGISTER;
+"WARP_SZ"  TC; yylval->int_value = WARPSZ_REG; return SPECIAL_REGISTER;
+
+[a-zA-Z_][a-zA-Z0-9_$]*  TC; yylval->string_value = strdup(yytext); return IDENTIFIER;
+[$%][a-zA-Z0-9_$]+  TC; yylval->string_value = strdup(yytext); return IDENTIFIER;
+
+[0-9]+\.[0-9]+ 	 TC; sscanf(yytext,"%lf", &yylval->double_value); return DOUBLE_OPERAND;
 	
-0[xX][0-9a-fA-F]+U? TC; CHECK_UNSIGNED; sscanf(yytext,"%x", &ptx_lval.int_value); return INT_OPERAND;
+0[xX][0-9a-fA-F]+U? TC; CHECK_UNSIGNED; sscanf(yytext,"%x", &yylval->int_value); return INT_OPERAND;
 0[0-7]+U?   	TC; printf("GPGPU-Sim: ERROR ** parsing octal not (yet) implemented\n"); abort(); return INT_OPERAND;
 0[bB][01]+U?  	TC; printf("GPGPU-Sim: ERROR ** parsing binary not (yet) implemented\n"); abort(); return INT_OPERAND;
-[-]?[0-9]+U?    TC; CHECK_UNSIGNED; ptx_lval.int_value =  atoi(yytext); return INT_OPERAND;
+[-]?[0-9]+U?    TC; CHECK_UNSIGNED; yylval->int_value =  atoi(yytext); return INT_OPERAND;
 
-0[fF][0-9a-fA-F]{8}  TC; sscanf(yytext+2,"%x", (unsigned*)(void*)&ptx_lval.float_value); return FLOAT_OPERAND;
-0[dD][0-9a-fA-F]{16}  TC; sscanf(yytext+2,"%Lx", (unsigned long long*)(void*)&ptx_lval.double_value); return DOUBLE_OPERAND;
+0[fF][0-9a-fA-F]{8}  TC; sscanf(yytext+2,"%x", (unsigned*)(void*)&yylval->float_value); return FLOAT_OPERAND;
+0[dD][0-9a-fA-F]{16}  TC; sscanf(yytext+2,"%Lx", (unsigned long long*)(void*)&yylval->double_value); return DOUBLE_OPERAND;
 
 \.s8   TC;  return S8_TYPE;
 \.s16  TC;  return S16_TYPE;
@@ -383,12 +403,12 @@ breakaddr  TC; ptx_lval.int_value = BREAKADDR_OP; return OPCODE;
 \.2d	TC; return GEOM_MODIFIER_2D;
 \.3d	TC; return GEOM_MODIFIER_3D;
 
-\.0	TC; ptx_lval.int_value = 0; return DIMENSION_MODIFIER;
-\.1	TC; ptx_lval.int_value = 1; return DIMENSION_MODIFIER;
-\.2	TC; ptx_lval.int_value = 2; return DIMENSION_MODIFIER;
-\.x	TC; ptx_lval.int_value = 0; return DIMENSION_MODIFIER;
-\.y	TC; ptx_lval.int_value = 1; return DIMENSION_MODIFIER;
-\.z	TC; ptx_lval.int_value = 2; return DIMENSION_MODIFIER;
+\.0	TC; yylval->int_value = 0; return DIMENSION_MODIFIER;
+\.1	TC; yylval->int_value = 1; return DIMENSION_MODIFIER;
+\.2	TC; yylval->int_value = 2; return DIMENSION_MODIFIER;
+\.x	TC; yylval->int_value = 0; return DIMENSION_MODIFIER;
+\.y	TC; yylval->int_value = 1; return DIMENSION_MODIFIER;
+\.z	TC; yylval->int_value = 2; return DIMENSION_MODIFIER;
 
 "-"	TC; return MINUS;
 "+"	TC; return PLUS;
@@ -411,7 +431,7 @@ breakaddr  TC; ptx_lval.int_value = BREAKADDR_OP; return OPCODE;
 
 "//"[^\n]* TC;	// eat single
 
-\n.*  col=0; strncpy(linebuf, yytext + 1, LINEBUF_SIZE); yyless( 1 );
+\n.*  recognizer->col=0; strncpy(recognizer->linebuf, yytext + 1, LINEBUF_SIZE); yyless( 1 );
 
 " " TC;
 "\t" TC;
@@ -439,28 +459,26 @@ breakaddr  TC; ptx_lval.int_value = BREAKADDR_OP; return OPCODE;
 }
 <IN_STRING>{
 "\"" 	TC; BEGIN(INITIAL); return STRING;
-[^\"]*	TC; ptx_lval.string_value = strdup(yytext); 
+[^\"]*	TC; yylval->string_value = strdup(yytext);
 }
 
 <*>\t@@DWARF.*\n
 
-<INITIAL,NOT_OPCODE,IN_FUNC_DECL>.  TC; ptx_error((const char*)NULL);
+<INITIAL,NOT_OPCODE,IN_FUNC_DECL>.  TC; ptx_error(yyscanner, recognizer, (const char*)NULL);
 %%
 
-extern int g_error_detected;
-extern const char *g_filename;
-
-int ptx_error( const char *s )
+int ptx_error( yyscan_t yyscanner, ptx_recognizer* recognizer, const char *s )
 {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
 	int i;
-	g_error_detected = 1;
+	recognizer->g_error_detected = 1;
 	fflush(stdout);
 	if( s != NULL )
-		printf("%s:%u: Syntax error:\n\n", g_filename, ptx_lineno );
-	printf("   %s\n", linebuf );
+		printf("%s:%u Syntax error:\n\n", recognizer->gpgpu_ctx->g_filename, yylineno );
+	printf("   %s\n", recognizer->linebuf );
 	printf("   ");
-	for( i=0; i < col-1; i++ ) {
-		if( linebuf[i] == '\t' ) printf("\t");
+	for( i=0; i < recognizer->col-1; i++ ) {
+		if( recognizer->linebuf[i] == '\t' ) printf("\t");
 		else printf(" ");
 	}
 			
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.c b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.c
index e2ec3d0c86..09932d95f3 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.c
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.c
@@ -1,8 +1,9 @@
-/* A Bison parser, made by GNU Bison 3.0.4.  */
+/* A Bison parser, made by GNU Bison 3.5.1.  */
 
 /* Bison implementation for Yacc-like parsers in C
 
-   Copyright (C) 1984, 1989-1990, 2000-2015 Free Software Foundation, Inc.
+   Copyright (C) 1984, 1989-1990, 2000-2015, 2018-2020 Free Software Foundation,
+   Inc.
 
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
@@ -40,17 +41,20 @@
    define necessary library symbols; they are noted "INFRINGES ON
    USER NAME SPACE" below.  */
 
+/* Undocumented macros, especially those whose name start with YY_,
+   are private implementation details.  Do not rely on them.  */
+
 /* Identify Bison output.  */
 #define YYBISON 1
 
 /* Bison version.  */
-#define YYBISON_VERSION "3.0.4"
+#define YYBISON_VERSION "3.5.1"
 
 /* Skeleton name.  */
 #define YYSKELETON_NAME "yacc.c"
 
 /* Pure parsers.  */
-#define YYPURE 0
+#define YYPURE 2
 
 /* Push parsers.  */
 #define YYPUSH 0
@@ -66,18 +70,33 @@
 #define yydebug         ptx_debug
 #define yynerrs         ptx_nerrs
 
-#define yylval          ptx_lval
-#define yychar          ptx_char
+/* First part of user prologue.  */
+#line 30 "ptx.y"
 
-/* Copy the first part of user declarations.  */
+typedef void * yyscan_t;
+class ptx_recognizer;
+#include "../../libcuda_sim/gpgpu_context.h"
 
-#line 75 "ptx.tab.c" /* yacc.c:339  */
+#line 81 "ptx.tab.c"
 
+# ifndef YY_CAST
+#  ifdef __cplusplus
+#   define YY_CAST(Type, Val) static_cast<Type> (Val)
+#   define YY_REINTERPRET_CAST(Type, Val) reinterpret_cast<Type> (Val)
+#  else
+#   define YY_CAST(Type, Val) ((Type) (Val))
+#   define YY_REINTERPRET_CAST(Type, Val) ((Type) (Val))
+#  endif
+# endif
 # ifndef YY_NULLPTR
-#  if defined __cplusplus && 201103L <= __cplusplus
-#   define YY_NULLPTR nullptr
+#  if defined __cplusplus
+#   if 201103L <= __cplusplus
+#    define YY_NULLPTR nullptr
+#   else
+#    define YY_NULLPTR 0
+#   endif
 #  else
-#   define YY_NULLPTR 0
+#   define YY_NULLPTR ((void*)0)
 #  endif
 # endif
 
@@ -89,15 +108,13 @@
 # define YYERROR_VERBOSE 0
 #endif
 
-/* In a future release of Bison, this section will be replaced
-   by #include "ptx.tab.h".  */
-#ifndef YY_PTX_HOME_CUDA_SIM_PTX_TAB_H_INCLUDED
-# define YY_PTX_HOME_CUDA_SIM_PTX_TAB_H_INCLUDED
+/* Use api.header.include to #include this header
+   instead of duplicating it here.  */
+#ifndef YY_PTX_PTX_TAB_H_INCLUDED
+# define YY_PTX_PTX_TAB_H_INCLUDED
 /* Debug traces.  */
 #ifndef YYDEBUG
 # define YYDEBUG 0
-// FIXME schi add it
-int ptx_debug = 0;
 #endif
 #if YYDEBUG
 extern int ptx_debug;
@@ -283,13 +300,186 @@ extern int ptx_debug;
     PRMT_ECR_MODE = 430
   };
 #endif
+/* Tokens.  */
+#define STRING 258
+#define OPCODE 259
+#define WMMA_DIRECTIVE 260
+#define LAYOUT 261
+#define CONFIGURATION 262
+#define ALIGN_DIRECTIVE 263
+#define BRANCHTARGETS_DIRECTIVE 264
+#define BYTE_DIRECTIVE 265
+#define CALLPROTOTYPE_DIRECTIVE 266
+#define CALLTARGETS_DIRECTIVE 267
+#define CONST_DIRECTIVE 268
+#define CONSTPTR_DIRECTIVE 269
+#define PTR_DIRECTIVE 270
+#define ENTRY_DIRECTIVE 271
+#define EXTERN_DIRECTIVE 272
+#define FILE_DIRECTIVE 273
+#define FUNC_DIRECTIVE 274
+#define GLOBAL_DIRECTIVE 275
+#define LOCAL_DIRECTIVE 276
+#define LOC_DIRECTIVE 277
+#define MAXNCTAPERSM_DIRECTIVE 278
+#define MAXNNREG_DIRECTIVE 279
+#define MAXNTID_DIRECTIVE 280
+#define MINNCTAPERSM_DIRECTIVE 281
+#define PARAM_DIRECTIVE 282
+#define PRAGMA_DIRECTIVE 283
+#define REG_DIRECTIVE 284
+#define REQNTID_DIRECTIVE 285
+#define SECTION_DIRECTIVE 286
+#define SHARED_DIRECTIVE 287
+#define SREG_DIRECTIVE 288
+#define SSTARR_DIRECTIVE 289
+#define STRUCT_DIRECTIVE 290
+#define SURF_DIRECTIVE 291
+#define TARGET_DIRECTIVE 292
+#define TEX_DIRECTIVE 293
+#define UNION_DIRECTIVE 294
+#define VERSION_DIRECTIVE 295
+#define ADDRESS_SIZE_DIRECTIVE 296
+#define VISIBLE_DIRECTIVE 297
+#define WEAK_DIRECTIVE 298
+#define IDENTIFIER 299
+#define INT_OPERAND 300
+#define FLOAT_OPERAND 301
+#define DOUBLE_OPERAND 302
+#define S8_TYPE 303
+#define S16_TYPE 304
+#define S32_TYPE 305
+#define S64_TYPE 306
+#define U8_TYPE 307
+#define U16_TYPE 308
+#define U32_TYPE 309
+#define U64_TYPE 310
+#define F16_TYPE 311
+#define F32_TYPE 312
+#define F64_TYPE 313
+#define FF64_TYPE 314
+#define B8_TYPE 315
+#define B16_TYPE 316
+#define B32_TYPE 317
+#define B64_TYPE 318
+#define BB64_TYPE 319
+#define BB128_TYPE 320
+#define PRED_TYPE 321
+#define TEXREF_TYPE 322
+#define SAMPLERREF_TYPE 323
+#define SURFREF_TYPE 324
+#define V2_TYPE 325
+#define V3_TYPE 326
+#define V4_TYPE 327
+#define COMMA 328
+#define PRED 329
+#define HALF_OPTION 330
+#define EXTP_OPTION 331
+#define EQ_OPTION 332
+#define NE_OPTION 333
+#define LT_OPTION 334
+#define LE_OPTION 335
+#define GT_OPTION 336
+#define GE_OPTION 337
+#define LO_OPTION 338
+#define LS_OPTION 339
+#define HI_OPTION 340
+#define HS_OPTION 341
+#define EQU_OPTION 342
+#define NEU_OPTION 343
+#define LTU_OPTION 344
+#define LEU_OPTION 345
+#define GTU_OPTION 346
+#define GEU_OPTION 347
+#define NUM_OPTION 348
+#define NAN_OPTION 349
+#define CF_OPTION 350
+#define SF_OPTION 351
+#define NSF_OPTION 352
+#define LEFT_SQUARE_BRACKET 353
+#define RIGHT_SQUARE_BRACKET 354
+#define WIDE_OPTION 355
+#define SPECIAL_REGISTER 356
+#define MINUS 357
+#define PLUS 358
+#define COLON 359
+#define SEMI_COLON 360
+#define EXCLAMATION 361
+#define PIPE 362
+#define RIGHT_BRACE 363
+#define LEFT_BRACE 364
+#define EQUALS 365
+#define PERIOD 366
+#define BACKSLASH 367
+#define DIMENSION_MODIFIER 368
+#define RN_OPTION 369
+#define RZ_OPTION 370
+#define RM_OPTION 371
+#define RP_OPTION 372
+#define RNI_OPTION 373
+#define RZI_OPTION 374
+#define RMI_OPTION 375
+#define RPI_OPTION 376
+#define UNI_OPTION 377
+#define GEOM_MODIFIER_1D 378
+#define GEOM_MODIFIER_2D 379
+#define GEOM_MODIFIER_3D 380
+#define SAT_OPTION 381
+#define FTZ_OPTION 382
+#define NEG_OPTION 383
+#define SYNC_OPTION 384
+#define RED_OPTION 385
+#define ARRIVE_OPTION 386
+#define ATOMIC_POPC 387
+#define ATOMIC_AND 388
+#define ATOMIC_OR 389
+#define ATOMIC_XOR 390
+#define ATOMIC_CAS 391
+#define ATOMIC_EXCH 392
+#define ATOMIC_ADD 393
+#define ATOMIC_INC 394
+#define ATOMIC_DEC 395
+#define ATOMIC_MIN 396
+#define ATOMIC_MAX 397
+#define LEFT_ANGLE_BRACKET 398
+#define RIGHT_ANGLE_BRACKET 399
+#define LEFT_PAREN 400
+#define RIGHT_PAREN 401
+#define APPROX_OPTION 402
+#define FULL_OPTION 403
+#define ANY_OPTION 404
+#define ALL_OPTION 405
+#define BALLOT_OPTION 406
+#define GLOBAL_OPTION 407
+#define CTA_OPTION 408
+#define SYS_OPTION 409
+#define EXIT_OPTION 410
+#define ABS_OPTION 411
+#define TO_OPTION 412
+#define CA_OPTION 413
+#define CG_OPTION 414
+#define CS_OPTION 415
+#define LU_OPTION 416
+#define CV_OPTION 417
+#define WB_OPTION 418
+#define WT_OPTION 419
+#define NC_OPTION 420
+#define UP_OPTION 421
+#define DOWN_OPTION 422
+#define BFLY_OPTION 423
+#define IDX_OPTION 424
+#define PRMT_F4E_MODE 425
+#define PRMT_B4E_MODE 426
+#define PRMT_RC8_MODE 427
+#define PRMT_RC16_MODE 428
+#define PRMT_ECL_MODE 429
+#define PRMT_ECR_MODE 430
 
 /* Value type.  */
 #if ! defined YYSTYPE && ! defined YYSTYPE_IS_DECLARED
-typedef union YYSTYPE YYSTYPE;
 union YYSTYPE
 {
-#line 30 "ptx.y" /* yacc.c:355  */
+#line 42 "ptx.y"
 
   double double_value;
   float  float_value;
@@ -297,59 +487,107 @@ union YYSTYPE
   char * string_value;
   void * ptr_value;
 
-#line 299 "ptx.tab.c" /* yacc.c:355  */
+#line 491 "ptx.tab.c"
+
 };
+typedef union YYSTYPE YYSTYPE;
 # define YYSTYPE_IS_TRIVIAL 1
 # define YYSTYPE_IS_DECLARED 1
 #endif
 
 
-extern YYSTYPE ptx_lval;
 
-// extern "C" int ptx_parse();
+int ptx_parse (yyscan_t scanner, ptx_recognizer* recognizer);
 
-#endif /* !YY_PTX_HOME_CUDA_SIM_PTX_TAB_H_INCLUDED  */
+#endif /* !YY_PTX_PTX_TAB_H_INCLUDED  */
 
-/* Copy the second part of user declarations.  */
-#line 215 "ptx.y" /* yacc.c:358  */
+/* Second part of user prologue.  */
+#line 227 "ptx.y"
 
   	#include "ptx_parser.h"
 	#include <stdlib.h>
 	#include <string.h>
 	#include <math.h>
-	void syntax_not_implemented();
-	extern int g_func_decl;
-	int ptx_lex(void);
-	int ptx_error(const char *);
+	void syntax_not_implemented(yyscan_t yyscanner, ptx_recognizer* recognizer);
+	int ptx_lex(YYSTYPE * yylval_param, yyscan_t yyscanner, ptx_recognizer* recognizer);
+	int ptx_error( yyscan_t yyscanner, ptx_recognizer* recognizer, const char *s );
+
+#line 516 "ptx.tab.c"
 
-#line 326 "ptx.tab.c" /* yacc.c:358  */
 
 #ifdef short
 # undef short
 #endif
 
-#ifdef YYTYPE_UINT8
-typedef YYTYPE_UINT8 yytype_uint8;
-#else
-typedef unsigned char yytype_uint8;
+/* On compilers that do not define __PTRDIFF_MAX__ etc., make sure
+   <limits.h> and (if available) <stdint.h> are included
+   so that the code can choose integer types of a good width.  */
+
+#ifndef __PTRDIFF_MAX__
+# include <limits.h> /* INFRINGES ON USER NAME SPACE */
+# if defined __STDC_VERSION__ && 199901 <= __STDC_VERSION__
+#  include <stdint.h> /* INFRINGES ON USER NAME SPACE */
+#  define YY_STDINT_H
+# endif
 #endif
 
-#ifdef YYTYPE_INT8
-typedef YYTYPE_INT8 yytype_int8;
+/* Narrow types that promote to a signed type and that can represent a
+   signed or unsigned integer of at least N bits.  In tables they can
+   save space and decrease cache pressure.  Promoting to a signed type
+   helps avoid bugs in integer arithmetic.  */
+
+#ifdef __INT_LEAST8_MAX__
+typedef __INT_LEAST8_TYPE__ yytype_int8;
+#elif defined YY_STDINT_H
+typedef int_least8_t yytype_int8;
 #else
 typedef signed char yytype_int8;
 #endif
 
-#ifdef YYTYPE_UINT16
-typedef YYTYPE_UINT16 yytype_uint16;
+#ifdef __INT_LEAST16_MAX__
+typedef __INT_LEAST16_TYPE__ yytype_int16;
+#elif defined YY_STDINT_H
+typedef int_least16_t yytype_int16;
 #else
-typedef unsigned short int yytype_uint16;
+typedef short yytype_int16;
 #endif
 
-#ifdef YYTYPE_INT16
-typedef YYTYPE_INT16 yytype_int16;
+#if defined __UINT_LEAST8_MAX__ && __UINT_LEAST8_MAX__ <= __INT_MAX__
+typedef __UINT_LEAST8_TYPE__ yytype_uint8;
+#elif (!defined __UINT_LEAST8_MAX__ && defined YY_STDINT_H \
+       && UINT_LEAST8_MAX <= INT_MAX)
+typedef uint_least8_t yytype_uint8;
+#elif !defined __UINT_LEAST8_MAX__ && UCHAR_MAX <= INT_MAX
+typedef unsigned char yytype_uint8;
 #else
-typedef short int yytype_int16;
+typedef short yytype_uint8;
+#endif
+
+#if defined __UINT_LEAST16_MAX__ && __UINT_LEAST16_MAX__ <= __INT_MAX__
+typedef __UINT_LEAST16_TYPE__ yytype_uint16;
+#elif (!defined __UINT_LEAST16_MAX__ && defined YY_STDINT_H \
+       && UINT_LEAST16_MAX <= INT_MAX)
+typedef uint_least16_t yytype_uint16;
+#elif !defined __UINT_LEAST16_MAX__ && USHRT_MAX <= INT_MAX
+typedef unsigned short yytype_uint16;
+#else
+typedef int yytype_uint16;
+#endif
+
+#ifndef YYPTRDIFF_T
+# if defined __PTRDIFF_TYPE__ && defined __PTRDIFF_MAX__
+#  define YYPTRDIFF_T __PTRDIFF_TYPE__
+#  define YYPTRDIFF_MAXIMUM __PTRDIFF_MAX__
+# elif defined PTRDIFF_MAX
+#  ifndef ptrdiff_t
+#   include <stddef.h> /* INFRINGES ON USER NAME SPACE */
+#  endif
+#  define YYPTRDIFF_T ptrdiff_t
+#  define YYPTRDIFF_MAXIMUM PTRDIFF_MAX
+# else
+#  define YYPTRDIFF_T long
+#  define YYPTRDIFF_MAXIMUM LONG_MAX
+# endif
 #endif
 
 #ifndef YYSIZE_T
@@ -357,15 +595,27 @@ typedef short int yytype_int16;
 #  define YYSIZE_T __SIZE_TYPE__
 # elif defined size_t
 #  define YYSIZE_T size_t
-# elif ! defined YYSIZE_T
+# elif defined __STDC_VERSION__ && 199901 <= __STDC_VERSION__
 #  include <stddef.h> /* INFRINGES ON USER NAME SPACE */
 #  define YYSIZE_T size_t
 # else
-#  define YYSIZE_T unsigned int
+#  define YYSIZE_T unsigned
 # endif
 #endif
 
-#define YYSIZE_MAXIMUM ((YYSIZE_T) -1)
+#define YYSIZE_MAXIMUM                                  \
+  YY_CAST (YYPTRDIFF_T,                                 \
+           (YYPTRDIFF_MAXIMUM < YY_CAST (YYSIZE_T, -1)  \
+            ? YYPTRDIFF_MAXIMUM                         \
+            : YY_CAST (YYSIZE_T, -1)))
+
+#define YYSIZEOF(X) YY_CAST (YYPTRDIFF_T, sizeof (X))
+
+/* Stored state numbers (used for stacks). */
+typedef yytype_int16 yy_state_t;
+
+/* State numbers in computations.  */
+typedef int yy_state_fast_t;
 
 #ifndef YY_
 # if defined YYENABLE_NLS && YYENABLE_NLS
@@ -379,30 +629,19 @@ typedef short int yytype_int16;
 # endif
 #endif
 
-#ifndef YY_ATTRIBUTE
-# if (defined __GNUC__                                               \
-      && (2 < __GNUC__ || (__GNUC__ == 2 && 96 <= __GNUC_MINOR__)))  \
-     || defined __SUNPRO_C && 0x5110 <= __SUNPRO_C
-#  define YY_ATTRIBUTE(Spec) __attribute__(Spec)
+#ifndef YY_ATTRIBUTE_PURE
+# if defined __GNUC__ && 2 < __GNUC__ + (96 <= __GNUC_MINOR__)
+#  define YY_ATTRIBUTE_PURE __attribute__ ((__pure__))
 # else
-#  define YY_ATTRIBUTE(Spec) /* empty */
+#  define YY_ATTRIBUTE_PURE
 # endif
 #endif
 
-#ifndef YY_ATTRIBUTE_PURE
-# define YY_ATTRIBUTE_PURE   YY_ATTRIBUTE ((__pure__))
-#endif
-
 #ifndef YY_ATTRIBUTE_UNUSED
-# define YY_ATTRIBUTE_UNUSED YY_ATTRIBUTE ((__unused__))
-#endif
-
-#if !defined _Noreturn \
-     && (!defined __STDC_VERSION__ || __STDC_VERSION__ < 201112)
-# if defined _MSC_VER && 1200 <= _MSC_VER
-#  define _Noreturn __declspec (noreturn)
+# if defined __GNUC__ && 2 < __GNUC__ + (7 <= __GNUC_MINOR__)
+#  define YY_ATTRIBUTE_UNUSED __attribute__ ((__unused__))
 # else
-#  define _Noreturn YY_ATTRIBUTE ((__noreturn__))
+#  define YY_ATTRIBUTE_UNUSED
 # endif
 #endif
 
@@ -413,13 +652,13 @@ typedef short int yytype_int16;
 # define YYUSE(E) /* empty */
 #endif
 
-#if defined __GNUC__ && 407 <= __GNUC__ * 100 + __GNUC_MINOR__
+#if defined __GNUC__ && ! defined __ICC && 407 <= __GNUC__ * 100 + __GNUC_MINOR__
 /* Suppress an incorrect diagnostic about yylval being uninitialized.  */
-# define YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN \
-    _Pragma ("GCC diagnostic push") \
-    _Pragma ("GCC diagnostic ignored \"-Wuninitialized\"")\
+# define YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN                            \
+    _Pragma ("GCC diagnostic push")                                     \
+    _Pragma ("GCC diagnostic ignored \"-Wuninitialized\"")              \
     _Pragma ("GCC diagnostic ignored \"-Wmaybe-uninitialized\"")
-# define YY_IGNORE_MAYBE_UNINITIALIZED_END \
+# define YY_IGNORE_MAYBE_UNINITIALIZED_END      \
     _Pragma ("GCC diagnostic pop")
 #else
 # define YY_INITIAL_VALUE(Value) Value
@@ -432,6 +671,20 @@ typedef short int yytype_int16;
 # define YY_INITIAL_VALUE(Value) /* Nothing. */
 #endif
 
+#if defined __cplusplus && defined __GNUC__ && ! defined __ICC && 6 <= __GNUC__
+# define YY_IGNORE_USELESS_CAST_BEGIN                          \
+    _Pragma ("GCC diagnostic push")                            \
+    _Pragma ("GCC diagnostic ignored \"-Wuseless-cast\"")
+# define YY_IGNORE_USELESS_CAST_END            \
+    _Pragma ("GCC diagnostic pop")
+#endif
+#ifndef YY_IGNORE_USELESS_CAST_BEGIN
+# define YY_IGNORE_USELESS_CAST_BEGIN
+# define YY_IGNORE_USELESS_CAST_END
+#endif
+
+
+#define YY_ASSERT(E) ((void) (0 && (E)))
 
 #if ! defined yyoverflow || YYERROR_VERBOSE
 
@@ -508,17 +761,17 @@ void free (void *); /* INFRINGES ON USER NAME SPACE */
 /* A type that is properly aligned for any stack member.  */
 union yyalloc
 {
-  yytype_int16 yyss_alloc;
+  yy_state_t yyss_alloc;
   YYSTYPE yyvs_alloc;
 };
 
 /* The size of the maximum gap between one aligned stack and the next.  */
-# define YYSTACK_GAP_MAXIMUM (sizeof (union yyalloc) - 1)
+# define YYSTACK_GAP_MAXIMUM (YYSIZEOF (union yyalloc) - 1)
 
 /* The size of an array large to enough to hold all stacks, each with
    N elements.  */
 # define YYSTACK_BYTES(N) \
-     ((N) * (sizeof (yytype_int16) + sizeof (YYSTYPE)) \
+     ((N) * (YYSIZEOF (yy_state_t) + YYSIZEOF (YYSTYPE)) \
       + YYSTACK_GAP_MAXIMUM)
 
 # define YYCOPY_NEEDED 1
@@ -531,11 +784,11 @@ union yyalloc
 # define YYSTACK_RELOCATE(Stack_alloc, Stack)                           \
     do                                                                  \
       {                                                                 \
-        YYSIZE_T yynewbytes;                                            \
+        YYPTRDIFF_T yynewbytes;                                         \
         YYCOPY (&yyptr->Stack_alloc, Stack, yysize);                    \
         Stack = &yyptr->Stack_alloc;                                    \
-        yynewbytes = yystacksize * sizeof (*Stack) + YYSTACK_GAP_MAXIMUM; \
-        yyptr += yynewbytes / sizeof (*yyptr);                          \
+        yynewbytes = yystacksize * YYSIZEOF (*Stack) + YYSTACK_GAP_MAXIMUM; \
+        yyptr += yynewbytes / YYSIZEOF (*yyptr);                        \
       }                                                                 \
     while (0)
 
@@ -547,12 +800,12 @@ union yyalloc
 # ifndef YYCOPY
 #  if defined __GNUC__ && 1 < __GNUC__
 #   define YYCOPY(Dst, Src, Count) \
-      __builtin_memcpy (Dst, Src, (Count) * sizeof (*(Src)))
+      __builtin_memcpy (Dst, Src, YY_CAST (YYSIZE_T, (Count)) * sizeof (*(Src)))
 #  else
 #   define YYCOPY(Dst, Src, Count)              \
       do                                        \
         {                                       \
-          YYSIZE_T yyi;                         \
+          YYPTRDIFF_T yyi;                      \
           for (yyi = 0; yyi < (Count); yyi++)   \
             (Dst)[yyi] = (Src)[yyi];            \
         }                                       \
@@ -575,16 +828,17 @@ union yyalloc
 /* YYNSTATES -- Number of states.  */
 #define YYNSTATES  460
 
-/* YYTRANSLATE[YYX] -- Symbol number corresponding to YYX as returned
-   by yylex, with out-of-bounds checking.  */
 #define YYUNDEFTOK  2
 #define YYMAXUTOK   430
 
+
+/* YYTRANSLATE(TOKEN-NUM) -- Symbol number corresponding to TOKEN-NUM
+   as returned by yylex, with out-of-bounds checking.  */
 #define YYTRANSLATE(YYX)                                                \
-  ((unsigned int) (YYX) <= YYMAXUTOK ? yytranslate[YYX] : YYUNDEFTOK)
+  (0 <= (YYX) && (YYX) <= YYMAXUTOK ? yytranslate[YYX] : YYUNDEFTOK)
 
 /* YYTRANSLATE[TOKEN-NUM] -- Symbol number corresponding to TOKEN-NUM
-   as returned by yylex, without out-of-bounds checking.  */
+   as returned by yylex.  */
 static const yytype_uint8 yytranslate[] =
 {
        0,     2,     2,     2,     2,     2,     2,     2,     2,     2,
@@ -635,39 +889,39 @@ static const yytype_uint8 yytranslate[] =
 
 #if YYDEBUG
   /* YYRLINE[YYN] -- Source line where rule number YYN was defined.  */
-static const yytype_uint16 yyrline[] =
+static const yytype_int16 yyrline[] =
 {
-       0,   228,   228,   229,   230,   231,   234,   234,   235,   235,
-     235,   238,   242,   243,   246,   247,   250,   250,   250,   251,
-     251,   252,   255,   255,   255,   256,   259,   260,   261,   262,
-     263,   264,   265,   266,   269,   270,   271,   271,   273,   273,
-     274,   274,   276,   277,   278,   280,   281,   282,   283,   285,
-     287,   289,   290,   291,   292,   293,   294,   294,   295,   295,
-     298,   299,   300,   301,   302,   303,   304,   305,   306,   307,
-     308,   309,   312,   313,   314,   315,   318,   320,   321,   323,
-     324,   336,   337,   340,   341,   343,   344,   345,   346,   347,
-     348,   351,   353,   354,   355,   358,   359,   360,   361,   362,
-     363,   364,   365,   368,   369,   372,   373,   374,   377,   378,
-     379,   380,   381,   382,   383,   384,   385,   386,   387,   388,
-     389,   390,   391,   392,   393,   394,   395,   396,   397,   398,
-     401,   402,   404,   405,   413,   415,   417,   418,   420,   421,
-     422,   424,   425,   426,   428,   428,   429,   430,   431,   432,
-     435,   435,   436,   438,   439,   440,   441,   442,   443,   444,
-     445,   446,   447,   448,   449,   450,   453,   454,   456,   457,
-     458,   459,   460,   461,   462,   463,   464,   465,   466,   467,
-     468,   469,   470,   471,   472,   473,   474,   475,   476,   477,
-     478,   479,   480,   481,   482,   483,   484,   485,   486,   487,
-     488,   489,   490,   491,   492,   493,   494,   495,   496,   497,
-     498,   501,   502,   503,   504,   505,   506,   507,   508,   509,
-     510,   511,   514,   515,   518,   519,   520,   521,   524,   525,
-     526,   527,   530,   531,   532,   533,   534,   535,   536,   537,
-     538,   539,   540,   541,   542,   543,   544,   545,   546,   547,
-     550,   551,   552,   553,   554,   555,   558,   559,   568,   569,
-     571,   572,   573,   574,   575,   576,   577,   578,   579,   580,
-     581,   582,   583,   584,   585,   586,   587,   588,   589,   590,
-     593,   594,   595,   596,   597,   600,   600,   605,   606,   609,
-     610,   611,   612,   613,   616,   617,   618,   619,   620,   621,
-     622,   625,   626,   627,   630,   631,   632,   633,   634
+       0,   239,   239,   240,   241,   242,   245,   245,   246,   246,
+     246,   249,   253,   254,   257,   258,   261,   261,   261,   262,
+     262,   263,   266,   266,   266,   267,   270,   271,   272,   273,
+     274,   275,   276,   277,   280,   281,   282,   282,   284,   284,
+     285,   285,   287,   288,   289,   291,   292,   293,   294,   296,
+     298,   300,   301,   302,   303,   304,   305,   305,   306,   306,
+     309,   310,   311,   312,   313,   314,   315,   316,   317,   318,
+     319,   320,   323,   324,   325,   326,   329,   331,   332,   334,
+     335,   347,   348,   351,   352,   354,   355,   356,   357,   358,
+     359,   362,   364,   365,   366,   369,   370,   371,   372,   373,
+     374,   375,   376,   379,   380,   383,   384,   385,   388,   389,
+     390,   391,   392,   393,   394,   395,   396,   397,   398,   399,
+     400,   401,   402,   403,   404,   405,   406,   407,   408,   409,
+     412,   413,   415,   416,   424,   426,   428,   429,   431,   432,
+     433,   435,   436,   437,   439,   439,   440,   441,   442,   443,
+     446,   446,   447,   449,   450,   451,   452,   453,   454,   455,
+     456,   457,   458,   459,   460,   461,   464,   465,   467,   468,
+     469,   470,   471,   472,   473,   474,   475,   476,   477,   478,
+     479,   480,   481,   482,   483,   484,   485,   486,   487,   488,
+     489,   490,   491,   492,   493,   494,   495,   496,   497,   498,
+     499,   500,   501,   502,   503,   504,   505,   506,   507,   508,
+     509,   512,   513,   514,   515,   516,   517,   518,   519,   520,
+     521,   522,   525,   526,   529,   530,   531,   532,   535,   536,
+     537,   538,   541,   542,   543,   544,   545,   546,   547,   548,
+     549,   550,   551,   552,   553,   554,   555,   556,   557,   558,
+     561,   562,   563,   564,   565,   566,   569,   570,   579,   580,
+     582,   583,   584,   585,   586,   587,   588,   589,   590,   591,
+     592,   593,   594,   595,   596,   597,   598,   599,   600,   601,
+     604,   605,   606,   607,   608,   611,   611,   616,   617,   620,
+     621,   622,   623,   624,   627,   628,   629,   630,   631,   632,
+     633,   636,   637,   638,   641,   642,   643,   644,   645
 };
 #endif
 
@@ -740,7 +994,7 @@ static const char *const yytname[] =
 # ifdef YYPRINT
 /* YYTOKNUM[NUM] -- (External) token number corresponding to the
    (internal) symbol number NUM (which must be that of a token).  */
-static const yytype_uint16 yytoknum[] =
+static const yytype_int16 yytoknum[] =
 {
        0,   256,   257,   258,   259,   260,   261,   262,   263,   264,
      265,   266,   267,   268,   269,   270,   271,   272,   273,   274,
@@ -763,14 +1017,14 @@ static const yytype_uint16 yytoknum[] =
 };
 # endif
 
-#define YYPACT_NINF -313
+#define YYPACT_NINF (-313)
 
-#define yypact_value_is_default(Yystate) \
-  (!!((Yystate) == (-313)))
+#define yypact_value_is_default(Yyn) \
+  ((Yyn) == YYPACT_NINF)
 
-#define YYTABLE_NINF -153
+#define YYTABLE_NINF (-153)
 
-#define yytable_value_is_error(Yytable_value) \
+#define yytable_value_is_error(Yyn) \
   0
 
   /* YYPACT[STATE-NUM] -- Index in YYTABLE of the portion describing
@@ -828,7 +1082,7 @@ static const yytype_int16 yypact[] =
   /* YYDEFACT[STATE-NUM] -- Default reduction number in state STATE-NUM.
      Performed when YYTABLE does not specify something else to do.  Zero
      means the default is an error.  */
-static const yytype_uint16 yydefact[] =
+static const yytype_int16 yydefact[] =
 {
        2,     0,     1,     0,    95,     0,    26,    89,     0,    29,
       96,    97,     0,    98,     0,    92,    99,    93,   100,   101,
@@ -976,7 +1230,7 @@ static const yytype_int16 yytable[] =
       90,    91,   185,   437,   336,   176,   159,   328
 };
 
-static const yytype_uint16 yycheck[] =
+static const yytype_int16 yycheck[] =
 {
      125,   125,   110,   165,   111,    44,     5,    44,    44,    45,
      322,    45,    46,    47,    13,    16,    98,    73,    19,   149,
@@ -1134,7 +1388,7 @@ static const yytype_uint8 yyr1[] =
 };
 
   /* YYR2[YYN] -- Number of symbols on the right hand side of rule YYN.  */
-static const yytype_uint8 yyr2[] =
+static const yytype_int8 yyr2[] =
 {
        0,     2,     0,     2,     2,     2,     0,     3,     0,     0,
        5,     6,     2,     2,     1,     2,     0,     0,     7,     0,
@@ -1182,22 +1436,22 @@ static const yytype_uint8 yyr2[] =
 
 #define YYRECOVERING()  (!!yyerrstatus)
 
-#define YYBACKUP(Token, Value)                                  \
-do                                                              \
-  if (yychar == YYEMPTY)                                        \
-    {                                                           \
-      yychar = (Token);                                         \
-      yylval = (Value);                                         \
-      YYPOPSTACK (yylen);                                       \
-      yystate = *yyssp;                                         \
-      goto yybackup;                                            \
-    }                                                           \
-  else                                                          \
-    {                                                           \
-      yyerror (YY_("syntax error: cannot back up")); \
-      YYERROR;                                                  \
-    }                                                           \
-while (0)
+#define YYBACKUP(Token, Value)                                    \
+  do                                                              \
+    if (yychar == YYEMPTY)                                        \
+      {                                                           \
+        yychar = (Token);                                         \
+        yylval = (Value);                                         \
+        YYPOPSTACK (yylen);                                       \
+        yystate = *yyssp;                                         \
+        goto yybackup;                                            \
+      }                                                           \
+    else                                                          \
+      {                                                           \
+        yyerror (scanner, recognizer, YY_("syntax error: cannot back up")); \
+        YYERROR;                                                  \
+      }                                                           \
+  while (0)
 
 /* Error token number */
 #define YYTERROR        1
@@ -1231,43 +1485,47 @@ do {                                                                      \
     {                                                                     \
       YYFPRINTF (stderr, "%s ", Title);                                   \
       yy_symbol_print (stderr,                                            \
-                  Type, Value); \
+                  Type, Value, scanner, recognizer); \
       YYFPRINTF (stderr, "\n");                                           \
     }                                                                     \
 } while (0)
 
 
-/*----------------------------------------.
-| Print this symbol's value on YYOUTPUT.  |
-`----------------------------------------*/
+/*-----------------------------------.
+| Print this symbol's value on YYO.  |
+`-----------------------------------*/
 
 static void
-yy_symbol_value_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
+yy_symbol_value_print (FILE *yyo, int yytype, YYSTYPE const * const yyvaluep, yyscan_t scanner, ptx_recognizer* recognizer)
 {
-  FILE *yyo = yyoutput;
-  YYUSE (yyo);
+  FILE *yyoutput = yyo;
+  YYUSE (yyoutput);
+  YYUSE (scanner);
+  YYUSE (recognizer);
   if (!yyvaluep)
     return;
 # ifdef YYPRINT
   if (yytype < YYNTOKENS)
-    YYPRINT (yyoutput, yytoknum[yytype], *yyvaluep);
+    YYPRINT (yyo, yytoknum[yytype], *yyvaluep);
 # endif
+  YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN
   YYUSE (yytype);
+  YY_IGNORE_MAYBE_UNINITIALIZED_END
 }
 
 
-/*--------------------------------.
-| Print this symbol on YYOUTPUT.  |
-`--------------------------------*/
+/*---------------------------.
+| Print this symbol on YYO.  |
+`---------------------------*/
 
 static void
-yy_symbol_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
+yy_symbol_print (FILE *yyo, int yytype, YYSTYPE const * const yyvaluep, yyscan_t scanner, ptx_recognizer* recognizer)
 {
-  YYFPRINTF (yyoutput, "%s %s (",
+  YYFPRINTF (yyo, "%s %s (",
              yytype < YYNTOKENS ? "token" : "nterm", yytname[yytype]);
 
-  yy_symbol_value_print (yyoutput, yytype, yyvaluep);
-  YYFPRINTF (yyoutput, ")");
+  yy_symbol_value_print (yyo, yytype, yyvaluep, scanner, recognizer);
+  YYFPRINTF (yyo, ")");
 }
 
 /*------------------------------------------------------------------.
@@ -1276,7 +1534,7 @@ yy_symbol_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
 `------------------------------------------------------------------*/
 
 static void
-yy_stack_print (yytype_int16 *yybottom, yytype_int16 *yytop)
+yy_stack_print (yy_state_t *yybottom, yy_state_t *yytop)
 {
   YYFPRINTF (stderr, "Stack now");
   for (; yybottom <= yytop; yybottom++)
@@ -1299,21 +1557,21 @@ do {                                                            \
 `------------------------------------------------*/
 
 static void
-yy_reduce_print (yytype_int16 *yyssp, YYSTYPE *yyvsp, int yyrule)
+yy_reduce_print (yy_state_t *yyssp, YYSTYPE *yyvsp, int yyrule, yyscan_t scanner, ptx_recognizer* recognizer)
 {
-  unsigned long int yylno = yyrline[yyrule];
+  int yylno = yyrline[yyrule];
   int yynrhs = yyr2[yyrule];
   int yyi;
-  YYFPRINTF (stderr, "Reducing stack by rule %d (line %lu):\n",
+  YYFPRINTF (stderr, "Reducing stack by rule %d (line %d):\n",
              yyrule - 1, yylno);
   /* The symbols being reduced.  */
   for (yyi = 0; yyi < yynrhs; yyi++)
     {
       YYFPRINTF (stderr, "   $%d = ", yyi + 1);
       yy_symbol_print (stderr,
-                       yystos[yyssp[yyi + 1 - yynrhs]],
-                       &(yyvsp[(yyi + 1) - (yynrhs)])
-                                              );
+                       yystos[+yyssp[yyi + 1 - yynrhs]],
+                       &yyvsp[(yyi + 1) - (yynrhs)]
+                                              , scanner, recognizer);
       YYFPRINTF (stderr, "\n");
     }
 }
@@ -1321,7 +1579,7 @@ yy_reduce_print (yytype_int16 *yyssp, YYSTYPE *yyvsp, int yyrule)
 # define YY_REDUCE_PRINT(Rule)          \
 do {                                    \
   if (yydebug)                          \
-    yy_reduce_print (yyssp, yyvsp, Rule); \
+    yy_reduce_print (yyssp, yyvsp, Rule, scanner, recognizer); \
 } while (0)
 
 /* Nonzero means print parse trace.  It is left uninitialized so that
@@ -1356,13 +1614,13 @@ int yydebug;
 
 # ifndef yystrlen
 #  if defined __GLIBC__ && defined _STRING_H
-#   define yystrlen strlen
+#   define yystrlen(S) (YY_CAST (YYPTRDIFF_T, strlen (S)))
 #  else
 /* Return the length of YYSTR.  */
-static YYSIZE_T
+static YYPTRDIFF_T
 yystrlen (const char *yystr)
 {
-  YYSIZE_T yylen;
+  YYPTRDIFF_T yylen;
   for (yylen = 0; yystr[yylen]; yylen++)
     continue;
   return yylen;
@@ -1398,12 +1656,12 @@ yystpcpy (char *yydest, const char *yysrc)
    backslash-backslash).  YYSTR is taken from yytname.  If YYRES is
    null, do not copy; instead, return the length of what the result
    would have been.  */
-static YYSIZE_T
+static YYPTRDIFF_T
 yytnamerr (char *yyres, const char *yystr)
 {
   if (*yystr == '"')
     {
-      YYSIZE_T yyn = 0;
+      YYPTRDIFF_T yyn = 0;
       char const *yyp = yystr;
 
       for (;;)
@@ -1416,7 +1674,10 @@ yytnamerr (char *yyres, const char *yystr)
           case '\\':
             if (*++yyp != '\\')
               goto do_not_strip_quotes;
-            /* Fall through.  */
+            else
+              goto append;
+
+          append:
           default:
             if (yyres)
               yyres[yyn] = *yyp;
@@ -1431,10 +1692,10 @@ yytnamerr (char *yyres, const char *yystr)
     do_not_strip_quotes: ;
     }
 
-  if (! yyres)
+  if (yyres)
+    return yystpcpy (yyres, yystr) - yyres;
+  else
     return yystrlen (yystr);
-
-  return yystpcpy (yyres, yystr) - yyres;
 }
 # endif
 
@@ -1447,19 +1708,19 @@ yytnamerr (char *yyres, const char *yystr)
    *YYMSG_ALLOC to the required number of bytes.  Return 2 if the
    required number of bytes is too large to store.  */
 static int
-yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
-                yytype_int16 *yyssp, int yytoken)
+yysyntax_error (YYPTRDIFF_T *yymsg_alloc, char **yymsg,
+                yy_state_t *yyssp, int yytoken)
 {
-  YYSIZE_T yysize0 = yytnamerr (YY_NULLPTR, yytname[yytoken]);
-  YYSIZE_T yysize = yysize0;
   enum { YYERROR_VERBOSE_ARGS_MAXIMUM = 5 };
   /* Internationalized format string. */
   const char *yyformat = YY_NULLPTR;
-  /* Arguments of yyformat. */
+  /* Arguments of yyformat: reported tokens (one for the "unexpected",
+     one per "expected"). */
   char const *yyarg[YYERROR_VERBOSE_ARGS_MAXIMUM];
-  /* Number of reported tokens (one for the "unexpected", one per
-     "expected"). */
+  /* Actual size of YYARG. */
   int yycount = 0;
+  /* Cumulated lengths of YYARG.  */
+  YYPTRDIFF_T yysize = 0;
 
   /* There are many possibilities here to consider:
      - If this state is a consistent state with a default action, then
@@ -1486,7 +1747,9 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
   */
   if (yytoken != YYEMPTY)
     {
-      int yyn = yypact[*yyssp];
+      int yyn = yypact[+*yyssp];
+      YYPTRDIFF_T yysize0 = yytnamerr (YY_NULLPTR, yytname[yytoken]);
+      yysize = yysize0;
       yyarg[yycount++] = yytname[yytoken];
       if (!yypact_value_is_default (yyn))
         {
@@ -1511,11 +1774,12 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
                   }
                 yyarg[yycount++] = yytname[yyx];
                 {
-                  YYSIZE_T yysize1 = yysize + yytnamerr (YY_NULLPTR, yytname[yyx]);
-                  if (! (yysize <= yysize1
-                         && yysize1 <= YYSTACK_ALLOC_MAXIMUM))
+                  YYPTRDIFF_T yysize1
+                    = yysize + yytnamerr (YY_NULLPTR, yytname[yyx]);
+                  if (yysize <= yysize1 && yysize1 <= YYSTACK_ALLOC_MAXIMUM)
+                    yysize = yysize1;
+                  else
                     return 2;
-                  yysize = yysize1;
                 }
               }
         }
@@ -1527,6 +1791,7 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
       case N:                               \
         yyformat = S;                       \
       break
+    default: /* Avoid compiler warnings. */
       YYCASE_(0, YY_("syntax error"));
       YYCASE_(1, YY_("syntax error, unexpected %s"));
       YYCASE_(2, YY_("syntax error, unexpected %s, expecting %s"));
@@ -1537,10 +1802,13 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
     }
 
   {
-    YYSIZE_T yysize1 = yysize + yystrlen (yyformat);
-    if (! (yysize <= yysize1 && yysize1 <= YYSTACK_ALLOC_MAXIMUM))
+    /* Don't count the "%s"s in the final size, but reserve room for
+       the terminator.  */
+    YYPTRDIFF_T yysize1 = yysize + (yystrlen (yyformat) - 2 * yycount) + 1;
+    if (yysize <= yysize1 && yysize1 <= YYSTACK_ALLOC_MAXIMUM)
+      yysize = yysize1;
+    else
       return 2;
-    yysize = yysize1;
   }
 
   if (*yymsg_alloc < yysize)
@@ -1566,8 +1834,8 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
         }
       else
         {
-          yyp++;
-          yyformat++;
+          ++yyp;
+          ++yyformat;
         }
   }
   return 0;
@@ -1579,9 +1847,11 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
 `-----------------------------------------------*/
 
 static void
-yydestruct (const char *yymsg, int yytype, YYSTYPE *yyvaluep)
+yydestruct (const char *yymsg, int yytype, YYSTYPE *yyvaluep, yyscan_t scanner, ptx_recognizer* recognizer)
 {
   YYUSE (yyvaluep);
+  YYUSE (scanner);
+  YYUSE (recognizer);
   if (!yymsg)
     yymsg = "Deleting";
   YY_SYMBOL_PRINT (yymsg, yytype, yyvaluep, yylocationp);
@@ -1594,23 +1864,27 @@ yydestruct (const char *yymsg, int yytype, YYSTYPE *yyvaluep)
 
 
 
-/* The lookahead symbol.  */
-int yychar;
-
-/* The semantic value of the lookahead symbol.  */
-YYSTYPE yylval;
-/* Number of syntax errors so far.  */
-int yynerrs;
-
-
 /*----------.
 | yyparse.  |
 `----------*/
 
 int
-yyparse (void)
+yyparse (yyscan_t scanner, ptx_recognizer* recognizer)
 {
-    int yystate;
+/* The lookahead symbol.  */
+int yychar;
+
+
+/* The semantic value of the lookahead symbol.  */
+/* Default value used for initialization, for pacifying older GCCs
+   or non-GCC compilers.  */
+YY_INITIAL_VALUE (static YYSTYPE yyval_default;)
+YYSTYPE yylval YY_INITIAL_VALUE (= yyval_default);
+
+    /* Number of syntax errors so far.  */
+    int yynerrs;
+
+    yy_state_fast_t yystate;
     /* Number of tokens to shift before error messages enabled.  */
     int yyerrstatus;
 
@@ -1622,16 +1896,16 @@ yyparse (void)
        to reallocate them elsewhere.  */
 
     /* The state stack.  */
-    yytype_int16 yyssa[YYINITDEPTH];
-    yytype_int16 *yyss;
-    yytype_int16 *yyssp;
+    yy_state_t yyssa[YYINITDEPTH];
+    yy_state_t *yyss;
+    yy_state_t *yyssp;
 
     /* The semantic value stack.  */
     YYSTYPE yyvsa[YYINITDEPTH];
     YYSTYPE *yyvs;
     YYSTYPE *yyvsp;
 
-    YYSIZE_T yystacksize;
+    YYPTRDIFF_T yystacksize;
 
   int yyn;
   int yyresult;
@@ -1645,7 +1919,7 @@ yyparse (void)
   /* Buffer for error messages, and its allocated size.  */
   char yymsgbuf[128];
   char *yymsg = yymsgbuf;
-  YYSIZE_T yymsg_alloc = sizeof yymsgbuf;
+  YYPTRDIFF_T yymsg_alloc = sizeof yymsgbuf;
 #endif
 
 #define YYPOPSTACK(N)   (yyvsp -= (N), yyssp -= (N))
@@ -1666,46 +1940,54 @@ yyparse (void)
   yychar = YYEMPTY; /* Cause a token to be read.  */
   goto yysetstate;
 
+
 /*------------------------------------------------------------.
-| yynewstate -- Push a new state, which is found in yystate.  |
+| yynewstate -- push a new state, which is found in yystate.  |
 `------------------------------------------------------------*/
- yynewstate:
+yynewstate:
   /* In all cases, when you get here, the value and location stacks
      have just been pushed.  So pushing a state here evens the stacks.  */
   yyssp++;
 
- yysetstate:
-  *yyssp = yystate;
+
+/*--------------------------------------------------------------------.
+| yysetstate -- set current state (the top of the stack) to yystate.  |
+`--------------------------------------------------------------------*/
+yysetstate:
+  YYDPRINTF ((stderr, "Entering state %d\n", yystate));
+  YY_ASSERT (0 <= yystate && yystate < YYNSTATES);
+  YY_IGNORE_USELESS_CAST_BEGIN
+  *yyssp = YY_CAST (yy_state_t, yystate);
+  YY_IGNORE_USELESS_CAST_END
 
   if (yyss + yystacksize - 1 <= yyssp)
+#if !defined yyoverflow && !defined YYSTACK_RELOCATE
+    goto yyexhaustedlab;
+#else
     {
       /* Get the current used size of the three stacks, in elements.  */
-      YYSIZE_T yysize = yyssp - yyss + 1;
+      YYPTRDIFF_T yysize = yyssp - yyss + 1;
 
-#ifdef yyoverflow
+# if defined yyoverflow
       {
         /* Give user a chance to reallocate the stack.  Use copies of
            these so that the &'s don't force the real ones into
            memory.  */
+        yy_state_t *yyss1 = yyss;
         YYSTYPE *yyvs1 = yyvs;
-        yytype_int16 *yyss1 = yyss;
 
         /* Each stack pointer address is followed by the size of the
            data in use in that stack, in bytes.  This used to be a
            conditional around just the two extra args, but that might
            be undefined if yyoverflow is a macro.  */
         yyoverflow (YY_("memory exhausted"),
-                    &yyss1, yysize * sizeof (*yyssp),
-                    &yyvs1, yysize * sizeof (*yyvsp),
+                    &yyss1, yysize * YYSIZEOF (*yyssp),
+                    &yyvs1, yysize * YYSIZEOF (*yyvsp),
                     &yystacksize);
-
         yyss = yyss1;
         yyvs = yyvs1;
       }
-#else /* no yyoverflow */
-# ifndef YYSTACK_RELOCATE
-      goto yyexhaustedlab;
-# else
+# else /* defined YYSTACK_RELOCATE */
       /* Extend the stack our own way.  */
       if (YYMAXDEPTH <= yystacksize)
         goto yyexhaustedlab;
@@ -1714,42 +1996,43 @@ yyparse (void)
         yystacksize = YYMAXDEPTH;
 
       {
-        yytype_int16 *yyss1 = yyss;
+        yy_state_t *yyss1 = yyss;
         union yyalloc *yyptr =
-          (union yyalloc *) YYSTACK_ALLOC (YYSTACK_BYTES (yystacksize));
+          YY_CAST (union yyalloc *,
+                   YYSTACK_ALLOC (YY_CAST (YYSIZE_T, YYSTACK_BYTES (yystacksize))));
         if (! yyptr)
           goto yyexhaustedlab;
         YYSTACK_RELOCATE (yyss_alloc, yyss);
         YYSTACK_RELOCATE (yyvs_alloc, yyvs);
-#  undef YYSTACK_RELOCATE
+# undef YYSTACK_RELOCATE
         if (yyss1 != yyssa)
           YYSTACK_FREE (yyss1);
       }
 # endif
-#endif /* no yyoverflow */
 
       yyssp = yyss + yysize - 1;
       yyvsp = yyvs + yysize - 1;
 
-      YYDPRINTF ((stderr, "Stack size increased to %lu\n",
-                  (unsigned long int) yystacksize));
+      YY_IGNORE_USELESS_CAST_BEGIN
+      YYDPRINTF ((stderr, "Stack size increased to %ld\n",
+                  YY_CAST (long, yystacksize)));
+      YY_IGNORE_USELESS_CAST_END
 
       if (yyss + yystacksize - 1 <= yyssp)
         YYABORT;
     }
-
-  YYDPRINTF ((stderr, "Entering state %d\n", yystate));
+#endif /* !defined yyoverflow && !defined YYSTACK_RELOCATE */
 
   if (yystate == YYFINAL)
     YYACCEPT;
 
   goto yybackup;
 
+
 /*-----------.
 | yybackup.  |
 `-----------*/
 yybackup:
-
   /* Do appropriate processing given the current state.  Read a
      lookahead token if we need one and don't already have one.  */
 
@@ -1764,7 +2047,7 @@ yybackup:
   if (yychar == YYEMPTY)
     {
       YYDPRINTF ((stderr, "Reading a token: "));
-      yychar = yylex ();
+      yychar = yylex (&yylval, scanner, recognizer);
     }
 
   if (yychar <= YYEOF)
@@ -1799,15 +2082,13 @@ yybackup:
 
   /* Shift the lookahead token.  */
   YY_SYMBOL_PRINT ("Shifting", yytoken, &yylval, &yylloc);
-
-  /* Discard the shifted token.  */
-  yychar = YYEMPTY;
-
   yystate = yyn;
   YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN
   *++yyvsp = yylval;
   YY_IGNORE_MAYBE_UNINITIALIZED_END
 
+  /* Discard the shifted token.  */
+  yychar = YYEMPTY;
   goto yynewstate;
 
 
@@ -1822,7 +2103,7 @@ yydefault:
 
 
 /*-----------------------------.
-| yyreduce -- Do a reduction.  |
+| yyreduce -- do a reduction.  |
 `-----------------------------*/
 yyreduce:
   /* yyn is the number of a rule to reduce with.  */
@@ -1842,384 +2123,384 @@ yyreduce:
   YY_REDUCE_PRINT (yyn);
   switch (yyn)
     {
-        case 6:
-#line 234 "ptx.y" /* yacc.c:1646  */
-    { set_symtab((yyvsp[0].ptr_value)); func_header(".skip"); }
-#line 1849 "ptx.tab.c" /* yacc.c:1646  */
+  case 6:
+#line 245 "ptx.y"
+                             { recognizer->set_symtab((yyvsp[0].ptr_value)); recognizer->func_header(".skip"); }
+#line 2130 "ptx.tab.c"
     break;
 
   case 7:
-#line 234 "ptx.y" /* yacc.c:1646  */
-    { end_function(); }
-#line 1855 "ptx.tab.c" /* yacc.c:1646  */
+#line 245 "ptx.y"
+                                                                                                               { recognizer->end_function(); }
+#line 2136 "ptx.tab.c"
     break;
 
   case 8:
-#line 235 "ptx.y" /* yacc.c:1646  */
-    { set_symtab((yyvsp[0].ptr_value)); }
-#line 1861 "ptx.tab.c" /* yacc.c:1646  */
+#line 246 "ptx.y"
+                        { recognizer->set_symtab((yyvsp[0].ptr_value)); }
+#line 2142 "ptx.tab.c"
     break;
 
   case 9:
-#line 235 "ptx.y" /* yacc.c:1646  */
-    { func_header(".skip"); }
-#line 1867 "ptx.tab.c" /* yacc.c:1646  */
+#line 246 "ptx.y"
+                                                                        { recognizer->func_header(".skip"); }
+#line 2148 "ptx.tab.c"
     break;
 
   case 10:
-#line 235 "ptx.y" /* yacc.c:1646  */
-    { end_function(); }
-#line 1873 "ptx.tab.c" /* yacc.c:1646  */
+#line 246 "ptx.y"
+                                                                                                                              { recognizer->end_function(); }
+#line 2154 "ptx.tab.c"
     break;
 
   case 11:
-#line 238 "ptx.y" /* yacc.c:1646  */
-    {func_header_info_int(".maxntid", (yyvsp[-4].int_value));
-										func_header_info_int(",", (yyvsp[-2].int_value));
-										func_header_info_int(",", (yyvsp[0].int_value)); 
-                                                                                maxnt_id((yyvsp[-4].int_value), (yyvsp[-2].int_value), (yyvsp[0].int_value));}
-#line 1882 "ptx.tab.c" /* yacc.c:1646  */
+#line 249 "ptx.y"
+                                                                              {recognizer->func_header_info_int(".maxntid", (yyvsp[-4].int_value));
+										recognizer->func_header_info_int(",", (yyvsp[-2].int_value));
+										recognizer->func_header_info_int(",", (yyvsp[0].int_value));
+                                                                                recognizer->maxnt_id((yyvsp[-4].int_value), (yyvsp[-2].int_value), (yyvsp[0].int_value));}
+#line 2163 "ptx.tab.c"
     break;
 
   case 12:
-#line 242 "ptx.y" /* yacc.c:1646  */
-    { func_header_info_int(".minnctapersm", (yyvsp[0].int_value)); printf("GPGPU-Sim: Warning: .minnctapersm ignored. \n"); }
-#line 1888 "ptx.tab.c" /* yacc.c:1646  */
+#line 253 "ptx.y"
+                                             { recognizer->func_header_info_int(".minnctapersm", (yyvsp[0].int_value)); printf("GPGPU-Sim: Warning: .minnctapersm ignored. \n"); }
+#line 2169 "ptx.tab.c"
     break;
 
   case 13:
-#line 243 "ptx.y" /* yacc.c:1646  */
-    { func_header_info_int(".maxnctapersm", (yyvsp[0].int_value)); printf("GPGPU-Sim: Warning: .maxnctapersm ignored. \n"); }
-#line 1894 "ptx.tab.c" /* yacc.c:1646  */
+#line 254 "ptx.y"
+                                             { recognizer->func_header_info_int(".maxnctapersm", (yyvsp[0].int_value)); printf("GPGPU-Sim: Warning: .maxnctapersm ignored. \n"); }
+#line 2175 "ptx.tab.c"
     break;
 
   case 16:
-#line 250 "ptx.y" /* yacc.c:1646  */
-    { start_function((yyvsp[-1].int_value)); func_header_info("(");}
-#line 1900 "ptx.tab.c" /* yacc.c:1646  */
+#line 261 "ptx.y"
+                                               { recognizer->start_function((yyvsp[-1].int_value)); recognizer->func_header_info("(");}
+#line 2181 "ptx.tab.c"
     break;
 
   case 17:
-#line 250 "ptx.y" /* yacc.c:1646  */
-    {func_header_info(")");}
-#line 1906 "ptx.tab.c" /* yacc.c:1646  */
+#line 261 "ptx.y"
+                                                                                                                                             {recognizer->func_header_info(")");}
+#line 2187 "ptx.tab.c"
     break;
 
   case 18:
-#line 250 "ptx.y" /* yacc.c:1646  */
-    { (yyval.ptr_value) = reset_symtab(); }
-#line 1912 "ptx.tab.c" /* yacc.c:1646  */
+#line 261 "ptx.y"
+                                                                                                                                                                                                       { (yyval.ptr_value) = recognizer->reset_symtab(); }
+#line 2193 "ptx.tab.c"
     break;
 
   case 19:
-#line 251 "ptx.y" /* yacc.c:1646  */
-    { start_function((yyvsp[0].int_value)); }
-#line 1918 "ptx.tab.c" /* yacc.c:1646  */
+#line 262 "ptx.y"
+                               { recognizer->start_function((yyvsp[0].int_value)); }
+#line 2199 "ptx.tab.c"
     break;
 
   case 20:
-#line 251 "ptx.y" /* yacc.c:1646  */
-    { (yyval.ptr_value) = reset_symtab(); }
-#line 1924 "ptx.tab.c" /* yacc.c:1646  */
+#line 262 "ptx.y"
+                                                                                        { (yyval.ptr_value) = recognizer->reset_symtab(); }
+#line 2205 "ptx.tab.c"
     break;
 
   case 21:
-#line 252 "ptx.y" /* yacc.c:1646  */
-    { start_function((yyvsp[0].int_value)); add_function_name(""); g_func_decl=0; (yyval.ptr_value) = reset_symtab(); }
-#line 1930 "ptx.tab.c" /* yacc.c:1646  */
+#line 263 "ptx.y"
+                               { recognizer->start_function((yyvsp[0].int_value)); recognizer->add_function_name(""); recognizer->g_func_decl=0; (yyval.ptr_value) = recognizer->reset_symtab(); }
+#line 2211 "ptx.tab.c"
     break;
 
   case 22:
-#line 255 "ptx.y" /* yacc.c:1646  */
-    { add_function_name((yyvsp[0].string_value)); }
-#line 1936 "ptx.tab.c" /* yacc.c:1646  */
+#line 266 "ptx.y"
+                                 { recognizer->add_function_name((yyvsp[0].string_value)); }
+#line 2217 "ptx.tab.c"
     break;
 
   case 23:
-#line 255 "ptx.y" /* yacc.c:1646  */
-    {func_header_info("(");}
-#line 1942 "ptx.tab.c" /* yacc.c:1646  */
+#line 266 "ptx.y"
+                                                                                   {recognizer->func_header_info("(");}
+#line 2223 "ptx.tab.c"
     break;
 
   case 24:
-#line 255 "ptx.y" /* yacc.c:1646  */
-    { g_func_decl=0; func_header_info(")"); }
-#line 1948 "ptx.tab.c" /* yacc.c:1646  */
+#line 266 "ptx.y"
+                                                                                                                                               { recognizer->g_func_decl=0; recognizer->func_header_info(")"); }
+#line 2229 "ptx.tab.c"
     break;
 
   case 25:
-#line 256 "ptx.y" /* yacc.c:1646  */
-    { add_function_name((yyvsp[0].string_value)); g_func_decl=0; }
-#line 1954 "ptx.tab.c" /* yacc.c:1646  */
+#line 267 "ptx.y"
+                     { recognizer->add_function_name((yyvsp[0].string_value)); recognizer->g_func_decl=0; }
+#line 2235 "ptx.tab.c"
     break;
 
   case 26:
-#line 259 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 1; g_func_decl=1; func_header(".entry"); }
-#line 1960 "ptx.tab.c" /* yacc.c:1646  */
+#line 270 "ptx.y"
+                                      { (yyval.int_value) = 1; recognizer->g_func_decl=1; recognizer->func_header(".entry"); }
+#line 2241 "ptx.tab.c"
     break;
 
   case 27:
-#line 260 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 1; g_func_decl=1; func_header(".entry"); }
-#line 1966 "ptx.tab.c" /* yacc.c:1646  */
+#line 271 "ptx.y"
+                                            { (yyval.int_value) = 1; recognizer->g_func_decl=1; recognizer->func_header(".entry"); }
+#line 2247 "ptx.tab.c"
     break;
 
   case 28:
-#line 261 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 1; g_func_decl=1; func_header(".entry"); }
-#line 1972 "ptx.tab.c" /* yacc.c:1646  */
+#line 272 "ptx.y"
+                                         { (yyval.int_value) = 1; recognizer->g_func_decl=1; recognizer->func_header(".entry"); }
+#line 2253 "ptx.tab.c"
     break;
 
   case 29:
-#line 262 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 0; g_func_decl=1; func_header(".func"); }
-#line 1978 "ptx.tab.c" /* yacc.c:1646  */
+#line 273 "ptx.y"
+                         { (yyval.int_value) = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+#line 2259 "ptx.tab.c"
     break;
 
   case 30:
-#line 263 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 0; g_func_decl=1; func_header(".func"); }
-#line 1984 "ptx.tab.c" /* yacc.c:1646  */
+#line 274 "ptx.y"
+                                           { (yyval.int_value) = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+#line 2265 "ptx.tab.c"
     break;
 
   case 31:
-#line 264 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 0; g_func_decl=1; func_header(".func"); }
-#line 1990 "ptx.tab.c" /* yacc.c:1646  */
+#line 275 "ptx.y"
+                                        { (yyval.int_value) = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+#line 2271 "ptx.tab.c"
     break;
 
   case 32:
-#line 265 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 2; g_func_decl=1; func_header(".func"); }
-#line 1996 "ptx.tab.c" /* yacc.c:1646  */
+#line 276 "ptx.y"
+                                          { (yyval.int_value) = 2; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+#line 2277 "ptx.tab.c"
     break;
 
   case 33:
-#line 266 "ptx.y" /* yacc.c:1646  */
-    { (yyval.int_value) = 0; g_func_decl=1; func_header(".func"); }
-#line 2002 "ptx.tab.c" /* yacc.c:1646  */
+#line 277 "ptx.y"
+                                        { (yyval.int_value) = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+#line 2283 "ptx.tab.c"
     break;
 
   case 35:
-#line 270 "ptx.y" /* yacc.c:1646  */
-    { add_directive(); }
-#line 2008 "ptx.tab.c" /* yacc.c:1646  */
+#line 281 "ptx.y"
+                      { recognizer->add_directive(); }
+#line 2289 "ptx.tab.c"
     break;
 
   case 36:
-#line 271 "ptx.y" /* yacc.c:1646  */
-    {func_header_info(",");}
-#line 2014 "ptx.tab.c" /* yacc.c:1646  */
+#line 282 "ptx.y"
+                           {recognizer->func_header_info(",");}
+#line 2295 "ptx.tab.c"
     break;
 
   case 37:
-#line 271 "ptx.y" /* yacc.c:1646  */
-    { add_directive(); }
-#line 2020 "ptx.tab.c" /* yacc.c:1646  */
+#line 282 "ptx.y"
+                                                                            { recognizer->add_directive(); }
+#line 2301 "ptx.tab.c"
     break;
 
   case 38:
-#line 273 "ptx.y" /* yacc.c:1646  */
-    { add_space_spec(param_space_unclassified,0); }
-#line 2026 "ptx.tab.c" /* yacc.c:1646  */
+#line 284 "ptx.y"
+                             { recognizer->add_space_spec(param_space_unclassified,0); }
+#line 2307 "ptx.tab.c"
     break;
 
   case 39:
-#line 273 "ptx.y" /* yacc.c:1646  */
-    { add_function_arg(); }
-#line 2032 "ptx.tab.c" /* yacc.c:1646  */
+#line 284 "ptx.y"
+                                                                                                                                { recognizer->add_function_arg(); }
+#line 2313 "ptx.tab.c"
     break;
 
   case 40:
-#line 274 "ptx.y" /* yacc.c:1646  */
-    { add_space_spec(reg_space,0); }
-#line 2038 "ptx.tab.c" /* yacc.c:1646  */
+#line 285 "ptx.y"
+                        { recognizer->add_space_spec(reg_space,0); }
+#line 2319 "ptx.tab.c"
     break;
 
   case 41:
-#line 274 "ptx.y" /* yacc.c:1646  */
-    { add_function_arg(); }
-#line 2044 "ptx.tab.c" /* yacc.c:1646  */
+#line 285 "ptx.y"
+                                                                                                   { recognizer->add_function_arg(); }
+#line 2325 "ptx.tab.c"
     break;
 
   case 45:
-#line 280 "ptx.y" /* yacc.c:1646  */
-    { add_ptr_spec(global_space); }
-#line 2050 "ptx.tab.c" /* yacc.c:1646  */
+#line 291 "ptx.y"
+                                 { recognizer->add_ptr_spec(global_space); }
+#line 2331 "ptx.tab.c"
     break;
 
   case 46:
-#line 281 "ptx.y" /* yacc.c:1646  */
-    { add_ptr_spec(local_space); }
-#line 2056 "ptx.tab.c" /* yacc.c:1646  */
+#line 292 "ptx.y"
+                                 { recognizer->add_ptr_spec(local_space); }
+#line 2337 "ptx.tab.c"
     break;
 
   case 47:
-#line 282 "ptx.y" /* yacc.c:1646  */
-    { add_ptr_spec(shared_space); }
-#line 2062 "ptx.tab.c" /* yacc.c:1646  */
+#line 293 "ptx.y"
+                                 { recognizer->add_ptr_spec(shared_space); }
+#line 2343 "ptx.tab.c"
     break;
 
   case 48:
-#line 283 "ptx.y" /* yacc.c:1646  */
-    { add_ptr_spec(global_space); }
-#line 2068 "ptx.tab.c" /* yacc.c:1646  */
+#line 294 "ptx.y"
+                                            { recognizer->add_ptr_spec(global_space); }
+#line 2349 "ptx.tab.c"
     break;
 
   case 51:
-#line 289 "ptx.y" /* yacc.c:1646  */
-    { add_directive(); }
-#line 2074 "ptx.tab.c" /* yacc.c:1646  */
+#line 300 "ptx.y"
+                                    { recognizer->add_directive(); }
+#line 2355 "ptx.tab.c"
     break;
 
   case 52:
-#line 290 "ptx.y" /* yacc.c:1646  */
-    {printf("Prototype statement detected. WARNING: this is not supported yet on GPGPU-SIM\n"); }
-#line 2080 "ptx.tab.c" /* yacc.c:1646  */
+#line 301 "ptx.y"
+                                     {printf("Prototype statement detected. WARNING: this is not supported yet on GPGPU-SIM\n"); }
+#line 2361 "ptx.tab.c"
     break;
 
   case 53:
-#line 291 "ptx.y" /* yacc.c:1646  */
-    { add_instruction(); }
-#line 2086 "ptx.tab.c" /* yacc.c:1646  */
+#line 302 "ptx.y"
+                                { recognizer->add_instruction(); }
+#line 2367 "ptx.tab.c"
     break;
 
   case 54:
-#line 292 "ptx.y" /* yacc.c:1646  */
-    { add_directive(); }
-#line 2092 "ptx.tab.c" /* yacc.c:1646  */
+#line 303 "ptx.y"
+                                             { recognizer->add_directive(); }
+#line 2373 "ptx.tab.c"
     break;
 
   case 55:
-#line 293 "ptx.y" /* yacc.c:1646  */
-    { add_instruction(); }
-#line 2098 "ptx.tab.c" /* yacc.c:1646  */
+#line 304 "ptx.y"
+                                               { recognizer->add_instruction(); }
+#line 2379 "ptx.tab.c"
     break;
 
   case 56:
-#line 294 "ptx.y" /* yacc.c:1646  */
-    {start_inst_group();}
-#line 2104 "ptx.tab.c" /* yacc.c:1646  */
+#line 305 "ptx.y"
+                         {recognizer->start_inst_group();}
+#line 2385 "ptx.tab.c"
     break;
 
   case 57:
-#line 294 "ptx.y" /* yacc.c:1646  */
-    {end_inst_group();}
-#line 2110 "ptx.tab.c" /* yacc.c:1646  */
+#line 305 "ptx.y"
+                                                                           {recognizer->end_inst_group();}
+#line 2391 "ptx.tab.c"
     break;
 
   case 58:
-#line 295 "ptx.y" /* yacc.c:1646  */
-    {start_inst_group();}
-#line 2116 "ptx.tab.c" /* yacc.c:1646  */
+#line 306 "ptx.y"
+          {recognizer->start_inst_group();}
+#line 2397 "ptx.tab.c"
     break;
 
   case 59:
-#line 295 "ptx.y" /* yacc.c:1646  */
-    {end_inst_group();}
-#line 2122 "ptx.tab.c" /* yacc.c:1646  */
+#line 306 "ptx.y"
+                                                            {recognizer->end_inst_group();}
+#line 2403 "ptx.tab.c"
     break;
 
   case 61:
-#line 299 "ptx.y" /* yacc.c:1646  */
-    { add_version_info((yyvsp[0].double_value), 0); }
-#line 2128 "ptx.tab.c" /* yacc.c:1646  */
+#line 310 "ptx.y"
+                                           { recognizer->add_version_info((yyvsp[0].double_value), 0); }
+#line 2409 "ptx.tab.c"
     break;
 
   case 62:
-#line 300 "ptx.y" /* yacc.c:1646  */
-    { add_version_info((yyvsp[-1].double_value),1); }
-#line 2134 "ptx.tab.c" /* yacc.c:1646  */
+#line 311 "ptx.y"
+                                                { recognizer->add_version_info((yyvsp[-1].double_value),1); }
+#line 2415 "ptx.tab.c"
     break;
 
   case 63:
-#line 301 "ptx.y" /* yacc.c:1646  */
-    {/*Do nothing*/}
-#line 2140 "ptx.tab.c" /* yacc.c:1646  */
+#line 312 "ptx.y"
+                                             {/*Do nothing*/}
+#line 2421 "ptx.tab.c"
     break;
 
   case 64:
-#line 302 "ptx.y" /* yacc.c:1646  */
-    { target_header2((yyvsp[-2].string_value),(yyvsp[0].string_value)); }
-#line 2146 "ptx.tab.c" /* yacc.c:1646  */
+#line 313 "ptx.y"
+                                                       { recognizer->target_header2((yyvsp[-2].string_value),(yyvsp[0].string_value)); }
+#line 2427 "ptx.tab.c"
     break;
 
   case 65:
-#line 303 "ptx.y" /* yacc.c:1646  */
-    { target_header3((yyvsp[-4].string_value),(yyvsp[-2].string_value),(yyvsp[0].string_value)); }
-#line 2152 "ptx.tab.c" /* yacc.c:1646  */
+#line 314 "ptx.y"
+                                                                        { recognizer->target_header3((yyvsp[-4].string_value),(yyvsp[-2].string_value),(yyvsp[0].string_value)); }
+#line 2433 "ptx.tab.c"
     break;
 
   case 66:
-#line 304 "ptx.y" /* yacc.c:1646  */
-    { target_header((yyvsp[0].string_value)); }
-#line 2158 "ptx.tab.c" /* yacc.c:1646  */
+#line 315 "ptx.y"
+                                      { recognizer->target_header((yyvsp[0].string_value)); }
+#line 2439 "ptx.tab.c"
     break;
 
   case 67:
-#line 305 "ptx.y" /* yacc.c:1646  */
-    { add_file((yyvsp[-1].int_value),(yyvsp[0].string_value)); }
-#line 2164 "ptx.tab.c" /* yacc.c:1646  */
+#line 316 "ptx.y"
+                                            { recognizer->add_file((yyvsp[-1].int_value),(yyvsp[0].string_value)); }
+#line 2445 "ptx.tab.c"
     break;
 
   case 68:
-#line 306 "ptx.y" /* yacc.c:1646  */
-    { add_file((yyvsp[-5].int_value),(yyvsp[-4].string_value)); }
-#line 2170 "ptx.tab.c" /* yacc.c:1646  */
+#line 317 "ptx.y"
+                                                                                { recognizer->add_file((yyvsp[-5].int_value),(yyvsp[-4].string_value)); }
+#line 2451 "ptx.tab.c"
     break;
 
   case 70:
-#line 308 "ptx.y" /* yacc.c:1646  */
-    { add_pragma((yyvsp[-1].string_value)); }
-#line 2176 "ptx.tab.c" /* yacc.c:1646  */
+#line 319 "ptx.y"
+                                             { recognizer->add_pragma((yyvsp[-1].string_value)); }
+#line 2457 "ptx.tab.c"
     break;
 
   case 71:
-#line 309 "ptx.y" /* yacc.c:1646  */
-    {/*Do nothing*/}
-#line 2182 "ptx.tab.c" /* yacc.c:1646  */
+#line 320 "ptx.y"
+                                   {/*Do nothing*/}
+#line 2463 "ptx.tab.c"
     break;
 
   case 72:
-#line 312 "ptx.y" /* yacc.c:1646  */
-    { add_variables(); }
-#line 2188 "ptx.tab.c" /* yacc.c:1646  */
+#line 323 "ptx.y"
+                                                    { recognizer->add_variables(); }
+#line 2469 "ptx.tab.c"
     break;
 
   case 73:
-#line 313 "ptx.y" /* yacc.c:1646  */
-    { add_variables(); }
-#line 2194 "ptx.tab.c" /* yacc.c:1646  */
+#line 324 "ptx.y"
+                                                                { recognizer->add_variables(); }
+#line 2475 "ptx.tab.c"
     break;
 
   case 74:
-#line 314 "ptx.y" /* yacc.c:1646  */
-    { add_variables(); }
-#line 2200 "ptx.tab.c" /* yacc.c:1646  */
+#line 325 "ptx.y"
+                                                               { recognizer->add_variables(); }
+#line 2481 "ptx.tab.c"
     break;
 
   case 75:
-#line 315 "ptx.y" /* yacc.c:1646  */
-    { add_constptr((yyvsp[-4].string_value), (yyvsp[-2].string_value), (yyvsp[0].int_value)); }
-#line 2206 "ptx.tab.c" /* yacc.c:1646  */
+#line 326 "ptx.y"
+                                                                           { recognizer->add_constptr((yyvsp[-4].string_value), (yyvsp[-2].string_value), (yyvsp[0].int_value)); }
+#line 2487 "ptx.tab.c"
     break;
 
   case 76:
-#line 318 "ptx.y" /* yacc.c:1646  */
-    { set_variable_type(); }
-#line 2212 "ptx.tab.c" /* yacc.c:1646  */
+#line 329 "ptx.y"
+                             { recognizer->set_variable_type(); }
+#line 2493 "ptx.tab.c"
     break;
 
   case 79:
-#line 323 "ptx.y" /* yacc.c:1646  */
-    { add_identifier((yyvsp[0].string_value),0,NON_ARRAY_IDENTIFIER); func_header_info((yyvsp[0].string_value));}
-#line 2218 "ptx.tab.c" /* yacc.c:1646  */
+#line 334 "ptx.y"
+                            { recognizer->add_identifier((yyvsp[0].string_value),0,NON_ARRAY_IDENTIFIER); recognizer->func_header_info((yyvsp[0].string_value));}
+#line 2499 "ptx.tab.c"
     break;
 
   case 80:
-#line 324 "ptx.y" /* yacc.c:1646  */
-    { func_header_info((yyvsp[-3].string_value)); func_header_info_int("<", (yyvsp[-1].int_value)); func_header_info(">");
+#line 335 "ptx.y"
+                                                                        { recognizer->func_header_info((yyvsp[-3].string_value)); recognizer->func_header_info_int("<", (yyvsp[-1].int_value)); recognizer->func_header_info(">");
 		int i,lbase,l;
 		char *id = NULL;
 		lbase = strlen((yyvsp[-3].string_value));
@@ -2227,1107 +2508,1108 @@ yyreduce:
 			l = lbase + (int)log10(i+1)+10;
 			id = (char*) malloc(l);
 			snprintf(id,l,"%s%u",(yyvsp[-3].string_value),i);
-			add_identifier(id,0,NON_ARRAY_IDENTIFIER); 
+			recognizer->add_identifier(id,0,NON_ARRAY_IDENTIFIER);
 		}
 		free((yyvsp[-3].string_value));
 	}
-#line 2235 "ptx.tab.c" /* yacc.c:1646  */
+#line 2516 "ptx.tab.c"
     break;
 
   case 81:
-#line 336 "ptx.y" /* yacc.c:1646  */
-    { add_identifier((yyvsp[-2].string_value),0,ARRAY_IDENTIFIER_NO_DIM); func_header_info((yyvsp[-2].string_value)); func_header_info("["); func_header_info("]");}
-#line 2241 "ptx.tab.c" /* yacc.c:1646  */
+#line 347 "ptx.y"
+                                                              { recognizer->add_identifier((yyvsp[-2].string_value),0,ARRAY_IDENTIFIER_NO_DIM); recognizer->func_header_info((yyvsp[-2].string_value)); recognizer->func_header_info("["); recognizer->func_header_info("]");}
+#line 2522 "ptx.tab.c"
     break;
 
   case 82:
-#line 337 "ptx.y" /* yacc.c:1646  */
-    { add_identifier((yyvsp[-3].string_value),(yyvsp[-1].int_value),ARRAY_IDENTIFIER); func_header_info((yyvsp[-3].string_value)); func_header_info_int("[",(yyvsp[-1].int_value)); func_header_info("]");}
-#line 2247 "ptx.tab.c" /* yacc.c:1646  */
+#line 348 "ptx.y"
+                                                                          { recognizer->add_identifier((yyvsp[-3].string_value),(yyvsp[-1].int_value),ARRAY_IDENTIFIER); recognizer->func_header_info((yyvsp[-3].string_value)); recognizer->func_header_info_int("[",(yyvsp[-1].int_value)); recognizer->func_header_info("]");}
+#line 2528 "ptx.tab.c"
     break;
 
   case 89:
-#line 347 "ptx.y" /* yacc.c:1646  */
-    { add_extern_spec(); }
-#line 2253 "ptx.tab.c" /* yacc.c:1646  */
+#line 358 "ptx.y"
+                           { recognizer->add_extern_spec(); }
+#line 2534 "ptx.tab.c"
     break;
 
   case 91:
-#line 351 "ptx.y" /* yacc.c:1646  */
-    { add_alignment_spec((yyvsp[0].int_value)); }
-#line 2259 "ptx.tab.c" /* yacc.c:1646  */
+#line 362 "ptx.y"
+                                        { recognizer->add_alignment_spec((yyvsp[0].int_value)); }
+#line 2540 "ptx.tab.c"
     break;
 
   case 92:
-#line 353 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(reg_space,0); }
-#line 2265 "ptx.tab.c" /* yacc.c:1646  */
+#line 364 "ptx.y"
+                          {  recognizer->add_space_spec(reg_space,0); }
+#line 2546 "ptx.tab.c"
     break;
 
   case 93:
-#line 354 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(reg_space,0); }
-#line 2271 "ptx.tab.c" /* yacc.c:1646  */
+#line 365 "ptx.y"
+                          {  recognizer->add_space_spec(reg_space,0); }
+#line 2552 "ptx.tab.c"
     break;
 
   case 95:
-#line 358 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(const_space,(yyvsp[0].int_value)); }
-#line 2277 "ptx.tab.c" /* yacc.c:1646  */
+#line 369 "ptx.y"
+                                  {  recognizer->add_space_spec(const_space,(yyvsp[0].int_value)); }
+#line 2558 "ptx.tab.c"
     break;
 
   case 96:
-#line 359 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(global_space,0); }
-#line 2283 "ptx.tab.c" /* yacc.c:1646  */
+#line 370 "ptx.y"
+                                  {  recognizer->add_space_spec(global_space,0); }
+#line 2564 "ptx.tab.c"
     break;
 
   case 97:
-#line 360 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(local_space,0); }
-#line 2289 "ptx.tab.c" /* yacc.c:1646  */
+#line 371 "ptx.y"
+                                  {  recognizer->add_space_spec(local_space,0); }
+#line 2570 "ptx.tab.c"
     break;
 
   case 98:
-#line 361 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(param_space_unclassified,0); }
-#line 2295 "ptx.tab.c" /* yacc.c:1646  */
+#line 372 "ptx.y"
+                                  {  recognizer->add_space_spec(param_space_unclassified,0); }
+#line 2576 "ptx.tab.c"
     break;
 
   case 99:
-#line 362 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(shared_space,0); }
-#line 2301 "ptx.tab.c" /* yacc.c:1646  */
+#line 373 "ptx.y"
+                                  {  recognizer->add_space_spec(shared_space,0); }
+#line 2582 "ptx.tab.c"
     break;
 
   case 100:
-#line 363 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(sstarr_space,0); }
-#line 2307 "ptx.tab.c" /* yacc.c:1646  */
+#line 374 "ptx.y"
+                              {  recognizer->add_space_spec(sstarr_space,0); }
+#line 2588 "ptx.tab.c"
     break;
 
   case 101:
-#line 364 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(surf_space,0); }
-#line 2313 "ptx.tab.c" /* yacc.c:1646  */
+#line 375 "ptx.y"
+                                  {  recognizer->add_space_spec(surf_space,0); }
+#line 2594 "ptx.tab.c"
     break;
 
   case 102:
-#line 365 "ptx.y" /* yacc.c:1646  */
-    {  add_space_spec(tex_space,0); }
-#line 2319 "ptx.tab.c" /* yacc.c:1646  */
+#line 376 "ptx.y"
+                                  {  recognizer->add_space_spec(tex_space,0); }
+#line 2600 "ptx.tab.c"
     break;
 
   case 105:
-#line 372 "ptx.y" /* yacc.c:1646  */
-    {  add_option(V2_TYPE); func_header_info(".v2");}
-#line 2325 "ptx.tab.c" /* yacc.c:1646  */
+#line 383 "ptx.y"
+                      {  recognizer->add_option(V2_TYPE); recognizer->func_header_info(".v2");}
+#line 2606 "ptx.tab.c"
     break;
 
   case 106:
-#line 373 "ptx.y" /* yacc.c:1646  */
-    {  add_option(V3_TYPE); func_header_info(".v3");}
-#line 2331 "ptx.tab.c" /* yacc.c:1646  */
+#line 384 "ptx.y"
+                      {  recognizer->add_option(V3_TYPE); recognizer->func_header_info(".v3");}
+#line 2612 "ptx.tab.c"
     break;
 
   case 107:
-#line 374 "ptx.y" /* yacc.c:1646  */
-    {  add_option(V4_TYPE); func_header_info(".v4");}
-#line 2337 "ptx.tab.c" /* yacc.c:1646  */
+#line 385 "ptx.y"
+                      {  recognizer->add_option(V4_TYPE); recognizer->func_header_info(".v4");}
+#line 2618 "ptx.tab.c"
     break;
 
   case 108:
-#line 377 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( S8_TYPE ); }
-#line 2343 "ptx.tab.c" /* yacc.c:1646  */
+#line 388 "ptx.y"
+                     { recognizer->add_scalar_type_spec( S8_TYPE ); }
+#line 2624 "ptx.tab.c"
     break;
 
   case 109:
-#line 378 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( S16_TYPE ); }
-#line 2349 "ptx.tab.c" /* yacc.c:1646  */
+#line 389 "ptx.y"
+                     { recognizer->add_scalar_type_spec( S16_TYPE ); }
+#line 2630 "ptx.tab.c"
     break;
 
   case 110:
-#line 379 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( S32_TYPE ); }
-#line 2355 "ptx.tab.c" /* yacc.c:1646  */
+#line 390 "ptx.y"
+                     { recognizer->add_scalar_type_spec( S32_TYPE ); }
+#line 2636 "ptx.tab.c"
     break;
 
   case 111:
-#line 380 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( S64_TYPE ); }
-#line 2361 "ptx.tab.c" /* yacc.c:1646  */
+#line 391 "ptx.y"
+                     { recognizer->add_scalar_type_spec( S64_TYPE ); }
+#line 2642 "ptx.tab.c"
     break;
 
   case 112:
-#line 381 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( U8_TYPE ); }
-#line 2367 "ptx.tab.c" /* yacc.c:1646  */
+#line 392 "ptx.y"
+                     { recognizer->add_scalar_type_spec( U8_TYPE ); }
+#line 2648 "ptx.tab.c"
     break;
 
   case 113:
-#line 382 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( U16_TYPE ); }
-#line 2373 "ptx.tab.c" /* yacc.c:1646  */
+#line 393 "ptx.y"
+                     { recognizer->add_scalar_type_spec( U16_TYPE ); }
+#line 2654 "ptx.tab.c"
     break;
 
   case 114:
-#line 383 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( U32_TYPE ); }
-#line 2379 "ptx.tab.c" /* yacc.c:1646  */
+#line 394 "ptx.y"
+                     { recognizer->add_scalar_type_spec( U32_TYPE ); }
+#line 2660 "ptx.tab.c"
     break;
 
   case 115:
-#line 384 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( U64_TYPE ); }
-#line 2385 "ptx.tab.c" /* yacc.c:1646  */
+#line 395 "ptx.y"
+                     { recognizer->add_scalar_type_spec( U64_TYPE ); }
+#line 2666 "ptx.tab.c"
     break;
 
   case 116:
-#line 385 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( F16_TYPE ); }
-#line 2391 "ptx.tab.c" /* yacc.c:1646  */
+#line 396 "ptx.y"
+                     { recognizer->add_scalar_type_spec( F16_TYPE ); }
+#line 2672 "ptx.tab.c"
     break;
 
   case 117:
-#line 386 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( F32_TYPE ); }
-#line 2397 "ptx.tab.c" /* yacc.c:1646  */
+#line 397 "ptx.y"
+                     { recognizer->add_scalar_type_spec( F32_TYPE ); }
+#line 2678 "ptx.tab.c"
     break;
 
   case 118:
-#line 387 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( F64_TYPE ); }
-#line 2403 "ptx.tab.c" /* yacc.c:1646  */
+#line 398 "ptx.y"
+                     { recognizer->add_scalar_type_spec( F64_TYPE ); }
+#line 2684 "ptx.tab.c"
     break;
 
   case 119:
-#line 388 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( FF64_TYPE ); }
-#line 2409 "ptx.tab.c" /* yacc.c:1646  */
+#line 399 "ptx.y"
+                      { recognizer->add_scalar_type_spec( FF64_TYPE ); }
+#line 2690 "ptx.tab.c"
     break;
 
   case 120:
-#line 389 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( B8_TYPE );  }
-#line 2415 "ptx.tab.c" /* yacc.c:1646  */
+#line 400 "ptx.y"
+                     { recognizer->add_scalar_type_spec( B8_TYPE );  }
+#line 2696 "ptx.tab.c"
     break;
 
   case 121:
-#line 390 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( B16_TYPE ); }
-#line 2421 "ptx.tab.c" /* yacc.c:1646  */
+#line 401 "ptx.y"
+                     { recognizer->add_scalar_type_spec( B16_TYPE ); }
+#line 2702 "ptx.tab.c"
     break;
 
   case 122:
-#line 391 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( B32_TYPE ); }
-#line 2427 "ptx.tab.c" /* yacc.c:1646  */
+#line 402 "ptx.y"
+                     { recognizer->add_scalar_type_spec( B32_TYPE ); }
+#line 2708 "ptx.tab.c"
     break;
 
   case 123:
-#line 392 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( B64_TYPE ); }
-#line 2433 "ptx.tab.c" /* yacc.c:1646  */
+#line 403 "ptx.y"
+                     { recognizer->add_scalar_type_spec( B64_TYPE ); }
+#line 2714 "ptx.tab.c"
     break;
 
   case 124:
-#line 393 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( BB64_TYPE ); }
-#line 2439 "ptx.tab.c" /* yacc.c:1646  */
+#line 404 "ptx.y"
+                      { recognizer->add_scalar_type_spec( BB64_TYPE ); }
+#line 2720 "ptx.tab.c"
     break;
 
   case 125:
-#line 394 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( BB128_TYPE ); }
-#line 2445 "ptx.tab.c" /* yacc.c:1646  */
+#line 405 "ptx.y"
+                       { recognizer->add_scalar_type_spec( BB128_TYPE ); }
+#line 2726 "ptx.tab.c"
     break;
 
   case 126:
-#line 395 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( PRED_TYPE ); }
-#line 2451 "ptx.tab.c" /* yacc.c:1646  */
+#line 406 "ptx.y"
+                     { recognizer->add_scalar_type_spec( PRED_TYPE ); }
+#line 2732 "ptx.tab.c"
     break;
 
   case 127:
-#line 396 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( TEXREF_TYPE ); }
-#line 2457 "ptx.tab.c" /* yacc.c:1646  */
+#line 407 "ptx.y"
+                       { recognizer->add_scalar_type_spec( TEXREF_TYPE ); }
+#line 2738 "ptx.tab.c"
     break;
 
   case 128:
-#line 397 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( SAMPLERREF_TYPE ); }
-#line 2463 "ptx.tab.c" /* yacc.c:1646  */
+#line 408 "ptx.y"
+                           { recognizer->add_scalar_type_spec( SAMPLERREF_TYPE ); }
+#line 2744 "ptx.tab.c"
     break;
 
   case 129:
-#line 398 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_type_spec( SURFREF_TYPE ); }
-#line 2469 "ptx.tab.c" /* yacc.c:1646  */
+#line 409 "ptx.y"
+                        { recognizer->add_scalar_type_spec( SURFREF_TYPE ); }
+#line 2750 "ptx.tab.c"
     break;
 
   case 130:
-#line 401 "ptx.y" /* yacc.c:1646  */
-    { add_array_initializer(); }
-#line 2475 "ptx.tab.c" /* yacc.c:1646  */
+#line 412 "ptx.y"
+                                                      { recognizer->add_array_initializer(); }
+#line 2756 "ptx.tab.c"
     break;
 
   case 131:
-#line 402 "ptx.y" /* yacc.c:1646  */
-    { syntax_not_implemented(); }
-#line 2481 "ptx.tab.c" /* yacc.c:1646  */
+#line 413 "ptx.y"
+                                                  { syntax_not_implemented(scanner, recognizer); }
+#line 2762 "ptx.tab.c"
     break;
 
   case 142:
-#line 425 "ptx.y" /* yacc.c:1646  */
-    { add_label((yyvsp[-1].string_value)); }
-#line 2487 "ptx.tab.c" /* yacc.c:1646  */
+#line 436 "ptx.y"
+                           { recognizer->add_label((yyvsp[-1].string_value)); }
+#line 2768 "ptx.tab.c"
     break;
 
   case 144:
-#line 428 "ptx.y" /* yacc.c:1646  */
-    { set_return(); }
-#line 2493 "ptx.tab.c" /* yacc.c:1646  */
+#line 439 "ptx.y"
+                                                        { recognizer->set_return(); }
+#line 2774 "ptx.tab.c"
     break;
 
   case 150:
-#line 435 "ptx.y" /* yacc.c:1646  */
-    { add_opcode((yyvsp[0].int_value)); }
-#line 2499 "ptx.tab.c" /* yacc.c:1646  */
+#line 446 "ptx.y"
+                    { recognizer->add_opcode((yyvsp[0].int_value)); }
+#line 2780 "ptx.tab.c"
     break;
 
   case 152:
-#line 436 "ptx.y" /* yacc.c:1646  */
-    { add_opcode((yyvsp[0].int_value)); }
-#line 2505 "ptx.tab.c" /* yacc.c:1646  */
+#line 447 "ptx.y"
+                 { recognizer->add_opcode((yyvsp[0].int_value)); }
+#line 2786 "ptx.tab.c"
     break;
 
   case 153:
-#line 438 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[0].string_value),0, -1); }
-#line 2511 "ptx.tab.c" /* yacc.c:1646  */
+#line 449 "ptx.y"
+                            { recognizer->add_pred((yyvsp[0].string_value),0, -1); }
+#line 2792 "ptx.tab.c"
     break;
 
   case 154:
-#line 439 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[0].string_value),1, -1); }
-#line 2517 "ptx.tab.c" /* yacc.c:1646  */
+#line 450 "ptx.y"
+                                      { recognizer->add_pred((yyvsp[0].string_value),1, -1); }
+#line 2798 "ptx.tab.c"
     break;
 
   case 155:
-#line 440 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,1); }
-#line 2523 "ptx.tab.c" /* yacc.c:1646  */
+#line 451 "ptx.y"
+                                     { recognizer->add_pred((yyvsp[-1].string_value),0,1); }
+#line 2804 "ptx.tab.c"
     break;
 
   case 156:
-#line 441 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,2); }
-#line 2529 "ptx.tab.c" /* yacc.c:1646  */
+#line 452 "ptx.y"
+                                     { recognizer->add_pred((yyvsp[-1].string_value),0,2); }
+#line 2810 "ptx.tab.c"
     break;
 
   case 157:
-#line 442 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,3); }
-#line 2535 "ptx.tab.c" /* yacc.c:1646  */
+#line 453 "ptx.y"
+                                     { recognizer->add_pred((yyvsp[-1].string_value),0,3); }
+#line 2816 "ptx.tab.c"
     break;
 
   case 158:
-#line 443 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,5); }
-#line 2541 "ptx.tab.c" /* yacc.c:1646  */
+#line 454 "ptx.y"
+                                     { recognizer->add_pred((yyvsp[-1].string_value),0,5); }
+#line 2822 "ptx.tab.c"
     break;
 
   case 159:
-#line 444 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,6); }
-#line 2547 "ptx.tab.c" /* yacc.c:1646  */
+#line 455 "ptx.y"
+                                     { recognizer->add_pred((yyvsp[-1].string_value),0,6); }
+#line 2828 "ptx.tab.c"
     break;
 
   case 160:
-#line 445 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,10); }
-#line 2553 "ptx.tab.c" /* yacc.c:1646  */
+#line 456 "ptx.y"
+                                      { recognizer->add_pred((yyvsp[-1].string_value),0,10); }
+#line 2834 "ptx.tab.c"
     break;
 
   case 161:
-#line 446 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,12); }
-#line 2559 "ptx.tab.c" /* yacc.c:1646  */
+#line 457 "ptx.y"
+                                      { recognizer->add_pred((yyvsp[-1].string_value),0,12); }
+#line 2840 "ptx.tab.c"
     break;
 
   case 162:
-#line 447 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,13); }
-#line 2565 "ptx.tab.c" /* yacc.c:1646  */
+#line 458 "ptx.y"
+                                      { recognizer->add_pred((yyvsp[-1].string_value),0,13); }
+#line 2846 "ptx.tab.c"
     break;
 
   case 163:
-#line 448 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,17); }
-#line 2571 "ptx.tab.c" /* yacc.c:1646  */
+#line 459 "ptx.y"
+                                     { recognizer->add_pred((yyvsp[-1].string_value),0,17); }
+#line 2852 "ptx.tab.c"
     break;
 
   case 164:
-#line 449 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,19); }
-#line 2577 "ptx.tab.c" /* yacc.c:1646  */
+#line 460 "ptx.y"
+                                     { recognizer->add_pred((yyvsp[-1].string_value),0,19); }
+#line 2858 "ptx.tab.c"
     break;
 
   case 165:
-#line 450 "ptx.y" /* yacc.c:1646  */
-    { add_pred((yyvsp[-1].string_value),0,28); }
-#line 2583 "ptx.tab.c" /* yacc.c:1646  */
+#line 461 "ptx.y"
+                                      { recognizer->add_pred((yyvsp[-1].string_value),0,28); }
+#line 2864 "ptx.tab.c"
     break;
 
   case 174:
-#line 462 "ptx.y" /* yacc.c:1646  */
-    { add_option(SYNC_OPTION); }
-#line 2589 "ptx.tab.c" /* yacc.c:1646  */
+#line 473 "ptx.y"
+                      { recognizer->add_option(SYNC_OPTION); }
+#line 2870 "ptx.tab.c"
     break;
 
   case 175:
-#line 463 "ptx.y" /* yacc.c:1646  */
-    { add_option(ARRIVE_OPTION); }
-#line 2595 "ptx.tab.c" /* yacc.c:1646  */
+#line 474 "ptx.y"
+                        { recognizer->add_option(ARRIVE_OPTION); }
+#line 2876 "ptx.tab.c"
     break;
 
   case 176:
-#line 464 "ptx.y" /* yacc.c:1646  */
-    { add_option(RED_OPTION); }
-#line 2601 "ptx.tab.c" /* yacc.c:1646  */
+#line 475 "ptx.y"
+                     { recognizer->add_option(RED_OPTION); }
+#line 2882 "ptx.tab.c"
     break;
 
   case 177:
-#line 465 "ptx.y" /* yacc.c:1646  */
-    { add_option(UNI_OPTION); }
-#line 2607 "ptx.tab.c" /* yacc.c:1646  */
+#line 476 "ptx.y"
+                     { recognizer->add_option(UNI_OPTION); }
+#line 2888 "ptx.tab.c"
     break;
 
   case 178:
-#line 466 "ptx.y" /* yacc.c:1646  */
-    { add_option(WIDE_OPTION); }
-#line 2613 "ptx.tab.c" /* yacc.c:1646  */
+#line 477 "ptx.y"
+                      { recognizer->add_option(WIDE_OPTION); }
+#line 2894 "ptx.tab.c"
     break;
 
   case 179:
-#line 467 "ptx.y" /* yacc.c:1646  */
-    { add_option(ANY_OPTION); }
-#line 2619 "ptx.tab.c" /* yacc.c:1646  */
+#line 478 "ptx.y"
+                     { recognizer->add_option(ANY_OPTION); }
+#line 2900 "ptx.tab.c"
     break;
 
   case 180:
-#line 468 "ptx.y" /* yacc.c:1646  */
-    { add_option(ALL_OPTION); }
-#line 2625 "ptx.tab.c" /* yacc.c:1646  */
+#line 479 "ptx.y"
+                     { recognizer->add_option(ALL_OPTION); }
+#line 2906 "ptx.tab.c"
     break;
 
   case 181:
-#line 469 "ptx.y" /* yacc.c:1646  */
-    { add_option(BALLOT_OPTION); }
-#line 2631 "ptx.tab.c" /* yacc.c:1646  */
+#line 480 "ptx.y"
+                        { recognizer->add_option(BALLOT_OPTION); }
+#line 2912 "ptx.tab.c"
     break;
 
   case 182:
-#line 470 "ptx.y" /* yacc.c:1646  */
-    { add_option(GLOBAL_OPTION); }
-#line 2637 "ptx.tab.c" /* yacc.c:1646  */
+#line 481 "ptx.y"
+                        { recognizer->add_option(GLOBAL_OPTION); }
+#line 2918 "ptx.tab.c"
     break;
 
   case 183:
-#line 471 "ptx.y" /* yacc.c:1646  */
-    { add_option(CTA_OPTION); }
-#line 2643 "ptx.tab.c" /* yacc.c:1646  */
+#line 482 "ptx.y"
+                     { recognizer->add_option(CTA_OPTION); }
+#line 2924 "ptx.tab.c"
     break;
 
   case 184:
-#line 472 "ptx.y" /* yacc.c:1646  */
-    { add_option(SYS_OPTION); }
-#line 2649 "ptx.tab.c" /* yacc.c:1646  */
+#line 483 "ptx.y"
+                     { recognizer->add_option(SYS_OPTION); }
+#line 2930 "ptx.tab.c"
     break;
 
   case 185:
-#line 473 "ptx.y" /* yacc.c:1646  */
-    { add_option(GEOM_MODIFIER_1D); }
-#line 2655 "ptx.tab.c" /* yacc.c:1646  */
+#line 484 "ptx.y"
+                           { recognizer->add_option(GEOM_MODIFIER_1D); }
+#line 2936 "ptx.tab.c"
     break;
 
   case 186:
-#line 474 "ptx.y" /* yacc.c:1646  */
-    { add_option(GEOM_MODIFIER_2D); }
-#line 2661 "ptx.tab.c" /* yacc.c:1646  */
+#line 485 "ptx.y"
+                           { recognizer->add_option(GEOM_MODIFIER_2D); }
+#line 2942 "ptx.tab.c"
     break;
 
   case 187:
-#line 475 "ptx.y" /* yacc.c:1646  */
-    { add_option(GEOM_MODIFIER_3D); }
-#line 2667 "ptx.tab.c" /* yacc.c:1646  */
+#line 486 "ptx.y"
+                           { recognizer->add_option(GEOM_MODIFIER_3D); }
+#line 2948 "ptx.tab.c"
     break;
 
   case 188:
-#line 476 "ptx.y" /* yacc.c:1646  */
-    { add_option(SAT_OPTION); }
-#line 2673 "ptx.tab.c" /* yacc.c:1646  */
+#line 487 "ptx.y"
+                     { recognizer->add_option(SAT_OPTION); }
+#line 2954 "ptx.tab.c"
     break;
 
   case 189:
-#line 477 "ptx.y" /* yacc.c:1646  */
-    { add_option(FTZ_OPTION); }
-#line 2679 "ptx.tab.c" /* yacc.c:1646  */
+#line 488 "ptx.y"
+                     { recognizer->add_option(FTZ_OPTION); }
+#line 2960 "ptx.tab.c"
     break;
 
   case 190:
-#line 478 "ptx.y" /* yacc.c:1646  */
-    { add_option(NEG_OPTION); }
-#line 2685 "ptx.tab.c" /* yacc.c:1646  */
+#line 489 "ptx.y"
+                     { recognizer->add_option(NEG_OPTION); }
+#line 2966 "ptx.tab.c"
     break;
 
   case 191:
-#line 479 "ptx.y" /* yacc.c:1646  */
-    { add_option(APPROX_OPTION); }
-#line 2691 "ptx.tab.c" /* yacc.c:1646  */
+#line 490 "ptx.y"
+                        { recognizer->add_option(APPROX_OPTION); }
+#line 2972 "ptx.tab.c"
     break;
 
   case 192:
-#line 480 "ptx.y" /* yacc.c:1646  */
-    { add_option(FULL_OPTION); }
-#line 2697 "ptx.tab.c" /* yacc.c:1646  */
+#line 491 "ptx.y"
+                      { recognizer->add_option(FULL_OPTION); }
+#line 2978 "ptx.tab.c"
     break;
 
   case 193:
-#line 481 "ptx.y" /* yacc.c:1646  */
-    { add_option(EXIT_OPTION); }
-#line 2703 "ptx.tab.c" /* yacc.c:1646  */
+#line 492 "ptx.y"
+                      { recognizer->add_option(EXIT_OPTION); }
+#line 2984 "ptx.tab.c"
     break;
 
   case 194:
-#line 482 "ptx.y" /* yacc.c:1646  */
-    { add_option(ABS_OPTION); }
-#line 2709 "ptx.tab.c" /* yacc.c:1646  */
+#line 493 "ptx.y"
+                     { recognizer->add_option(ABS_OPTION); }
+#line 2990 "ptx.tab.c"
     break;
 
   case 196:
-#line 484 "ptx.y" /* yacc.c:1646  */
-    { add_option(TO_OPTION); }
-#line 2715 "ptx.tab.c" /* yacc.c:1646  */
+#line 495 "ptx.y"
+                    { recognizer->add_option(TO_OPTION); }
+#line 2996 "ptx.tab.c"
     break;
 
   case 197:
-#line 485 "ptx.y" /* yacc.c:1646  */
-    { add_option(HALF_OPTION); }
-#line 2721 "ptx.tab.c" /* yacc.c:1646  */
+#line 496 "ptx.y"
+                      { recognizer->add_option(HALF_OPTION); }
+#line 3002 "ptx.tab.c"
     break;
 
   case 198:
-#line 486 "ptx.y" /* yacc.c:1646  */
-    { add_option(EXTP_OPTION); }
-#line 2727 "ptx.tab.c" /* yacc.c:1646  */
+#line 497 "ptx.y"
+                      { recognizer->add_option(EXTP_OPTION); }
+#line 3008 "ptx.tab.c"
     break;
 
   case 199:
-#line 487 "ptx.y" /* yacc.c:1646  */
-    { add_option(CA_OPTION); }
-#line 2733 "ptx.tab.c" /* yacc.c:1646  */
+#line 498 "ptx.y"
+                    { recognizer->add_option(CA_OPTION); }
+#line 3014 "ptx.tab.c"
     break;
 
   case 200:
-#line 488 "ptx.y" /* yacc.c:1646  */
-    { add_option(CG_OPTION); }
-#line 2739 "ptx.tab.c" /* yacc.c:1646  */
+#line 499 "ptx.y"
+                    { recognizer->add_option(CG_OPTION); }
+#line 3020 "ptx.tab.c"
     break;
 
   case 201:
-#line 489 "ptx.y" /* yacc.c:1646  */
-    { add_option(CS_OPTION); }
-#line 2745 "ptx.tab.c" /* yacc.c:1646  */
+#line 500 "ptx.y"
+                    { recognizer->add_option(CS_OPTION); }
+#line 3026 "ptx.tab.c"
     break;
 
   case 202:
-#line 490 "ptx.y" /* yacc.c:1646  */
-    { add_option(LU_OPTION); }
-#line 2751 "ptx.tab.c" /* yacc.c:1646  */
+#line 501 "ptx.y"
+                    { recognizer->add_option(LU_OPTION); }
+#line 3032 "ptx.tab.c"
     break;
 
   case 203:
-#line 491 "ptx.y" /* yacc.c:1646  */
-    { add_option(CV_OPTION); }
-#line 2757 "ptx.tab.c" /* yacc.c:1646  */
+#line 502 "ptx.y"
+                    { recognizer->add_option(CV_OPTION); }
+#line 3038 "ptx.tab.c"
     break;
 
   case 204:
-#line 492 "ptx.y" /* yacc.c:1646  */
-    { add_option(WB_OPTION); }
-#line 2763 "ptx.tab.c" /* yacc.c:1646  */
+#line 503 "ptx.y"
+                    { recognizer->add_option(WB_OPTION); }
+#line 3044 "ptx.tab.c"
     break;
 
   case 205:
-#line 493 "ptx.y" /* yacc.c:1646  */
-    { add_option(WT_OPTION); }
-#line 2769 "ptx.tab.c" /* yacc.c:1646  */
+#line 504 "ptx.y"
+                    { recognizer->add_option(WT_OPTION); }
+#line 3050 "ptx.tab.c"
     break;
 
   case 206:
-#line 494 "ptx.y" /* yacc.c:1646  */
-    { add_option(NC_OPTION); }
-#line 2775 "ptx.tab.c" /* yacc.c:1646  */
+#line 505 "ptx.y"
+                    { recognizer->add_option(NC_OPTION); }
+#line 3056 "ptx.tab.c"
     break;
 
   case 207:
-#line 495 "ptx.y" /* yacc.c:1646  */
-    { add_option(UP_OPTION); }
-#line 2781 "ptx.tab.c" /* yacc.c:1646  */
+#line 506 "ptx.y"
+                    { recognizer->add_option(UP_OPTION); }
+#line 3062 "ptx.tab.c"
     break;
 
   case 208:
-#line 496 "ptx.y" /* yacc.c:1646  */
-    { add_option(DOWN_OPTION); }
-#line 2787 "ptx.tab.c" /* yacc.c:1646  */
+#line 507 "ptx.y"
+                      { recognizer->add_option(DOWN_OPTION); }
+#line 3068 "ptx.tab.c"
     break;
 
   case 209:
-#line 497 "ptx.y" /* yacc.c:1646  */
-    { add_option(BFLY_OPTION); }
-#line 2793 "ptx.tab.c" /* yacc.c:1646  */
+#line 508 "ptx.y"
+                      { recognizer->add_option(BFLY_OPTION); }
+#line 3074 "ptx.tab.c"
     break;
 
   case 210:
-#line 498 "ptx.y" /* yacc.c:1646  */
-    { add_option(IDX_OPTION); }
-#line 2799 "ptx.tab.c" /* yacc.c:1646  */
+#line 509 "ptx.y"
+                     { recognizer->add_option(IDX_OPTION); }
+#line 3080 "ptx.tab.c"
     break;
 
   case 211:
-#line 501 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_AND); }
-#line 2805 "ptx.tab.c" /* yacc.c:1646  */
+#line 512 "ptx.y"
+                                  { recognizer->add_option(ATOMIC_AND); }
+#line 3086 "ptx.tab.c"
     break;
 
   case 212:
-#line 502 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_POPC); }
-#line 2811 "ptx.tab.c" /* yacc.c:1646  */
+#line 513 "ptx.y"
+                      { recognizer->add_option(ATOMIC_POPC); }
+#line 3092 "ptx.tab.c"
     break;
 
   case 213:
-#line 503 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_OR); }
-#line 2817 "ptx.tab.c" /* yacc.c:1646  */
+#line 514 "ptx.y"
+                    { recognizer->add_option(ATOMIC_OR); }
+#line 3098 "ptx.tab.c"
     break;
 
   case 214:
-#line 504 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_XOR); }
-#line 2823 "ptx.tab.c" /* yacc.c:1646  */
+#line 515 "ptx.y"
+                     { recognizer->add_option(ATOMIC_XOR); }
+#line 3104 "ptx.tab.c"
     break;
 
   case 215:
-#line 505 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_CAS); }
-#line 2829 "ptx.tab.c" /* yacc.c:1646  */
+#line 516 "ptx.y"
+                     { recognizer->add_option(ATOMIC_CAS); }
+#line 3110 "ptx.tab.c"
     break;
 
   case 216:
-#line 506 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_EXCH); }
-#line 2835 "ptx.tab.c" /* yacc.c:1646  */
+#line 517 "ptx.y"
+                      { recognizer->add_option(ATOMIC_EXCH); }
+#line 3116 "ptx.tab.c"
     break;
 
   case 217:
-#line 507 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_ADD); }
-#line 2841 "ptx.tab.c" /* yacc.c:1646  */
+#line 518 "ptx.y"
+                     { recognizer->add_option(ATOMIC_ADD); }
+#line 3122 "ptx.tab.c"
     break;
 
   case 218:
-#line 508 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_INC); }
-#line 2847 "ptx.tab.c" /* yacc.c:1646  */
+#line 519 "ptx.y"
+                     { recognizer->add_option(ATOMIC_INC); }
+#line 3128 "ptx.tab.c"
     break;
 
   case 219:
-#line 509 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_DEC); }
-#line 2853 "ptx.tab.c" /* yacc.c:1646  */
+#line 520 "ptx.y"
+                     { recognizer->add_option(ATOMIC_DEC); }
+#line 3134 "ptx.tab.c"
     break;
 
   case 220:
-#line 510 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_MIN); }
-#line 2859 "ptx.tab.c" /* yacc.c:1646  */
+#line 521 "ptx.y"
+                     { recognizer->add_option(ATOMIC_MIN); }
+#line 3140 "ptx.tab.c"
     break;
 
   case 221:
-#line 511 "ptx.y" /* yacc.c:1646  */
-    { add_option(ATOMIC_MAX); }
-#line 2865 "ptx.tab.c" /* yacc.c:1646  */
+#line 522 "ptx.y"
+                     { recognizer->add_option(ATOMIC_MAX); }
+#line 3146 "ptx.tab.c"
     break;
 
   case 224:
-#line 518 "ptx.y" /* yacc.c:1646  */
-    { add_option(RN_OPTION); }
-#line 2871 "ptx.tab.c" /* yacc.c:1646  */
+#line 529 "ptx.y"
+                                        { recognizer->add_option(RN_OPTION); }
+#line 3152 "ptx.tab.c"
     break;
 
   case 225:
-#line 519 "ptx.y" /* yacc.c:1646  */
-    { add_option(RZ_OPTION); }
-#line 2877 "ptx.tab.c" /* yacc.c:1646  */
+#line 530 "ptx.y"
+                    { recognizer->add_option(RZ_OPTION); }
+#line 3158 "ptx.tab.c"
     break;
 
   case 226:
-#line 520 "ptx.y" /* yacc.c:1646  */
-    { add_option(RM_OPTION); }
-#line 2883 "ptx.tab.c" /* yacc.c:1646  */
+#line 531 "ptx.y"
+                    { recognizer->add_option(RM_OPTION); }
+#line 3164 "ptx.tab.c"
     break;
 
   case 227:
-#line 521 "ptx.y" /* yacc.c:1646  */
-    { add_option(RP_OPTION); }
-#line 2889 "ptx.tab.c" /* yacc.c:1646  */
+#line 532 "ptx.y"
+                    { recognizer->add_option(RP_OPTION); }
+#line 3170 "ptx.tab.c"
     break;
 
   case 228:
-#line 524 "ptx.y" /* yacc.c:1646  */
-    { add_option(RNI_OPTION); }
-#line 2895 "ptx.tab.c" /* yacc.c:1646  */
+#line 535 "ptx.y"
+                                  { recognizer->add_option(RNI_OPTION); }
+#line 3176 "ptx.tab.c"
     break;
 
   case 229:
-#line 525 "ptx.y" /* yacc.c:1646  */
-    { add_option(RZI_OPTION); }
-#line 2901 "ptx.tab.c" /* yacc.c:1646  */
+#line 536 "ptx.y"
+                     { recognizer->add_option(RZI_OPTION); }
+#line 3182 "ptx.tab.c"
     break;
 
   case 230:
-#line 526 "ptx.y" /* yacc.c:1646  */
-    { add_option(RMI_OPTION); }
-#line 2907 "ptx.tab.c" /* yacc.c:1646  */
+#line 537 "ptx.y"
+                     { recognizer->add_option(RMI_OPTION); }
+#line 3188 "ptx.tab.c"
     break;
 
   case 231:
-#line 527 "ptx.y" /* yacc.c:1646  */
-    { add_option(RPI_OPTION); }
-#line 2913 "ptx.tab.c" /* yacc.c:1646  */
+#line 538 "ptx.y"
+                     { recognizer->add_option(RPI_OPTION); }
+#line 3194 "ptx.tab.c"
     break;
 
   case 232:
-#line 530 "ptx.y" /* yacc.c:1646  */
-    { add_option(EQ_OPTION); }
-#line 2919 "ptx.tab.c" /* yacc.c:1646  */
+#line 541 "ptx.y"
+                       { recognizer->add_option(EQ_OPTION); }
+#line 3200 "ptx.tab.c"
     break;
 
   case 233:
-#line 531 "ptx.y" /* yacc.c:1646  */
-    { add_option(NE_OPTION); }
-#line 2925 "ptx.tab.c" /* yacc.c:1646  */
+#line 542 "ptx.y"
+                    { recognizer->add_option(NE_OPTION); }
+#line 3206 "ptx.tab.c"
     break;
 
   case 234:
-#line 532 "ptx.y" /* yacc.c:1646  */
-    { add_option(LT_OPTION); }
-#line 2931 "ptx.tab.c" /* yacc.c:1646  */
+#line 543 "ptx.y"
+                    { recognizer->add_option(LT_OPTION); }
+#line 3212 "ptx.tab.c"
     break;
 
   case 235:
-#line 533 "ptx.y" /* yacc.c:1646  */
-    { add_option(LE_OPTION); }
-#line 2937 "ptx.tab.c" /* yacc.c:1646  */
+#line 544 "ptx.y"
+                    { recognizer->add_option(LE_OPTION); }
+#line 3218 "ptx.tab.c"
     break;
 
   case 236:
-#line 534 "ptx.y" /* yacc.c:1646  */
-    { add_option(GT_OPTION); }
-#line 2943 "ptx.tab.c" /* yacc.c:1646  */
+#line 545 "ptx.y"
+                    { recognizer->add_option(GT_OPTION); }
+#line 3224 "ptx.tab.c"
     break;
 
   case 237:
-#line 535 "ptx.y" /* yacc.c:1646  */
-    { add_option(GE_OPTION); }
-#line 2949 "ptx.tab.c" /* yacc.c:1646  */
+#line 546 "ptx.y"
+                    { recognizer->add_option(GE_OPTION); }
+#line 3230 "ptx.tab.c"
     break;
 
   case 238:
-#line 536 "ptx.y" /* yacc.c:1646  */
-    { add_option(LO_OPTION); }
-#line 2955 "ptx.tab.c" /* yacc.c:1646  */
+#line 547 "ptx.y"
+                    { recognizer->add_option(LO_OPTION); }
+#line 3236 "ptx.tab.c"
     break;
 
   case 239:
-#line 537 "ptx.y" /* yacc.c:1646  */
-    { add_option(LS_OPTION); }
-#line 2961 "ptx.tab.c" /* yacc.c:1646  */
+#line 548 "ptx.y"
+                    { recognizer->add_option(LS_OPTION); }
+#line 3242 "ptx.tab.c"
     break;
 
   case 240:
-#line 538 "ptx.y" /* yacc.c:1646  */
-    { add_option(HI_OPTION); }
-#line 2967 "ptx.tab.c" /* yacc.c:1646  */
+#line 549 "ptx.y"
+                    { recognizer->add_option(HI_OPTION); }
+#line 3248 "ptx.tab.c"
     break;
 
   case 241:
-#line 539 "ptx.y" /* yacc.c:1646  */
-    { add_option(HS_OPTION); }
-#line 2973 "ptx.tab.c" /* yacc.c:1646  */
+#line 550 "ptx.y"
+                     { recognizer->add_option(HS_OPTION); }
+#line 3254 "ptx.tab.c"
     break;
 
   case 242:
-#line 540 "ptx.y" /* yacc.c:1646  */
-    { add_option(EQU_OPTION); }
-#line 2979 "ptx.tab.c" /* yacc.c:1646  */
+#line 551 "ptx.y"
+                     { recognizer->add_option(EQU_OPTION); }
+#line 3260 "ptx.tab.c"
     break;
 
   case 243:
-#line 541 "ptx.y" /* yacc.c:1646  */
-    { add_option(NEU_OPTION); }
-#line 2985 "ptx.tab.c" /* yacc.c:1646  */
+#line 552 "ptx.y"
+                     { recognizer->add_option(NEU_OPTION); }
+#line 3266 "ptx.tab.c"
     break;
 
   case 244:
-#line 542 "ptx.y" /* yacc.c:1646  */
-    { add_option(LTU_OPTION); }
-#line 2991 "ptx.tab.c" /* yacc.c:1646  */
+#line 553 "ptx.y"
+                     { recognizer->add_option(LTU_OPTION); }
+#line 3272 "ptx.tab.c"
     break;
 
   case 245:
-#line 543 "ptx.y" /* yacc.c:1646  */
-    { add_option(LEU_OPTION); }
-#line 2997 "ptx.tab.c" /* yacc.c:1646  */
+#line 554 "ptx.y"
+                     { recognizer->add_option(LEU_OPTION); }
+#line 3278 "ptx.tab.c"
     break;
 
   case 246:
-#line 544 "ptx.y" /* yacc.c:1646  */
-    { add_option(GTU_OPTION); }
-#line 3003 "ptx.tab.c" /* yacc.c:1646  */
+#line 555 "ptx.y"
+                     { recognizer->add_option(GTU_OPTION); }
+#line 3284 "ptx.tab.c"
     break;
 
   case 247:
-#line 545 "ptx.y" /* yacc.c:1646  */
-    { add_option(GEU_OPTION); }
-#line 3009 "ptx.tab.c" /* yacc.c:1646  */
+#line 556 "ptx.y"
+                     { recognizer->add_option(GEU_OPTION); }
+#line 3290 "ptx.tab.c"
     break;
 
   case 248:
-#line 546 "ptx.y" /* yacc.c:1646  */
-    { add_option(NUM_OPTION); }
-#line 3015 "ptx.tab.c" /* yacc.c:1646  */
+#line 557 "ptx.y"
+                     { recognizer->add_option(NUM_OPTION); }
+#line 3296 "ptx.tab.c"
     break;
 
   case 249:
-#line 547 "ptx.y" /* yacc.c:1646  */
-    { add_option(NAN_OPTION); }
-#line 3021 "ptx.tab.c" /* yacc.c:1646  */
+#line 558 "ptx.y"
+                     { recognizer->add_option(NAN_OPTION); }
+#line 3302 "ptx.tab.c"
     break;
 
   case 250:
-#line 550 "ptx.y" /* yacc.c:1646  */
-    { add_option( PRMT_F4E_MODE); }
-#line 3027 "ptx.tab.c" /* yacc.c:1646  */
+#line 561 "ptx.y"
+                         { recognizer->add_option( PRMT_F4E_MODE); }
+#line 3308 "ptx.tab.c"
     break;
 
   case 251:
-#line 551 "ptx.y" /* yacc.c:1646  */
-    { add_option( PRMT_B4E_MODE); }
-#line 3033 "ptx.tab.c" /* yacc.c:1646  */
+#line 562 "ptx.y"
+                         { recognizer->add_option( PRMT_B4E_MODE); }
+#line 3314 "ptx.tab.c"
     break;
 
   case 252:
-#line 552 "ptx.y" /* yacc.c:1646  */
-    { add_option( PRMT_RC8_MODE); }
-#line 3039 "ptx.tab.c" /* yacc.c:1646  */
+#line 563 "ptx.y"
+                         { recognizer->add_option( PRMT_RC8_MODE); }
+#line 3320 "ptx.tab.c"
     break;
 
   case 253:
-#line 553 "ptx.y" /* yacc.c:1646  */
-    { add_option( PRMT_RC16_MODE);}
-#line 3045 "ptx.tab.c" /* yacc.c:1646  */
+#line 564 "ptx.y"
+                         { recognizer->add_option( PRMT_RC16_MODE);}
+#line 3326 "ptx.tab.c"
     break;
 
   case 254:
-#line 554 "ptx.y" /* yacc.c:1646  */
-    { add_option( PRMT_ECL_MODE); }
-#line 3051 "ptx.tab.c" /* yacc.c:1646  */
+#line 565 "ptx.y"
+                         { recognizer->add_option( PRMT_ECL_MODE); }
+#line 3332 "ptx.tab.c"
     break;
 
   case 255:
-#line 555 "ptx.y" /* yacc.c:1646  */
-    { add_option( PRMT_ECR_MODE); }
-#line 3057 "ptx.tab.c" /* yacc.c:1646  */
+#line 566 "ptx.y"
+                         { recognizer->add_option( PRMT_ECR_MODE); }
+#line 3338 "ptx.tab.c"
     break;
 
   case 256:
-#line 558 "ptx.y" /* yacc.c:1646  */
-    {add_space_spec(global_space,0);add_ptr_spec(global_space); add_wmma_option((yyvsp[-2].int_value));add_wmma_option((yyvsp[-1].int_value));add_wmma_option((yyvsp[0].int_value));}
-#line 3063 "ptx.tab.c" /* yacc.c:1646  */
+#line 569 "ptx.y"
+                                              {recognizer->add_space_spec(global_space,0);recognizer->add_ptr_spec(global_space); recognizer->add_wmma_option((yyvsp[-2].int_value));recognizer->add_wmma_option((yyvsp[-1].int_value));recognizer->add_wmma_option((yyvsp[0].int_value));}
+#line 3344 "ptx.tab.c"
     break;
 
   case 257:
-#line 559 "ptx.y" /* yacc.c:1646  */
-    {add_wmma_option((yyvsp[-3].int_value));add_wmma_option((yyvsp[-2].int_value));add_wmma_option((yyvsp[-1].int_value));add_wmma_option((yyvsp[0].int_value));}
-#line 3069 "ptx.tab.c" /* yacc.c:1646  */
+#line 570 "ptx.y"
+                                                    {recognizer->add_wmma_option((yyvsp[-3].int_value));recognizer->add_wmma_option((yyvsp[-2].int_value));recognizer->add_wmma_option((yyvsp[-1].int_value));recognizer->add_wmma_option((yyvsp[0].int_value));}
+#line 3350 "ptx.tab.c"
     break;
 
   case 260:
-#line 571 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_operand( (yyvsp[0].string_value) ); }
-#line 3075 "ptx.tab.c" /* yacc.c:1646  */
+#line 582 "ptx.y"
+                     { recognizer->add_scalar_operand( (yyvsp[0].string_value) ); }
+#line 3356 "ptx.tab.c"
     break;
 
   case 261:
-#line 572 "ptx.y" /* yacc.c:1646  */
-    { add_neg_pred_operand( (yyvsp[0].string_value) ); }
-#line 3081 "ptx.tab.c" /* yacc.c:1646  */
+#line 583 "ptx.y"
+                                 { recognizer->add_neg_pred_operand( (yyvsp[0].string_value) ); }
+#line 3362 "ptx.tab.c"
     break;
 
   case 262:
-#line 573 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_operand( (yyvsp[0].string_value) ); change_operand_neg(); }
-#line 3087 "ptx.tab.c" /* yacc.c:1646  */
+#line 584 "ptx.y"
+                            { recognizer->add_scalar_operand( (yyvsp[0].string_value) ); recognizer->change_operand_neg(); }
+#line 3368 "ptx.tab.c"
     break;
 
   case 267:
-#line 578 "ptx.y" /* yacc.c:1646  */
-    { change_operand_neg(); }
-#line 3093 "ptx.tab.c" /* yacc.c:1646  */
+#line 589 "ptx.y"
+                               { recognizer->change_operand_neg(); }
+#line 3374 "ptx.tab.c"
     break;
 
   case 269:
-#line 580 "ptx.y" /* yacc.c:1646  */
-    { add_address_operand((yyvsp[-2].string_value),(yyvsp[0].int_value)); }
-#line 3099 "ptx.tab.c" /* yacc.c:1646  */
+#line 591 "ptx.y"
+                                      { recognizer->add_address_operand((yyvsp[-2].string_value),(yyvsp[0].int_value)); }
+#line 3380 "ptx.tab.c"
     break;
 
   case 270:
-#line 581 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_operand( (yyvsp[-1].string_value) ); change_operand_lohi(1);}
-#line 3105 "ptx.tab.c" /* yacc.c:1646  */
+#line 592 "ptx.y"
+                               { recognizer->add_scalar_operand( (yyvsp[-1].string_value) ); recognizer->change_operand_lohi(1);}
+#line 3386 "ptx.tab.c"
     break;
 
   case 271:
-#line 582 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_operand( (yyvsp[-1].string_value) ); change_operand_lohi(1); change_operand_neg();}
-#line 3111 "ptx.tab.c" /* yacc.c:1646  */
+#line 593 "ptx.y"
+                                     { recognizer->add_scalar_operand( (yyvsp[-1].string_value) ); recognizer->change_operand_lohi(1); recognizer->change_operand_neg();}
+#line 3392 "ptx.tab.c"
     break;
 
   case 272:
-#line 583 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_operand( (yyvsp[-1].string_value) ); change_operand_lohi(2);}
-#line 3117 "ptx.tab.c" /* yacc.c:1646  */
+#line 594 "ptx.y"
+                               { recognizer->add_scalar_operand( (yyvsp[-1].string_value) ); recognizer->change_operand_lohi(2);}
+#line 3398 "ptx.tab.c"
     break;
 
   case 273:
-#line 584 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_operand( (yyvsp[-1].string_value) ); change_operand_lohi(2); change_operand_neg();}
-#line 3123 "ptx.tab.c" /* yacc.c:1646  */
+#line 595 "ptx.y"
+                                     { recognizer->add_scalar_operand( (yyvsp[-1].string_value) ); recognizer->change_operand_lohi(2); recognizer->change_operand_neg();}
+#line 3404 "ptx.tab.c"
     break;
 
   case 274:
-#line 585 "ptx.y" /* yacc.c:1646  */
-    { add_2vector_operand((yyvsp[-2].string_value),(yyvsp[0].string_value)); change_double_operand_type(-1);}
-#line 3129 "ptx.tab.c" /* yacc.c:1646  */
+#line 596 "ptx.y"
+                                     { recognizer->add_2vector_operand((yyvsp[-2].string_value),(yyvsp[0].string_value)); recognizer->change_double_operand_type(-1);}
+#line 3410 "ptx.tab.c"
     break;
 
   case 275:
-#line 586 "ptx.y" /* yacc.c:1646  */
-    { add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); change_double_operand_type(-1); change_operand_lohi(1);}
-#line 3135 "ptx.tab.c" /* yacc.c:1646  */
+#line 597 "ptx.y"
+                                               { recognizer->add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(-1); recognizer->change_operand_lohi(1);}
+#line 3416 "ptx.tab.c"
     break;
 
   case 276:
-#line 587 "ptx.y" /* yacc.c:1646  */
-    { add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); change_double_operand_type(-1); change_operand_lohi(2);}
-#line 3141 "ptx.tab.c" /* yacc.c:1646  */
+#line 598 "ptx.y"
+                                               { recognizer->add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(-1); recognizer->change_operand_lohi(2);}
+#line 3422 "ptx.tab.c"
     break;
 
   case 277:
-#line 588 "ptx.y" /* yacc.c:1646  */
-    { add_2vector_operand((yyvsp[-2].string_value),(yyvsp[0].string_value)); change_double_operand_type(-3);}
-#line 3147 "ptx.tab.c" /* yacc.c:1646  */
+#line 599 "ptx.y"
+                                          { recognizer->add_2vector_operand((yyvsp[-2].string_value),(yyvsp[0].string_value)); recognizer->change_double_operand_type(-3);}
+#line 3428 "ptx.tab.c"
     break;
 
   case 278:
-#line 589 "ptx.y" /* yacc.c:1646  */
-    { add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); change_double_operand_type(-3); change_operand_lohi(1);}
-#line 3153 "ptx.tab.c" /* yacc.c:1646  */
+#line 600 "ptx.y"
+                                                    { recognizer->add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(-3); recognizer->change_operand_lohi(1);}
+#line 3434 "ptx.tab.c"
     break;
 
   case 279:
-#line 590 "ptx.y" /* yacc.c:1646  */
-    { add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); change_double_operand_type(-3); change_operand_lohi(2);}
-#line 3159 "ptx.tab.c" /* yacc.c:1646  */
+#line 601 "ptx.y"
+                                                    { recognizer->add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(-3); recognizer->change_operand_lohi(2);}
+#line 3440 "ptx.tab.c"
     break;
 
   case 280:
-#line 593 "ptx.y" /* yacc.c:1646  */
-    { add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
-#line 3165 "ptx.tab.c" /* yacc.c:1646  */
+#line 604 "ptx.y"
+                                                                   { recognizer->add_2vector_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
+#line 3446 "ptx.tab.c"
     break;
 
   case 281:
-#line 594 "ptx.y" /* yacc.c:1646  */
-    { add_3vector_operand((yyvsp[-5].string_value),(yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
-#line 3171 "ptx.tab.c" /* yacc.c:1646  */
+#line 605 "ptx.y"
+                                                                                      { recognizer->add_3vector_operand((yyvsp[-5].string_value),(yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
+#line 3452 "ptx.tab.c"
     break;
 
   case 282:
-#line 595 "ptx.y" /* yacc.c:1646  */
-    { add_4vector_operand((yyvsp[-7].string_value),(yyvsp[-5].string_value),(yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
-#line 3177 "ptx.tab.c" /* yacc.c:1646  */
+#line 606 "ptx.y"
+                                                                                                       { recognizer->add_4vector_operand((yyvsp[-7].string_value),(yyvsp[-5].string_value),(yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
+#line 3458 "ptx.tab.c"
     break;
 
   case 283:
-#line 596 "ptx.y" /* yacc.c:1646  */
-    { add_8vector_operand((yyvsp[-15].string_value),(yyvsp[-13].string_value),(yyvsp[-11].string_value),(yyvsp[-9].string_value),(yyvsp[-7].string_value),(yyvsp[-5].string_value),(yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
-#line 3183 "ptx.tab.c" /* yacc.c:1646  */
+#line 607 "ptx.y"
+                                                                                                                                                                           { recognizer->add_8vector_operand((yyvsp[-15].string_value),(yyvsp[-13].string_value),(yyvsp[-11].string_value),(yyvsp[-9].string_value),(yyvsp[-7].string_value),(yyvsp[-5].string_value),(yyvsp[-3].string_value),(yyvsp[-1].string_value)); }
+#line 3464 "ptx.tab.c"
     break;
 
   case 284:
-#line 597 "ptx.y" /* yacc.c:1646  */
-    { add_1vector_operand((yyvsp[-1].string_value)); }
-#line 3189 "ptx.tab.c" /* yacc.c:1646  */
+#line 608 "ptx.y"
+                                                    { recognizer->add_1vector_operand((yyvsp[-1].string_value)); }
+#line 3470 "ptx.tab.c"
     break;
 
   case 285:
-#line 600 "ptx.y" /* yacc.c:1646  */
-    { add_scalar_operand((yyvsp[-1].string_value)); }
-#line 3195 "ptx.tab.c" /* yacc.c:1646  */
+#line 611 "ptx.y"
+                                                  { recognizer->add_scalar_operand((yyvsp[-1].string_value)); }
+#line 3476 "ptx.tab.c"
     break;
 
   case 287:
-#line 605 "ptx.y" /* yacc.c:1646  */
-    { add_builtin_operand((yyvsp[-1].int_value),(yyvsp[0].int_value)); }
-#line 3201 "ptx.tab.c" /* yacc.c:1646  */
+#line 616 "ptx.y"
+                                                     { recognizer->add_builtin_operand((yyvsp[-1].int_value),(yyvsp[0].int_value)); }
+#line 3482 "ptx.tab.c"
     break;
 
   case 288:
-#line 606 "ptx.y" /* yacc.c:1646  */
-    { add_builtin_operand((yyvsp[0].int_value),-1); }
-#line 3207 "ptx.tab.c" /* yacc.c:1646  */
+#line 617 "ptx.y"
+                           { recognizer->add_builtin_operand((yyvsp[0].int_value),-1); }
+#line 3488 "ptx.tab.c"
     break;
 
   case 289:
-#line 609 "ptx.y" /* yacc.c:1646  */
-    { add_memory_operand(); }
-#line 3213 "ptx.tab.c" /* yacc.c:1646  */
+#line 620 "ptx.y"
+                                                                             { recognizer->add_memory_operand(); }
+#line 3494 "ptx.tab.c"
     break;
 
   case 290:
-#line 610 "ptx.y" /* yacc.c:1646  */
-    { add_memory_operand(); change_memory_addr_space((yyvsp[-3].string_value)); }
-#line 3219 "ptx.tab.c" /* yacc.c:1646  */
+#line 621 "ptx.y"
+                                                                                 { recognizer->add_memory_operand(); recognizer->change_memory_addr_space((yyvsp[-3].string_value)); }
+#line 3500 "ptx.tab.c"
     break;
 
   case 291:
-#line 611 "ptx.y" /* yacc.c:1646  */
-    { change_memory_addr_space((yyvsp[-3].string_value)); }
-#line 3225 "ptx.tab.c" /* yacc.c:1646  */
+#line 622 "ptx.y"
+                                                                              { recognizer->change_memory_addr_space((yyvsp[-3].string_value)); }
+#line 3506 "ptx.tab.c"
     break;
 
   case 292:
-#line 612 "ptx.y" /* yacc.c:1646  */
-    { change_memory_addr_space((yyvsp[-3].string_value)); add_memory_operand();}
-#line 3231 "ptx.tab.c" /* yacc.c:1646  */
+#line 623 "ptx.y"
+                                                                           { recognizer->change_memory_addr_space((yyvsp[-3].string_value)); recognizer->add_memory_operand();}
+#line 3512 "ptx.tab.c"
     break;
 
   case 293:
-#line 613 "ptx.y" /* yacc.c:1646  */
-    { change_operand_neg(); }
-#line 3237 "ptx.tab.c" /* yacc.c:1646  */
+#line 624 "ptx.y"
+                               { recognizer->change_operand_neg(); }
+#line 3518 "ptx.tab.c"
     break;
 
   case 294:
-#line 616 "ptx.y" /* yacc.c:1646  */
-    { add_double_operand((yyvsp[-2].string_value),(yyvsp[0].string_value)); change_double_operand_type(1); }
-#line 3243 "ptx.tab.c" /* yacc.c:1646  */
+#line 627 "ptx.y"
+                                          { recognizer->add_double_operand((yyvsp[-2].string_value),(yyvsp[0].string_value)); recognizer->change_double_operand_type(1); }
+#line 3524 "ptx.tab.c"
     break;
 
   case 295:
-#line 617 "ptx.y" /* yacc.c:1646  */
-    { add_double_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); change_double_operand_type(1); change_operand_lohi(1); }
-#line 3249 "ptx.tab.c" /* yacc.c:1646  */
+#line 628 "ptx.y"
+                                               { recognizer->add_double_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(1); recognizer->change_operand_lohi(1); }
+#line 3530 "ptx.tab.c"
     break;
 
   case 296:
-#line 618 "ptx.y" /* yacc.c:1646  */
-    { add_double_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); change_double_operand_type(1); change_operand_lohi(2); }
-#line 3255 "ptx.tab.c" /* yacc.c:1646  */
+#line 629 "ptx.y"
+                                               { recognizer->add_double_operand((yyvsp[-3].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(1); recognizer->change_operand_lohi(2); }
+#line 3536 "ptx.tab.c"
     break;
 
   case 297:
-#line 619 "ptx.y" /* yacc.c:1646  */
-    { add_double_operand((yyvsp[-3].string_value),(yyvsp[0].string_value)); change_double_operand_type(2); }
-#line 3261 "ptx.tab.c" /* yacc.c:1646  */
+#line 630 "ptx.y"
+                                             { recognizer->add_double_operand((yyvsp[-3].string_value),(yyvsp[0].string_value)); recognizer->change_double_operand_type(2); }
+#line 3542 "ptx.tab.c"
     break;
 
   case 298:
-#line 620 "ptx.y" /* yacc.c:1646  */
-    { add_double_operand((yyvsp[-4].string_value),(yyvsp[-1].string_value)); change_double_operand_type(2); change_operand_lohi(1); }
-#line 3267 "ptx.tab.c" /* yacc.c:1646  */
+#line 631 "ptx.y"
+                                                      { recognizer->add_double_operand((yyvsp[-4].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(2); recognizer->change_operand_lohi(1); }
+#line 3548 "ptx.tab.c"
     break;
 
   case 299:
-#line 621 "ptx.y" /* yacc.c:1646  */
-    { add_double_operand((yyvsp[-4].string_value),(yyvsp[-1].string_value)); change_double_operand_type(2); change_operand_lohi(2); }
-#line 3273 "ptx.tab.c" /* yacc.c:1646  */
+#line 632 "ptx.y"
+                                                      { recognizer->add_double_operand((yyvsp[-4].string_value),(yyvsp[-1].string_value)); recognizer->change_double_operand_type(2); recognizer->change_operand_lohi(2); }
+#line 3554 "ptx.tab.c"
     break;
 
   case 300:
-#line 622 "ptx.y" /* yacc.c:1646  */
-    { add_address_operand((yyvsp[-3].string_value),(yyvsp[0].int_value)); change_double_operand_type(3); }
-#line 3279 "ptx.tab.c" /* yacc.c:1646  */
+#line 633 "ptx.y"
+                                              { recognizer->add_address_operand((yyvsp[-3].string_value),(yyvsp[0].int_value)); recognizer->change_double_operand_type(3); }
+#line 3560 "ptx.tab.c"
     break;
 
   case 301:
-#line 625 "ptx.y" /* yacc.c:1646  */
-    { add_literal_int((yyvsp[0].int_value)); }
-#line 3285 "ptx.tab.c" /* yacc.c:1646  */
+#line 636 "ptx.y"
+                              { recognizer->add_literal_int((yyvsp[0].int_value)); }
+#line 3566 "ptx.tab.c"
     break;
 
   case 302:
-#line 626 "ptx.y" /* yacc.c:1646  */
-    { add_literal_float((yyvsp[0].float_value)); }
-#line 3291 "ptx.tab.c" /* yacc.c:1646  */
+#line 637 "ptx.y"
+                        { recognizer->add_literal_float((yyvsp[0].float_value)); }
+#line 3572 "ptx.tab.c"
     break;
 
   case 303:
-#line 627 "ptx.y" /* yacc.c:1646  */
-    { add_literal_double((yyvsp[0].double_value)); }
-#line 3297 "ptx.tab.c" /* yacc.c:1646  */
+#line 638 "ptx.y"
+                         { recognizer->add_literal_double((yyvsp[0].double_value)); }
+#line 3578 "ptx.tab.c"
     break;
 
   case 304:
-#line 630 "ptx.y" /* yacc.c:1646  */
-    { add_address_operand((yyvsp[0].string_value),0); }
-#line 3303 "ptx.tab.c" /* yacc.c:1646  */
+#line 641 "ptx.y"
+                               { recognizer->add_address_operand((yyvsp[0].string_value),0); }
+#line 3584 "ptx.tab.c"
     break;
 
   case 305:
-#line 631 "ptx.y" /* yacc.c:1646  */
-    { add_address_operand((yyvsp[-1].string_value),0); change_operand_lohi(1);}
-#line 3309 "ptx.tab.c" /* yacc.c:1646  */
+#line 642 "ptx.y"
+                               { recognizer->add_address_operand((yyvsp[-1].string_value),0); recognizer->change_operand_lohi(1);}
+#line 3590 "ptx.tab.c"
     break;
 
   case 306:
-#line 632 "ptx.y" /* yacc.c:1646  */
-    { add_address_operand((yyvsp[-1].string_value),0); change_operand_lohi(2); }
-#line 3315 "ptx.tab.c" /* yacc.c:1646  */
+#line 643 "ptx.y"
+                               { recognizer->add_address_operand((yyvsp[-1].string_value),0); recognizer->change_operand_lohi(2); }
+#line 3596 "ptx.tab.c"
     break;
 
   case 307:
-#line 633 "ptx.y" /* yacc.c:1646  */
-    { add_address_operand((yyvsp[-2].string_value),(yyvsp[0].int_value)); }
-#line 3321 "ptx.tab.c" /* yacc.c:1646  */
+#line 644 "ptx.y"
+                                      { recognizer->add_address_operand((yyvsp[-2].string_value),(yyvsp[0].int_value)); }
+#line 3602 "ptx.tab.c"
     break;
 
   case 308:
-#line 634 "ptx.y" /* yacc.c:1646  */
-    { add_address_operand2((yyvsp[0].int_value)); }
-#line 3327 "ptx.tab.c" /* yacc.c:1646  */
+#line 645 "ptx.y"
+                      { recognizer->add_address_operand2((yyvsp[0].int_value)); }
+#line 3608 "ptx.tab.c"
     break;
 
 
-#line 3331 "ptx.tab.c" /* yacc.c:1646  */
+#line 3612 "ptx.tab.c"
+
       default: break;
     }
   /* User semantic actions sometimes alter yychar, and that requires
@@ -3352,14 +3634,13 @@ yyreduce:
   /* Now 'shift' the result of the reduction.  Determine what state
      that goes to, based on the state we popped back to and the rule
      number reduced by.  */
-
-  yyn = yyr1[yyn];
-
-  yystate = yypgoto[yyn - YYNTOKENS] + *yyssp;
-  if (0 <= yystate && yystate <= YYLAST && yycheck[yystate] == *yyssp)
-    yystate = yytable[yystate];
-  else
-    yystate = yydefgoto[yyn - YYNTOKENS];
+  {
+    const int yylhs = yyr1[yyn] - YYNTOKENS;
+    const int yyi = yypgoto[yylhs] + *yyssp;
+    yystate = (0 <= yyi && yyi <= YYLAST && yycheck[yyi] == *yyssp
+               ? yytable[yyi]
+               : yydefgoto[yylhs]);
+  }
 
   goto yynewstate;
 
@@ -3377,7 +3658,7 @@ yyerrlab:
     {
       ++yynerrs;
 #if ! YYERROR_VERBOSE
-      yyerror (YY_("syntax error"));
+      yyerror (scanner, recognizer, YY_("syntax error"));
 #else
 # define YYSYNTAX_ERROR yysyntax_error (&yymsg_alloc, &yymsg, \
                                         yyssp, yytoken)
@@ -3391,7 +3672,7 @@ yyerrlab:
           {
             if (yymsg != yymsgbuf)
               YYSTACK_FREE (yymsg);
-            yymsg = (char *) YYSTACK_ALLOC (yymsg_alloc);
+            yymsg = YY_CAST (char *, YYSTACK_ALLOC (YY_CAST (YYSIZE_T, yymsg_alloc)));
             if (!yymsg)
               {
                 yymsg = yymsgbuf;
@@ -3404,7 +3685,7 @@ yyerrlab:
                 yymsgp = yymsg;
               }
           }
-        yyerror (yymsgp);
+        yyerror (scanner, recognizer, yymsgp);
         if (yysyntax_error_status == 2)
           goto yyexhaustedlab;
       }
@@ -3428,7 +3709,7 @@ yyerrlab:
       else
         {
           yydestruct ("Error: discarding",
-                      yytoken, &yylval);
+                      yytoken, &yylval, scanner, recognizer);
           yychar = YYEMPTY;
         }
     }
@@ -3442,12 +3723,10 @@ yyerrlab:
 | yyerrorlab -- error raised explicitly by YYERROR.  |
 `---------------------------------------------------*/
 yyerrorlab:
-
-  /* Pacify compilers like GCC when the user code never invokes
-     YYERROR and the label yyerrorlab therefore never appears in user
-     code.  */
-  if (/*CONSTCOND*/ 0)
-     goto yyerrorlab;
+  /* Pacify compilers when the user code never invokes YYERROR and the
+     label yyerrorlab therefore never appears in user code.  */
+  if (0)
+    YYERROR;
 
   /* Do not reclaim the symbols of the rule whose action triggered
      this YYERROR.  */
@@ -3484,7 +3763,7 @@ yyerrlab1:
 
 
       yydestruct ("Error: popping",
-                  yystos[yystate], yyvsp);
+                  yystos[yystate], yyvsp, scanner, recognizer);
       YYPOPSTACK (1);
       yystate = *yyssp;
       YY_STACK_PRINT (yyss, yyssp);
@@ -3509,6 +3788,7 @@ yyacceptlab:
   yyresult = 0;
   goto yyreturn;
 
+
 /*-----------------------------------.
 | yyabortlab -- YYABORT comes here.  |
 `-----------------------------------*/
@@ -3516,16 +3796,21 @@ yyabortlab:
   yyresult = 1;
   goto yyreturn;
 
+
 #if !defined yyoverflow || YYERROR_VERBOSE
 /*-------------------------------------------------.
 | yyexhaustedlab -- memory exhaustion comes here.  |
 `-------------------------------------------------*/
 yyexhaustedlab:
-  yyerror (YY_("memory exhausted"));
+  yyerror (scanner, recognizer, YY_("memory exhausted"));
   yyresult = 2;
   /* Fall through.  */
 #endif
 
+
+/*-----------------------------------------------------.
+| yyreturn -- parsing is finished, return the result.  |
+`-----------------------------------------------------*/
 yyreturn:
   if (yychar != YYEMPTY)
     {
@@ -3533,7 +3818,7 @@ yyreturn:
          user semantic actions for why this is necessary.  */
       yytoken = YYTRANSLATE (yychar);
       yydestruct ("Cleanup: discarding lookahead",
-                  yytoken, &yylval);
+                  yytoken, &yylval, scanner, recognizer);
     }
   /* Do not reclaim the symbols of the rule whose action triggered
      this YYABORT or YYACCEPT.  */
@@ -3542,7 +3827,7 @@ yyreturn:
   while (yyssp != yyss)
     {
       yydestruct ("Cleanup: popping",
-                  yystos[*yyssp], yyvsp);
+                  yystos[+*yyssp], yyvsp, scanner, recognizer);
       YYPOPSTACK (1);
     }
 #ifndef yyoverflow
@@ -3555,15 +3840,12 @@ yyreturn:
 #endif
   return yyresult;
 }
-#line 637 "ptx.y" /* yacc.c:1906  */
-
+#line 648 "ptx.y"
 
-extern int ptx_lineno;
-extern const char *g_filename;
 
-void syntax_not_implemented()
+void syntax_not_implemented(yyscan_t yyscanner, ptx_recognizer* recognizer)
 {
-	printf("Parse error (%s:%u): this syntax is not (yet) implemented:\n",g_filename,ptx_lineno);
-	ptx_error(NULL);
+	printf("Parse error (%s): this syntax is not (yet) implemented:\n", recognizer->gpgpu_ctx->g_filename);
+	ptx_error(yyscanner, recognizer, NULL);
 	abort();
 }
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.h
index 58fac6b318..50a17b0dc9 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.tab.h
@@ -1,8 +1,9 @@
-/* A Bison parser, made by GNU Bison 3.0.4.  */
+/* A Bison parser, made by GNU Bison 3.5.1.  */
 
 /* Bison interface for Yacc-like parsers in C
 
-   Copyright (C) 1984, 1989-1990, 2000-2015 Free Software Foundation, Inc.
+   Copyright (C) 1984, 1989-1990, 2000-2015, 2018-2020 Free Software Foundation,
+   Inc.
 
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
@@ -30,8 +31,11 @@
    This special exception was added by the Free Software Foundation in
    version 2.2 of Bison.  */
 
-#ifndef YY_PTX_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTX_TAB_H_INCLUDED
-# define YY_PTX_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTX_TAB_H_INCLUDED
+/* Undocumented macros, especially those whose name start with YY_,
+   are private implementation details.  Do not rely on them.  */
+
+#ifndef YY_PTX_PTX_TAB_H_INCLUDED
+# define YY_PTX_PTX_TAB_H_INCLUDED
 /* Debug traces.  */
 #ifndef YYDEBUG
 # define YYDEBUG 0
@@ -220,13 +224,186 @@ extern int ptx_debug;
     PRMT_ECR_MODE = 430
   };
 #endif
+/* Tokens.  */
+#define STRING 258
+#define OPCODE 259
+#define WMMA_DIRECTIVE 260
+#define LAYOUT 261
+#define CONFIGURATION 262
+#define ALIGN_DIRECTIVE 263
+#define BRANCHTARGETS_DIRECTIVE 264
+#define BYTE_DIRECTIVE 265
+#define CALLPROTOTYPE_DIRECTIVE 266
+#define CALLTARGETS_DIRECTIVE 267
+#define CONST_DIRECTIVE 268
+#define CONSTPTR_DIRECTIVE 269
+#define PTR_DIRECTIVE 270
+#define ENTRY_DIRECTIVE 271
+#define EXTERN_DIRECTIVE 272
+#define FILE_DIRECTIVE 273
+#define FUNC_DIRECTIVE 274
+#define GLOBAL_DIRECTIVE 275
+#define LOCAL_DIRECTIVE 276
+#define LOC_DIRECTIVE 277
+#define MAXNCTAPERSM_DIRECTIVE 278
+#define MAXNNREG_DIRECTIVE 279
+#define MAXNTID_DIRECTIVE 280
+#define MINNCTAPERSM_DIRECTIVE 281
+#define PARAM_DIRECTIVE 282
+#define PRAGMA_DIRECTIVE 283
+#define REG_DIRECTIVE 284
+#define REQNTID_DIRECTIVE 285
+#define SECTION_DIRECTIVE 286
+#define SHARED_DIRECTIVE 287
+#define SREG_DIRECTIVE 288
+#define SSTARR_DIRECTIVE 289
+#define STRUCT_DIRECTIVE 290
+#define SURF_DIRECTIVE 291
+#define TARGET_DIRECTIVE 292
+#define TEX_DIRECTIVE 293
+#define UNION_DIRECTIVE 294
+#define VERSION_DIRECTIVE 295
+#define ADDRESS_SIZE_DIRECTIVE 296
+#define VISIBLE_DIRECTIVE 297
+#define WEAK_DIRECTIVE 298
+#define IDENTIFIER 299
+#define INT_OPERAND 300
+#define FLOAT_OPERAND 301
+#define DOUBLE_OPERAND 302
+#define S8_TYPE 303
+#define S16_TYPE 304
+#define S32_TYPE 305
+#define S64_TYPE 306
+#define U8_TYPE 307
+#define U16_TYPE 308
+#define U32_TYPE 309
+#define U64_TYPE 310
+#define F16_TYPE 311
+#define F32_TYPE 312
+#define F64_TYPE 313
+#define FF64_TYPE 314
+#define B8_TYPE 315
+#define B16_TYPE 316
+#define B32_TYPE 317
+#define B64_TYPE 318
+#define BB64_TYPE 319
+#define BB128_TYPE 320
+#define PRED_TYPE 321
+#define TEXREF_TYPE 322
+#define SAMPLERREF_TYPE 323
+#define SURFREF_TYPE 324
+#define V2_TYPE 325
+#define V3_TYPE 326
+#define V4_TYPE 327
+#define COMMA 328
+#define PRED 329
+#define HALF_OPTION 330
+#define EXTP_OPTION 331
+#define EQ_OPTION 332
+#define NE_OPTION 333
+#define LT_OPTION 334
+#define LE_OPTION 335
+#define GT_OPTION 336
+#define GE_OPTION 337
+#define LO_OPTION 338
+#define LS_OPTION 339
+#define HI_OPTION 340
+#define HS_OPTION 341
+#define EQU_OPTION 342
+#define NEU_OPTION 343
+#define LTU_OPTION 344
+#define LEU_OPTION 345
+#define GTU_OPTION 346
+#define GEU_OPTION 347
+#define NUM_OPTION 348
+#define NAN_OPTION 349
+#define CF_OPTION 350
+#define SF_OPTION 351
+#define NSF_OPTION 352
+#define LEFT_SQUARE_BRACKET 353
+#define RIGHT_SQUARE_BRACKET 354
+#define WIDE_OPTION 355
+#define SPECIAL_REGISTER 356
+#define MINUS 357
+#define PLUS 358
+#define COLON 359
+#define SEMI_COLON 360
+#define EXCLAMATION 361
+#define PIPE 362
+#define RIGHT_BRACE 363
+#define LEFT_BRACE 364
+#define EQUALS 365
+#define PERIOD 366
+#define BACKSLASH 367
+#define DIMENSION_MODIFIER 368
+#define RN_OPTION 369
+#define RZ_OPTION 370
+#define RM_OPTION 371
+#define RP_OPTION 372
+#define RNI_OPTION 373
+#define RZI_OPTION 374
+#define RMI_OPTION 375
+#define RPI_OPTION 376
+#define UNI_OPTION 377
+#define GEOM_MODIFIER_1D 378
+#define GEOM_MODIFIER_2D 379
+#define GEOM_MODIFIER_3D 380
+#define SAT_OPTION 381
+#define FTZ_OPTION 382
+#define NEG_OPTION 383
+#define SYNC_OPTION 384
+#define RED_OPTION 385
+#define ARRIVE_OPTION 386
+#define ATOMIC_POPC 387
+#define ATOMIC_AND 388
+#define ATOMIC_OR 389
+#define ATOMIC_XOR 390
+#define ATOMIC_CAS 391
+#define ATOMIC_EXCH 392
+#define ATOMIC_ADD 393
+#define ATOMIC_INC 394
+#define ATOMIC_DEC 395
+#define ATOMIC_MIN 396
+#define ATOMIC_MAX 397
+#define LEFT_ANGLE_BRACKET 398
+#define RIGHT_ANGLE_BRACKET 399
+#define LEFT_PAREN 400
+#define RIGHT_PAREN 401
+#define APPROX_OPTION 402
+#define FULL_OPTION 403
+#define ANY_OPTION 404
+#define ALL_OPTION 405
+#define BALLOT_OPTION 406
+#define GLOBAL_OPTION 407
+#define CTA_OPTION 408
+#define SYS_OPTION 409
+#define EXIT_OPTION 410
+#define ABS_OPTION 411
+#define TO_OPTION 412
+#define CA_OPTION 413
+#define CG_OPTION 414
+#define CS_OPTION 415
+#define LU_OPTION 416
+#define CV_OPTION 417
+#define WB_OPTION 418
+#define WT_OPTION 419
+#define NC_OPTION 420
+#define UP_OPTION 421
+#define DOWN_OPTION 422
+#define BFLY_OPTION 423
+#define IDX_OPTION 424
+#define PRMT_F4E_MODE 425
+#define PRMT_B4E_MODE 426
+#define PRMT_RC8_MODE 427
+#define PRMT_RC16_MODE 428
+#define PRMT_ECL_MODE 429
+#define PRMT_ECR_MODE 430
 
 /* Value type.  */
 #if ! defined YYSTYPE && ! defined YYSTYPE_IS_DECLARED
-
 union YYSTYPE
 {
-#line 30 "ptx.y" /* yacc.c:1909  */
+#line 42 "ptx.y"
 
   double double_value;
   float  float_value;
@@ -234,18 +411,16 @@ union YYSTYPE
   char * string_value;
   void * ptr_value;
 
-#line 238 "ptx.tab.h" /* yacc.c:1909  */
-};
+#line 415 "ptx.tab.h"
 
+};
 typedef union YYSTYPE YYSTYPE;
 # define YYSTYPE_IS_TRIVIAL 1
 # define YYSTYPE_IS_DECLARED 1
 #endif
 
 
-extern YYSTYPE ptx_lval;
 
-// extern "C" int ptx_parse(void);
-// extern "C" int ptx_parse();
+// int ptx_parse (yyscan_t scanner, ptx_recognizer* recognizer);
 
-#endif /* !YY_PTX_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTX_TAB_H_INCLUDED  */
+#endif /* !YY_PTX_PTX_TAB_H_INCLUDED  */
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.y b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.y
index 260564f6be..0b856d228d 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.y
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx.y
@@ -27,6 +27,18 @@ OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
 
+%{
+typedef void * yyscan_t;
+class ptx_recognizer;
+#include "../../libcuda_sim/gpgpu_context.h"
+%}
+
+%define api.pure full
+%parse-param {yyscan_t scanner}
+%parse-param {ptx_recognizer* recognizer}
+%lex-param {yyscan_t scanner}
+%lex-param {ptx_recognizer* recognizer}
+
 %union {
   double double_value;
   float  float_value;
@@ -217,10 +229,9 @@ OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 	#include <stdlib.h>
 	#include <string.h>
 	#include <math.h>
-	void syntax_not_implemented();
-	extern int g_func_decl;
-	int ptx_lex(void);
-	int ptx_error(const char *);
+	void syntax_not_implemented(yyscan_t yyscanner, ptx_recognizer* recognizer);
+	int ptx_lex(YYSTYPE * yylval_param, yyscan_t yyscanner, ptx_recognizer* recognizer);
+	int ptx_error( yyscan_t yyscanner, ptx_recognizer* recognizer, const char *s );
 %}
 
 %%
@@ -231,97 +242,97 @@ input:	/* empty */
 	| input function_decl
 	;
 
-function_defn: function_decl { set_symtab($1); func_header(".skip"); } statement_block { end_function(); }
-	| function_decl { set_symtab($1); } block_spec_list { func_header(".skip"); } statement_block { end_function(); }
+function_defn: function_decl { recognizer->set_symtab($1); recognizer->func_header(".skip"); } statement_block { recognizer->end_function(); }
+	| function_decl { recognizer->set_symtab($1); } block_spec_list { recognizer->func_header(".skip"); } statement_block { recognizer->end_function(); }
 	;
 
-block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND {func_header_info_int(".maxntid", $2);
-										func_header_info_int(",", $4);
-										func_header_info_int(",", $6); 
-                                                                                maxnt_id($2, $4, $6);}
-	| MINNCTAPERSM_DIRECTIVE INT_OPERAND { func_header_info_int(".minnctapersm", $2); printf("GPGPU-Sim: Warning: .minnctapersm ignored. \n"); }
-	| MAXNCTAPERSM_DIRECTIVE INT_OPERAND { func_header_info_int(".maxnctapersm", $2); printf("GPGPU-Sim: Warning: .maxnctapersm ignored. \n"); }
+block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND {recognizer->func_header_info_int(".maxntid", $2);
+										recognizer->func_header_info_int(",", $4);
+										recognizer->func_header_info_int(",", $6);
+                                                                                recognizer->maxnt_id($2, $4, $6);}
+	| MINNCTAPERSM_DIRECTIVE INT_OPERAND { recognizer->func_header_info_int(".minnctapersm", $2); printf("GPGPU-Sim: Warning: .minnctapersm ignored. \n"); }
+	| MAXNCTAPERSM_DIRECTIVE INT_OPERAND { recognizer->func_header_info_int(".maxnctapersm", $2); printf("GPGPU-Sim: Warning: .maxnctapersm ignored. \n"); }
 	;
 
 block_spec_list: block_spec
 	| block_spec_list block_spec
 	;
 
-function_decl: function_decl_header LEFT_PAREN { start_function($1); func_header_info("(");} param_entry RIGHT_PAREN {func_header_info(")");} function_ident_param { $$ = reset_symtab(); }
-	| function_decl_header { start_function($1); } function_ident_param { $$ = reset_symtab(); }
-	| function_decl_header { start_function($1); add_function_name(""); g_func_decl=0; $$ = reset_symtab(); }
+function_decl: function_decl_header LEFT_PAREN { recognizer->start_function($1); recognizer->func_header_info("(");} param_entry RIGHT_PAREN {recognizer->func_header_info(")");} function_ident_param { $$ = recognizer->reset_symtab(); }
+	| function_decl_header { recognizer->start_function($1); } function_ident_param { $$ = recognizer->reset_symtab(); }
+	| function_decl_header { recognizer->start_function($1); recognizer->add_function_name(""); recognizer->g_func_decl=0; $$ = recognizer->reset_symtab(); }
 	;
 
-function_ident_param: IDENTIFIER { add_function_name($1); } LEFT_PAREN {func_header_info("(");} param_list RIGHT_PAREN { g_func_decl=0; func_header_info(")"); } 
-	| IDENTIFIER { add_function_name($1); g_func_decl=0; } 
+function_ident_param: IDENTIFIER { recognizer->add_function_name($1); } LEFT_PAREN {recognizer->func_header_info("(");} param_list RIGHT_PAREN { recognizer->g_func_decl=0; recognizer->func_header_info(")"); }
+	| IDENTIFIER { recognizer->add_function_name($1); recognizer->g_func_decl=0; }
 	;
 
-function_decl_header: ENTRY_DIRECTIVE { $$ = 1; g_func_decl=1; func_header(".entry"); }
-	| VISIBLE_DIRECTIVE ENTRY_DIRECTIVE { $$ = 1; g_func_decl=1; func_header(".entry"); }
-	| WEAK_DIRECTIVE ENTRY_DIRECTIVE { $$ = 1; g_func_decl=1; func_header(".entry"); }
-	| FUNC_DIRECTIVE { $$ = 0; g_func_decl=1; func_header(".func"); }
-	| VISIBLE_DIRECTIVE FUNC_DIRECTIVE { $$ = 0; g_func_decl=1; func_header(".func"); }
-	| WEAK_DIRECTIVE FUNC_DIRECTIVE { $$ = 0; g_func_decl=1; func_header(".func"); }
-	| EXTERN_DIRECTIVE FUNC_DIRECTIVE { $$ = 2; g_func_decl=1; func_header(".func"); }
-	| WEAK_DIRECTIVE FUNC_DIRECTIVE { $$ = 0; g_func_decl=1; func_header(".func"); }
+function_decl_header: ENTRY_DIRECTIVE { $$ = 1; recognizer->g_func_decl=1; recognizer->func_header(".entry"); }
+	| VISIBLE_DIRECTIVE ENTRY_DIRECTIVE { $$ = 1; recognizer->g_func_decl=1; recognizer->func_header(".entry"); }
+	| WEAK_DIRECTIVE ENTRY_DIRECTIVE { $$ = 1; recognizer->g_func_decl=1; recognizer->func_header(".entry"); }
+	| FUNC_DIRECTIVE { $$ = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+	| VISIBLE_DIRECTIVE FUNC_DIRECTIVE { $$ = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+	| WEAK_DIRECTIVE FUNC_DIRECTIVE { $$ = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+	| EXTERN_DIRECTIVE FUNC_DIRECTIVE { $$ = 2; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
+	| WEAK_DIRECTIVE FUNC_DIRECTIVE { $$ = 0; recognizer->g_func_decl=1; recognizer->func_header(".func"); }
 	;
 
 param_list: /*empty*/
-	| param_entry { add_directive(); }
-	| param_list COMMA {func_header_info(",");} param_entry { add_directive(); }
+	| param_entry { recognizer->add_directive(); }
+	| param_list COMMA {recognizer->func_header_info(",");} param_entry { recognizer->add_directive(); }
 
-param_entry: PARAM_DIRECTIVE { add_space_spec(param_space_unclassified,0); } variable_spec ptr_spec identifier_spec { add_function_arg(); }
-	| REG_DIRECTIVE { add_space_spec(reg_space,0); } variable_spec identifier_spec { add_function_arg(); }
+param_entry: PARAM_DIRECTIVE { recognizer->add_space_spec(param_space_unclassified,0); } variable_spec ptr_spec identifier_spec { recognizer->add_function_arg(); }
+	| REG_DIRECTIVE { recognizer->add_space_spec(reg_space,0); } variable_spec identifier_spec { recognizer->add_function_arg(); }
 
 ptr_spec: /*empty*/
         | PTR_DIRECTIVE ptr_space_spec ptr_align_spec
         | PTR_DIRECTIVE ptr_align_spec
 
-ptr_space_spec: GLOBAL_DIRECTIVE { add_ptr_spec(global_space); }
-              | LOCAL_DIRECTIVE  { add_ptr_spec(local_space); }
-              | SHARED_DIRECTIVE { add_ptr_spec(shared_space); }
-			  | CONST_DIRECTIVE { add_ptr_spec(global_space); }
+ptr_space_spec: GLOBAL_DIRECTIVE { recognizer->add_ptr_spec(global_space); }
+              | LOCAL_DIRECTIVE  { recognizer->add_ptr_spec(local_space); }
+              | SHARED_DIRECTIVE { recognizer->add_ptr_spec(shared_space); }
+			  | CONST_DIRECTIVE { recognizer->add_ptr_spec(global_space); }
 
 ptr_align_spec: ALIGN_DIRECTIVE INT_OPERAND
 
 statement_block: LEFT_BRACE statement_list RIGHT_BRACE 
 
-statement_list: directive_statement { add_directive(); }
-	| statement_list prototype_block {printf("Prototype statement detected. WARNING: this is not supported yet on GPGPU-SIM\n"); }
-	| instruction_statement { add_instruction(); }
-	| statement_list directive_statement { add_directive(); }
-	| statement_list instruction_statement { add_instruction(); }
-	| statement_list {start_inst_group();} statement_block {end_inst_group();}
-	| {start_inst_group();} statement_block {end_inst_group();}
+statement_list: directive_statement { recognizer->add_directive(); }
+    | statement_list prototype_block {printf("Prototype statement detected. WARNING: this is not supported yet on GPGPU-SIM\n"); }
+	| instruction_statement { recognizer->add_instruction(); }
+	| statement_list directive_statement { recognizer->add_directive(); }
+	| statement_list instruction_statement { recognizer->add_instruction(); }
+	| statement_list {recognizer->start_inst_group();} statement_block {recognizer->end_inst_group();}
+	| {recognizer->start_inst_group();} statement_block {recognizer->end_inst_group();}
 	;
 
 directive_statement: variable_declaration SEMI_COLON
-	| VERSION_DIRECTIVE DOUBLE_OPERAND { add_version_info($2, 0); }
-	| VERSION_DIRECTIVE DOUBLE_OPERAND PLUS { add_version_info($2,1); }
+	| VERSION_DIRECTIVE DOUBLE_OPERAND { recognizer->add_version_info($2, 0); }
+	| VERSION_DIRECTIVE DOUBLE_OPERAND PLUS { recognizer->add_version_info($2,1); }
 	| ADDRESS_SIZE_DIRECTIVE INT_OPERAND {/*Do nothing*/}
-	| TARGET_DIRECTIVE IDENTIFIER COMMA IDENTIFIER { target_header2($2,$4); }
-	| TARGET_DIRECTIVE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER { target_header3($2,$4,$6); }
-	| TARGET_DIRECTIVE IDENTIFIER { target_header($2); }
-	| FILE_DIRECTIVE INT_OPERAND STRING { add_file($2,$3); } 
-	| FILE_DIRECTIVE INT_OPERAND STRING COMMA INT_OPERAND COMMA INT_OPERAND { add_file($2,$3); } 	
+	| TARGET_DIRECTIVE IDENTIFIER COMMA IDENTIFIER { recognizer->target_header2($2,$4); }
+	| TARGET_DIRECTIVE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER { recognizer->target_header3($2,$4,$6); }
+	| TARGET_DIRECTIVE IDENTIFIER { recognizer->target_header($2); }
+	| FILE_DIRECTIVE INT_OPERAND STRING { recognizer->add_file($2,$3); }
+	| FILE_DIRECTIVE INT_OPERAND STRING COMMA INT_OPERAND COMMA INT_OPERAND { recognizer->add_file($2,$3); }
 	| LOC_DIRECTIVE INT_OPERAND INT_OPERAND INT_OPERAND 
-	| PRAGMA_DIRECTIVE STRING SEMI_COLON { add_pragma($2); }
+	| PRAGMA_DIRECTIVE STRING SEMI_COLON { recognizer->add_pragma($2); }
 	| function_decl SEMI_COLON {/*Do nothing*/}
 	;
 
-variable_declaration: variable_spec identifier_list { add_variables(); }
-	| variable_spec identifier_spec EQUALS initializer_list { add_variables(); }
-	| variable_spec identifier_spec EQUALS literal_operand { add_variables(); }
-	| CONSTPTR_DIRECTIVE IDENTIFIER COMMA IDENTIFIER COMMA INT_OPERAND { add_constptr($2, $4, $6); }
+variable_declaration: variable_spec identifier_list { recognizer->add_variables(); }
+	| variable_spec identifier_spec EQUALS initializer_list { recognizer->add_variables(); }
+	| variable_spec identifier_spec EQUALS literal_operand { recognizer->add_variables(); }
+	| CONSTPTR_DIRECTIVE IDENTIFIER COMMA IDENTIFIER COMMA INT_OPERAND { recognizer->add_constptr($2, $4, $6); }
 	;
 
-variable_spec: var_spec_list { set_variable_type(); }
+variable_spec: var_spec_list { recognizer->set_variable_type(); }
 
 identifier_list: identifier_spec
 	| identifier_list COMMA identifier_spec;
 
-identifier_spec: IDENTIFIER { add_identifier($1,0,NON_ARRAY_IDENTIFIER); func_header_info($1);}
-	| IDENTIFIER LEFT_ANGLE_BRACKET INT_OPERAND RIGHT_ANGLE_BRACKET { func_header_info($1); func_header_info_int("<", $3); func_header_info(">");
+identifier_spec: IDENTIFIER { recognizer->add_identifier($1,0,NON_ARRAY_IDENTIFIER); recognizer->func_header_info($1);}
+	| IDENTIFIER LEFT_ANGLE_BRACKET INT_OPERAND RIGHT_ANGLE_BRACKET { recognizer->func_header_info($1); recognizer->func_header_info_int("<", $3); recognizer->func_header_info(">");
 		int i,lbase,l;
 		char *id = NULL;
 		lbase = strlen($1);
@@ -329,12 +340,12 @@ identifier_spec: IDENTIFIER { add_identifier($1,0,NON_ARRAY_IDENTIFIER); func_he
 			l = lbase + (int)log10(i+1)+10;
 			id = (char*) malloc(l);
 			snprintf(id,l,"%s%u",$1,i);
-			add_identifier(id,0,NON_ARRAY_IDENTIFIER); 
+			recognizer->add_identifier(id,0,NON_ARRAY_IDENTIFIER);
 		}
 		free($1);
 	}
-	| IDENTIFIER LEFT_SQUARE_BRACKET RIGHT_SQUARE_BRACKET { add_identifier($1,0,ARRAY_IDENTIFIER_NO_DIM); func_header_info($1); func_header_info("["); func_header_info("]");}
-	| IDENTIFIER LEFT_SQUARE_BRACKET INT_OPERAND RIGHT_SQUARE_BRACKET { add_identifier($1,$3,ARRAY_IDENTIFIER); func_header_info($1); func_header_info_int("[",$3); func_header_info("]");}
+	| IDENTIFIER LEFT_SQUARE_BRACKET RIGHT_SQUARE_BRACKET { recognizer->add_identifier($1,0,ARRAY_IDENTIFIER_NO_DIM); recognizer->func_header_info($1); recognizer->func_header_info("["); recognizer->func_header_info("]");}
+	| IDENTIFIER LEFT_SQUARE_BRACKET INT_OPERAND RIGHT_SQUARE_BRACKET { recognizer->add_identifier($1,$3,ARRAY_IDENTIFIER); recognizer->func_header_info($1); recognizer->func_header_info_int("[",$3); recognizer->func_header_info("]");}
 	;
 
 var_spec_list: var_spec 
@@ -344,62 +355,62 @@ var_spec: space_spec
 	| type_spec
 	| align_spec
 	| VISIBLE_DIRECTIVE
-	| EXTERN_DIRECTIVE { add_extern_spec(); }
+	| EXTERN_DIRECTIVE { recognizer->add_extern_spec(); }
     | WEAK_DIRECTIVE
 	;
 
-align_spec: ALIGN_DIRECTIVE INT_OPERAND { add_alignment_spec($2); }
+align_spec: ALIGN_DIRECTIVE INT_OPERAND { recognizer->add_alignment_spec($2); }
 
-space_spec: REG_DIRECTIVE {  add_space_spec(reg_space,0); }
-	| SREG_DIRECTIVE  {  add_space_spec(reg_space,0); }
+space_spec: REG_DIRECTIVE {  recognizer->add_space_spec(reg_space,0); }
+	| SREG_DIRECTIVE  {  recognizer->add_space_spec(reg_space,0); }
 	| addressable_spec
 	;
 
-addressable_spec: CONST_DIRECTIVE {  add_space_spec(const_space,$1); }
-	| GLOBAL_DIRECTIVE 	  {  add_space_spec(global_space,0); }
-	| LOCAL_DIRECTIVE 	  {  add_space_spec(local_space,0); }
-	| PARAM_DIRECTIVE 	  {  add_space_spec(param_space_unclassified,0); }
-	| SHARED_DIRECTIVE 	  {  add_space_spec(shared_space,0); }
-	| SSTARR_DIRECTIVE    {  add_space_spec(sstarr_space,0); }
-	| SURF_DIRECTIVE 	  {  add_space_spec(surf_space,0); }
-	| TEX_DIRECTIVE 	  {  add_space_spec(tex_space,0); }
+addressable_spec: CONST_DIRECTIVE {  recognizer->add_space_spec(const_space,$1); }
+	| GLOBAL_DIRECTIVE 	  {  recognizer->add_space_spec(global_space,0); }
+	| LOCAL_DIRECTIVE 	  {  recognizer->add_space_spec(local_space,0); }
+	| PARAM_DIRECTIVE 	  {  recognizer->add_space_spec(param_space_unclassified,0); }
+	| SHARED_DIRECTIVE 	  {  recognizer->add_space_spec(shared_space,0); }
+	| SSTARR_DIRECTIVE    {  recognizer->add_space_spec(sstarr_space,0); }
+	| SURF_DIRECTIVE 	  {  recognizer->add_space_spec(surf_space,0); }
+	| TEX_DIRECTIVE 	  {  recognizer->add_space_spec(tex_space,0); }
 	;
 
 type_spec: scalar_type 
 	|  vector_spec scalar_type 
 	;
 
-vector_spec:  V2_TYPE {  add_option(V2_TYPE); func_header_info(".v2");}
-	| V3_TYPE     {  add_option(V3_TYPE); func_header_info(".v3");}
-	| V4_TYPE     {  add_option(V4_TYPE); func_header_info(".v4");}
-	;
-
-scalar_type: S8_TYPE { add_scalar_type_spec( S8_TYPE ); }
-	| S16_TYPE   { add_scalar_type_spec( S16_TYPE ); }
-	| S32_TYPE   { add_scalar_type_spec( S32_TYPE ); }
-	| S64_TYPE   { add_scalar_type_spec( S64_TYPE ); }
-	| U8_TYPE    { add_scalar_type_spec( U8_TYPE ); }
-	| U16_TYPE   { add_scalar_type_spec( U16_TYPE ); }
-	| U32_TYPE   { add_scalar_type_spec( U32_TYPE ); }
-	| U64_TYPE   { add_scalar_type_spec( U64_TYPE ); }
-	| F16_TYPE   { add_scalar_type_spec( F16_TYPE ); }
-	| F32_TYPE   { add_scalar_type_spec( F32_TYPE ); }
-	| F64_TYPE   { add_scalar_type_spec( F64_TYPE ); }
-	| FF64_TYPE   { add_scalar_type_spec( FF64_TYPE ); }
-	| B8_TYPE    { add_scalar_type_spec( B8_TYPE );  }
-	| B16_TYPE   { add_scalar_type_spec( B16_TYPE ); }
-	| B32_TYPE   { add_scalar_type_spec( B32_TYPE ); }
-	| B64_TYPE   { add_scalar_type_spec( B64_TYPE ); }
-	| BB64_TYPE   { add_scalar_type_spec( BB64_TYPE ); }
-	| BB128_TYPE   { add_scalar_type_spec( BB128_TYPE ); }
-	| PRED_TYPE  { add_scalar_type_spec( PRED_TYPE ); }
-	| TEXREF_TYPE  { add_scalar_type_spec( TEXREF_TYPE ); }
-	| SAMPLERREF_TYPE  { add_scalar_type_spec( SAMPLERREF_TYPE ); }
-	| SURFREF_TYPE  { add_scalar_type_spec( SURFREF_TYPE ); }
-	;
-
-initializer_list: LEFT_BRACE literal_list RIGHT_BRACE { add_array_initializer(); } 
-	| LEFT_BRACE initializer_list RIGHT_BRACE { syntax_not_implemented(); }
+vector_spec:  V2_TYPE {  recognizer->add_option(V2_TYPE); recognizer->func_header_info(".v2");}
+	| V3_TYPE     {  recognizer->add_option(V3_TYPE); recognizer->func_header_info(".v3");}
+	| V4_TYPE     {  recognizer->add_option(V4_TYPE); recognizer->func_header_info(".v4");}
+	;
+
+scalar_type: S8_TYPE { recognizer->add_scalar_type_spec( S8_TYPE ); }
+	| S16_TYPE   { recognizer->add_scalar_type_spec( S16_TYPE ); }
+	| S32_TYPE   { recognizer->add_scalar_type_spec( S32_TYPE ); }
+	| S64_TYPE   { recognizer->add_scalar_type_spec( S64_TYPE ); }
+	| U8_TYPE    { recognizer->add_scalar_type_spec( U8_TYPE ); }
+	| U16_TYPE   { recognizer->add_scalar_type_spec( U16_TYPE ); }
+	| U32_TYPE   { recognizer->add_scalar_type_spec( U32_TYPE ); }
+	| U64_TYPE   { recognizer->add_scalar_type_spec( U64_TYPE ); }
+	| F16_TYPE   { recognizer->add_scalar_type_spec( F16_TYPE ); }
+	| F32_TYPE   { recognizer->add_scalar_type_spec( F32_TYPE ); }
+	| F64_TYPE   { recognizer->add_scalar_type_spec( F64_TYPE ); }
+	| FF64_TYPE   { recognizer->add_scalar_type_spec( FF64_TYPE ); }
+	| B8_TYPE    { recognizer->add_scalar_type_spec( B8_TYPE );  }
+	| B16_TYPE   { recognizer->add_scalar_type_spec( B16_TYPE ); }
+	| B32_TYPE   { recognizer->add_scalar_type_spec( B32_TYPE ); }
+	| B64_TYPE   { recognizer->add_scalar_type_spec( B64_TYPE ); }
+	| BB64_TYPE   { recognizer->add_scalar_type_spec( BB64_TYPE ); }
+	| BB128_TYPE   { recognizer->add_scalar_type_spec( BB128_TYPE ); }
+	| PRED_TYPE  { recognizer->add_scalar_type_spec( PRED_TYPE ); }
+	| TEXREF_TYPE  { recognizer->add_scalar_type_spec( TEXREF_TYPE ); }
+	| SAMPLERREF_TYPE  { recognizer->add_scalar_type_spec( SAMPLERREF_TYPE ); }
+	| SURFREF_TYPE  { recognizer->add_scalar_type_spec( SURFREF_TYPE ); }
+	;
+
+initializer_list: LEFT_BRACE literal_list RIGHT_BRACE { recognizer->add_array_initializer(); }
+	| LEFT_BRACE initializer_list RIGHT_BRACE { syntax_not_implemented(scanner, recognizer); }
 
 literal_list: literal_operand
 	| literal_list COMMA literal_operand;
@@ -422,32 +433,32 @@ prototype_param: /* empty */
 	       | PARAM_DIRECTIVE B32_TYPE IDENTIFIER
 
 instruction_statement:  instruction SEMI_COLON
-	| IDENTIFIER COLON { add_label($1); }    
+	| IDENTIFIER COLON { recognizer->add_label($1); }
 	| pred_spec instruction SEMI_COLON;
 
-instruction: opcode_spec LEFT_PAREN operand RIGHT_PAREN { set_return(); } COMMA operand COMMA LEFT_PAREN operand_list RIGHT_PAREN
+instruction: opcode_spec LEFT_PAREN operand RIGHT_PAREN { recognizer->set_return(); } COMMA operand COMMA LEFT_PAREN operand_list RIGHT_PAREN
 	| opcode_spec operand COMMA LEFT_PAREN operand_list RIGHT_PAREN
 	| opcode_spec operand COMMA LEFT_PAREN RIGHT_PAREN
 	| opcode_spec operand_list 
 	| opcode_spec
 	;
 
-opcode_spec: OPCODE { add_opcode($1); } option_list
-	| OPCODE { add_opcode($1); }
+opcode_spec: OPCODE { recognizer->add_opcode($1); } option_list
+	| OPCODE { recognizer->add_opcode($1); }
 
-pred_spec: PRED IDENTIFIER  { add_pred($2,0, -1); }
-	| PRED EXCLAMATION IDENTIFIER { add_pred($3,1, -1); } 
-	| PRED IDENTIFIER LT_OPTION  { add_pred($2,0,1); }
-	| PRED IDENTIFIER EQ_OPTION  { add_pred($2,0,2); }
-	| PRED IDENTIFIER LE_OPTION  { add_pred($2,0,3); }
-	| PRED IDENTIFIER NE_OPTION  { add_pred($2,0,5); }
-	| PRED IDENTIFIER GE_OPTION  { add_pred($2,0,6); }
-	| PRED IDENTIFIER EQU_OPTION  { add_pred($2,0,10); }
-	| PRED IDENTIFIER GTU_OPTION  { add_pred($2,0,12); }
-	| PRED IDENTIFIER NEU_OPTION  { add_pred($2,0,13); }
-	| PRED IDENTIFIER CF_OPTION  { add_pred($2,0,17); }
-	| PRED IDENTIFIER SF_OPTION  { add_pred($2,0,19); }
-	| PRED IDENTIFIER NSF_OPTION  { add_pred($2,0,28); }
+pred_spec: PRED IDENTIFIER  { recognizer->add_pred($2,0, -1); }
+	| PRED EXCLAMATION IDENTIFIER { recognizer->add_pred($3,1, -1); }
+	| PRED IDENTIFIER LT_OPTION  { recognizer->add_pred($2,0,1); }
+	| PRED IDENTIFIER EQ_OPTION  { recognizer->add_pred($2,0,2); }
+	| PRED IDENTIFIER LE_OPTION  { recognizer->add_pred($2,0,3); }
+	| PRED IDENTIFIER NE_OPTION  { recognizer->add_pred($2,0,5); }
+	| PRED IDENTIFIER GE_OPTION  { recognizer->add_pred($2,0,6); }
+	| PRED IDENTIFIER EQU_OPTION  { recognizer->add_pred($2,0,10); }
+	| PRED IDENTIFIER GTU_OPTION  { recognizer->add_pred($2,0,12); }
+	| PRED IDENTIFIER NEU_OPTION  { recognizer->add_pred($2,0,13); }
+	| PRED IDENTIFIER CF_OPTION  { recognizer->add_pred($2,0,17); }
+	| PRED IDENTIFIER SF_OPTION  { recognizer->add_pred($2,0,19); }
+	| PRED IDENTIFIER NSF_OPTION  { recognizer->add_pred($2,0,28); }
 	;
 
 option_list: option
@@ -459,108 +470,108 @@ option: type_spec
 	| rounding_mode
 	| wmma_spec 
 	| prmt_spec 
-	| SYNC_OPTION { add_option(SYNC_OPTION); }	
-	| ARRIVE_OPTION { add_option(ARRIVE_OPTION); }
-	| RED_OPTION { add_option(RED_OPTION); }	
-	| UNI_OPTION { add_option(UNI_OPTION); }
-	| WIDE_OPTION { add_option(WIDE_OPTION); }
-	| ANY_OPTION { add_option(ANY_OPTION); }
-	| ALL_OPTION { add_option(ALL_OPTION); }
-	| BALLOT_OPTION { add_option(BALLOT_OPTION); }
-	| GLOBAL_OPTION { add_option(GLOBAL_OPTION); }
-	| CTA_OPTION { add_option(CTA_OPTION); }
-	| SYS_OPTION { add_option(SYS_OPTION); }
-	| GEOM_MODIFIER_1D { add_option(GEOM_MODIFIER_1D); }
-	| GEOM_MODIFIER_2D { add_option(GEOM_MODIFIER_2D); }
-	| GEOM_MODIFIER_3D { add_option(GEOM_MODIFIER_3D); }
-	| SAT_OPTION { add_option(SAT_OPTION); }
- 	| FTZ_OPTION { add_option(FTZ_OPTION); } 
- 	| NEG_OPTION { add_option(NEG_OPTION); } 
-	| APPROX_OPTION { add_option(APPROX_OPTION); }
-	| FULL_OPTION { add_option(FULL_OPTION); }
-	| EXIT_OPTION { add_option(EXIT_OPTION); }
-	| ABS_OPTION { add_option(ABS_OPTION); }
+	| SYNC_OPTION { recognizer->add_option(SYNC_OPTION); }
+	| ARRIVE_OPTION { recognizer->add_option(ARRIVE_OPTION); }
+	| RED_OPTION { recognizer->add_option(RED_OPTION); }
+	| UNI_OPTION { recognizer->add_option(UNI_OPTION); }
+	| WIDE_OPTION { recognizer->add_option(WIDE_OPTION); }
+	| ANY_OPTION { recognizer->add_option(ANY_OPTION); }
+	| ALL_OPTION { recognizer->add_option(ALL_OPTION); }
+	| BALLOT_OPTION { recognizer->add_option(BALLOT_OPTION); }
+	| GLOBAL_OPTION { recognizer->add_option(GLOBAL_OPTION); }
+	| CTA_OPTION { recognizer->add_option(CTA_OPTION); }
+	| SYS_OPTION { recognizer->add_option(SYS_OPTION); }
+	| GEOM_MODIFIER_1D { recognizer->add_option(GEOM_MODIFIER_1D); }
+	| GEOM_MODIFIER_2D { recognizer->add_option(GEOM_MODIFIER_2D); }
+	| GEOM_MODIFIER_3D { recognizer->add_option(GEOM_MODIFIER_3D); }
+	| SAT_OPTION { recognizer->add_option(SAT_OPTION); }
+	| FTZ_OPTION { recognizer->add_option(FTZ_OPTION); }
+	| NEG_OPTION { recognizer->add_option(NEG_OPTION); }
+	| APPROX_OPTION { recognizer->add_option(APPROX_OPTION); }
+	| FULL_OPTION { recognizer->add_option(FULL_OPTION); }
+	| EXIT_OPTION { recognizer->add_option(EXIT_OPTION); }
+	| ABS_OPTION { recognizer->add_option(ABS_OPTION); }
 	| atomic_operation_spec ;
-	| TO_OPTION { add_option(TO_OPTION); }
-	| HALF_OPTION { add_option(HALF_OPTION); }
-	| EXTP_OPTION { add_option(EXTP_OPTION); }
-	| CA_OPTION { add_option(CA_OPTION); }
-	| CG_OPTION { add_option(CG_OPTION); }
-	| CS_OPTION { add_option(CS_OPTION); }
-	| LU_OPTION { add_option(LU_OPTION); }
-	| CV_OPTION { add_option(CV_OPTION); }
-	| WB_OPTION { add_option(WB_OPTION); }
-	| WT_OPTION { add_option(WT_OPTION); }
-	| NC_OPTION { add_option(NC_OPTION); }
-	| UP_OPTION { add_option(UP_OPTION); }
-	| DOWN_OPTION { add_option(DOWN_OPTION); }
-	| BFLY_OPTION { add_option(BFLY_OPTION); }
-	| IDX_OPTION { add_option(IDX_OPTION); }
-	;
-
-atomic_operation_spec: ATOMIC_AND { add_option(ATOMIC_AND); } 
-	| ATOMIC_POPC { add_option(ATOMIC_POPC); }
-	| ATOMIC_OR { add_option(ATOMIC_OR); } 
-	| ATOMIC_XOR { add_option(ATOMIC_XOR); } 
-	| ATOMIC_CAS { add_option(ATOMIC_CAS); } 
-	| ATOMIC_EXCH { add_option(ATOMIC_EXCH); } 
-	| ATOMIC_ADD { add_option(ATOMIC_ADD); } 
-	| ATOMIC_INC { add_option(ATOMIC_INC); } 
-	| ATOMIC_DEC { add_option(ATOMIC_DEC); } 
-	| ATOMIC_MIN { add_option(ATOMIC_MIN); } 
-	| ATOMIC_MAX { add_option(ATOMIC_MAX); } 
+	| TO_OPTION { recognizer->add_option(TO_OPTION); }
+	| HALF_OPTION { recognizer->add_option(HALF_OPTION); }
+	| EXTP_OPTION { recognizer->add_option(EXTP_OPTION); }
+	| CA_OPTION { recognizer->add_option(CA_OPTION); }
+	| CG_OPTION { recognizer->add_option(CG_OPTION); }
+	| CS_OPTION { recognizer->add_option(CS_OPTION); }
+	| LU_OPTION { recognizer->add_option(LU_OPTION); }
+	| CV_OPTION { recognizer->add_option(CV_OPTION); }
+	| WB_OPTION { recognizer->add_option(WB_OPTION); }
+	| WT_OPTION { recognizer->add_option(WT_OPTION); }
+	| NC_OPTION { recognizer->add_option(NC_OPTION); }
+	| UP_OPTION { recognizer->add_option(UP_OPTION); }
+	| DOWN_OPTION { recognizer->add_option(DOWN_OPTION); }
+	| BFLY_OPTION { recognizer->add_option(BFLY_OPTION); }
+	| IDX_OPTION { recognizer->add_option(IDX_OPTION); }
+	;
+
+atomic_operation_spec: ATOMIC_AND { recognizer->add_option(ATOMIC_AND); }
+	| ATOMIC_POPC { recognizer->add_option(ATOMIC_POPC); }
+	| ATOMIC_OR { recognizer->add_option(ATOMIC_OR); }
+	| ATOMIC_XOR { recognizer->add_option(ATOMIC_XOR); }
+	| ATOMIC_CAS { recognizer->add_option(ATOMIC_CAS); }
+	| ATOMIC_EXCH { recognizer->add_option(ATOMIC_EXCH); }
+	| ATOMIC_ADD { recognizer->add_option(ATOMIC_ADD); }
+	| ATOMIC_INC { recognizer->add_option(ATOMIC_INC); }
+	| ATOMIC_DEC { recognizer->add_option(ATOMIC_DEC); }
+	| ATOMIC_MIN { recognizer->add_option(ATOMIC_MIN); }
+	| ATOMIC_MAX { recognizer->add_option(ATOMIC_MAX); }
 	;
 
 rounding_mode: floating_point_rounding_mode
 	| integer_rounding_mode;
 
 
-floating_point_rounding_mode: RN_OPTION { add_option(RN_OPTION); } 
- 	| RZ_OPTION { add_option(RZ_OPTION); } 
- 	| RM_OPTION { add_option(RM_OPTION); } 
- 	| RP_OPTION { add_option(RP_OPTION); } 
+floating_point_rounding_mode: RN_OPTION { recognizer->add_option(RN_OPTION); }
+	| RZ_OPTION { recognizer->add_option(RZ_OPTION); }
+	| RM_OPTION { recognizer->add_option(RM_OPTION); }
+	| RP_OPTION { recognizer->add_option(RP_OPTION); }
 	;
 
-integer_rounding_mode: RNI_OPTION { add_option(RNI_OPTION); } 
-	| RZI_OPTION { add_option(RZI_OPTION); } 
- 	| RMI_OPTION { add_option(RMI_OPTION); } 
- 	| RPI_OPTION { add_option(RPI_OPTION); } 
+integer_rounding_mode: RNI_OPTION { recognizer->add_option(RNI_OPTION); }
+	| RZI_OPTION { recognizer->add_option(RZI_OPTION); }
+	| RMI_OPTION { recognizer->add_option(RMI_OPTION); }
+	| RPI_OPTION { recognizer->add_option(RPI_OPTION); }
 	;
 
-compare_spec:EQ_OPTION { add_option(EQ_OPTION); } 
-	| NE_OPTION { add_option(NE_OPTION); } 
-	| LT_OPTION { add_option(LT_OPTION); } 
-	| LE_OPTION { add_option(LE_OPTION); } 
-	| GT_OPTION { add_option(GT_OPTION); } 
-	| GE_OPTION { add_option(GE_OPTION); } 
-	| LO_OPTION { add_option(LO_OPTION); } 
-	| LS_OPTION { add_option(LS_OPTION); } 
-	| HI_OPTION { add_option(HI_OPTION); } 
-	| HS_OPTION  { add_option(HS_OPTION); } 
-	| EQU_OPTION { add_option(EQU_OPTION); } 
-	| NEU_OPTION { add_option(NEU_OPTION); } 
-	| LTU_OPTION { add_option(LTU_OPTION); } 
-	| LEU_OPTION { add_option(LEU_OPTION); } 
-	| GTU_OPTION { add_option(GTU_OPTION); } 
-	| GEU_OPTION { add_option(GEU_OPTION); } 
-	| NUM_OPTION { add_option(NUM_OPTION); } 
-	| NAN_OPTION { add_option(NAN_OPTION); } 
+compare_spec:EQ_OPTION { recognizer->add_option(EQ_OPTION); }
+	| NE_OPTION { recognizer->add_option(NE_OPTION); }
+	| LT_OPTION { recognizer->add_option(LT_OPTION); }
+	| LE_OPTION { recognizer->add_option(LE_OPTION); }
+	| GT_OPTION { recognizer->add_option(GT_OPTION); }
+	| GE_OPTION { recognizer->add_option(GE_OPTION); }
+	| LO_OPTION { recognizer->add_option(LO_OPTION); }
+	| LS_OPTION { recognizer->add_option(LS_OPTION); }
+	| HI_OPTION { recognizer->add_option(HI_OPTION); }
+	| HS_OPTION  { recognizer->add_option(HS_OPTION); }
+	| EQU_OPTION { recognizer->add_option(EQU_OPTION); }
+	| NEU_OPTION { recognizer->add_option(NEU_OPTION); }
+	| LTU_OPTION { recognizer->add_option(LTU_OPTION); }
+	| LEU_OPTION { recognizer->add_option(LEU_OPTION); }
+	| GTU_OPTION { recognizer->add_option(GTU_OPTION); }
+	| GEU_OPTION { recognizer->add_option(GEU_OPTION); }
+	| NUM_OPTION { recognizer->add_option(NUM_OPTION); }
+	| NAN_OPTION { recognizer->add_option(NAN_OPTION); }
 	;
 
-prmt_spec: PRMT_F4E_MODE { add_option( PRMT_F4E_MODE); }
-	|  PRMT_B4E_MODE { add_option( PRMT_B4E_MODE); }
-	|  PRMT_RC8_MODE { add_option( PRMT_RC8_MODE); }
-	|  PRMT_RC16_MODE{ add_option( PRMT_RC16_MODE);}
-	|  PRMT_ECL_MODE { add_option( PRMT_ECL_MODE); }
-	|  PRMT_ECR_MODE { add_option( PRMT_ECR_MODE); }
+prmt_spec: PRMT_F4E_MODE { recognizer->add_option( PRMT_F4E_MODE); }
+	|  PRMT_B4E_MODE { recognizer->add_option( PRMT_B4E_MODE); }
+	|  PRMT_RC8_MODE { recognizer->add_option( PRMT_RC8_MODE); }
+	|  PRMT_RC16_MODE{ recognizer->add_option( PRMT_RC16_MODE);}
+	|  PRMT_ECL_MODE { recognizer->add_option( PRMT_ECL_MODE); }
+	|  PRMT_ECR_MODE { recognizer->add_option( PRMT_ECR_MODE); }
 	;
 
-wmma_spec: WMMA_DIRECTIVE LAYOUT CONFIGURATION{add_space_spec(global_space,0);add_ptr_spec(global_space); add_wmma_option($1);add_wmma_option($2);add_wmma_option($3);}
-	| WMMA_DIRECTIVE LAYOUT LAYOUT CONFIGURATION{add_wmma_option($1);add_wmma_option($2);add_wmma_option($3);add_wmma_option($4);}
+wmma_spec: WMMA_DIRECTIVE LAYOUT CONFIGURATION{recognizer->add_space_spec(global_space,0);recognizer->add_ptr_spec(global_space); recognizer->add_wmma_option($1);recognizer->add_wmma_option($2);recognizer->add_wmma_option($3);}
+	| WMMA_DIRECTIVE LAYOUT LAYOUT CONFIGURATION{recognizer->add_wmma_option($1);recognizer->add_wmma_option($2);recognizer->add_wmma_option($3);recognizer->add_wmma_option($4);}
 	;
 
-vp_spec: WMMA_DIRECTIVE LAYOUT CONFIGURATION{add_space_spec(global_space,0);add_ptr_spec(global_space);add_wmma_option($1);add_wmma_option($2);add_wmma_option($3);}
-	| WMMA_DIRECTIVE LAYOUT LAYOUT CONFIGURATION{add_wmma_option($1);add_wmma_option($2);add_wmma_option($3);add_wmma_option($4);}
+vp_spec: WMMA_DIRECTIVE LAYOUT CONFIGURATION{recognizer->add_space_spec(global_space,0);recognizer->add_ptr_spec(global_space);recognizer->add_wmma_option($1);recognizer->add_wmma_option($2);recognizer->add_wmma_option($3);}
+	| WMMA_DIRECTIVE LAYOUT LAYOUT CONFIGURATION{recognizer->add_wmma_option($1);recognizer->add_wmma_option($2);recognizer->add_wmma_option($3);recognizer->add_wmma_option($4);}
 	;
 
 
@@ -568,80 +579,77 @@ vp_spec: WMMA_DIRECTIVE LAYOUT CONFIGURATION{add_space_spec(global_space,0);add_
 operand_list: operand
 	| operand COMMA operand_list;
 
-operand: IDENTIFIER  { add_scalar_operand( $1 ); }
-	| EXCLAMATION IDENTIFIER { add_neg_pred_operand( $2 ); }
-	| MINUS IDENTIFIER  { add_scalar_operand( $2 ); change_operand_neg(); }
+operand: IDENTIFIER  { recognizer->add_scalar_operand( $1 ); }
+	| EXCLAMATION IDENTIFIER { recognizer->add_neg_pred_operand( $2 ); }
+	| MINUS IDENTIFIER  { recognizer->add_scalar_operand( $2 ); recognizer->change_operand_neg(); }
 	| memory_operand
 	| literal_operand
 	| builtin_operand
 	| vector_operand
-	| MINUS vector_operand { change_operand_neg(); }
+	| MINUS vector_operand { recognizer->change_operand_neg(); }
 	| tex_operand
-	| IDENTIFIER PLUS INT_OPERAND { add_address_operand($1,$3); }
-	| IDENTIFIER LO_OPTION { add_scalar_operand( $1 ); change_operand_lohi(1);}
-	| MINUS IDENTIFIER LO_OPTION { add_scalar_operand( $2 ); change_operand_lohi(1); change_operand_neg();}
-	| IDENTIFIER HI_OPTION { add_scalar_operand( $1 ); change_operand_lohi(2);}
-	| MINUS IDENTIFIER HI_OPTION { add_scalar_operand( $2 ); change_operand_lohi(2); change_operand_neg();}
-	| IDENTIFIER PIPE IDENTIFIER { add_2vector_operand($1,$3); change_double_operand_type(-1);}
-	| IDENTIFIER PIPE IDENTIFIER LO_OPTION { add_2vector_operand($1,$3); change_double_operand_type(-1); change_operand_lohi(1);}
-	| IDENTIFIER PIPE IDENTIFIER HI_OPTION { add_2vector_operand($1,$3); change_double_operand_type(-1); change_operand_lohi(2);}
-	| IDENTIFIER BACKSLASH IDENTIFIER { add_2vector_operand($1,$3); change_double_operand_type(-3);}
-	| IDENTIFIER BACKSLASH IDENTIFIER LO_OPTION { add_2vector_operand($1,$3); change_double_operand_type(-3); change_operand_lohi(1);}
-	| IDENTIFIER BACKSLASH IDENTIFIER HI_OPTION { add_2vector_operand($1,$3); change_double_operand_type(-3); change_operand_lohi(2);}
-	;
-
-vector_operand: LEFT_BRACE IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { add_2vector_operand($2,$4); }
-		| LEFT_BRACE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { add_3vector_operand($2,$4,$6); }
-		| LEFT_BRACE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { add_4vector_operand($2,$4,$6,$8); }
-		| LEFT_BRACE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { add_8vector_operand($2,$4,$6,$8,$10,$12,$14,$16); }
-		| LEFT_BRACE IDENTIFIER RIGHT_BRACE { add_1vector_operand($2); }
-	;
-
-tex_operand: LEFT_SQUARE_BRACKET IDENTIFIER COMMA { add_scalar_operand($2); }
+	| IDENTIFIER PLUS INT_OPERAND { recognizer->add_address_operand($1,$3); }
+	| IDENTIFIER LO_OPTION { recognizer->add_scalar_operand( $1 ); recognizer->change_operand_lohi(1);}
+	| MINUS IDENTIFIER LO_OPTION { recognizer->add_scalar_operand( $2 ); recognizer->change_operand_lohi(1); recognizer->change_operand_neg();}
+	| IDENTIFIER HI_OPTION { recognizer->add_scalar_operand( $1 ); recognizer->change_operand_lohi(2);}
+	| MINUS IDENTIFIER HI_OPTION { recognizer->add_scalar_operand( $2 ); recognizer->change_operand_lohi(2); recognizer->change_operand_neg();}
+	| IDENTIFIER PIPE IDENTIFIER { recognizer->add_2vector_operand($1,$3); recognizer->change_double_operand_type(-1);}
+	| IDENTIFIER PIPE IDENTIFIER LO_OPTION { recognizer->add_2vector_operand($1,$3); recognizer->change_double_operand_type(-1); recognizer->change_operand_lohi(1);}
+	| IDENTIFIER PIPE IDENTIFIER HI_OPTION { recognizer->add_2vector_operand($1,$3); recognizer->change_double_operand_type(-1); recognizer->change_operand_lohi(2);}
+	| IDENTIFIER BACKSLASH IDENTIFIER { recognizer->add_2vector_operand($1,$3); recognizer->change_double_operand_type(-3);}
+	| IDENTIFIER BACKSLASH IDENTIFIER LO_OPTION { recognizer->add_2vector_operand($1,$3); recognizer->change_double_operand_type(-3); recognizer->change_operand_lohi(1);}
+	| IDENTIFIER BACKSLASH IDENTIFIER HI_OPTION { recognizer->add_2vector_operand($1,$3); recognizer->change_double_operand_type(-3); recognizer->change_operand_lohi(2);}
+	;
+
+vector_operand: LEFT_BRACE IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { recognizer->add_2vector_operand($2,$4); }
+		| LEFT_BRACE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { recognizer->add_3vector_operand($2,$4,$6); }
+		| LEFT_BRACE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { recognizer->add_4vector_operand($2,$4,$6,$8); }
+		| LEFT_BRACE IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER COMMA IDENTIFIER RIGHT_BRACE { recognizer->add_8vector_operand($2,$4,$6,$8,$10,$12,$14,$16); }
+		| LEFT_BRACE IDENTIFIER RIGHT_BRACE { recognizer->add_1vector_operand($2); }
+	;
+
+tex_operand: LEFT_SQUARE_BRACKET IDENTIFIER COMMA { recognizer->add_scalar_operand($2); }
 		vector_operand 
 	     RIGHT_SQUARE_BRACKET
 	;
 
-builtin_operand: SPECIAL_REGISTER DIMENSION_MODIFIER { add_builtin_operand($1,$2); }
-        | SPECIAL_REGISTER { add_builtin_operand($1,-1); }
+builtin_operand: SPECIAL_REGISTER DIMENSION_MODIFIER { recognizer->add_builtin_operand($1,$2); }
+        | SPECIAL_REGISTER { recognizer->add_builtin_operand($1,-1); }
 	;
 
-memory_operand : LEFT_SQUARE_BRACKET address_expression RIGHT_SQUARE_BRACKET { add_memory_operand(); }
-	| IDENTIFIER LEFT_SQUARE_BRACKET address_expression RIGHT_SQUARE_BRACKET { add_memory_operand(); change_memory_addr_space($1); }
-	| IDENTIFIER LEFT_SQUARE_BRACKET literal_operand RIGHT_SQUARE_BRACKET { change_memory_addr_space($1); }
-	| IDENTIFIER LEFT_SQUARE_BRACKET twin_operand RIGHT_SQUARE_BRACKET { change_memory_addr_space($1); add_memory_operand();}
-        | MINUS memory_operand { change_operand_neg(); }
+memory_operand : LEFT_SQUARE_BRACKET address_expression RIGHT_SQUARE_BRACKET { recognizer->add_memory_operand(); }
+	| IDENTIFIER LEFT_SQUARE_BRACKET address_expression RIGHT_SQUARE_BRACKET { recognizer->add_memory_operand(); recognizer->change_memory_addr_space($1); }
+	| IDENTIFIER LEFT_SQUARE_BRACKET literal_operand RIGHT_SQUARE_BRACKET { recognizer->change_memory_addr_space($1); }
+	| IDENTIFIER LEFT_SQUARE_BRACKET twin_operand RIGHT_SQUARE_BRACKET { recognizer->change_memory_addr_space($1); recognizer->add_memory_operand();}
+        | MINUS memory_operand { recognizer->change_operand_neg(); }
 	;
 
-twin_operand : IDENTIFIER PLUS IDENTIFIER { add_double_operand($1,$3); change_double_operand_type(1); }
-	| IDENTIFIER PLUS IDENTIFIER LO_OPTION { add_double_operand($1,$3); change_double_operand_type(1); change_operand_lohi(1); }
-	| IDENTIFIER PLUS IDENTIFIER HI_OPTION { add_double_operand($1,$3); change_double_operand_type(1); change_operand_lohi(2); }
-	| IDENTIFIER PLUS EQUALS IDENTIFIER  { add_double_operand($1,$4); change_double_operand_type(2); }
-	| IDENTIFIER PLUS EQUALS IDENTIFIER LO_OPTION { add_double_operand($1,$4); change_double_operand_type(2); change_operand_lohi(1); }
-	| IDENTIFIER PLUS EQUALS IDENTIFIER HI_OPTION { add_double_operand($1,$4); change_double_operand_type(2); change_operand_lohi(2); }
-	| IDENTIFIER PLUS EQUALS INT_OPERAND  { add_address_operand($1,$4); change_double_operand_type(3); }
+twin_operand : IDENTIFIER PLUS IDENTIFIER { recognizer->add_double_operand($1,$3); recognizer->change_double_operand_type(1); }
+	| IDENTIFIER PLUS IDENTIFIER LO_OPTION { recognizer->add_double_operand($1,$3); recognizer->change_double_operand_type(1); recognizer->change_operand_lohi(1); }
+	| IDENTIFIER PLUS IDENTIFIER HI_OPTION { recognizer->add_double_operand($1,$3); recognizer->change_double_operand_type(1); recognizer->change_operand_lohi(2); }
+	| IDENTIFIER PLUS EQUALS IDENTIFIER  { recognizer->add_double_operand($1,$4); recognizer->change_double_operand_type(2); }
+	| IDENTIFIER PLUS EQUALS IDENTIFIER LO_OPTION { recognizer->add_double_operand($1,$4); recognizer->change_double_operand_type(2); recognizer->change_operand_lohi(1); }
+	| IDENTIFIER PLUS EQUALS IDENTIFIER HI_OPTION { recognizer->add_double_operand($1,$4); recognizer->change_double_operand_type(2); recognizer->change_operand_lohi(2); }
+	| IDENTIFIER PLUS EQUALS INT_OPERAND  { recognizer->add_address_operand($1,$4); recognizer->change_double_operand_type(3); }
 	;
 
-literal_operand : INT_OPERAND { add_literal_int($1); }
-	| FLOAT_OPERAND { add_literal_float($1); }
-	| DOUBLE_OPERAND { add_literal_double($1); }
+literal_operand : INT_OPERAND { recognizer->add_literal_int($1); }
+	| FLOAT_OPERAND { recognizer->add_literal_float($1); }
+	| DOUBLE_OPERAND { recognizer->add_literal_double($1); }
 	;
 
-address_expression: IDENTIFIER { add_address_operand($1,0); }
-	| IDENTIFIER LO_OPTION { add_address_operand($1,0); change_operand_lohi(1);}
-	| IDENTIFIER HI_OPTION { add_address_operand($1,0); change_operand_lohi(2); }
-	| IDENTIFIER PLUS INT_OPERAND { add_address_operand($1,$3); }
-	| INT_OPERAND { add_address_operand2($1); }
+address_expression: IDENTIFIER { recognizer->add_address_operand($1,0); }
+	| IDENTIFIER LO_OPTION { recognizer->add_address_operand($1,0); recognizer->change_operand_lohi(1);}
+	| IDENTIFIER HI_OPTION { recognizer->add_address_operand($1,0); recognizer->change_operand_lohi(2); }
+	| IDENTIFIER PLUS INT_OPERAND { recognizer->add_address_operand($1,$3); }
+	| INT_OPERAND { recognizer->add_address_operand2($1); }
 	;
 
 %%
 
-extern int ptx_lineno;
-extern const char *g_filename;
-
-void syntax_not_implemented()
+void syntax_not_implemented(yyscan_t yyscanner, ptx_recognizer* recognizer)
 {
-	printf("Parse error (%s:%u): this syntax is not (yet) implemented:\n",g_filename,ptx_lineno);
-	ptx_error(NULL);
+	printf("Parse error (%s): this syntax is not (yet) implemented:\n", recognizer->gpgpu_ctx->g_filename);
+	ptx_error(yyscanner, recognizer, NULL);
 	abort();
 }
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.cc
index 22fa7f77a3..e20f693a3a 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.cc
@@ -26,55 +26,51 @@
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-#include "ptx_parser.h"
 #include "ptx_ir.h"
-#include "ptx.tab.h"
-#include "opcodes.h"
+#include "ptx_parser.h"
+typedef void *yyscan_t;
+#include <assert.h>
 #include <stdio.h>
 #include <stdlib.h>
-#include <list>
-#include <assert.h>
 #include <algorithm>
+#include <list>
 #include "assert.h"
+#include "opcodes.h"
+#include "ptx.tab.h"
 
+#include "../../libcuda_sim/gpgpu_context.h"
 #include "cuda-sim.h"
 
 #define STR_SIZE 1024
 
-unsigned symbol::sm_next_uid = 1;
+const ptx_instruction *gpgpu_context::pc_to_instruction(unsigned pc) {
+  if (pc < s_g_pc_to_insn.size())
+    return s_g_pc_to_insn[pc];
+  else
+    return NULL;
+}
 
-unsigned symbol::get_uid()
-{
-   unsigned result = sm_next_uid++;
-   return result;
+unsigned symbol::get_uid() {
+  unsigned result = (gpgpu_ctx->symbol_sm_next_uid)++;
+  return result;
 }
 
-void symbol::add_initializer( const std::list<operand_info> &init )
-{
-   m_initializer = init;
+void symbol::add_initializer(const std::list<operand_info> &init) {
+  m_initializer = init;
 }
 
-void symbol::print_info(FILE *fp) const
-{
-   fprintf(fp,"uid:%u, decl:%s, type:%p, ", m_uid, m_decl_location.c_str(), m_type );
-   if( m_address_valid ) 
-      fprintf(fp,"<address valid>, ");
-   if( m_is_label )
-      fprintf(fp," is_label ");
-   if( m_is_shared )
-      fprintf(fp," is_shared ");
-   if( m_is_const )
-      fprintf(fp," is_const ");
-   if( m_is_global )
-      fprintf(fp," is_global ");
-   if( m_is_local )
-      fprintf(fp," is_local ");
-   if( m_is_tex )
-      fprintf(fp," is_tex ");
-   if( m_is_func_addr )
-      fprintf(fp," is_func_addr ");
-   if( m_function ) 
-      fprintf(fp," %p ", m_function );
+void symbol::print_info(FILE *fp) const {
+  fprintf(fp, "uid:%u, decl:%s, type:%p, ", m_uid, m_decl_location.c_str(),
+          m_type);
+  if (m_address_valid) fprintf(fp, "<address valid>, ");
+  if (m_is_label) fprintf(fp, " is_label ");
+  if (m_is_shared) fprintf(fp, " is_shared ");
+  if (m_is_const) fprintf(fp, " is_const ");
+  if (m_is_global) fprintf(fp, " is_global ");
+  if (m_is_local) fprintf(fp, " is_local ");
+  if (m_is_tex) fprintf(fp, " is_tex ");
+  if (m_is_func_addr) fprintf(fp, " is_func_addr ");
+  if (m_function) fprintf(fp, " %p ", m_function);
 }
 
 symbol_table::symbol_table() { assert(0); }
@@ -210,8 +206,8 @@ symbol_table *symbol_table::end_inst_group() {
   return sym_table;
 }
 
-
-void register_ptx_function( const char *name, function_info *impl ); // either libcuda or libopencl
+void register_ptx_function(const char *name,
+                           function_info *impl);  // either libcuda or libopencl
 
 bool symbol_table::add_function_decl(const char *name, int entry_point,
                                      function_info **func_info,
@@ -322,846 +318,953 @@ void symbol_table::dump() {
   printf("\n");
 }
 
-unsigned operand_info::sm_next_uid=1;
-
-unsigned operand_info::get_uid()
-{
-   unsigned result = sm_next_uid++;
-   return result;
+unsigned operand_info::get_uid() {
+  unsigned result = (gpgpu_ctx->operand_info_sm_next_uid)++;
+  return result;
 }
 
-std::list<ptx_instruction*>::iterator function_info::find_next_real_instruction( std::list<ptx_instruction*>::iterator i)
-{
-   while( (i != m_instructions.end()) && (*i)->is_label() ) 
-      i++;
-   return i;
+std::list<ptx_instruction *>::iterator
+function_info::find_next_real_instruction(
+    std::list<ptx_instruction *>::iterator i) {
+  while ((i != m_instructions.end()) && (*i)->is_label()) i++;
+  return i;
 }
 
-void function_info::create_basic_blocks()
-{
-   std::list<ptx_instruction*> leaders;
-   std::list<ptx_instruction*>::iterator i, l;
-
-   // first instruction is a leader
-   i=m_instructions.begin();
-   leaders.push_back(*i);
-   i++;
-   while( i!=m_instructions.end() ) {
-      ptx_instruction *pI = *i;
-      if( pI->is_label() ) {
-         leaders.push_back(pI);
-         i = find_next_real_instruction(++i);
-      } else {
-         switch( pI->get_opcode() ) {
-         case BRA_OP: case RET_OP: case EXIT_OP: case RETP_OP: case BREAK_OP: 
+void function_info::create_basic_blocks() {
+  std::list<ptx_instruction *> leaders;
+  std::list<ptx_instruction *>::iterator i, l;
+
+  // first instruction is a leader
+  i = m_instructions.begin();
+  leaders.push_back(*i);
+  i++;
+  while (i != m_instructions.end()) {
+    ptx_instruction *pI = *i;
+    if (pI->is_label()) {
+      leaders.push_back(pI);
+      i = find_next_real_instruction(++i);
+    } else {
+      switch (pI->get_opcode()) {
+        case BRA_OP:
+        case RET_OP:
+        case EXIT_OP:
+        case RETP_OP:
+        case BREAK_OP:
+          i++;
+          if (i != m_instructions.end()) leaders.push_back(*i);
+          i = find_next_real_instruction(i);
+          break;
+        case CALL_OP:
+        case CALLP_OP:
+          if (pI->has_pred()) {
+            printf("GPGPU-Sim PTX: Warning found predicated call\n");
             i++;
-            if( i != m_instructions.end() ) 
-               leaders.push_back(*i);
+            if (i != m_instructions.end()) leaders.push_back(*i);
             i = find_next_real_instruction(i);
-            break;
-         case CALL_OP: case CALLP_OP:
-            if( pI->has_pred() ) {
-               printf("GPGPU-Sim PTX: Warning found predicated call\n");
-               i++;
-               if( i != m_instructions.end() ) 
-                  leaders.push_back(*i);
-               i = find_next_real_instruction(i);
-            } else i++;
-            break;
-         default:
+          } else
             i++;
-         }
-      } 
-   }
-
-   if( leaders.empty() ) {
-      printf("GPGPU-Sim PTX: Function \'%s\' has no basic blocks\n", m_name.c_str());
-      return;
-   }
-
-   unsigned bb_id = 0;
-   l=leaders.begin();
-   i=m_instructions.begin();
-   m_basic_blocks.push_back( new basic_block_t(bb_id++,*find_next_real_instruction(i),NULL,1,0) );
-   ptx_instruction *last_real_inst=*(l++);
-
-   for( ; i!=m_instructions.end(); i++ ) {
-      ptx_instruction *pI = *i;
-      if( l != leaders.end() && *i == *l ) {
-         // found start of next basic block
-         m_basic_blocks.back()->ptx_end = last_real_inst;
-         if( find_next_real_instruction(i) != m_instructions.end() ) { // if not bogus trailing label
-            m_basic_blocks.push_back( new basic_block_t(bb_id++,*find_next_real_instruction(i),NULL,0,0) );
-            last_real_inst = *find_next_real_instruction(i);
-         }
-         // start search for next leader
-         l++;
+          break;
+        default:
+          i++;
       }
-      pI->assign_bb( m_basic_blocks.back() );
-      if( !pI->is_label() ) last_real_inst = pI;
-   }
-   m_basic_blocks.back()->ptx_end = last_real_inst;
-   m_basic_blocks.push_back( /*exit basic block*/ new basic_block_t(bb_id,NULL,NULL,0,1) );
+    }
+  }
+
+  if (leaders.empty()) {
+    printf("GPGPU-Sim PTX: Function \'%s\' has no basic blocks\n",
+           m_name.c_str());
+    return;
+  }
+
+  unsigned bb_id = 0;
+  l = leaders.begin();
+  i = m_instructions.begin();
+  m_basic_blocks.push_back(
+      new basic_block_t(bb_id++, *find_next_real_instruction(i), NULL, 1, 0));
+  ptx_instruction *last_real_inst = *(l++);
+
+  for (; i != m_instructions.end(); i++) {
+    ptx_instruction *pI = *i;
+    if (l != leaders.end() && *i == *l) {
+      // found start of next basic block
+      m_basic_blocks.back()->ptx_end = last_real_inst;
+      if (find_next_real_instruction(i) !=
+          m_instructions.end()) {  // if not bogus trailing label
+        m_basic_blocks.push_back(new basic_block_t(
+            bb_id++, *find_next_real_instruction(i), NULL, 0, 0));
+        last_real_inst = *find_next_real_instruction(i);
+      }
+      // start search for next leader
+      l++;
+    }
+    pI->assign_bb(m_basic_blocks.back());
+    if (!pI->is_label()) last_real_inst = pI;
+  }
+  m_basic_blocks.back()->ptx_end = last_real_inst;
+  m_basic_blocks.push_back(
+      /*exit basic block*/ new basic_block_t(bb_id, NULL, NULL, 0, 1));
 }
 
-void function_info::print_basic_blocks()
-{
-   printf("Printing basic blocks for function \'%s\':\n", m_name.c_str() );
-   std::list<ptx_instruction*>::iterator ptx_itr;
-   unsigned last_bb=0;
-   for (ptx_itr = m_instructions.begin();ptx_itr != m_instructions.end(); ptx_itr++) {
-      if( (*ptx_itr)->get_bb() ) {
-         if( (*ptx_itr)->get_bb()->bb_id != last_bb ) {
-            printf("\n");
-            last_bb = (*ptx_itr)->get_bb()->bb_id;
-         }
-         printf("bb_%02u\t: ", (*ptx_itr)->get_bb()->bb_id);
-         (*ptx_itr)->print_insn();
-         printf("\n");
+void function_info::print_basic_blocks() {
+  printf("Printing basic blocks for function \'%s\':\n", m_name.c_str());
+  std::list<ptx_instruction *>::iterator ptx_itr;
+  unsigned last_bb = 0;
+  for (ptx_itr = m_instructions.begin(); ptx_itr != m_instructions.end();
+       ptx_itr++) {
+    if ((*ptx_itr)->get_bb()) {
+      if ((*ptx_itr)->get_bb()->bb_id != last_bb) {
+        printf("\n");
+        last_bb = (*ptx_itr)->get_bb()->bb_id;
       }
-   }
-   printf("\nSummary of basic blocks for \'%s\':\n", m_name.c_str() );
-   std::vector<basic_block_t*>::iterator bb_itr;
-   for (bb_itr = m_basic_blocks.begin();bb_itr != m_basic_blocks.end(); bb_itr++) {
-      printf("bb_%02u\t:", (*bb_itr)->bb_id);
-      if ((*bb_itr)->ptx_begin)
-         printf(" first: %s\t", ((*bb_itr)->ptx_begin)->get_opcode_cstr());
-      else printf(" first: NULL\t");
-      if ((*bb_itr)->ptx_end) {
-         printf(" last: %s\t", ((*bb_itr)->ptx_end)->get_opcode_cstr());
-      } else printf(" last: NULL\t");
+      printf("bb_%02u\t: ", (*ptx_itr)->get_bb()->bb_id);
+      (*ptx_itr)->print_insn();
       printf("\n");
-   }
-   printf("\n");
+    }
+  }
+  printf("\nSummary of basic blocks for \'%s\':\n", m_name.c_str());
+  std::vector<basic_block_t *>::iterator bb_itr;
+  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
+       bb_itr++) {
+    printf("bb_%02u\t:", (*bb_itr)->bb_id);
+    if ((*bb_itr)->ptx_begin)
+      printf(" first: %s\t", ((*bb_itr)->ptx_begin)->get_opcode_cstr());
+    else
+      printf(" first: NULL\t");
+    if ((*bb_itr)->ptx_end) {
+      printf(" last: %s\t", ((*bb_itr)->ptx_end)->get_opcode_cstr());
+    } else
+      printf(" last: NULL\t");
+    printf("\n");
+  }
+  printf("\n");
 }
 
-void function_info::print_basic_block_links()
-{
-   printf("Printing basic blocks links for function \'%s\':\n", m_name.c_str() );
-   std::vector<basic_block_t*>::iterator bb_itr;
-   for (bb_itr = m_basic_blocks.begin();bb_itr != m_basic_blocks.end(); bb_itr++) {
-      printf("ID: %d\t:", (*bb_itr)->bb_id);
-      if ( !(*bb_itr)->predecessor_ids.empty() ) {
-         printf("Predecessors:");
-         std::set<int>::iterator p;
-         for (p= (*bb_itr)->predecessor_ids.begin();p != (*bb_itr)->predecessor_ids.end();p++) {
-            printf(" %d", *p);
-         }
-         printf("\t");
+void function_info::print_basic_block_links() {
+  printf("Printing basic blocks links for function \'%s\':\n", m_name.c_str());
+  std::vector<basic_block_t *>::iterator bb_itr;
+  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
+       bb_itr++) {
+    printf("ID: %d\t:", (*bb_itr)->bb_id);
+    if (!(*bb_itr)->predecessor_ids.empty()) {
+      printf("Predecessors:");
+      std::set<int>::iterator p;
+      for (p = (*bb_itr)->predecessor_ids.begin();
+           p != (*bb_itr)->predecessor_ids.end(); p++) {
+        printf(" %d", *p);
       }
-      if ( !(*bb_itr)->successor_ids.empty() ) {
-         printf("Successors:");
-         std::set<int>::iterator s;
-         for (s= (*bb_itr)->successor_ids.begin();s != (*bb_itr)->successor_ids.end();s++) {
-            printf(" %d", *s);
-         }
+      printf("\t");
+    }
+    if (!(*bb_itr)->successor_ids.empty()) {
+      printf("Successors:");
+      std::set<int>::iterator s;
+      for (s = (*bb_itr)->successor_ids.begin();
+           s != (*bb_itr)->successor_ids.end(); s++) {
+        printf(" %d", *s);
       }
-      printf("\n");
-   }
+    }
+    printf("\n");
+  }
 }
-operand_info* function_info::find_break_target( ptx_instruction * p_break_insn ) //find the target of a break instruction 
+operand_info *function_info::find_break_target(
+    ptx_instruction *p_break_insn)  // find the target of a break instruction
 {
-   const basic_block_t *break_bb = p_break_insn->get_bb(); 
-   // go through the dominator tree 
-   for(const basic_block_t *p_bb = break_bb; 
-       p_bb->immediatedominator_id != -1; 
-       p_bb = m_basic_blocks[p_bb->immediatedominator_id]) 
-   {
-      // reverse search through instructions in basic block for breakaddr instruction 
-      unsigned insn_addr = p_bb->ptx_end->get_m_instr_mem_index(); 
-      while (insn_addr >= p_bb->ptx_begin->get_m_instr_mem_index()) { 
-         ptx_instruction *pI = m_instr_mem[insn_addr]; 
-         insn_addr -= 1; 
-         if (pI == NULL) continue; // temporary solution for variable size instructions 
-         if (pI->get_opcode() == BREAKADDR_OP) {
-            return &(pI->dst()); 
-         }
+  const basic_block_t *break_bb = p_break_insn->get_bb();
+  // go through the dominator tree
+  for (const basic_block_t *p_bb = break_bb; p_bb->immediatedominator_id != -1;
+       p_bb = m_basic_blocks[p_bb->immediatedominator_id]) {
+    // reverse search through instructions in basic block for breakaddr
+    // instruction
+    unsigned insn_addr = p_bb->ptx_end->get_m_instr_mem_index();
+    while (insn_addr >= p_bb->ptx_begin->get_m_instr_mem_index()) {
+      ptx_instruction *pI = m_instr_mem[insn_addr];
+      insn_addr -= 1;
+      if (pI == NULL)
+        continue;  // temporary solution for variable size instructions
+      if (pI->get_opcode() == BREAKADDR_OP) {
+        return &(pI->dst());
       }
-   }
+    }
+  }
 
-   assert(0); 
+  assert(0);
 
-   // lazy fallback: just traverse backwards? 
-   for (int insn_addr = p_break_insn->get_m_instr_mem_index(); 
-        insn_addr >= 0; insn_addr--) 
-   { 
-      ptx_instruction *pI = m_instr_mem[insn_addr]; 
-      if (pI->get_opcode() == BREAKADDR_OP) {
-         return &(pI->dst()); 
-      }
-   }
+  // lazy fallback: just traverse backwards?
+  for (int insn_addr = p_break_insn->get_m_instr_mem_index(); insn_addr >= 0;
+       insn_addr--) {
+    ptx_instruction *pI = m_instr_mem[insn_addr];
+    if (pI->get_opcode() == BREAKADDR_OP) {
+      return &(pI->dst());
+    }
+  }
 
-   return NULL; 
+  return NULL;
 }
-void function_info::connect_basic_blocks( ) //iterate across m_basic_blocks of function, connecting basic blocks together
+void function_info::connect_basic_blocks()  // iterate across m_basic_blocks of
+                                            // function, connecting basic blocks
+                                            // together
 {
-   std::vector<basic_block_t*>::iterator bb_itr;
-   std::vector<basic_block_t*>::iterator bb_target_itr;
-   basic_block_t* exit_bb = m_basic_blocks.back();
-
-   //start from first basic block, which we know is the entry point
-   bb_itr = m_basic_blocks.begin(); 
-   for (bb_itr = m_basic_blocks.begin();bb_itr != m_basic_blocks.end(); bb_itr++) {
-      ptx_instruction *pI = (*bb_itr)->ptx_end;
-      if ((*bb_itr)->is_exit) //reached last basic block, no successors to link 
-         continue;
-      if (pI->get_opcode() == RETP_OP || pI->get_opcode() == RET_OP || pI->get_opcode() == EXIT_OP ) {
-         (*bb_itr)->successor_ids.insert(exit_bb->bb_id);
-         exit_bb->predecessor_ids.insert((*bb_itr)->bb_id);
-         if( pI->has_pred() ) {
-            printf("GPGPU-Sim PTX: Warning detected predicated return/exit.\n");
-            // if predicated, add link to next block
-            unsigned next_addr = pI->get_m_instr_mem_index() + pI->inst_size();
-            if( next_addr < m_instr_mem_size && m_instr_mem[next_addr] ) {
-               basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
-               (*bb_itr)->successor_ids.insert(next_bb->bb_id);
-               next_bb->predecessor_ids.insert((*bb_itr)->bb_id);
-            }
-         }
-         continue;
-      } else if (pI->get_opcode() == BRA_OP) {
-         //find successor and link that basic_block to this one
-         operand_info &target = pI->dst(); //get operand, e.g. target name
-         unsigned addr = labels[ target.name() ];
-         ptx_instruction *target_pI = m_instr_mem[addr];
-         basic_block_t *target_bb = target_pI->get_bb();
-         (*bb_itr)->successor_ids.insert(target_bb->bb_id);
-         target_bb->predecessor_ids.insert((*bb_itr)->bb_id);
-      } 
-
-      if ( !(pI->get_opcode()==BRA_OP && (!pI->has_pred())) ) { 
-         // if basic block does not end in an unpredicated branch, 
-         // then next basic block is also successor
-         // (this is better than testing for .uni)
-         unsigned next_addr = pI->get_m_instr_mem_index() + pI->inst_size();
-         basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
-         (*bb_itr)->successor_ids.insert(next_bb->bb_id);
-         next_bb->predecessor_ids.insert((*bb_itr)->bb_id);
-      } else
-         assert(pI->get_opcode() == BRA_OP);
-   }
+  std::vector<basic_block_t *>::iterator bb_itr;
+  std::vector<basic_block_t *>::iterator bb_target_itr;
+  basic_block_t *exit_bb = m_basic_blocks.back();
+
+  // start from first basic block, which we know is the entry point
+  bb_itr = m_basic_blocks.begin();
+  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
+       bb_itr++) {
+    ptx_instruction *pI = (*bb_itr)->ptx_end;
+    if ((*bb_itr)->is_exit)  // reached last basic block, no successors to link
+      continue;
+    if (pI->get_opcode() == RETP_OP || pI->get_opcode() == RET_OP ||
+        pI->get_opcode() == EXIT_OP) {
+      (*bb_itr)->successor_ids.insert(exit_bb->bb_id);
+      exit_bb->predecessor_ids.insert((*bb_itr)->bb_id);
+      if (pI->has_pred()) {
+        printf("GPGPU-Sim PTX: Warning detected predicated return/exit.\n");
+        // if predicated, add link to next block
+        unsigned next_addr = pI->get_m_instr_mem_index() + pI->inst_size();
+        if (next_addr < m_instr_mem_size && m_instr_mem[next_addr]) {
+          basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
+          (*bb_itr)->successor_ids.insert(next_bb->bb_id);
+          next_bb->predecessor_ids.insert((*bb_itr)->bb_id);
+        }
+      }
+      continue;
+    } else if (pI->get_opcode() == BRA_OP) {
+      // find successor and link that basic_block to this one
+      operand_info &target = pI->dst();  // get operand, e.g. target name
+      unsigned addr = labels[target.name()];
+      ptx_instruction *target_pI = m_instr_mem[addr];
+      basic_block_t *target_bb = target_pI->get_bb();
+      (*bb_itr)->successor_ids.insert(target_bb->bb_id);
+      target_bb->predecessor_ids.insert((*bb_itr)->bb_id);
+    }
+
+    if (!(pI->get_opcode() == BRA_OP && (!pI->has_pred()))) {
+      // if basic block does not end in an unpredicated branch,
+      // then next basic block is also successor
+      // (this is better than testing for .uni)
+      unsigned next_addr = pI->get_m_instr_mem_index() + pI->inst_size();
+      basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
+      (*bb_itr)->successor_ids.insert(next_bb->bb_id);
+      next_bb->predecessor_ids.insert((*bb_itr)->bb_id);
+    } else
+      assert(pI->get_opcode() == BRA_OP);
+  }
 }
-bool function_info::connect_break_targets() //connecting break instructions with proper targets
+bool function_info::connect_break_targets()  // connecting break instructions
+                                             // with proper targets
 {
-   std::vector<basic_block_t*>::iterator bb_itr;
-   std::vector<basic_block_t*>::iterator bb_target_itr;
-   bool modified = false; 
-
-   //start from first basic block, which we know is the entry point
-   bb_itr = m_basic_blocks.begin(); 
-   for (bb_itr = m_basic_blocks.begin();bb_itr != m_basic_blocks.end(); bb_itr++) {
-      basic_block_t *p_bb = *bb_itr; 
-      ptx_instruction *pI = p_bb->ptx_end;
-      if (p_bb->is_exit) //reached last basic block, no successors to link 
-         continue;
-      if (pI->get_opcode() == BREAK_OP) {
-         // backup existing successor_ids for stability check
-         std::set<int> orig_successor_ids = p_bb->successor_ids; 
-
-         // erase the previous linkage with old successors 
-         for(std::set<int>::iterator succ_ids = p_bb->successor_ids.begin(); succ_ids != p_bb->successor_ids.end(); ++succ_ids) {
-            basic_block_t *successor_bb = m_basic_blocks[*succ_ids];
-            successor_bb->predecessor_ids.erase(p_bb->bb_id); 
-         }
-         p_bb->successor_ids.clear(); 
-
-         //find successor and link that basic_block to this one
-         //successor of a break is set by an preceeding breakaddr instruction 
-         operand_info *target = find_break_target(pI); 
-         unsigned addr = labels[ target->name() ];
-         ptx_instruction *target_pI = m_instr_mem[addr];
-         basic_block_t *target_bb = target_pI->get_bb();
-         p_bb->successor_ids.insert(target_bb->bb_id);
-         target_bb->predecessor_ids.insert(p_bb->bb_id);
-
-         if (pI->has_pred()) {
-            // predicated break - add link to next basic block
-            unsigned next_addr = pI->get_m_instr_mem_index() + pI->inst_size();
-            basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
-            p_bb->successor_ids.insert(next_bb->bb_id);
-            next_bb->predecessor_ids.insert(p_bb->bb_id);
-         }
-
-         modified = modified || (orig_successor_ids != p_bb->successor_ids); 
+  std::vector<basic_block_t *>::iterator bb_itr;
+  std::vector<basic_block_t *>::iterator bb_target_itr;
+  bool modified = false;
+
+  // start from first basic block, which we know is the entry point
+  bb_itr = m_basic_blocks.begin();
+  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
+       bb_itr++) {
+    basic_block_t *p_bb = *bb_itr;
+    ptx_instruction *pI = p_bb->ptx_end;
+    if (p_bb->is_exit)  // reached last basic block, no successors to link
+      continue;
+    if (pI->get_opcode() == BREAK_OP) {
+      // backup existing successor_ids for stability check
+      std::set<int> orig_successor_ids = p_bb->successor_ids;
+
+      // erase the previous linkage with old successors
+      for (std::set<int>::iterator succ_ids = p_bb->successor_ids.begin();
+           succ_ids != p_bb->successor_ids.end(); ++succ_ids) {
+        basic_block_t *successor_bb = m_basic_blocks[*succ_ids];
+        successor_bb->predecessor_ids.erase(p_bb->bb_id);
+      }
+      p_bb->successor_ids.clear();
+
+      // find successor and link that basic_block to this one
+      // successor of a break is set by an preceeding breakaddr instruction
+      operand_info *target = find_break_target(pI);
+      unsigned addr = labels[target->name()];
+      ptx_instruction *target_pI = m_instr_mem[addr];
+      basic_block_t *target_bb = target_pI->get_bb();
+      p_bb->successor_ids.insert(target_bb->bb_id);
+      target_bb->predecessor_ids.insert(p_bb->bb_id);
+
+      if (pI->has_pred()) {
+        // predicated break - add link to next basic block
+        unsigned next_addr = pI->get_m_instr_mem_index() + pI->inst_size();
+        basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
+        p_bb->successor_ids.insert(next_bb->bb_id);
+        next_bb->predecessor_ids.insert(p_bb->bb_id);
       }
-   }
 
-   return modified; 
+      modified = modified || (orig_successor_ids != p_bb->successor_ids);
+    }
+  }
+
+  return modified;
 }
-void function_info::do_pdom() 
-{
-   create_basic_blocks();
-   connect_basic_blocks();
-   bool modified = false; 
-   do {
-      find_dominators();
-      find_idominators();
-      modified = connect_break_targets(); 
-   } while (modified == true);
-
-   if ( g_debug_execution>=50 ) {
-      print_basic_blocks();
-      print_basic_block_links();
-      print_basic_block_dot();
-   }
-   if ( g_debug_execution>=2 ) {
-      print_dominators();
-   }
-   find_postdominators();
-   find_ipostdominators();
-   if ( g_debug_execution>=50 ) {
-      print_postdominators();
-      print_ipostdominators();
-   }
-   printf("GPGPU-Sim PTX: pre-decoding instructions for \'%s\'...\n", m_name.c_str() );
-   for ( unsigned ii=0; ii < m_n; ii += m_instr_mem[ii]->inst_size() ) { // handle branch instructions
-      ptx_instruction *pI = m_instr_mem[ii];
-      pI->pre_decode();
-   }
-   printf("GPGPU-Sim PTX: ... done pre-decoding instructions for \'%s\'.\n", m_name.c_str() );
-   fflush(stdout);
-   m_assembled = true;
+void function_info::do_pdom() {
+  create_basic_blocks();
+  connect_basic_blocks();
+  bool modified = false;
+  do {
+    find_dominators();
+    find_idominators();
+    modified = connect_break_targets();
+  } while (modified == true);
+
+  if (g_debug_execution >= 50) {
+    print_basic_blocks();
+    print_basic_block_links();
+    print_basic_block_dot();
+  }
+  if (g_debug_execution >= 2) {
+    print_dominators();
+  }
+  find_postdominators();
+  find_ipostdominators();
+  if (g_debug_execution >= 50) {
+    print_postdominators();
+    print_ipostdominators();
+  }
+  printf("GPGPU-Sim PTX: pre-decoding instructions for \'%s\'...\n",
+         m_name.c_str());
+  for (unsigned ii = 0; ii < m_n;
+       ii += m_instr_mem[ii]->inst_size()) {  // handle branch instructions
+    ptx_instruction *pI = m_instr_mem[ii];
+    pI->pre_decode();
+  }
+  printf("GPGPU-Sim PTX: ... done pre-decoding instructions for \'%s\'.\n",
+         m_name.c_str());
+  fflush(stdout);
+  m_assembled = true;
 }
-void intersect( std::set<int> &A, const std::set<int> &B )
-{
-   // return intersection of A and B in A
-   for( std::set<int>::iterator a=A.begin(); a!=A.end(); ) {    
-      std::set<int>::iterator a_next = a;
-      a_next++;
-      if( B.find(*a) == B.end() ) {
-         A.erase(*a);
-         a = a_next;
-      } else 
-         a++;
-   }
+void intersect(std::set<int> &A, const std::set<int> &B) {
+  // return intersection of A and B in A
+  for (std::set<int>::iterator a = A.begin(); a != A.end();) {
+    std::set<int>::iterator a_next = a;
+    a_next++;
+    if (B.find(*a) == B.end()) {
+      A.erase(*a);
+      a = a_next;
+    } else
+      a++;
+  }
 }
 
-bool is_equal( const std::set<int> &A, const std::set<int> &B )
-{
-   if( A.size() != B.size() ) 
-      return false;
-   for( std::set<int>::iterator b=B.begin(); b!=B.end(); b++ ) 
-      if( A.find(*b) == A.end() ) 
-         return false;
-   return true;
+bool is_equal(const std::set<int> &A, const std::set<int> &B) {
+  if (A.size() != B.size()) return false;
+  for (std::set<int>::iterator b = B.begin(); b != B.end(); b++)
+    if (A.find(*b) == A.end()) return false;
+  return true;
 }
 
-void print_set(const std::set<int> &A)
-{
-   std::set<int>::iterator a;
-   for (a= A.begin(); a != A.end(); a++) {
-      printf("%d ", (*a));
-   }
-   printf("\n");
+void print_set(const std::set<int> &A) {
+  std::set<int>::iterator a;
+  for (a = A.begin(); a != A.end(); a++) {
+    printf("%d ", (*a));
+  }
+  printf("\n");
 }
 
-void function_info::find_dominators( )
-{  
-   // find dominators using algorithm of Muchnick's Adv. Compiler Design & Implemmntation Fig 7.14 
-   printf("GPGPU-Sim PTX: Finding dominators for \'%s\'...\n", m_name.c_str() );
-   fflush(stdout);
-   assert( m_basic_blocks.size() >= 2 ); // must have a distinquished entry block
-   std::vector<basic_block_t*>::iterator bb_itr = m_basic_blocks.begin();
-   (*bb_itr)->dominator_ids.insert((*bb_itr)->bb_id);  // the only dominator of the entry block is the entry
-   //copy all basic blocks to all dominator lists EXCEPT for the entry block
-   for (++bb_itr;bb_itr != m_basic_blocks.end(); bb_itr++) { 
-      for (unsigned i = 0; i < m_basic_blocks.size(); i++) 
-         (*bb_itr)->dominator_ids.insert(i);
-   }
-   bool change = true;
-   while (change) {
-      change = false;
-      for ( int h = 1/*skip entry*/; h < m_basic_blocks.size(); ++h ) {
-         assert( m_basic_blocks[h]->bb_id == (unsigned)h );
-         std::set<int> T;
-         for (unsigned i=0;i< m_basic_blocks.size();i++) 
-            T.insert(i);
-         for ( std::set<int>::iterator s = m_basic_blocks[h]->predecessor_ids.begin();s != m_basic_blocks[h]->predecessor_ids.end();s++) 
-            intersect(T, m_basic_blocks[*s]->dominator_ids);
-         T.insert(h);
-         if (!is_equal(T, m_basic_blocks[h]->dominator_ids)) {
-            change = true;
-            m_basic_blocks[h]->dominator_ids = T;
-         }
+void function_info::find_dominators() {
+  // find dominators using algorithm of Muchnick's Adv. Compiler Design &
+  // Implemmntation Fig 7.14
+  printf("GPGPU-Sim PTX: Finding dominators for \'%s\'...\n", m_name.c_str());
+  fflush(stdout);
+  assert(m_basic_blocks.size() >= 2);  // must have a distinquished entry block
+  std::vector<basic_block_t *>::iterator bb_itr = m_basic_blocks.begin();
+  (*bb_itr)->dominator_ids.insert(
+      (*bb_itr)->bb_id);  // the only dominator of the entry block is the entry
+  // copy all basic blocks to all dominator lists EXCEPT for the entry block
+  for (++bb_itr; bb_itr != m_basic_blocks.end(); bb_itr++) {
+    for (unsigned i = 0; i < m_basic_blocks.size(); i++)
+      (*bb_itr)->dominator_ids.insert(i);
+  }
+  bool change = true;
+  while (change) {
+    change = false;
+    for (int h = 1 /*skip entry*/; h < m_basic_blocks.size(); ++h) {
+      assert(m_basic_blocks[h]->bb_id == (unsigned)h);
+      std::set<int> T;
+      for (unsigned i = 0; i < m_basic_blocks.size(); i++) T.insert(i);
+      for (std::set<int>::iterator s =
+               m_basic_blocks[h]->predecessor_ids.begin();
+           s != m_basic_blocks[h]->predecessor_ids.end(); s++)
+        intersect(T, m_basic_blocks[*s]->dominator_ids);
+      T.insert(h);
+      if (!is_equal(T, m_basic_blocks[h]->dominator_ids)) {
+        change = true;
+        m_basic_blocks[h]->dominator_ids = T;
       }
-   }
-   //clean the basic block of dominators of it has no predecessors -- except for entry block
-   bb_itr = m_basic_blocks.begin();
-   for (++bb_itr;bb_itr != m_basic_blocks.end(); bb_itr++) {
-	  if ((*bb_itr)->predecessor_ids.empty())
-         (*bb_itr)->dominator_ids.clear();
-   }
+    }
+  }
+  // clean the basic block of dominators of it has no predecessors -- except for
+  // entry block
+  bb_itr = m_basic_blocks.begin();
+  for (++bb_itr; bb_itr != m_basic_blocks.end(); bb_itr++) {
+    if ((*bb_itr)->predecessor_ids.empty()) (*bb_itr)->dominator_ids.clear();
+  }
 }
 
-void function_info::find_postdominators( )
-{  
-   // find postdominators using algorithm of Muchnick's Adv. Compiler Design & Implemmntation Fig 7.14 
-   printf("GPGPU-Sim PTX: Finding postdominators for \'%s\'...\n", m_name.c_str() );
-   fflush(stdout);
-   assert( m_basic_blocks.size() >= 2 ); // must have a distinquished exit block
-   std::vector<basic_block_t*>::reverse_iterator bb_itr = m_basic_blocks.rbegin();
-   (*bb_itr)->postdominator_ids.insert((*bb_itr)->bb_id);  // the only postdominator of the exit block is the exit
-   for (++bb_itr;bb_itr != m_basic_blocks.rend();bb_itr++) { //copy all basic blocks to all postdominator lists EXCEPT for the exit block
-      for (unsigned i=0; i<m_basic_blocks.size(); i++) 
-         (*bb_itr)->postdominator_ids.insert(i);
-   }
-   bool change = true;
-   while (change) {
-      change = false;
-      for ( int h = m_basic_blocks.size()-2/*skip exit*/; h >= 0 ; --h ) {
-         assert( m_basic_blocks[h]->bb_id == (unsigned)h );
-         std::set<int> T;
-         for (unsigned i=0;i< m_basic_blocks.size();i++) 
-            T.insert(i);
-         for ( std::set<int>::iterator s = m_basic_blocks[h]->successor_ids.begin();s != m_basic_blocks[h]->successor_ids.end();s++) 
-            intersect(T, m_basic_blocks[*s]->postdominator_ids);
-         T.insert(h);
-         if (!is_equal(T,m_basic_blocks[h]->postdominator_ids)) {
-            change = true;
-            m_basic_blocks[h]->postdominator_ids = T;
-         }
+void function_info::find_postdominators() {
+  // find postdominators using algorithm of Muchnick's Adv. Compiler Design &
+  // Implemmntation Fig 7.14
+  printf("GPGPU-Sim PTX: Finding postdominators for \'%s\'...\n",
+         m_name.c_str());
+  fflush(stdout);
+  assert(m_basic_blocks.size() >= 2);  // must have a distinquished exit block
+  std::vector<basic_block_t *>::reverse_iterator bb_itr =
+      m_basic_blocks.rbegin();
+  (*bb_itr)->postdominator_ids.insert(
+      (*bb_itr)
+          ->bb_id);  // the only postdominator of the exit block is the exit
+  for (++bb_itr; bb_itr != m_basic_blocks.rend();
+       bb_itr++) {  // copy all basic blocks to all postdominator lists EXCEPT
+                    // for the exit block
+    for (unsigned i = 0; i < m_basic_blocks.size(); i++)
+      (*bb_itr)->postdominator_ids.insert(i);
+  }
+  bool change = true;
+  while (change) {
+    change = false;
+    for (int h = m_basic_blocks.size() - 2 /*skip exit*/; h >= 0; --h) {
+      assert(m_basic_blocks[h]->bb_id == (unsigned)h);
+      std::set<int> T;
+      for (unsigned i = 0; i < m_basic_blocks.size(); i++) T.insert(i);
+      for (std::set<int>::iterator s = m_basic_blocks[h]->successor_ids.begin();
+           s != m_basic_blocks[h]->successor_ids.end(); s++)
+        intersect(T, m_basic_blocks[*s]->postdominator_ids);
+      T.insert(h);
+      if (!is_equal(T, m_basic_blocks[h]->postdominator_ids)) {
+        change = true;
+        m_basic_blocks[h]->postdominator_ids = T;
       }
-   }
+    }
+  }
 }
 
-void function_info::find_ipostdominators( )
-{  
-   // find immediate postdominator blocks, using algorithm of
-   // Muchnick's Adv. Compiler Design & Implemmntation Fig 7.15 
-   printf("GPGPU-Sim PTX: Finding immediate postdominators for \'%s\'...\n", m_name.c_str() );
-   fflush(stdout);
-   assert( m_basic_blocks.size() >= 2 ); // must have a distinquished exit block
-   for (unsigned i=0; i<m_basic_blocks.size(); i++) { //initialize Tmp(n) to all pdoms of n except for n
-      m_basic_blocks[i]->Tmp_ids = m_basic_blocks[i]->postdominator_ids;
-      assert( m_basic_blocks[i]->bb_id == i );
-      m_basic_blocks[i]->Tmp_ids.erase(i);
-   }
-   for ( int n = m_basic_blocks.size()-2; n >=0;--n) {
-      // point iterator to basic block before the exit
-      for( std::set<int>::iterator s=m_basic_blocks[n]->Tmp_ids.begin(); s != m_basic_blocks[n]->Tmp_ids.end(); s++ ) {
-         int bb_s = *s;
-         for( std::set<int>::iterator t=m_basic_blocks[n]->Tmp_ids.begin(); t != m_basic_blocks[n]->Tmp_ids.end(); ) {
-            std::set<int>::iterator t_next = t; t_next++; // might erase thing pointed to be t, invalidating iterator t
-            if( *s == *t ) {
-               t = t_next;
-               continue;
-            }
-            int bb_t = *t;
-            if( m_basic_blocks[bb_s]->postdominator_ids.find(bb_t) != m_basic_blocks[bb_s]->postdominator_ids.end() ) 
-                m_basic_blocks[n]->Tmp_ids.erase(bb_t);
-            t = t_next;
-         }
-      }
-   }
-   unsigned num_ipdoms=0;
-   for ( int n = m_basic_blocks.size()-1; n >=0;--n) {
-      assert( m_basic_blocks[n]->Tmp_ids.size() <= 1 ); 
-         // if the above assert fails we have an error in either postdominator 
-         // computation, the flow graph does not have a unique exit, or some other error
-      if( !m_basic_blocks[n]->Tmp_ids.empty() ) {
-         m_basic_blocks[n]->immediatepostdominator_id = *m_basic_blocks[n]->Tmp_ids.begin();
-         num_ipdoms++;
+void function_info::find_ipostdominators() {
+  // find immediate postdominator blocks, using algorithm of
+  // Muchnick's Adv. Compiler Design & Implemmntation Fig 7.15
+  printf("GPGPU-Sim PTX: Finding immediate postdominators for \'%s\'...\n",
+         m_name.c_str());
+  fflush(stdout);
+  assert(m_basic_blocks.size() >= 2);  // must have a distinquished exit block
+  for (unsigned i = 0; i < m_basic_blocks.size();
+       i++) {  // initialize Tmp(n) to all pdoms of n except for n
+    m_basic_blocks[i]->Tmp_ids = m_basic_blocks[i]->postdominator_ids;
+    assert(m_basic_blocks[i]->bb_id == i);
+    m_basic_blocks[i]->Tmp_ids.erase(i);
+  }
+  for (int n = m_basic_blocks.size() - 2; n >= 0; --n) {
+    // point iterator to basic block before the exit
+    for (std::set<int>::iterator s = m_basic_blocks[n]->Tmp_ids.begin();
+         s != m_basic_blocks[n]->Tmp_ids.end(); s++) {
+      int bb_s = *s;
+      for (std::set<int>::iterator t = m_basic_blocks[n]->Tmp_ids.begin();
+           t != m_basic_blocks[n]->Tmp_ids.end();) {
+        std::set<int>::iterator t_next = t;
+        t_next++;  // might erase thing pointed to be t, invalidating iterator t
+        if (*s == *t) {
+          t = t_next;
+          continue;
+        }
+        int bb_t = *t;
+        if (m_basic_blocks[bb_s]->postdominator_ids.find(bb_t) !=
+            m_basic_blocks[bb_s]->postdominator_ids.end())
+          m_basic_blocks[n]->Tmp_ids.erase(bb_t);
+        t = t_next;
       }
-   }
-   assert( num_ipdoms == m_basic_blocks.size()-1 ); 
-      // the exit node does not have an immediate post dominator, but everyone else should
+    }
+  }
+  unsigned num_ipdoms = 0;
+  for (int n = m_basic_blocks.size() - 1; n >= 0; --n) {
+    assert(m_basic_blocks[n]->Tmp_ids.size() <= 1);
+    // if the above assert fails we have an error in either postdominator
+    // computation, the flow graph does not have a unique exit, or some other
+    // error
+    if (!m_basic_blocks[n]->Tmp_ids.empty()) {
+      m_basic_blocks[n]->immediatepostdominator_id =
+          *m_basic_blocks[n]->Tmp_ids.begin();
+      num_ipdoms++;
+    }
+  }
+  assert(num_ipdoms == m_basic_blocks.size() - 1);
+  // the exit node does not have an immediate post dominator, but everyone else
+  // should
 }
 
-void function_info::find_idominators( )
-{  
-   // find immediate dominator blocks, using algorithm of
-   // Muchnick's Adv. Compiler Design & Implemmntation Fig 7.15 
-   printf("GPGPU-Sim PTX: Finding immediate dominators for \'%s\'...\n", m_name.c_str() );
-   fflush(stdout);
-   assert( m_basic_blocks.size() >= 2 ); // must have a distinquished entry block
-   for (unsigned i=0; i<m_basic_blocks.size(); i++) { //initialize Tmp(n) to all doms of n except for n
-      m_basic_blocks[i]->Tmp_ids = m_basic_blocks[i]->dominator_ids;
-      assert( m_basic_blocks[i]->bb_id == i );
-      m_basic_blocks[i]->Tmp_ids.erase(i);
-   }
-   for ( int n = 0; n < m_basic_blocks.size(); ++n) {
-      // point iterator to basic block before the exit
-      for( std::set<int>::iterator s=m_basic_blocks[n]->Tmp_ids.begin(); s != m_basic_blocks[n]->Tmp_ids.end(); s++ ) {
-         int bb_s = *s;
-         for( std::set<int>::iterator t=m_basic_blocks[n]->Tmp_ids.begin(); t != m_basic_blocks[n]->Tmp_ids.end(); ) {
-            std::set<int>::iterator t_next = t; t_next++; // might erase thing pointed to be t, invalidating iterator t
-            if( *s == *t ) {
-               t = t_next;
-               continue;
-            }
-            int bb_t = *t;
-            if( m_basic_blocks[bb_s]->dominator_ids.find(bb_t) != m_basic_blocks[bb_s]->dominator_ids.end() ) 
-                m_basic_blocks[n]->Tmp_ids.erase(bb_t);
-            t = t_next;
-         }
-      }
-   }
-   unsigned num_idoms=0;
-   unsigned num_nopred = 0;
-   for ( int n = 0; n < m_basic_blocks.size(); ++n) {
-      //assert( m_basic_blocks[n]->Tmp_ids.size() <= 1 );
-         // if the above assert fails we have an error in either dominator
-         // computation, the flow graph does not have a unique entry, or some other error
-      if( !m_basic_blocks[n]->Tmp_ids.empty() ) {
-         m_basic_blocks[n]->immediatedominator_id = *m_basic_blocks[n]->Tmp_ids.begin();
-         num_idoms++;
-      } else if (m_basic_blocks[n]->predecessor_ids.empty()) {
-    	  num_nopred += 1;
+void function_info::find_idominators() {
+  // find immediate dominator blocks, using algorithm of
+  // Muchnick's Adv. Compiler Design & Implemmntation Fig 7.15
+  printf("GPGPU-Sim PTX: Finding immediate dominators for \'%s\'...\n",
+         m_name.c_str());
+  fflush(stdout);
+  assert(m_basic_blocks.size() >= 2);  // must have a distinquished entry block
+  for (unsigned i = 0; i < m_basic_blocks.size();
+       i++) {  // initialize Tmp(n) to all doms of n except for n
+    m_basic_blocks[i]->Tmp_ids = m_basic_blocks[i]->dominator_ids;
+    assert(m_basic_blocks[i]->bb_id == i);
+    m_basic_blocks[i]->Tmp_ids.erase(i);
+  }
+  for (int n = 0; n < m_basic_blocks.size(); ++n) {
+    // point iterator to basic block before the exit
+    for (std::set<int>::iterator s = m_basic_blocks[n]->Tmp_ids.begin();
+         s != m_basic_blocks[n]->Tmp_ids.end(); s++) {
+      int bb_s = *s;
+      for (std::set<int>::iterator t = m_basic_blocks[n]->Tmp_ids.begin();
+           t != m_basic_blocks[n]->Tmp_ids.end();) {
+        std::set<int>::iterator t_next = t;
+        t_next++;  // might erase thing pointed to be t, invalidating iterator t
+        if (*s == *t) {
+          t = t_next;
+          continue;
+        }
+        int bb_t = *t;
+        if (m_basic_blocks[bb_s]->dominator_ids.find(bb_t) !=
+            m_basic_blocks[bb_s]->dominator_ids.end())
+          m_basic_blocks[n]->Tmp_ids.erase(bb_t);
+        t = t_next;
       }
-   }
-   assert( num_idoms == m_basic_blocks.size()-num_nopred );
-      // the entry node does not have an immediate dominator, but everyone else should
+    }
+  }
+  unsigned num_idoms = 0;
+  unsigned num_nopred = 0;
+  for (int n = 0; n < m_basic_blocks.size(); ++n) {
+    // assert( m_basic_blocks[n]->Tmp_ids.size() <= 1 );
+    // if the above assert fails we have an error in either dominator
+    // computation, the flow graph does not have a unique entry, or some other
+    // error
+    if (!m_basic_blocks[n]->Tmp_ids.empty()) {
+      m_basic_blocks[n]->immediatedominator_id =
+          *m_basic_blocks[n]->Tmp_ids.begin();
+      num_idoms++;
+    } else if (m_basic_blocks[n]->predecessor_ids.empty()) {
+      num_nopred += 1;
+    }
+  }
+  assert(num_idoms == m_basic_blocks.size() - num_nopred);
+  // the entry node does not have an immediate dominator, but everyone else
+  // should
 }
 
-void function_info::print_dominators()
-{
-   printf("Printing dominators for function \'%s\':\n", m_name.c_str() );
-   std::vector<int>::iterator bb_itr;
-   for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
-      printf("ID: %d\t:", i);
-      for( std::set<int>::iterator j=m_basic_blocks[i]->dominator_ids.begin(); j!=m_basic_blocks[i]->dominator_ids.end(); j++) 
-         printf(" %d", *j );
-      printf("\n");
-   }
+void function_info::print_dominators() {
+  printf("Printing dominators for function \'%s\':\n", m_name.c_str());
+  std::vector<int>::iterator bb_itr;
+  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
+    printf("ID: %d\t:", i);
+    for (std::set<int>::iterator j = m_basic_blocks[i]->dominator_ids.begin();
+         j != m_basic_blocks[i]->dominator_ids.end(); j++)
+      printf(" %d", *j);
+    printf("\n");
+  }
 }
 
-void function_info::print_postdominators()
-{
-   printf("Printing postdominators for function \'%s\':\n", m_name.c_str() );
-   std::vector<int>::iterator bb_itr;
-   for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
-      printf("ID: %d\t:", i);
-      for( std::set<int>::iterator j=m_basic_blocks[i]->postdominator_ids.begin(); j!=m_basic_blocks[i]->postdominator_ids.end(); j++) 
-         printf(" %d", *j );
-      printf("\n");
-   }
+void function_info::print_postdominators() {
+  printf("Printing postdominators for function \'%s\':\n", m_name.c_str());
+  std::vector<int>::iterator bb_itr;
+  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
+    printf("ID: %d\t:", i);
+    for (std::set<int>::iterator j =
+             m_basic_blocks[i]->postdominator_ids.begin();
+         j != m_basic_blocks[i]->postdominator_ids.end(); j++)
+      printf(" %d", *j);
+    printf("\n");
+  }
 }
 
-void function_info::print_ipostdominators()
-{
-   printf("Printing immediate postdominators for function \'%s\':\n", m_name.c_str() );
-   std::vector<int>::iterator bb_itr;
-   for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
-      printf("ID: %d\t:", i);
-      printf("%d\n", m_basic_blocks[i]->immediatepostdominator_id);
-   }
+void function_info::print_ipostdominators() {
+  printf("Printing immediate postdominators for function \'%s\':\n",
+         m_name.c_str());
+  std::vector<int>::iterator bb_itr;
+  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
+    printf("ID: %d\t:", i);
+    printf("%d\n", m_basic_blocks[i]->immediatepostdominator_id);
+  }
 }
 
-void function_info::print_idominators()
-{
-   printf("Printing immediate dominators for function \'%s\':\n", m_name.c_str() );
-   std::vector<int>::iterator bb_itr;
-   for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
-      printf("ID: %d\t:", i);
-      printf("%d\n", m_basic_blocks[i]->immediatedominator_id);
-   }
+void function_info::print_idominators() {
+  printf("Printing immediate dominators for function \'%s\':\n",
+         m_name.c_str());
+  std::vector<int>::iterator bb_itr;
+  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
+    printf("ID: %d\t:", i);
+    printf("%d\n", m_basic_blocks[i]->immediatedominator_id);
+  }
 }
 
-unsigned function_info::get_num_reconvergence_pairs()
-{
-   if (!num_reconvergence_pairs) {
-      if( m_basic_blocks.size() == 0 ) 
-         return 0;
-      for (unsigned i=0; i< (m_basic_blocks.size()-1); i++) { //last basic block containing exit obviously won't have a pair
-         if (m_basic_blocks[i]->ptx_end->get_opcode() == BRA_OP) {
-            num_reconvergence_pairs++;
-         }
+unsigned function_info::get_num_reconvergence_pairs() {
+  if (!num_reconvergence_pairs) {
+    if (m_basic_blocks.size() == 0) return 0;
+    for (unsigned i = 0; i < (m_basic_blocks.size() - 1);
+         i++) {  // last basic block containing exit obviously won't have a pair
+      if (m_basic_blocks[i]->ptx_end->get_opcode() == BRA_OP) {
+        num_reconvergence_pairs++;
       }
-   }
-   return num_reconvergence_pairs;
+    }
+  }
+  return num_reconvergence_pairs;
 }
 
-void function_info::get_reconvergence_pairs(gpgpu_recon_t* recon_points)
-{
-   unsigned idx=0; //array index
-   if( m_basic_blocks.size() == 0 ) 
-       return;
-   for (unsigned i=0; i< (m_basic_blocks.size()-1); i++) { //last basic block containing exit obviously won't have a pair
+void function_info::get_reconvergence_pairs(gpgpu_recon_t *recon_points) {
+  unsigned idx = 0;  // array index
+  if (m_basic_blocks.size() == 0) return;
+  for (unsigned i = 0; i < (m_basic_blocks.size() - 1);
+       i++) {  // last basic block containing exit obviously won't have a pair
 #ifdef DEBUG_GET_RECONVERG_PAIRS
-      printf("i=%d\n", i); fflush(stdout);
+    printf("i=%d\n", i);
+    fflush(stdout);
 #endif
-      if (m_basic_blocks[i]->ptx_end->get_opcode() == BRA_OP) {
+    if (m_basic_blocks[i]->ptx_end->get_opcode() == BRA_OP) {
 #ifdef DEBUG_GET_RECONVERG_PAIRS
-         printf("\tbranch!\n");
-         printf("\tbb_id=%d; ipdom=%d\n", m_basic_blocks[i]->bb_id, m_basic_blocks[i]->immediatepostdominator_id);
-         printf("\tm_instr_mem index=%d\n", m_basic_blocks[i]->ptx_end->get_m_instr_mem_index());
-         fflush(stdout);
+      printf("\tbranch!\n");
+      printf("\tbb_id=%d; ipdom=%d\n", m_basic_blocks[i]->bb_id,
+             m_basic_blocks[i]->immediatepostdominator_id);
+      printf("\tm_instr_mem index=%d\n",
+             m_basic_blocks[i]->ptx_end->get_m_instr_mem_index());
+      fflush(stdout);
 #endif
-         recon_points[idx].source_pc = m_basic_blocks[i]->ptx_end->get_PC();
-         recon_points[idx].source_inst = m_basic_blocks[i]->ptx_end;
+      recon_points[idx].source_pc = m_basic_blocks[i]->ptx_end->get_PC();
+      recon_points[idx].source_inst = m_basic_blocks[i]->ptx_end;
 #ifdef DEBUG_GET_RECONVERG_PAIRS
-         printf("\trecon_points[idx].source_pc=%d\n", recon_points[idx].source_pc);
+      printf("\trecon_points[idx].source_pc=%d\n", recon_points[idx].source_pc);
 #endif
-         if( m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]->ptx_begin ) {
-            recon_points[idx].target_pc = m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]->ptx_begin->get_PC();
-            recon_points[idx].target_inst = m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]->ptx_begin;
-         } else {
-            // reconverge after function return
-            recon_points[idx].target_pc = -2;
-            recon_points[idx].target_inst = NULL;
-         }
+      if (m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
+              ->ptx_begin) {
+        recon_points[idx].target_pc =
+            m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
+                ->ptx_begin->get_PC();
+        recon_points[idx].target_inst =
+            m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
+                ->ptx_begin;
+      } else {
+        // reconverge after function return
+        recon_points[idx].target_pc = -2;
+        recon_points[idx].target_inst = NULL;
+      }
 #ifdef DEBUG_GET_RECONVERG_PAIRS
-         m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]->ptx_begin->print_insn();
-         printf("\trecon_points[idx].target_pc=%d\n", recon_points[idx].target_pc); fflush(stdout);
+      m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
+          ->ptx_begin->print_insn();
+      printf("\trecon_points[idx].target_pc=%d\n", recon_points[idx].target_pc);
+      fflush(stdout);
 #endif
-         idx++;
-      }
-   }
+      idx++;
+    }
+  }
 }
 
 // interface with graphviz (print the graph in DOT language) for plotting
-void function_info::print_basic_block_dot()
-{
-   printf("Basic Block in DOT\n");
-   printf("digraph %s {\n", m_name.c_str());
-   std::vector<basic_block_t*>::iterator bb_itr;
-   for (bb_itr = m_basic_blocks.begin();bb_itr != m_basic_blocks.end(); bb_itr++) {
-      printf("\t");
-      std::set<int>::iterator s;
-      for (s = (*bb_itr)->successor_ids.begin();s != (*bb_itr)->successor_ids.end();s++) {
-         unsigned succ_bb = *s;
-         printf("%d -> %d; ", (*bb_itr)->bb_id, succ_bb );
-      }
-      printf("\n");
-   }
-   printf("}\n");
+void function_info::print_basic_block_dot() {
+  printf("Basic Block in DOT\n");
+  printf("digraph %s {\n", m_name.c_str());
+  std::vector<basic_block_t *>::iterator bb_itr;
+  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
+       bb_itr++) {
+    printf("\t");
+    std::set<int>::iterator s;
+    for (s = (*bb_itr)->successor_ids.begin();
+         s != (*bb_itr)->successor_ids.end(); s++) {
+      unsigned succ_bb = *s;
+      printf("%d -> %d; ", (*bb_itr)->bb_id, succ_bb);
+    }
+    printf("\n");
+  }
+  printf("}\n");
 }
 
-unsigned ptx_kernel_shmem_size( void *kernel_impl )
-{
-   function_info *f = (function_info*)kernel_impl;
-   const struct gpgpu_ptx_sim_info *kernel_info = f->get_kernel_info();
-   return kernel_info->smem;
+unsigned ptx_kernel_shmem_size(void *kernel_impl) {
+  function_info *f = (function_info *)kernel_impl;
+  const struct gpgpu_ptx_sim_info *kernel_info = f->get_kernel_info();
+  return kernel_info->smem;
 }
 
-unsigned ptx_kernel_nregs( void *kernel_impl )
-{
-   function_info *f = (function_info*)kernel_impl;
-   const struct gpgpu_ptx_sim_info *kernel_info = f->get_kernel_info();
-   return kernel_info->regs;
+unsigned ptx_kernel_nregs(void *kernel_impl) {
+  function_info *f = (function_info *)kernel_impl;
+  const struct gpgpu_ptx_sim_info *kernel_info = f->get_kernel_info();
+  return kernel_info->regs;
 }
 
-unsigned type_info_key::type_decode( size_t &size, int &basic_type ) const
-{
-   int type = scalar_type();
-   return type_decode(type,size,basic_type);
+unsigned type_info_key::type_decode(size_t &size, int &basic_type) const {
+  int type = scalar_type();
+  return type_decode(type, size, basic_type);
 }
 
-unsigned type_info_key::type_decode( int type, size_t &size, int &basic_type )
-{
-   switch ( type ) {
-   case S8_TYPE:  size=8;  basic_type=1; return 0;
-   case S16_TYPE: size=16; basic_type=1; return 1;
-   case S32_TYPE: size=32; basic_type=1; return 2;
-   case S64_TYPE: size=64; basic_type=1; return 3;
-   case U8_TYPE:  size=8;  basic_type=0; return 4;
-   case U16_TYPE: size=16; basic_type=0; return 5;
-   case U32_TYPE: size=32; basic_type=0; return 6;
-   case U64_TYPE: size=64; basic_type=0; return 7;
-   case F16_TYPE: size=16; basic_type=-1; return 8;
-   case F32_TYPE: size=32; basic_type=-1; return 9;
-   case F64_TYPE: size=64; basic_type=-1; return 10;
-   case FF64_TYPE: size=64; basic_type=-1; return 10;
-   case PRED_TYPE: size=1; basic_type=2; return 11;
-   case B8_TYPE:  size=8;  basic_type=0; return 12;
-   case B16_TYPE: size=16; basic_type=0; return 13;
-   case B32_TYPE: size=32; basic_type=0; return 14;
-   case B64_TYPE: size=64; basic_type=0; return 15;
-   case BB64_TYPE: size=64; basic_type=0; return 15;
-   case BB128_TYPE: size=128; basic_type=0; return 16;
-   case TEXREF_TYPE: case SAMPLERREF_TYPE: case SURFREF_TYPE:
-      size=32; basic_type=3; return 16;
-   default: 
-      printf("ERROR ** type_decode() does not know about \"%s\"\n", decode_token(type) ); 
-      assert(0); 
+unsigned type_info_key::type_decode(int type, size_t &size, int &basic_type) {
+  switch (type) {
+    case S8_TYPE:
+      size = 8;
+      basic_type = 1;
+      return 0;
+    case S16_TYPE:
+      size = 16;
+      basic_type = 1;
+      return 1;
+    case S32_TYPE:
+      size = 32;
+      basic_type = 1;
+      return 2;
+    case S64_TYPE:
+      size = 64;
+      basic_type = 1;
+      return 3;
+    case U8_TYPE:
+      size = 8;
+      basic_type = 0;
+      return 4;
+    case U16_TYPE:
+      size = 16;
+      basic_type = 0;
+      return 5;
+    case U32_TYPE:
+      size = 32;
+      basic_type = 0;
+      return 6;
+    case U64_TYPE:
+      size = 64;
+      basic_type = 0;
+      return 7;
+    case F16_TYPE:
+      size = 16;
+      basic_type = -1;
+      return 8;
+    case F32_TYPE:
+      size = 32;
+      basic_type = -1;
+      return 9;
+    case F64_TYPE:
+      size = 64;
+      basic_type = -1;
+      return 10;
+    case FF64_TYPE:
+      size = 64;
+      basic_type = -1;
+      return 10;
+    case PRED_TYPE:
+      size = 1;
+      basic_type = 2;
+      return 11;
+    case B8_TYPE:
+      size = 8;
+      basic_type = 0;
+      return 12;
+    case B16_TYPE:
+      size = 16;
+      basic_type = 0;
+      return 13;
+    case B32_TYPE:
+      size = 32;
+      basic_type = 0;
+      return 14;
+    case B64_TYPE:
+      size = 64;
+      basic_type = 0;
+      return 15;
+    case BB64_TYPE:
+      size = 64;
+      basic_type = 0;
+      return 15;
+    case BB128_TYPE:
+      size = 128;
+      basic_type = 0;
+      return 16;
+    case TEXREF_TYPE:
+    case SAMPLERREF_TYPE:
+    case SURFREF_TYPE:
+      size = 32;
+      basic_type = 3;
+      return 16;
+    default:
+      printf("ERROR ** type_decode() does not know about \"%s\"\n",
+             decode_token(type));
+      assert(0);
       return 0xDEADBEEF;
-   }
+  }
 }
 
-arg_buffer_t copy_arg_to_buffer(ptx_thread_info * thread, operand_info actual_param_op, const symbol * formal_param)
-{
-   if( actual_param_op.is_reg() )  {
-      ptx_reg_t value = thread->get_reg(actual_param_op.get_symbol());
-      return arg_buffer_t(formal_param,actual_param_op,value);
-   } else if ( actual_param_op.is_param_local() ) {
-      unsigned size=formal_param->get_size_in_bytes();
-      addr_t frame_offset = actual_param_op.get_symbol()->get_address();
-      addr_t from_addr = thread->get_local_mem_stack_pointer() + frame_offset;
-      char buffer[1024];
-      assert(size<1024); 
-      thread->m_local_mem->read(from_addr,size,buffer);
-      return arg_buffer_t(formal_param,actual_param_op,buffer,size);
-   } else {
-      printf("GPGPU-Sim PTX: ERROR ** need to add support for this operand type in call/return\n");
-      abort();
-   }
+arg_buffer_t copy_arg_to_buffer(ptx_thread_info *thread,
+                                operand_info actual_param_op,
+                                const symbol *formal_param) {
+  if (actual_param_op.is_reg()) {
+    ptx_reg_t value = thread->get_reg(actual_param_op.get_symbol());
+    return arg_buffer_t(formal_param, actual_param_op, value);
+  } else if (actual_param_op.is_param_local()) {
+    unsigned size = formal_param->get_size_in_bytes();
+    addr_t frame_offset = actual_param_op.get_symbol()->get_address();
+    addr_t from_addr = thread->get_local_mem_stack_pointer() + frame_offset;
+    char buffer[1024];
+    assert(size < 1024);
+    thread->m_local_mem->read(from_addr, size, buffer);
+    return arg_buffer_t(formal_param, actual_param_op, buffer, size);
+  } else {
+    printf(
+        "GPGPU-Sim PTX: ERROR ** need to add support for this operand type in "
+        "call/return\n");
+    abort();
+  }
 }
 
-void copy_args_into_buffer_list( const ptx_instruction * pI, 
-                                 ptx_thread_info * thread, 
-                                 const function_info * target_func, 
-                                 arg_buffer_list_t &arg_values ) 
-{
-   unsigned n_return = target_func->has_return();
-   unsigned n_args = target_func->num_args();
-   for( unsigned arg=0; arg < n_args; arg ++ ) {
-      const operand_info &actual_param_op = pI->operand_lookup(n_return+1+arg);
-      const symbol *formal_param = target_func->get_arg(arg);
-      arg_values.push_back( copy_arg_to_buffer(thread, actual_param_op, formal_param) );
-   }
+void copy_args_into_buffer_list(const ptx_instruction *pI,
+                                ptx_thread_info *thread,
+                                const function_info *target_func,
+                                arg_buffer_list_t &arg_values) {
+  unsigned n_return = target_func->has_return();
+  unsigned n_args = target_func->num_args();
+  for (unsigned arg = 0; arg < n_args; arg++) {
+    const operand_info &actual_param_op =
+        pI->operand_lookup(n_return + 1 + arg);
+    const symbol *formal_param = target_func->get_arg(arg);
+    arg_values.push_back(
+        copy_arg_to_buffer(thread, actual_param_op, formal_param));
+  }
 }
 
-void copy_buffer_to_frame(ptx_thread_info * thread, const arg_buffer_t &a) 
-{
-   if( a.is_reg() ) {
-      ptx_reg_t value = a.get_reg();
-      operand_info dst_reg = operand_info(a.get_dst()); 
-      thread->set_reg(dst_reg.get_symbol(),value);
-   } else {  
-      const void *buffer = a.get_param_buffer();
-      size_t size = a.get_param_buffer_size();
-      const symbol *dst = a.get_dst();
-      addr_t frame_offset = dst->get_address();
-      addr_t to_addr = thread->get_local_mem_stack_pointer() + frame_offset;
-      thread->m_local_mem->write(to_addr,size,buffer,NULL,NULL);
-   }
+void copy_buffer_to_frame(ptx_thread_info *thread, const arg_buffer_t &a) {
+  if (a.is_reg()) {
+    ptx_reg_t value = a.get_reg();
+    operand_info dst_reg =
+        operand_info(a.get_dst(), thread->get_gpu()->gpgpu_ctx);
+    thread->set_reg(dst_reg.get_symbol(), value);
+  } else {
+    const void *buffer = a.get_param_buffer();
+    size_t size = a.get_param_buffer_size();
+    const symbol *dst = a.get_dst();
+    addr_t frame_offset = dst->get_address();
+    addr_t to_addr = thread->get_local_mem_stack_pointer() + frame_offset;
+    thread->m_local_mem->write(to_addr, size, buffer, NULL, NULL);
+  }
 }
 
-void copy_buffer_list_into_frame(ptx_thread_info * thread, arg_buffer_list_t &arg_values) 
-{
-   arg_buffer_list_t::iterator a;
-   for( a=arg_values.begin(); a != arg_values.end(); a++ ) {
-      copy_buffer_to_frame(thread, *a);
-   }
+void copy_buffer_list_into_frame(ptx_thread_info *thread,
+                                 arg_buffer_list_t &arg_values) {
+  arg_buffer_list_t::iterator a;
+  for (a = arg_values.begin(); a != arg_values.end(); a++) {
+    copy_buffer_to_frame(thread, *a);
+  }
 }
 
-
-
-static std::list<operand_info> check_operands( int opcode,
-                                        const std::list<int> &scalar_type,
-                                        const std::list<operand_info> &operands )
-{
-   static int g_warn_literal_operands_two_type_inst;
-    if( (opcode == CVT_OP) || (opcode == SET_OP) || (opcode == SLCT_OP) || (opcode == TEX_OP) || (opcode==MMA_OP) || (opcode == DP4A_OP)) {
-        // just make sure these do not have have const operands... 
-        if( !g_warn_literal_operands_two_type_inst ) {
-            std::list<operand_info>::const_iterator o;
-            for( o = operands.begin(); o != operands.end(); o++ ) {
-                const operand_info &op = *o;
-                if( op.is_literal() ) {
-                    printf("GPGPU-Sim PTX: PTX uses two scalar type intruction with literal operand.\n");
-                    g_warn_literal_operands_two_type_inst = 1;
-                }
-            }
+static std::list<operand_info> check_operands(
+    int opcode, const std::list<int> &scalar_type,
+    const std::list<operand_info> &operands, gpgpu_context *ctx) {
+  static int g_warn_literal_operands_two_type_inst;
+  if ((opcode == CVT_OP) || (opcode == SET_OP) || (opcode == SLCT_OP) ||
+      (opcode == TEX_OP) || (opcode == MMA_OP) || (opcode == DP4A_OP) ||
+      (opcode == VMIN_OP) || (opcode == VMAX_OP)) {
+    // just make sure these do not have have const operands...
+    if (!g_warn_literal_operands_two_type_inst) {
+      std::list<operand_info>::const_iterator o;
+      for (o = operands.begin(); o != operands.end(); o++) {
+        const operand_info &op = *o;
+        if (op.is_literal()) {
+          printf(
+              "GPGPU-Sim PTX: PTX uses two scalar type intruction with literal "
+              "operand.\n");
+          g_warn_literal_operands_two_type_inst = 1;
         }
-    } else {
-        assert( scalar_type.size() < 2 );
-        if( scalar_type.size() == 1 ) {
-            std::list<operand_info> result;
-            int inst_type = scalar_type.front();
-            std::list<operand_info>::const_iterator o;
-            for( o = operands.begin(); o != operands.end(); o++ ) {
-                const operand_info &op = *o;
-                if( op.is_literal() ) {
-                    if( (op.get_type() == double_op_t) && (inst_type == F32_TYPE) ) {
-                        ptx_reg_t v = op.get_literal_value();
-                        float u = (float)v.f64;
-                        operand_info n(u);
-                        result.push_back(n);
-                    } else {
-                        result.push_back(op);
-                    }
-                } else {
-                        result.push_back(op);
-                }
-            }
-            return result;
-        } 
+      }
+    }
+  } else {
+    assert(scalar_type.size() < 2);
+    if (scalar_type.size() == 1) {
+      std::list<operand_info> result;
+      int inst_type = scalar_type.front();
+      std::list<operand_info>::const_iterator o;
+      for (o = operands.begin(); o != operands.end(); o++) {
+        const operand_info &op = *o;
+        if (op.is_literal()) {
+          if ((op.get_type() == double_op_t) && (inst_type == F32_TYPE)) {
+            ptx_reg_t v = op.get_literal_value();
+            float u = (float)v.f64;
+            operand_info n(u, ctx);
+            result.push_back(n);
+          } else {
+            result.push_back(op);
+          }
+        } else {
+          result.push_back(op);
+        }
+      }
+      return result;
     }
-    return operands;
+  }
+  return operands;
 }
 
-
-ptx_instruction::ptx_instruction( int opcode, 
-                                  const symbol *pred, 
-                                  int neg_pred, 
-                                  int pred_mod,
-                                  symbol *label,
-                                  const std::list<operand_info> &operands, 
-                                  const operand_info &return_var,
-                                  const std::list<int> &options, 
-                                  const std::list<int> &wmma_options, 
-                                  const std::list<int> &scalar_type,
-                                  memory_space_t space_spec,
-                                  const char *file, 
-                                  unsigned line,
-                                  const char *source,
-                                  const core_config *config ) : warp_inst_t(config)
-{
-   m_uid = ++g_num_ptx_inst_uid;
-   m_PC = 0;
-   m_opcode = opcode;
-   m_pred = pred;
-   m_neg_pred = neg_pred;
-   m_pred_mod = pred_mod;
-   m_label = label;
-   const std::list<operand_info> checked_operands = check_operands(opcode,scalar_type,operands);
-   m_operands.insert(m_operands.begin(), checked_operands.begin(), checked_operands.end() );
-   m_return_var = return_var;
-   m_options = options;
-   m_wmma_options = wmma_options;
-   m_wide = false;
-   m_hi = false;
-   m_lo = false;
-   m_uni = false;
-   m_exit = false;
-   m_abs = false;
-   m_neg = false;
-   m_to_option = false;
-   m_cache_option = 0;
-   m_rounding_mode = RN_OPTION;
-   m_compare_op = -1;
-   m_saturation_mode = 0;
-   m_geom_spec = 0;
-   m_vector_spec = 0;
-   m_atomic_spec = 0;
-   m_membar_level = 0;
-   m_inst_size = 8; // bytes
-   int rr=0;
-   std::list<int>::const_iterator i;
-   unsigned n=1;
-   for ( i=wmma_options.begin(); i!= wmma_options.end(); i++, n++ ) {
-      int last_ptx_inst_option = *i;
-      switch ( last_ptx_inst_option ) {
-      		case SYNC_OPTION:
-      		case LOAD_A:
-      		case LOAD_B:
-      		case LOAD_C:
-      		case STORE_D:
-      		case MMA:
-      		  m_wmma_type=last_ptx_inst_option;
-      		  break;
-      		case ROW:
-      		case COL:
-      		  m_wmma_layout[rr++]=last_ptx_inst_option;
-      		  break;
-      		case M16N16K16:
-			break;
-      		default:
-      		   assert(0);
-      		   break;
-	}
-   }
-   rr=0;
-   n=1;
-   for ( i=options.begin(); i!= options.end(); i++, n++ ) {
-      int last_ptx_inst_option = *i;
-      switch ( last_ptx_inst_option ) {
+ptx_instruction::ptx_instruction(
+    int opcode, const symbol *pred, int neg_pred, int pred_mod, symbol *label,
+    const std::list<operand_info> &operands, const operand_info &return_var,
+    const std::list<int> &options, const std::list<int> &wmma_options,
+    const std::list<int> &scalar_type, memory_space_t space_spec,
+    const char *file, unsigned line, const char *source,
+    const core_config *config, gpgpu_context *ctx)
+    : warp_inst_t(config), m_return_var(ctx) {
+  gpgpu_ctx = ctx;
+  m_uid = ++(ctx->g_num_ptx_inst_uid);
+  m_PC = 0;
+  m_opcode = opcode;
+  m_pred = pred;
+  m_neg_pred = neg_pred;
+  m_pred_mod = pred_mod;
+  m_label = label;
+  const std::list<operand_info> checked_operands =
+      check_operands(opcode, scalar_type, operands, ctx);
+  m_operands.insert(m_operands.begin(), checked_operands.begin(),
+                    checked_operands.end());
+  m_return_var = return_var;
+  m_options = options;
+  m_wmma_options = wmma_options;
+  m_wide = false;
+  m_hi = false;
+  m_lo = false;
+  m_uni = false;
+  m_exit = false;
+  m_abs = false;
+  m_neg = false;
+  m_to_option = false;
+  m_cache_option = 0;
+  m_rounding_mode = RN_OPTION;
+  m_compare_op = -1;
+  m_saturation_mode = 0;
+  m_geom_spec = 0;
+  m_vector_spec = 0;
+  m_atomic_spec = 0;
+  m_membar_level = 0;
+  m_inst_size = 8;  // bytes
+  int rr = 0;
+  std::list<int>::const_iterator i;
+  unsigned n = 1;
+  for (i = wmma_options.begin(); i != wmma_options.end(); i++, n++) {
+    int last_ptx_inst_option = *i;
+    switch (last_ptx_inst_option) {
+      case SYNC_OPTION:
+      case LOAD_A:
+      case LOAD_B:
+      case LOAD_C:
+      case STORE_D:
+      case MMA:
+        m_wmma_type = last_ptx_inst_option;
+        break;
+      case ROW:
+      case COL:
+        m_wmma_layout[rr++] = last_ptx_inst_option;
+        break;
+      case M16N16K16:
+      case M32N8K16:
+      case M8N32K16:
+        break;
+      default:
+        assert(0);
+        break;
+    }
+  }
+  rr = 0;
+  n = 1;
+  for (i = options.begin(); i != options.end(); i++, n++) {
+    int last_ptx_inst_option = *i;
+    switch (last_ptx_inst_option) {
       case SYNC_OPTION:
       case ARRIVE_OPTION:
       case RED_OPTION:
-          m_barrier_op = last_ptx_inst_option;
-          break;
+        m_barrier_op = last_ptx_inst_option;
+        break;
       case EQU_OPTION:
       case NEU_OPTION:
       case LTU_OPTION:
@@ -1176,16 +1279,16 @@ ptx_instruction::ptx_instruction( int opcode,
       case GE_OPTION:
       case LS_OPTION:
       case HS_OPTION:
-         m_compare_op = last_ptx_inst_option;
-         break;
+        m_compare_op = last_ptx_inst_option;
+        break;
       case NUM_OPTION:
       case NAN_OPTION:
-    	  m_compare_op = last_ptx_inst_option;
+        m_compare_op = last_ptx_inst_option;
         // assert(0); // finish this
-         break;
+        break;
       case SAT_OPTION:
-         m_saturation_mode = 1;
-         break;
+        m_saturation_mode = 1;
+        break;
       case RNI_OPTION:
       case RZI_OPTION:
       case RMI_OPTION:
@@ -1194,38 +1297,39 @@ ptx_instruction::ptx_instruction( int opcode,
       case RZ_OPTION:
       case RM_OPTION:
       case RP_OPTION:
-         m_rounding_mode = last_ptx_inst_option;
-         break;
+        m_rounding_mode = last_ptx_inst_option;
+        break;
       case HI_OPTION:
-         m_compare_op = last_ptx_inst_option;
-         m_hi = true;
-         assert( !m_lo ); 
-         assert( !m_wide );
-         break;
+        m_compare_op = last_ptx_inst_option;
+        m_hi = true;
+        assert(!m_lo);
+        assert(!m_wide);
+        break;
       case LO_OPTION:
-         m_compare_op = last_ptx_inst_option;
-         m_lo = true;
-         assert( !m_hi );
-         assert( !m_wide );
-         break;
+        m_compare_op = last_ptx_inst_option;
+        m_lo = true;
+        assert(!m_hi);
+        assert(!m_wide);
+        break;
       case WIDE_OPTION:
-         m_wide = true;
-         assert( !m_lo ); 
-         assert( !m_hi ); 
-         break;
+        m_wide = true;
+        assert(!m_lo);
+        assert(!m_hi);
+        break;
       case UNI_OPTION:
-         m_uni = true; // don't care... < now we DO care when constructing flowgraph>
-         break;
+        m_uni = true;  // don't care... < now we DO care when constructing
+                       // flowgraph>
+        break;
       case GEOM_MODIFIER_1D:
       case GEOM_MODIFIER_2D:
       case GEOM_MODIFIER_3D:
-         m_geom_spec = last_ptx_inst_option;
-         break;
+        m_geom_spec = last_ptx_inst_option;
+        break;
       case V2_TYPE:
       case V3_TYPE:
       case V4_TYPE:
-         m_vector_spec = last_ptx_inst_option;
-         break;
+        m_vector_spec = last_ptx_inst_option;
+        break;
       case ATOMIC_AND:
       case ATOMIC_OR:
       case ATOMIC_XOR:
@@ -1236,219 +1340,229 @@ ptx_instruction::ptx_instruction( int opcode,
       case ATOMIC_DEC:
       case ATOMIC_MIN:
       case ATOMIC_MAX:
-         m_atomic_spec = last_ptx_inst_option;
-         break;
+        m_atomic_spec = last_ptx_inst_option;
+        break;
       case APPROX_OPTION:
-         break;
+        break;
       case FULL_OPTION:
-         break;
+        break;
       case ANY_OPTION:
-         m_vote_mode = vote_any;
-         break;
+        m_vote_mode = vote_any;
+        break;
       case ALL_OPTION:
-         m_vote_mode = vote_all;
-         break;
+        m_vote_mode = vote_all;
+        break;
       case BALLOT_OPTION:
-         m_vote_mode = vote_ballot;
-         break;
+        m_vote_mode = vote_ballot;
+        break;
       case GLOBAL_OPTION:
-         m_membar_level = GLOBAL_OPTION;
-         break;
+        m_membar_level = GLOBAL_OPTION;
+        break;
       case CTA_OPTION:
-         m_membar_level = CTA_OPTION;
-         break;
+        m_membar_level = CTA_OPTION;
+        break;
       case SYS_OPTION:
-         m_membar_level = SYS_OPTION;
-         break;
+        m_membar_level = SYS_OPTION;
+        break;
       case FTZ_OPTION:
-         break;
+        break;
       case EXIT_OPTION:
-         m_exit = true;
-         break;
+        m_exit = true;
+        break;
       case ABS_OPTION:
-         m_abs = true;
-         break;
+        m_abs = true;
+        break;
       case NEG_OPTION:
-         m_neg = true;
-         break;
+        m_neg = true;
+        break;
       case TO_OPTION:
-         m_to_option = true;
-         break;
-      case CA_OPTION: case CG_OPTION: case CS_OPTION: case LU_OPTION: case CV_OPTION:
-         m_cache_option = last_ptx_inst_option;
-         break;
+        m_to_option = true;
+        break;
+      case CA_OPTION:
+      case CG_OPTION:
+      case CS_OPTION:
+      case LU_OPTION:
+      case CV_OPTION:
+      case WB_OPTION: 
+      case WT_OPTION:
+        m_cache_option = last_ptx_inst_option;
+        break;
       case HALF_OPTION:
-         m_inst_size = 4; // bytes
-         break;
+        m_inst_size = 4;  // bytes
+        break;
       case EXTP_OPTION:
-             break;
+        break;
       case NC_OPTION:
-             m_cache_option = last_ptx_inst_option;
-             break;
+        m_cache_option = last_ptx_inst_option;
+        break;
       case UP_OPTION:
       case DOWN_OPTION:
       case BFLY_OPTION:
       case IDX_OPTION:
-		  m_shfl_op = last_ptx_inst_option;
-		  break;
+        m_shfl_op = last_ptx_inst_option;
+        break;
       case PRMT_F4E_MODE:
       case PRMT_B4E_MODE:
       case PRMT_RC8_MODE:
       case PRMT_ECL_MODE:
       case PRMT_ECR_MODE:
       case PRMT_RC16_MODE:
-		  m_prmt_op = last_ptx_inst_option;
-		  break;
+        m_prmt_op = last_ptx_inst_option;
+        break;
       default:
-         assert(0);
-         break;
-      }
-   }
-   m_scalar_type = scalar_type;
-   m_space_spec = space_spec;
-   if( ( opcode == ST_OP || opcode == LD_OP || opcode == LDU_OP ) && (space_spec == undefined_space) ) {
-      m_space_spec = generic_space;
-   } 
-   for( std::vector<operand_info>::const_iterator i=m_operands.begin(); i!=m_operands.end(); ++i) {
-       const operand_info &op = *i;
-       if( op.get_addr_space() != undefined_space ) 
-           m_space_spec = op.get_addr_space(); // TODO: can have more than one memory space for ptxplus (g8x) inst
-   }
-   if( opcode == TEX_OP ) 
-       m_space_spec = tex_space;
-   
-   m_source_file = file?file:"<unknown>";
-   m_source_line = line;
-   m_source = source;
-   // Trim tabs
-   m_source.erase( std::remove( m_source.begin(), m_source.end(), '\t' ), m_source.end() );
-
-   if (opcode == CALL_OP) {
-       const operand_info &target  = func_addr();
-       assert( target.is_function_address() );
-       const symbol *func_addr = target.get_symbol();
-       const function_info *target_func = func_addr->get_pc();
-       std::string fname = target_func->get_name();
-
-       if (fname =="vprintf"){
-           m_is_printf = true;
-       }
-       if(fname == "cudaStreamCreateWithFlags")
-           m_is_cdp = 1;
-       if(fname == "cudaGetParameterBufferV2")
-           m_is_cdp = 2;
-       if(fname == "cudaLaunchDeviceV2")
-           m_is_cdp = 4;
-
-   }
+        assert(0);
+        break;
+    }
+  }
+  m_scalar_type = scalar_type;
+  m_space_spec = space_spec;
+  if ((opcode == ST_OP || opcode == LD_OP || opcode == LDU_OP) &&
+      (space_spec == undefined_space)) {
+    m_space_spec = generic_space;
+  }
+  for (std::vector<operand_info>::const_iterator i = m_operands.begin();
+       i != m_operands.end(); ++i) {
+    const operand_info &op = *i;
+    if (op.get_addr_space() != undefined_space)
+      m_space_spec =
+          op.get_addr_space();  // TODO: can have more than one memory space for
+                                // ptxplus (g8x) inst
+  }
+  if (opcode == TEX_OP) m_space_spec = tex_space;
+
+  m_source_file = file ? file : "<unknown>";
+  m_source_line = line;
+  m_source = source;
+  // Trim tabs
+  m_source.erase(std::remove(m_source.begin(), m_source.end(), '\t'),
+                 m_source.end());
+
+  if (opcode == CALL_OP) {
+    const operand_info &target = func_addr();
+    assert(target.is_function_address());
+    const symbol *func_addr = target.get_symbol();
+    const function_info *target_func = func_addr->get_pc();
+    std::string fname = target_func->get_name();
+
+    if (fname == "vprintf") {
+      m_is_printf = true;
+    }
+    if (fname == "cudaStreamCreateWithFlags") m_is_cdp = 1;
+    if (fname == "cudaGetParameterBufferV2") m_is_cdp = 2;
+    if (fname == "cudaLaunchDeviceV2") m_is_cdp = 4;
+  }
 }
 
-void ptx_instruction::print_insn() const
-{
-   print_insn(stdout);
-   fflush(stdout);
+void ptx_instruction::print_insn() const {
+  print_insn(stdout);
+  fflush(stdout);
 }
 
-void ptx_instruction::print_insn( FILE *fp ) const
-{
-    fprintf( fp, "%s", to_string().c_str() );
+void ptx_instruction::print_insn(FILE *fp) const {
+  fprintf(fp, "%s", to_string().c_str());
 }
 
-std::string ptx_instruction::to_string() const
-{
-   char buf[ STR_SIZE ];
-   unsigned used_bytes = 0;
-   if( !is_label() ) {
-      used_bytes += snprintf( buf + used_bytes, STR_SIZE - used_bytes, " PC=0x%03x ", m_PC );
-   } else {
-      used_bytes += snprintf( buf + used_bytes, STR_SIZE - used_bytes, "                " );
-   }
-   used_bytes += snprintf( buf + used_bytes, STR_SIZE - used_bytes,
-                           "(%s:%d) %s",
-                           m_source_file.c_str(), m_source_line,
-                           m_source.c_str() );
-   return std::string( buf );
+std::string ptx_instruction::to_string() const {
+  char buf[STR_SIZE];
+  unsigned used_bytes = 0;
+  if (!is_label()) {
+    used_bytes +=
+        snprintf(buf + used_bytes, STR_SIZE - used_bytes, " PC=0x%03x ", m_PC);
+  } else {
+    used_bytes +=
+        snprintf(buf + used_bytes, STR_SIZE - used_bytes, "                ");
+  }
+  used_bytes +=
+      snprintf(buf + used_bytes, STR_SIZE - used_bytes, "(%s:%d) %s",
+               m_source_file.c_str(), m_source_line, m_source.c_str());
+  return std::string(buf);
+}
+operand_info ptx_instruction::get_pred() const {
+  return operand_info(m_pred, gpgpu_ctx);
 }
 
-unsigned function_info::sm_next_uid = 1;
-
-function_info::function_info(int entry_point ) 
-{
-   m_uid = sm_next_uid++;
-   m_entry_point = (entry_point==1)?true:false;
-   m_extern = (entry_point==2)?true:false;
-   num_reconvergence_pairs = 0;
-   m_symtab = NULL;
-   m_assembled = false;
-   m_return_var_sym = NULL; 
-   m_kernel_info.cmem = 0;
-   m_kernel_info.lmem = 0;
-   m_kernel_info.regs = 0;
-   m_kernel_info.smem = 0;
-   m_local_mem_framesize = 0;
-   m_args_aligned_size = -1;
-   pdom_done = false; //initialize it to false
+function_info::function_info(int entry_point, gpgpu_context *ctx) {
+  gpgpu_ctx = ctx;
+  m_uid = (gpgpu_ctx->function_info_sm_next_uid)++;
+  m_entry_point = (entry_point == 1) ? true : false;
+  m_extern = (entry_point == 2) ? true : false;
+  num_reconvergence_pairs = 0;
+  m_symtab = NULL;
+  m_assembled = false;
+  m_return_var_sym = NULL;
+  m_kernel_info.cmem = 0;
+  m_kernel_info.lmem = 0;
+  m_kernel_info.regs = 0;
+  m_kernel_info.smem = 0;
+  m_local_mem_framesize = 0;
+  m_args_aligned_size = -1;
+  pdom_done = false;  // initialize it to false
 }
 
-unsigned function_info::print_insn( unsigned pc, FILE * fp ) const
-{
-   unsigned inst_size=1; // return offset to next instruction or 1 if unknown
-   unsigned index = pc - m_start_PC;
-   char command[1024];
-   char buffer[1024];
-   memset(command, 0, 1024);
-   memset(buffer, 0, 1024);
-   snprintf(command,1024,"c++filt -p %s",m_name.c_str());
-   FILE *p = popen(command,"r");
-   buffer[0]=0;
-   fgets(buffer, 1023, p);
-   // Remove trailing "\n" in buffer
-   char *c;
-   if ((c=strchr(buffer, '\n')) != NULL) *c = '\0';
-   fprintf(fp,"%s",buffer);
-   if ( index >= m_instr_mem_size ) {
-      fprintf(fp, "<past last instruction (max pc=%u)>", m_start_PC + m_instr_mem_size - 1 );
-   } else {
-      if ( m_instr_mem[index] != NULL ) {
-         m_instr_mem[index]->print_insn(fp);
-         inst_size = m_instr_mem[index]->isize;
-      } else
-         fprintf(fp, "<no instruction at pc = %u>", pc );
-   }
-   pclose(p);
-   return inst_size;
+unsigned function_info::print_insn(unsigned pc, FILE *fp) const {
+  unsigned inst_size = 1;  // return offset to next instruction or 1 if unknown
+  unsigned index = pc - m_start_PC;
+  char command[1024];
+  char buffer[1024];
+  memset(command, 0, 1024);
+  memset(buffer, 0, 1024);
+  snprintf(command, 1024, "c++filt -p %s", m_name.c_str());
+  FILE *p = popen(command, "r");
+  buffer[0] = 0;
+  assert(fgets(buffer, 1023, p) != NULL);
+  // Remove trailing "\n" in buffer
+  char *c;
+  if ((c = strchr(buffer, '\n')) != NULL) *c = '\0';
+  fprintf(fp, "%s", buffer);
+  if (index >= m_instr_mem_size) {
+    fprintf(fp, "<past last instruction (max pc=%u)>",
+            m_start_PC + m_instr_mem_size - 1);
+  } else {
+    if (m_instr_mem[index] != NULL) {
+      m_instr_mem[index]->print_insn(fp);
+      inst_size = m_instr_mem[index]->isize;
+    } else
+      fprintf(fp, "<no instruction at pc = %u>", pc);
+  }
+  pclose(p);
+  return inst_size;
 }
 
-std::string function_info::get_insn_str( unsigned pc ) const
-{
-   unsigned index = pc - m_start_PC;
-   if ( index >= m_instr_mem_size ) {
+std::string function_info::get_insn_str(unsigned pc) const {
+  unsigned index = pc - m_start_PC;
+  if (index >= m_instr_mem_size) {
+    char buff[STR_SIZE];
+    buff[STR_SIZE - 1] = '\0';
+    snprintf(buff, STR_SIZE, "<past last instruction (max pc=%u)>",
+             m_start_PC + m_instr_mem_size - 1);
+    return std::string(buff);
+  } else {
+    if (m_instr_mem[index] != NULL) {
+      return m_instr_mem[index]->to_string();
+    } else {
       char buff[STR_SIZE];
-      buff[STR_SIZE-1] = '\0';
-      snprintf(buff, STR_SIZE, "<past last instruction (max pc=%u)>", m_start_PC + m_instr_mem_size - 1 );
+      buff[STR_SIZE - 1] = '\0';
+      snprintf(buff, STR_SIZE, "<no instruction at pc = %u>", pc);
       return std::string(buff);
-   } else {
-      if ( m_instr_mem[index] != NULL ) {
-         return m_instr_mem[index]->to_string();
-      } else {
-         char buff[STR_SIZE];
-         buff[STR_SIZE-1] = '\0';
-         snprintf(buff, STR_SIZE, "<no instruction at pc = %u>", pc );
-         return std::string(buff);
-      }
-   }
+    }
+  }
 }
 
-void gpgpu_ptx_assemble( std::string kname, void *kinfo )
-{
-    function_info *func_info = (function_info *)kinfo;
-    if((function_info *)kinfo == NULL) {
-       printf("GPGPU-Sim PTX: Warning - missing function definition \'%s\'\n", kname.c_str());
-       return;
-    }
-    if( func_info->is_extern() ) {
-       printf("GPGPU-Sim PTX: skipping assembly for extern declared function \'%s\'\n", func_info->get_name().c_str() );
-       return;
-    }
-    func_info->ptx_assemble();
+void gpgpu_ptx_assemble(std::string kname, void *kinfo) {
+  function_info *func_info = (function_info *)kinfo;
+  if ((function_info *)kinfo == NULL) {
+    printf("GPGPU-Sim PTX: Warning - missing function definition \'%s\'\n",
+           kname.c_str());
+    return;
+  }
+  if (func_info->is_extern()) {
+    printf(
+        "GPGPU-Sim PTX: skipping assembly for extern declared function "
+        "\'%s\'\n",
+        func_info->get_name().c_str());
+    return;
+  }
+  func_info->ptx_assemble();
 }
+
+
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.h
index c67cea6ca5..9140eb6997 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_ir.h
@@ -28,285 +28,278 @@
 #ifndef ptx_ir_INCLUDED
 #define ptx_ir_INCLUDED
 
-#ifndef LIBCUDA
 #include "../abstract_hardware_model.h"
-#else
-#include "../libcuda/abstract_hardware_model.h"
-#endif
 
-#include <cstdlib>
-#include <cstring>
-#include <string>
+#include <assert.h>
+//#include <cstdlib>
+#include <string.h>
 #include <list>
 #include <map>
+#include <string>
 #include <vector>
-#include <assert.h>
 
 //#include "ptx.tab.h"
 #include "ptx_sim.h"
 
 #include "memory.h"
 
+class gpgpu_context;
+
 class type_info_key {
-public:
-   type_info_key()
-   {
-      m_is_non_arch_reg = false;
-      m_init = false;
-   }
-   type_info_key( memory_space_t space_spec, int scalar_type_spec, int vector_spec, int alignment_spec, int extern_spec, int array_dim )
-   {
-      m_is_non_arch_reg = false;
-      m_init = true;
-      m_space_spec = space_spec; 
-      m_scalar_type_spec = scalar_type_spec;
-      m_vector_spec = vector_spec;
-      m_alignment_spec = alignment_spec;
-      m_extern_spec = extern_spec;
-      m_array_dim = array_dim;
-      m_is_function = 0;
-   } 
-   void set_is_func()
-   { 
-      assert(!m_init);
-      m_init = true;
-      m_space_spec = undefined_space; 
-      m_scalar_type_spec = 0;
-      m_vector_spec = 0;
-      m_alignment_spec = 0;
-      m_extern_spec = 0;
-      m_array_dim = 0; 
-      m_is_function = 1;
-   }
-
-   void set_array_dim( int array_dim ) { m_array_dim = array_dim; }
-   int get_array_dim() const { assert(m_init); return m_array_dim; }
-   void set_is_non_arch_reg() { m_is_non_arch_reg = true;  }
-
-   bool is_non_arch_reg() const { return m_is_non_arch_reg; }
-   bool is_reg() const { return m_space_spec == reg_space;} 
-   bool is_param_kernel() const { return m_space_spec == param_space_kernel;}
-   bool is_param_local() const { return m_space_spec == param_space_local; }
-   bool is_param_unclassified() const { return m_space_spec == param_space_unclassified; }
-   bool is_global() const { return m_space_spec == global_space;}
-   bool is_local() const { return m_space_spec == local_space;}
-   bool is_shared() const { return m_space_spec == shared_space;}
-   bool is_const() const { return m_space_spec.get_type() == const_space;}
-   bool is_tex() const { return m_space_spec == tex_space;}
-   bool is_func_addr() const { return m_is_function?true:false; }
-   int  scalar_type() const { return m_scalar_type_spec;}
-   int  get_alignment_spec() const { return m_alignment_spec;}
-   unsigned type_decode( size_t &size, int &t ) const;
-   static unsigned type_decode( int type, size_t &size, int &t );
-   memory_space_t get_memory_space() const { return m_space_spec; }
-private:
-   bool m_init;
-   memory_space_t m_space_spec; 
-   int m_scalar_type_spec;
-   int m_vector_spec;
-   int m_alignment_spec;
-   int m_extern_spec;
-   int m_array_dim;
-   int m_is_function;
-   bool m_is_non_arch_reg;
-
-   friend struct type_info_key_compare;
+ public:
+  type_info_key() {
+    m_is_non_arch_reg = false;
+    m_init = false;
+  }
+  type_info_key(memory_space_t space_spec, int scalar_type_spec,
+                int vector_spec, int alignment_spec, int extern_spec,
+                int array_dim) {
+    m_is_non_arch_reg = false;
+    m_init = true;
+    m_space_spec = space_spec;
+    m_scalar_type_spec = scalar_type_spec;
+    m_vector_spec = vector_spec;
+    m_alignment_spec = alignment_spec;
+    m_extern_spec = extern_spec;
+    m_array_dim = array_dim;
+    m_is_function = 0;
+  }
+  void set_is_func() {
+    assert(!m_init);
+    m_init = true;
+    m_space_spec = undefined_space;
+    m_scalar_type_spec = 0;
+    m_vector_spec = 0;
+    m_alignment_spec = 0;
+    m_extern_spec = 0;
+    m_array_dim = 0;
+    m_is_function = 1;
+  }
+
+  void set_array_dim(int array_dim) { m_array_dim = array_dim; }
+  int get_array_dim() const {
+    assert(m_init);
+    return m_array_dim;
+  }
+  void set_is_non_arch_reg() { m_is_non_arch_reg = true; }
+
+  bool is_non_arch_reg() const { return m_is_non_arch_reg; }
+  bool is_reg() const { return m_space_spec == reg_space; }
+  bool is_param_kernel() const { return m_space_spec == param_space_kernel; }
+  bool is_param_local() const { return m_space_spec == param_space_local; }
+  bool is_param_unclassified() const {
+    return m_space_spec == param_space_unclassified;
+  }
+  bool is_global() const { return m_space_spec == global_space; }
+  bool is_local() const { return m_space_spec == local_space; }
+  bool is_shared() const { return m_space_spec == shared_space; }
+  bool is_const() const { return m_space_spec.get_type() == const_space; }
+  bool is_tex() const { return m_space_spec == tex_space; }
+  bool is_func_addr() const { return m_is_function ? true : false; }
+  int scalar_type() const { return m_scalar_type_spec; }
+  int get_alignment_spec() const { return m_alignment_spec; }
+  unsigned type_decode(size_t &size, int &t) const;
+  static unsigned type_decode(int type, size_t &size, int &t);
+  memory_space_t get_memory_space() const { return m_space_spec; }
+
+ private:
+  bool m_init;
+  memory_space_t m_space_spec;
+  int m_scalar_type_spec;
+  int m_vector_spec;
+  int m_alignment_spec;
+  int m_extern_spec;
+  int m_array_dim;
+  int m_is_function;
+  bool m_is_non_arch_reg;
+
+  friend struct type_info_key_compare;
 };
 
 class symbol_table;
 
 struct type_info_key_compare {
-   bool operator()( const type_info_key &a, const type_info_key &b ) const
-   {
-      assert( a.m_init && b.m_init );
-      if ( a.m_space_spec < b.m_space_spec ) return true;
-      if ( a.m_scalar_type_spec < b.m_scalar_type_spec ) return true;
-      if ( a.m_vector_spec < b.m_vector_spec ) return true;
-      if ( a.m_alignment_spec < b.m_alignment_spec ) return true;
-      if ( a.m_extern_spec < b.m_extern_spec ) return true;
-      if ( a.m_array_dim < b.m_array_dim ) return true;
-      if ( a.m_is_function < b.m_is_function ) return true;
-
-      return false;
-   }
+  bool operator()(const type_info_key &a, const type_info_key &b) const {
+    assert(a.m_init && b.m_init);
+    if (a.m_space_spec < b.m_space_spec) return true;
+    if (a.m_scalar_type_spec < b.m_scalar_type_spec) return true;
+    if (a.m_vector_spec < b.m_vector_spec) return true;
+    if (a.m_alignment_spec < b.m_alignment_spec) return true;
+    if (a.m_extern_spec < b.m_extern_spec) return true;
+    if (a.m_array_dim < b.m_array_dim) return true;
+    if (a.m_is_function < b.m_is_function) return true;
+
+    return false;
+  }
 };
 
 class type_info {
-public:
-   type_info( symbol_table *scope, type_info_key t )
-   {
-      m_type_info = t;
-   }
-   const type_info_key &get_key() const { return m_type_info;}
-
-private:
-   symbol_table *m_scope;
-   type_info_key m_type_info;
+ public:
+  type_info(symbol_table *scope, type_info_key t) { m_type_info = t; }
+  const type_info_key &get_key() const { return m_type_info; }
+
+ private:
+  symbol_table *m_scope;
+  type_info_key m_type_info;
 };
 
 enum operand_type {
-   reg_t, vector_t, builtin_t, address_t, memory_t, float_op_t, double_op_t, int_t, 
-   unsigned_t, symbolic_t, label_t, v_reg_t, v_float_op_t, v_double_op_t,
-   v_int_t, v_unsigned_t, undef_t
+  reg_t,
+  vector_t,
+  builtin_t,
+  address_t,
+  memory_t,
+  float_op_t,
+  double_op_t,
+  int_t,
+  unsigned_t,
+  symbolic_t,
+  label_t,
+  v_reg_t,
+  v_float_op_t,
+  v_double_op_t,
+  v_int_t,
+  v_unsigned_t,
+  undef_t
 };
 
 class operand_info;
 
 class symbol {
-public:
-   symbol( const char *name, const type_info *type, const char *location, unsigned size ) 
-   {
-      m_uid = get_uid();
-      m_name = name;
-      m_decl_location = location;
-      m_type = type;
-      m_size = size;
-      m_address_valid = false;
-      m_is_label = false;
-      m_is_shared = false;
-      m_is_const = false;
-      m_is_global = false;
-      m_is_local = false;
-      m_is_param_local = false;
-      m_is_param_kernel = false;
-      m_is_tex = false;
-      m_is_func_addr = false;
-      m_reg_num_valid = false;
-      m_function = NULL;
-      m_reg_num=(unsigned)-1;
-      m_arch_reg_num=(unsigned)-1;
-      m_address=(unsigned)-1;
-      m_initializer.clear();
-      if ( type ) m_is_shared = type->get_key().is_shared();
-      if ( type ) m_is_const = type->get_key().is_const();
-      if ( type ) m_is_global = type->get_key().is_global();
-      if ( type ) m_is_local = type->get_key().is_local();
-      if ( type ) m_is_param_local = type->get_key().is_param_local();
-      if ( type ) m_is_param_kernel = type->get_key().is_param_kernel();
-      if ( type ) m_is_tex = type->get_key().is_tex();
-      if ( type ) m_is_func_addr = type->get_key().is_func_addr();
-   }
-   unsigned get_size_in_bytes() const
-   {
-      return m_size;
-   }
-   const std::string &name() const { return m_name;}
-   const std::string &decl_location() const { return m_decl_location;} 
-   const type_info *type() const { return m_type;}
-   addr_t get_address() const 
-   { 
-      assert( m_is_label || !m_type->get_key().is_reg() ); // todo : other assertions
-      assert( m_address_valid );
-      return m_address;
-   }
-   function_info *get_pc() const
-   {
-      return m_function;
-   }
-   void set_regno( unsigned regno, unsigned arch_regno )
-   {
-      m_reg_num_valid = true;
-      m_reg_num = regno;
-      m_arch_reg_num = arch_regno;
-   }
-
-   void set_address( addr_t addr )
-   {
-      m_address_valid = true;
-      m_address = addr;
-   }
-   void set_label_address( addr_t addr)
-   {
-      m_address_valid = true;
-      m_address = addr;
-      m_is_label = true;
-   }
-   void set_function( function_info *func )
-   {
-      m_function = func;
-      m_is_func_addr = true; 
-   }
-
-   bool is_label() const { return m_is_label;}
-   bool is_shared() const { return m_is_shared;}
-   bool is_sstarr() const { return m_is_sstarr;}
-   bool is_const() const { return m_is_const;}
-   bool is_global() const { return m_is_global;}
-   bool is_local() const { return m_is_local;}
-   bool is_param_local() const { return m_is_param_local; }
-   bool is_param_kernel() const { return m_is_param_kernel; }
-   bool is_tex() const { return m_is_tex;}
-   bool is_func_addr() const { return m_is_func_addr; }
-   bool is_reg() const
-   {
-       if ( m_type == NULL ) {
-           return false;
-       }
-      return m_type->get_key().is_reg(); 
-   }
-   bool is_non_arch_reg() const
-   {
-       if ( m_type == NULL ) {
-           return false;
-       }
-      return m_type->get_key().is_non_arch_reg(); 
-   }
-
-   void add_initializer( const std::list<operand_info> &init );
-   bool has_initializer() const 
-   {
-      return m_initializer.size() > 0; 
-   }
-   std::list<operand_info> get_initializer() const
-   {
-      return m_initializer;
-   }
-   unsigned reg_num() const
-   {
-      assert( m_reg_num_valid );
-      return m_reg_num; 
-   }
-   unsigned arch_reg_num() const
-   {
-      assert( m_reg_num_valid );
-      return m_arch_reg_num; 
-   }
-   void print_info(FILE *fp) const;
-   unsigned uid() const { return m_uid; }
-
-private:
-   unsigned get_uid();
-   unsigned m_uid;
-   const type_info *m_type;
-   unsigned m_size; // in bytes
-   std::string m_name;
-   std::string m_decl_location;
-
-   unsigned m_address;
-   function_info *m_function; // used for function symbols
-
-   bool m_address_valid;
-   bool m_is_label;
-   bool m_is_shared;
-   bool m_is_sstarr;
-   bool m_is_const;
-   bool m_is_global;
-   bool m_is_local;
-   bool m_is_param_local;
-   bool m_is_param_kernel;
-   bool m_is_tex;
-   bool m_is_func_addr;
-   unsigned m_reg_num; 
-   unsigned m_arch_reg_num; 
-   bool m_reg_num_valid; 
-
-   std::list<operand_info> m_initializer;
-   static unsigned sm_next_uid;
-   
+ public:
+  symbol(const char *name, const type_info *type, const char *location,
+         unsigned size, gpgpu_context *ctx) {
+    gpgpu_ctx = ctx;
+    m_uid = get_uid();
+    m_name = name;
+    m_decl_location = location;
+    m_type = type;
+    m_size = size;
+    m_address_valid = false;
+    m_is_label = false;
+    m_is_shared = false;
+    m_is_const = false;
+    m_is_global = false;
+    m_is_local = false;
+    m_is_param_local = false;
+    m_is_param_kernel = false;
+    m_is_tex = false;
+    m_is_func_addr = false;
+    m_reg_num_valid = false;
+    m_function = NULL;
+    m_reg_num = (unsigned)-1;
+    m_arch_reg_num = (unsigned)-1;
+    m_address = (unsigned)-1;
+    m_initializer.clear();
+    if (type) m_is_shared = type->get_key().is_shared();
+    if (type) m_is_const = type->get_key().is_const();
+    if (type) m_is_global = type->get_key().is_global();
+    if (type) m_is_local = type->get_key().is_local();
+    if (type) m_is_param_local = type->get_key().is_param_local();
+    if (type) m_is_param_kernel = type->get_key().is_param_kernel();
+    if (type) m_is_tex = type->get_key().is_tex();
+    if (type) m_is_func_addr = type->get_key().is_func_addr();
+  }
+  unsigned get_size_in_bytes() const { return m_size; }
+  const std::string &name() const { return m_name; }
+  const std::string &decl_location() const { return m_decl_location; }
+  const type_info *type() const { return m_type; }
+  addr_t get_address() const {
+    assert(m_is_label ||
+           !m_type->get_key().is_reg());  // todo : other assertions
+    assert(m_address_valid);
+    return m_address;
+  }
+  function_info *get_pc() const { return m_function; }
+  void set_regno(unsigned regno, unsigned arch_regno) {
+    m_reg_num_valid = true;
+    m_reg_num = regno;
+    m_arch_reg_num = arch_regno;
+  }
+
+  void set_address(addr_t addr) {
+    m_address_valid = true;
+    m_address = addr;
+  }
+  void set_label_address(addr_t addr) {
+    m_address_valid = true;
+    m_address = addr;
+    m_is_label = true;
+  }
+  void set_function(function_info *func) {
+    m_function = func;
+    m_is_func_addr = true;
+  }
+
+  bool is_label() const { return m_is_label; }
+  bool is_shared() const { return m_is_shared; }
+  bool is_sstarr() const { return m_is_sstarr; }
+  bool is_const() const { return m_is_const; }
+  bool is_global() const { return m_is_global; }
+  bool is_local() const { return m_is_local; }
+  bool is_param_local() const { return m_is_param_local; }
+  bool is_param_kernel() const { return m_is_param_kernel; }
+  bool is_tex() const { return m_is_tex; }
+  bool is_func_addr() const { return m_is_func_addr; }
+  bool is_reg() const {
+    if (m_type == NULL) {
+      return false;
+    }
+    return m_type->get_key().is_reg();
+  }
+  bool is_non_arch_reg() const {
+    if (m_type == NULL) {
+      return false;
+    }
+    return m_type->get_key().is_non_arch_reg();
+  }
+
+  void add_initializer(const std::list<operand_info> &init);
+  bool has_initializer() const { return m_initializer.size() > 0; }
+  std::list<operand_info> get_initializer() const { return m_initializer; }
+  unsigned reg_num() const {
+    assert(m_reg_num_valid);
+    return m_reg_num;
+  }
+  unsigned arch_reg_num() const {
+    assert(m_reg_num_valid);
+    return m_arch_reg_num;
+  }
+  void print_info(FILE *fp) const;
+  unsigned uid() const { return m_uid; }
+
+ private:
+  gpgpu_context *gpgpu_ctx;
+  unsigned get_uid();
+  unsigned m_uid;
+  const type_info *m_type;
+  unsigned m_size;  // in bytes
+  std::string m_name;
+  std::string m_decl_location;
+
+  unsigned m_address;
+  function_info *m_function;  // used for function symbols
+
+  bool m_address_valid;
+  bool m_is_label;
+  bool m_is_shared;
+  bool m_is_sstarr;
+  bool m_is_const;
+  bool m_is_global;
+  bool m_is_local;
+  bool m_is_param_local;
+  bool m_is_param_kernel;
+  bool m_is_tex;
+  bool m_is_func_addr;
+  unsigned m_reg_num;
+  unsigned m_arch_reg_num;
+  bool m_reg_num_valid;
+
+  std::list<operand_info> m_initializer;
 };
 
 class symbol_table {
-public:
+ public:
   symbol_table();
   symbol_table(const char *scope_name, unsigned entry_point,
                symbol_table *parent, gpgpu_context *ctx);
@@ -385,1217 +378,1261 @@ public:
 };
 
 class operand_info {
-public:
-   operand_info()
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = false;
-      m_immediate_address=false;
-      m_addr_offset = 0;
-      m_value.m_symbolic=NULL;
-   }
-   operand_info( const symbol *addr )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      if ( addr->is_label() ) {
-         m_type = label_t;
-      } else if ( addr->is_shared() ) {
-         m_type = symbolic_t;
-      } else if ( addr->is_const() ) {
-         m_type = symbolic_t;
-      } else if ( addr->is_global() ) {
-         m_type = symbolic_t;
-      } else if ( addr->is_local() ) {
-         m_type = symbolic_t;
-      } else if ( addr->is_param_local() ) {
-         m_type = symbolic_t;
-      } else if ( addr->is_param_kernel() ) {
-         m_type = symbolic_t;
-      } else if ( addr->is_tex() ) {
-         m_type = symbolic_t;
-      } else if ( addr->is_func_addr() ) {
-         m_type = symbolic_t;
-      } else if ( !addr->is_reg() ) {
-         m_type = symbolic_t;
-      } else {
-         m_type = reg_t;
-      }
-      
-      m_is_non_arch_reg = addr->is_non_arch_reg();
-      m_value.m_symbolic = addr;
-      m_addr_offset = 0;
-      m_vector = false;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( const symbol *addr1, const symbol *addr2 )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_type = memory_t;
-      m_value.m_vector_symbolic = new const symbol*[8];
-      m_value.m_vector_symbolic[0] = addr1;
-      m_value.m_vector_symbolic[1] = addr2;
-      m_value.m_vector_symbolic[2] = NULL;
-      m_value.m_vector_symbolic[3] = NULL;
-      m_value.m_vector_symbolic[4] = NULL;
-      m_value.m_vector_symbolic[5] = NULL;
-      m_value.m_vector_symbolic[6] = NULL;
-      m_value.m_vector_symbolic[7] = NULL;
-      m_addr_offset = 0;
-      m_vector = false;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( int builtin_id, int dim_mod )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = false;
-      m_type = builtin_t;
-      m_value.m_int = builtin_id;
-      m_addr_offset = dim_mod;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( const symbol *addr, int offset )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = false;
-      m_type = address_t;
-      m_value.m_symbolic = addr;
-      m_addr_offset = offset;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( unsigned x )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = false;
-      m_type = unsigned_t;
-      m_value.m_unsigned = x;
-      m_addr_offset = x;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=true;
-   }
-   operand_info( int x )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = false;
-      m_type = int_t;
-      m_value.m_int = x;
-      m_addr_offset = 0;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( float x )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = false;
-      m_type = float_op_t;
-      m_value.m_float = x;
-      m_addr_offset = 0;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( double x )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = false;
-      m_type = double_op_t;
-      m_value.m_double = x;
-      m_addr_offset = 0;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( const symbol *s1, const symbol *s2, const symbol *s3, const symbol *s4 )
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = true;
-      m_type = vector_t;
-      m_value.m_vector_symbolic = new const symbol*[8];
-      m_value.m_vector_symbolic[0] = s1;
-      m_value.m_vector_symbolic[1] = s2;
-      m_value.m_vector_symbolic[2] = s3;
-      m_value.m_vector_symbolic[3] = s4;
-      m_value.m_vector_symbolic[4] = NULL;
-      m_value.m_vector_symbolic[5] = NULL;
-      m_value.m_vector_symbolic[6] = NULL;
-      m_value.m_vector_symbolic[7] = NULL;
-      m_addr_offset = 0;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-   operand_info( const symbol *s1, const symbol *s2, const symbol *s3, const symbol *s4 ,const symbol *s5,const symbol *s6,const symbol *s7, const symbol *s8)
-   {
-      init();
-      m_is_non_arch_reg = false;
-      m_addr_space = undefined_space;
-      m_operand_lohi = 0;
-      m_double_operand_type = 0;
-      m_operand_neg = false;
-      m_const_mem_offset = 0;
-      m_uid = get_uid();
-      m_valid = true;
-      m_vector = true;
-      m_type = vector_t;
-      m_value.m_vector_symbolic = new const symbol*[8];
-      m_value.m_vector_symbolic[0] = s1;
-      m_value.m_vector_symbolic[1] = s2;
-      m_value.m_vector_symbolic[2] = s3;
-      m_value.m_vector_symbolic[3] = s4;
-      m_value.m_vector_symbolic[4] = s5;
-      m_value.m_vector_symbolic[5] = s6;
-      m_value.m_vector_symbolic[6] = s7;
-      m_value.m_vector_symbolic[7] = s8;
-      m_addr_offset = 0;
-      m_neg_pred = false;
-      m_is_return_var = false;
-      m_immediate_address=false;
-   }
-
-   void init()
-   {
-       m_uid=(unsigned)-1;
-       m_valid=false;
-       m_vector=false;
-       m_type=undef_t;
-       m_immediate_address=false;
-       m_addr_space=undefined_space;
-       m_operand_lohi=0;
-       m_double_operand_type=0;
-       m_operand_neg=false;
-       m_const_mem_offset=(unsigned)-1;
-       m_value.m_int=0;
-       m_value.m_unsigned=(unsigned)-1;
-       m_value.m_float=0;
-       m_value.m_double=0;
-       for(unsigned i=0; i<4; i++){
-           m_value.m_vint[i]=0;
-           m_value.m_vunsigned[i]=0;
-           m_value.m_vfloat[i]=0;
-           m_value.m_vdouble[i]=0;
-       }
-       m_value.m_symbolic=NULL;
-       m_value.m_vector_symbolic=NULL;
-       m_addr_offset=0;
-       m_neg_pred=0;
-       m_is_return_var=0;
-       m_is_non_arch_reg=0;
-
-   }
-   void make_memory_operand() { m_type = memory_t;}
-   void set_return() { m_is_return_var = true; }
-   void set_immediate_addr() {m_immediate_address=true;}
-   const std::string &name() const
-   {
-      assert( m_type == symbolic_t || m_type == reg_t || m_type == address_t || m_type == memory_t || m_type == label_t);
-      return m_value.m_symbolic->name();
-   }
-
-   unsigned get_vect_nelem() const
-   {
-      assert( is_vector() );
-      if( !m_value.m_vector_symbolic[0] ) return 0;
-      if( !m_value.m_vector_symbolic[1] ) return 1;
-      if( !m_value.m_vector_symbolic[2] ) return 2;
-      if( !m_value.m_vector_symbolic[3] ) return 3;
-      if( !m_value.m_vector_symbolic[4] ) return 4;
-      if( !m_value.m_vector_symbolic[5] ) return 5;
-      if( !m_value.m_vector_symbolic[6] ) return 6;
-      if( !m_value.m_vector_symbolic[7] ) return 7;
-      return 8;
-   }
-
-   const symbol* vec_symbol(int idx) const 
-   {
-      assert(idx < 8);
-      const symbol *result = m_value.m_vector_symbolic[idx];
-      assert( result != NULL );
-      return result;
-   }
-
-   const std::string &vec_name1() const
-   {
-      assert( m_type == vector_t);
-      return m_value.m_vector_symbolic[0]->name();
-   }
-
-   const std::string &vec_name2() const
-   {
-      assert( m_type == vector_t);
-      return m_value.m_vector_symbolic[1]->name();
-   }
-
-   const std::string &vec_name3() const
-   {
-      assert( m_type == vector_t);
-      return m_value.m_vector_symbolic[2]->name();
-   }
-
-   const std::string &vec_name4() const
-   {
-      assert( m_type == vector_t);
-      return m_value.m_vector_symbolic[3]->name();
-   }
-
-   bool is_reg() const
-   {
-      if ( m_type == reg_t ) {
-         return true;
-      }
-      if ( m_type != symbolic_t ) {
-         return false;
-      }
-      return m_value.m_symbolic->type()->get_key().is_reg();
-   }
-   bool is_param_local() const
-   {
-      if ( m_type != symbolic_t ) 
-         return false;
-      return m_value.m_symbolic->type()->get_key().is_param_local();
-   }
-
-   bool is_param_kernel() const
-   {
-      if ( m_type != symbolic_t ) 
-         return false;
-      return m_value.m_symbolic->type()->get_key().is_param_kernel();
-   }
-
-   bool is_vector() const
-   {
-      if ( m_vector) return true;
+ public:
+  operand_info(gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = false;
+    m_immediate_address = false;
+    m_addr_offset = 0;
+    m_value.m_symbolic = NULL;
+  }
+  operand_info(const symbol *addr, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    if (addr->is_label()) {
+      m_type = label_t;
+    } else if (addr->is_shared()) {
+      m_type = symbolic_t;
+    } else if (addr->is_const()) {
+      m_type = symbolic_t;
+    } else if (addr->is_global()) {
+      m_type = symbolic_t;
+    } else if (addr->is_local()) {
+      m_type = symbolic_t;
+    } else if (addr->is_param_local()) {
+      m_type = symbolic_t;
+    } else if (addr->is_param_kernel()) {
+      m_type = symbolic_t;
+    } else if (addr->is_tex()) {
+      m_type = symbolic_t;
+    } else if (addr->is_func_addr()) {
+      m_type = symbolic_t;
+    } else if (!addr->is_reg()) {
+      m_type = symbolic_t;
+    } else {
+      m_type = reg_t;
+    }
+
+    m_is_non_arch_reg = addr->is_non_arch_reg();
+    m_value.m_symbolic = addr;
+    m_addr_offset = 0;
+    m_vector = false;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(const symbol *addr1, const symbol *addr2, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_type = memory_t;
+    m_value.m_vector_symbolic = new const symbol *[8];
+    m_value.m_vector_symbolic[0] = addr1;
+    m_value.m_vector_symbolic[1] = addr2;
+    m_value.m_vector_symbolic[2] = NULL;
+    m_value.m_vector_symbolic[3] = NULL;
+    m_value.m_vector_symbolic[4] = NULL;
+    m_value.m_vector_symbolic[5] = NULL;
+    m_value.m_vector_symbolic[6] = NULL;
+    m_value.m_vector_symbolic[7] = NULL;
+    m_addr_offset = 0;
+    m_vector = false;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(int builtin_id, int dim_mod, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = false;
+    m_type = builtin_t;
+    m_value.m_int = builtin_id;
+    m_addr_offset = dim_mod;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(const symbol *addr, int offset, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = false;
+    m_type = address_t;
+    m_value.m_symbolic = addr;
+    m_addr_offset = offset;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(unsigned x, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = false;
+    m_type = unsigned_t;
+    m_value.m_unsigned = x;
+    m_addr_offset = x;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = true;
+  }
+  operand_info(int x, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = false;
+    m_type = int_t;
+    m_value.m_int = x;
+    m_addr_offset = 0;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(float x, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = false;
+    m_type = float_op_t;
+    m_value.m_float = x;
+    m_addr_offset = 0;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(double x, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = false;
+    m_type = double_op_t;
+    m_value.m_double = x;
+    m_addr_offset = 0;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(const symbol *s1, const symbol *s2, const symbol *s3,
+               const symbol *s4, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = true;
+    m_type = vector_t;
+    m_value.m_vector_symbolic = new const symbol *[8];
+    m_value.m_vector_symbolic[0] = s1;
+    m_value.m_vector_symbolic[1] = s2;
+    m_value.m_vector_symbolic[2] = s3;
+    m_value.m_vector_symbolic[3] = s4;
+    m_value.m_vector_symbolic[4] = NULL;
+    m_value.m_vector_symbolic[5] = NULL;
+    m_value.m_vector_symbolic[6] = NULL;
+    m_value.m_vector_symbolic[7] = NULL;
+    m_addr_offset = 0;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+  operand_info(const symbol *s1, const symbol *s2, const symbol *s3,
+               const symbol *s4, const symbol *s5, const symbol *s6,
+               const symbol *s7, const symbol *s8, gpgpu_context *ctx) {
+    init(ctx);
+    m_is_non_arch_reg = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = 0;
+    m_uid = get_uid();
+    m_valid = true;
+    m_vector = true;
+    m_type = vector_t;
+    m_value.m_vector_symbolic = new const symbol *[8];
+    m_value.m_vector_symbolic[0] = s1;
+    m_value.m_vector_symbolic[1] = s2;
+    m_value.m_vector_symbolic[2] = s3;
+    m_value.m_vector_symbolic[3] = s4;
+    m_value.m_vector_symbolic[4] = s5;
+    m_value.m_vector_symbolic[5] = s6;
+    m_value.m_vector_symbolic[6] = s7;
+    m_value.m_vector_symbolic[7] = s8;
+    m_addr_offset = 0;
+    m_neg_pred = false;
+    m_is_return_var = false;
+    m_immediate_address = false;
+  }
+
+  void init(gpgpu_context *ctx) {
+    gpgpu_ctx = ctx;
+    m_uid = (unsigned)-1;
+    m_valid = false;
+    m_vector = false;
+    m_type = undef_t;
+    m_immediate_address = false;
+    m_addr_space = undefined_space;
+    m_operand_lohi = 0;
+    m_double_operand_type = 0;
+    m_operand_neg = false;
+    m_const_mem_offset = (unsigned)-1;
+    m_value.m_int = 0;
+    m_value.m_unsigned = (unsigned)-1;
+    m_value.m_float = 0;
+    m_value.m_double = 0;
+    for (unsigned i = 0; i < 4; i++) {
+      m_value.m_vint[i] = 0;
+      m_value.m_vunsigned[i] = 0;
+      m_value.m_vfloat[i] = 0;
+      m_value.m_vdouble[i] = 0;
+    }
+    m_value.m_symbolic = NULL;
+    m_value.m_vector_symbolic = NULL;
+    m_addr_offset = 0;
+    m_neg_pred = 0;
+    m_is_return_var = 0;
+    m_is_non_arch_reg = 0;
+  }
+  void make_memory_operand() { m_type = memory_t; }
+  void set_return() { m_is_return_var = true; }
+  void set_immediate_addr() { m_immediate_address = true; }
+  const std::string &name() const {
+    assert(m_type == symbolic_t || m_type == reg_t || m_type == address_t ||
+           m_type == memory_t || m_type == label_t);
+    return m_value.m_symbolic->name();
+  }
+
+  unsigned get_vect_nelem() const {
+    assert(is_vector());
+    if (!m_value.m_vector_symbolic[0]) return 0;
+    if (!m_value.m_vector_symbolic[1]) return 1;
+    if (!m_value.m_vector_symbolic[2]) return 2;
+    if (!m_value.m_vector_symbolic[3]) return 3;
+    if (!m_value.m_vector_symbolic[4]) return 4;
+    if (!m_value.m_vector_symbolic[5]) return 5;
+    if (!m_value.m_vector_symbolic[6]) return 6;
+    if (!m_value.m_vector_symbolic[7]) return 7;
+    return 8;
+  }
+
+  const symbol *vec_symbol(int idx) const {
+    assert(idx < 8);
+    const symbol *result = m_value.m_vector_symbolic[idx];
+    assert(result != NULL);
+    return result;
+  }
+
+  const std::string &vec_name1() const {
+    assert(m_type == vector_t);
+    return m_value.m_vector_symbolic[0]->name();
+  }
+
+  const std::string &vec_name2() const {
+    assert(m_type == vector_t);
+    return m_value.m_vector_symbolic[1]->name();
+  }
+
+  const std::string &vec_name3() const {
+    assert(m_type == vector_t);
+    return m_value.m_vector_symbolic[2]->name();
+  }
+
+  const std::string &vec_name4() const {
+    assert(m_type == vector_t);
+    return m_value.m_vector_symbolic[3]->name();
+  }
+
+  bool is_reg() const {
+    if (m_type == reg_t) {
+      return true;
+    }
+    if (m_type != symbolic_t) {
       return false;
-   }
-   int reg_num() const { return m_value.m_symbolic->reg_num();}
-   int reg1_num() const { return m_value.m_vector_symbolic[0]->reg_num();}
-   int reg2_num() const { return m_value.m_vector_symbolic[1]->reg_num();}
-   int reg3_num() const { return m_value.m_vector_symbolic[2]?m_value.m_vector_symbolic[2]->reg_num():0; }
-   int reg4_num() const { return m_value.m_vector_symbolic[3]?m_value.m_vector_symbolic[3]->reg_num():0; }
-   int reg5_num() const { return m_value.m_vector_symbolic[4]?m_value.m_vector_symbolic[4]->reg_num():0; }
-   int reg6_num() const { return m_value.m_vector_symbolic[5]?m_value.m_vector_symbolic[5]->reg_num():0; }
-   int reg7_num() const { return m_value.m_vector_symbolic[6]?m_value.m_vector_symbolic[6]->reg_num():0; }
-   int reg8_num() const { return m_value.m_vector_symbolic[7]?m_value.m_vector_symbolic[7]->reg_num():0; }
-   int arch_reg_num() const { return m_value.m_symbolic->arch_reg_num(); }
-   int arch_reg_num(unsigned n) const { return (m_value.m_vector_symbolic[n])? m_value.m_vector_symbolic[n]->arch_reg_num() : -1; }
-   bool is_label() const { return m_type == label_t;}
-   bool is_builtin() const { return m_type == builtin_t;}
-
-   // Memory operand used in ld / st instructions (ex. [__var1])
-   bool is_memory_operand() const { return m_type == memory_t;}
-
-   // Memory operand with immediate access (ex. s[0x0004] or g[$r1+=0x0004])
-   // This is used by the PTXPlus extension. The operand is assigned an address space during parsing.
-   bool is_memory_operand2() const { 
-      return (m_addr_space!=undefined_space); 
-   }
-
-   bool is_immediate_address() const {
-       return   m_immediate_address;
-   }
-
-   bool is_literal() const { return m_type == int_t ||
-      m_type == float_op_t ||
-      m_type == double_op_t ||
-      m_type == unsigned_t;} 
-   bool is_shared() const {
-      if ( !(m_type == symbolic_t || m_type == address_t || m_type == memory_t) ) {
-         return false;
-      }
-      return  m_value.m_symbolic->is_shared();
-   }
-   bool is_sstarr() const { return m_value.m_symbolic->is_sstarr();}
-   bool is_const() const { return m_value.m_symbolic->is_const();}
-   bool is_global() const { return m_value.m_symbolic->is_global();}
-   bool is_local() const { return m_value.m_symbolic->is_local();}
-   bool is_tex() const { return m_value.m_symbolic->is_tex();}
-   bool is_return_var() const { return m_is_return_var; }
-
-   bool is_function_address() const
-   {
-      if( m_type != symbolic_t ) {
-         return false;
-      }
-      return m_value.m_symbolic->is_func_addr();
-   }
-
-   ptx_reg_t get_literal_value() const
-   {
-      ptx_reg_t result;
-      switch ( m_type ) {
-      case int_t:         result.s64 = m_value.m_int; break;
-      case float_op_t:    result.f32 = m_value.m_float; break;
-      case double_op_t:   result.f64 = m_value.m_double; break; 
-      case unsigned_t:    result.u32 = m_value.m_unsigned; break;
+    }
+    return m_value.m_symbolic->type()->get_key().is_reg();
+  }
+  bool is_param_local() const {
+    if (m_type != symbolic_t) return false;
+    return m_value.m_symbolic->type()->get_key().is_param_local();
+  }
+
+  bool is_param_kernel() const {
+    if (m_type != symbolic_t) return false;
+    return m_value.m_symbolic->type()->get_key().is_param_kernel();
+  }
+
+  bool is_vector() const {
+    if (m_vector) return true;
+    return false;
+  }
+  int reg_num() const { return m_value.m_symbolic->reg_num(); }
+  int reg1_num() const { return m_value.m_vector_symbolic[0]->reg_num(); }
+  int reg2_num() const { return m_value.m_vector_symbolic[1]->reg_num(); }
+  int reg3_num() const {
+    return m_value.m_vector_symbolic[2]
+               ? m_value.m_vector_symbolic[2]->reg_num()
+               : 0;
+  }
+  int reg4_num() const {
+    return m_value.m_vector_symbolic[3]
+               ? m_value.m_vector_symbolic[3]->reg_num()
+               : 0;
+  }
+  int reg5_num() const {
+    return m_value.m_vector_symbolic[4]
+               ? m_value.m_vector_symbolic[4]->reg_num()
+               : 0;
+  }
+  int reg6_num() const {
+    return m_value.m_vector_symbolic[5]
+               ? m_value.m_vector_symbolic[5]->reg_num()
+               : 0;
+  }
+  int reg7_num() const {
+    return m_value.m_vector_symbolic[6]
+               ? m_value.m_vector_symbolic[6]->reg_num()
+               : 0;
+  }
+  int reg8_num() const {
+    return m_value.m_vector_symbolic[7]
+               ? m_value.m_vector_symbolic[7]->reg_num()
+               : 0;
+  }
+  int arch_reg_num() const { return m_value.m_symbolic->arch_reg_num(); }
+  int arch_reg_num(unsigned n) const {
+    return (m_value.m_vector_symbolic[n])
+               ? m_value.m_vector_symbolic[n]->arch_reg_num()
+               : -1;
+  }
+  bool is_label() const { return m_type == label_t; }
+  bool is_builtin() const { return m_type == builtin_t; }
+
+  // Memory operand used in ld / st instructions (ex. [__var1])
+  bool is_memory_operand() const { return m_type == memory_t; }
+
+  // Memory operand with immediate access (ex. s[0x0004] or g[$r1+=0x0004])
+  // This is used by the PTXPlus extension. The operand is assigned an address
+  // space during parsing.
+  bool is_memory_operand2() const { return (m_addr_space != undefined_space); }
+
+  bool is_immediate_address() const { return m_immediate_address; }
+
+  bool is_literal() const {
+    return m_type == int_t || m_type == float_op_t || m_type == double_op_t ||
+           m_type == unsigned_t;
+  }
+  bool is_shared() const {
+    if (!(m_type == symbolic_t || m_type == address_t || m_type == memory_t)) {
+      return false;
+    }
+    return m_value.m_symbolic->is_shared();
+  }
+  bool is_sstarr() const { return m_value.m_symbolic->is_sstarr(); }
+  bool is_const() const { return m_value.m_symbolic->is_const(); }
+  bool is_global() const { return m_value.m_symbolic->is_global(); }
+  bool is_local() const { return m_value.m_symbolic->is_local(); }
+  bool is_tex() const { return m_value.m_symbolic->is_tex(); }
+  bool is_return_var() const { return m_is_return_var; }
+
+  bool is_function_address() const {
+    if (m_type != symbolic_t) {
+      return false;
+    }
+    return m_value.m_symbolic->is_func_addr();
+  }
+
+  ptx_reg_t get_literal_value() const {
+    ptx_reg_t result;
+    switch (m_type) {
+      case int_t:
+        result.s64 = m_value.m_int;
+        break;
+      case float_op_t:
+        result.f32 = m_value.m_float;
+        break;
+      case double_op_t:
+        result.f64 = m_value.m_double;
+        break;
+      case unsigned_t:
+        result.u32 = m_value.m_unsigned;
+        break;
       default:
-         assert(0);
-         break;
-      } 
-      return result;
-   }
-   int get_int() const { return m_value.m_int;}
-   int get_addr_offset() const { return m_addr_offset;}
-   const symbol *get_symbol() const { return m_value.m_symbolic;}
-   void set_type( enum operand_type type ) 
-   {
-      m_type = type;
-   }
-   enum operand_type get_type() const {
-      return m_type;
-   }
-   void set_neg_pred()
-   {
-      assert( m_valid );
-      m_neg_pred = true;
-   }
-   bool is_neg_pred() const { return m_neg_pred; }
-   bool is_valid() const { return m_valid; }
-
-   void set_addr_space(enum _memory_space_t set_value) { m_addr_space = set_value; }
-   enum _memory_space_t get_addr_space() const { return m_addr_space; }
-   void set_operand_lohi(int set_value) { m_operand_lohi = set_value; }
-   int get_operand_lohi() const { return m_operand_lohi; }
-   void set_double_operand_type(int set_value) {  m_double_operand_type = set_value; }
-   int get_double_operand_type() const { return  m_double_operand_type; }
-   void set_operand_neg() { m_operand_neg = true; }
-   bool get_operand_neg() const { return m_operand_neg; }
-   void set_const_mem_offset(addr_t set_value) { m_const_mem_offset = set_value; }
-   addr_t get_const_mem_offset() const { return m_const_mem_offset; }
-   bool is_non_arch_reg() const { return m_is_non_arch_reg; }
-
-private:
-   unsigned m_uid;
-   bool m_valid;
-   bool m_vector;
-   enum operand_type m_type;
-   bool m_immediate_address;
-   enum _memory_space_t m_addr_space;
-   int m_operand_lohi;
-   int m_double_operand_type;
-   bool m_operand_neg;
-   addr_t m_const_mem_offset;
-   union {
-      int             m_int;
-      unsigned int    m_unsigned;
-      float           m_float;
-      double          m_double;
-      int             m_vint[4];
-      unsigned int    m_vunsigned[4];
-      float           m_vfloat[4];
-      double          m_vdouble[4];
-      const symbol*  m_symbolic;
-      const symbol**  m_vector_symbolic;
-   } m_value;
-
-   int m_addr_offset;
-
-   bool m_neg_pred;
-   bool m_is_return_var;
-   bool m_is_non_arch_reg;
-
-   static unsigned sm_next_uid;
-   unsigned get_uid();
+        assert(0);
+        break;
+    }
+    return result;
+  }
+  int get_int() const { return m_value.m_int; }
+  int get_addr_offset() const { return m_addr_offset; }
+  const symbol *get_symbol() const { return m_value.m_symbolic; }
+  void set_type(enum operand_type type) { m_type = type; }
+  enum operand_type get_type() const { return m_type; }
+  void set_neg_pred() {
+    assert(m_valid);
+    m_neg_pred = true;
+  }
+  bool is_neg_pred() const { return m_neg_pred; }
+  bool is_valid() const { return m_valid; }
+
+  void set_addr_space(enum _memory_space_t set_value) {
+    m_addr_space = set_value;
+  }
+  enum _memory_space_t get_addr_space() const { return m_addr_space; }
+  void set_operand_lohi(int set_value) { m_operand_lohi = set_value; }
+  int get_operand_lohi() const { return m_operand_lohi; }
+  void set_double_operand_type(int set_value) {
+    m_double_operand_type = set_value;
+  }
+  int get_double_operand_type() const { return m_double_operand_type; }
+  void set_operand_neg() { m_operand_neg = true; }
+  bool get_operand_neg() const { return m_operand_neg; }
+  void set_const_mem_offset(addr_t set_value) {
+    m_const_mem_offset = set_value;
+  }
+  addr_t get_const_mem_offset() const { return m_const_mem_offset; }
+  bool is_non_arch_reg() const { return m_is_non_arch_reg; }
+
+ private:
+  gpgpu_context *gpgpu_ctx;
+  unsigned m_uid;
+  bool m_valid;
+  bool m_vector;
+  enum operand_type m_type;
+  bool m_immediate_address;
+  enum _memory_space_t m_addr_space;
+  int m_operand_lohi;
+  int m_double_operand_type;
+  bool m_operand_neg;
+  addr_t m_const_mem_offset;
+  union {
+    int m_int;
+    unsigned int m_unsigned;
+    float m_float;
+    double m_double;
+    int m_vint[4];
+    unsigned int m_vunsigned[4];
+    float m_vfloat[4];
+    double m_vdouble[4];
+    const symbol *m_symbolic;
+    const symbol **m_vector_symbolic;
+  } m_value;
+
+  int m_addr_offset;
+
+  bool m_neg_pred;
+  bool m_is_return_var;
+  bool m_is_non_arch_reg;
+
+  unsigned get_uid();
 };
 
 extern const char *g_opcode_string[];
-extern unsigned g_num_ptx_inst_uid;
 struct basic_block_t {
-   basic_block_t( unsigned ID, ptx_instruction *begin, ptx_instruction *end, bool entry, bool ex)
-   {
-      bb_id = ID;
-      ptx_begin = begin;
-      ptx_end = end;
-      is_entry=entry;
-      is_exit=ex;
-      immediatepostdominator_id = -1;
-      immediatedominator_id = -1;
-   }
-
-   ptx_instruction* ptx_begin;
-   ptx_instruction* ptx_end;
-   std::set<int> predecessor_ids; //indices of other basic blocks in m_basic_blocks array
-   std::set<int> successor_ids;
-   std::set<int> postdominator_ids;
-   std::set<int> dominator_ids;
-   std::set<int> Tmp_ids;
-   int immediatepostdominator_id;
-   int immediatedominator_id;
-   bool is_entry;
-   bool is_exit;
-   unsigned bb_id;
-
-   // if this basic block dom B
-   bool dom(const basic_block_t *B) {
-      return (B->dominator_ids.find(this->bb_id) != B->dominator_ids.end());
-   }
-
-   // if this basic block pdom B
-   bool pdom(const basic_block_t *B) {
-      return (B->postdominator_ids.find(this->bb_id) != B->postdominator_ids.end());
-   }
+  basic_block_t(unsigned ID, ptx_instruction *begin, ptx_instruction *end,
+                bool entry, bool ex) {
+    bb_id = ID;
+    ptx_begin = begin;
+    ptx_end = end;
+    is_entry = entry;
+    is_exit = ex;
+    immediatepostdominator_id = -1;
+    immediatedominator_id = -1;
+  }
+
+  ptx_instruction *ptx_begin;
+  ptx_instruction *ptx_end;
+  std::set<int>
+      predecessor_ids;  // indices of other basic blocks in m_basic_blocks array
+  std::set<int> successor_ids;
+  std::set<int> postdominator_ids;
+  std::set<int> dominator_ids;
+  std::set<int> Tmp_ids;
+  int immediatepostdominator_id;
+  int immediatedominator_id;
+  bool is_entry;
+  bool is_exit;
+  unsigned bb_id;
+
+  // if this basic block dom B
+  bool dom(const basic_block_t *B) {
+    return (B->dominator_ids.find(this->bb_id) != B->dominator_ids.end());
+  }
+
+  // if this basic block pdom B
+  bool pdom(const basic_block_t *B) {
+    return (B->postdominator_ids.find(this->bb_id) !=
+            B->postdominator_ids.end());
+  }
 };
 
 struct gpgpu_recon_t {
-   address_type source_pc;
-   address_type target_pc;
-   class ptx_instruction* source_inst;
-   class ptx_instruction* target_inst;
+  address_type source_pc;
+  address_type target_pc;
+  class ptx_instruction* source_inst;
+  class ptx_instruction* target_inst;
 };
 
 class ptx_instruction : public warp_inst_t {
-public:
-    ptx_instruction( int opcode, 
-                    const symbol *pred, 
-                    int neg_pred, 
-                    int pred_mod,
-                    symbol *label,
-                    const std::list<operand_info> &operands, 
-                    const operand_info &return_var,
-                    const std::list<int> &options, 
-                    const std::list<int> &wmma_options, 
-                    const std::list<int> &scalar_type,
-                    memory_space_t space_spec,
-                    const char *file, 
-                    unsigned line,
-                    const char *source,
-                    const core_config *config );
-
-   void print_insn() const;
-   virtual void print_insn( FILE *fp ) const;
-   std::string to_string() const;
-   unsigned inst_size() const { return m_inst_size; }
-   unsigned uid() const { return m_uid;}
-   int get_opcode() const { return m_opcode;}
-   const char *get_opcode_cstr() const 
-   {
-      if ( m_opcode != -1 ) {
-         return g_opcode_string[m_opcode]; 
-      } else {
-         return "label";
-      }
-   }
-   const char *source_file() const { return m_source_file.c_str();} 
-   unsigned source_line() const { return m_source_line;}
-   unsigned get_num_operands() const { return m_operands.size();}
-   bool has_pred() const { return m_pred != NULL;}
-   operand_info get_pred() const { return operand_info( m_pred );}
-   bool get_pred_neg() const { return m_neg_pred;}
-   int get_pred_mod() const { return m_pred_mod;}
-   const char *get_source() const { return m_source.c_str();}
-
-   typedef std::vector<operand_info>::const_iterator const_iterator;
-
-   const_iterator op_iter_begin() const 
-   { 
-      return m_operands.begin();
-   }
-
-   const_iterator op_iter_end() const 
-   { 
-      return m_operands.end();
-   }
-
-   const operand_info &dst() const 
-   { 
-      assert( !m_operands.empty() );
+ public:
+  ptx_instruction(int opcode, const symbol *pred, int neg_pred, int pred_mod,
+                  symbol *label, const std::list<operand_info> &operands,
+                  const operand_info &return_var, const std::list<int> &options,
+                  const std::list<int> &wmma_options,
+                  const std::list<int> &scalar_type, memory_space_t space_spec,
+                  const char *file, unsigned line, const char *source,
+                  const core_config *config, gpgpu_context *ctx);
+
+  void print_insn() const;
+  virtual void print_insn(FILE *fp) const;
+  // virtual void print_coasm(function_info* finfo, FILE *fp) const ;
+  std::string to_string() const;
+  unsigned inst_size() const { return m_inst_size; }
+  unsigned uid() const { return m_uid; }
+  int get_opcode() const { return m_opcode; }
+  const char *get_opcode_cstr() const {
+    if (m_opcode != -1) {
+      return g_opcode_string[m_opcode];
+    } else {
+      return "label";
+    }
+  }
+  const char *source_file() const { return m_source_file.c_str(); }
+  unsigned source_line() const { return m_source_line; }
+  unsigned get_num_operands() const { return m_operands.size(); }
+  bool has_pred() const { return m_pred != NULL; }
+  operand_info get_pred() const;
+  bool get_pred_neg() const { return m_neg_pred; }
+  int get_pred_mod() const { return m_pred_mod; }
+  const char *get_source() const { return m_source.c_str(); }
+
+  const std::list<int> get_scalar_type() const { return m_scalar_type; }
+  const std::list<int> get_options() const { return m_options; }
+
+  typedef std::vector<operand_info>::const_iterator const_iterator;
+
+  const_iterator op_iter_begin() const { return m_operands.begin(); }
+
+  const_iterator op_iter_end() const { return m_operands.end(); }
+
+  const operand_info &dst() const {
+    assert(!m_operands.empty());
+    return m_operands[0];
+  }
+
+  const operand_info &func_addr() const {
+    assert(!m_operands.empty());
+    if (!m_operands[0].is_return_var()) {
       return m_operands[0];
-   }
-
-   const operand_info &func_addr() const
-   {
-      assert( !m_operands.empty() );
-      if( !m_operands[0].is_return_var() ) {
-         return m_operands[0];
-      } else {
-         assert( m_operands.size() >= 2 );
-         return m_operands[1];
-      }
-   }
-
-   operand_info &dst() 
-   { 
-      assert( !m_operands.empty() );
-      return m_operands[0];
-   }
-
-   const operand_info &src1() const 
-   { 
-      assert( m_operands.size() > 1 );
+    } else {
+      assert(m_operands.size() >= 2);
       return m_operands[1];
-   }
-
-   const operand_info &src2() const 
-   { 
-      assert( m_operands.size() > 2 );
-      return m_operands[2];
-   }
-
-   const operand_info &src3() const 
-   { 
-      assert( m_operands.size() > 3 );
-      return m_operands[3];
-   }
-   const operand_info &src4() const 
-   { 
-      assert( m_operands.size() > 4 );
-      return m_operands[4];
-   }
-   const operand_info &src5() const 
-   { 
-      assert( m_operands.size() > 5 );
-      return m_operands[5];
-   }
-   const operand_info &src6() const 
-   { 
-      assert( m_operands.size() > 6 );
-      return m_operands[6];
-   }
-   const operand_info &src7() const 
-   { 
-      assert( m_operands.size() > 7 );
-      return m_operands[7];
-   }
-   const operand_info &src8() const 
-   { 
-      assert( m_operands.size() > 8 );
-      return m_operands[8];
-   }
-
-   const operand_info &operand_lookup( unsigned n ) const
-   {
-      assert( n < m_operands.size() );
-      return m_operands[n];
-   }
-   bool has_return() const
-   {
-      return m_return_var.is_valid();
-   }
-
-   memory_space_t get_space() const { return m_space_spec;}
-   unsigned get_vector() const { return m_vector_spec;}
-   unsigned get_atomic() const { return m_atomic_spec;}
-
-   int get_wmma_type() const {
-      return m_wmma_type;
-   }
-   int get_wmma_layout(int index) const {
-      return m_wmma_layout[index];//0->Matrix D,1->Matrix C
-   }
-   int get_type() const 
-   {
-      assert( !m_scalar_type.empty() );
-      return m_scalar_type.front();
-   }
-
-   int get_type2() const 
-   {
-      assert( m_scalar_type.size()==2 );
-      return m_scalar_type.back();
-   }
-
-   void assign_bb(basic_block_t* basic_block) //assign instruction to a basic block
-   {
-      m_basic_block = basic_block;
-   }
-   basic_block_t* get_bb() { return m_basic_block;}
-   void set_m_instr_mem_index(unsigned index) {
-      m_instr_mem_index = index; 
-   }
-   void set_PC( addr_t PC )
-   {
-       m_PC = PC;
-   }
-   addr_t get_PC() const
-   {
-       return m_PC;
-   }
-
-   unsigned get_m_instr_mem_index() { return m_instr_mem_index;}
-   unsigned get_cmpop() const { return m_compare_op;}
-   const symbol *get_label() const { return m_label;}
-   bool is_label() const { if(m_label){ assert(m_opcode==-1);return true;} return false;}
-   bool is_hi() const { return m_hi;}
-   bool is_lo() const { return m_lo;}
-   bool is_wide() const { return m_wide;}
-   bool is_uni() const { return m_uni;}
-   bool is_exit() const { return m_exit;}
-   bool is_abs() const { return m_abs;}
-   bool is_neg() const { return m_neg;}
-   bool is_to() const { return m_to_option; }
-   unsigned cache_option() const { return m_cache_option; }
-   unsigned rounding_mode() const { return m_rounding_mode;}
-   unsigned saturation_mode() const { return m_saturation_mode;}
-   unsigned dimension() const { return m_geom_spec;}
-   unsigned barrier_op() const {return m_barrier_op;}
-   unsigned shfl_op() const {return m_shfl_op;}
-   unsigned prmt_op() const {return m_prmt_op;}
-   enum vote_mode_t { vote_any, vote_all, vote_uni, vote_ballot };
-   enum vote_mode_t vote_mode() const { return m_vote_mode; }
-
-   int membar_level() const { return m_membar_level; }
-
-   bool has_memory_read() const {
-      if( m_opcode == LD_OP || m_opcode == LDU_OP || m_opcode == TEX_OP|| m_opcode==MMA_LD_OP ) 
-         return true;
-      // Check PTXPlus operand type below
-      // Source operands are memory operands
-      ptx_instruction::const_iterator op=op_iter_begin();
-      for ( int n=0; op != op_iter_end(); op++, n++ ) { //process operands
-         if( n > 0 && op->is_memory_operand2()) // source operands only
-            return true;
-      }
-      return false;
-   }
-   bool has_memory_write() const {
-      if( m_opcode == ST_OP || m_opcode==MMA_ST_OP ) return true;
-      // Check PTXPlus operand type below
-      // Destination operand is a memory operand
-      ptx_instruction::const_iterator op=op_iter_begin();
-      for ( int n=0; (op!=op_iter_end() && n<1); op++, n++ ) { //process operands
-         if( n==0 && op->is_memory_operand2()) // source operands only
-            return true;
-      }
-      return false;
-   }
-
-
-private:
-   void set_opcode_and_latency();
-   void set_bar_type();
-   void set_fp_or_int_archop();
-   void set_mul_div_or_other_archop();
-
-   basic_block_t        *m_basic_block;
-   unsigned          m_uid;
-   addr_t            m_PC;
-   std::string             m_source_file;
-   unsigned                m_source_line;
-   std::string          m_source;
-
-   const symbol           *m_pred;
-   bool                    m_neg_pred;
-   int                    m_pred_mod;
-   int                     m_opcode;
-   const symbol           *m_label;
-   std::vector<operand_info> m_operands;
-   operand_info m_return_var;
-
-   std::list<int>          m_options;
-   std::list<int>          m_wmma_options;
-   bool                m_wide;
-   bool                m_hi;
-   bool                m_lo;
-   bool                m_exit;
-   bool                m_abs;
-   bool                m_neg;
-   bool                m_uni; //if branch instruction, this evaluates to true for uniform branches (ie jumps)
-   bool                m_to_option;
-   unsigned            m_cache_option;
-   int      m_wmma_type;
-   int      m_wmma_layout[2];
-   int      m_wmma_configuration;
-   unsigned            m_rounding_mode;
-   unsigned            m_compare_op;
-   unsigned            m_saturation_mode;
-   unsigned 		   m_barrier_op;
-   unsigned			   m_shfl_op;
-   unsigned			   m_prmt_op;
-
-   std::list<int>          m_scalar_type;
-   memory_space_t m_space_spec;
-   int m_geom_spec;
-   int m_vector_spec;
-   int m_atomic_spec;
-   enum vote_mode_t m_vote_mode;
-   int m_membar_level;
-   int m_instr_mem_index; //index into m_instr_mem array
-   unsigned m_inst_size; // bytes
-
-   virtual void pre_decode();
-   friend class function_info;
-   static unsigned g_num_ptx_inst_uid;
+    }
+  }
+
+  operand_info &dst() {
+    assert(!m_operands.empty());
+    return m_operands[0];
+  }
+
+  const operand_info &src1() const {
+    assert(m_operands.size() > 1);
+    return m_operands[1];
+  }
+
+  const operand_info &src2() const {
+    assert(m_operands.size() > 2);
+    return m_operands[2];
+  }
+
+  const operand_info &src3() const {
+    assert(m_operands.size() > 3);
+    return m_operands[3];
+  }
+  const operand_info &src4() const {
+    assert(m_operands.size() > 4);
+    return m_operands[4];
+  }
+  const operand_info &src5() const {
+    assert(m_operands.size() > 5);
+    return m_operands[5];
+  }
+  const operand_info &src6() const {
+    assert(m_operands.size() > 6);
+    return m_operands[6];
+  }
+  const operand_info &src7() const {
+    assert(m_operands.size() > 7);
+    return m_operands[7];
+  }
+  const operand_info &src8() const {
+    assert(m_operands.size() > 8);
+    return m_operands[8];
+  }
+
+  const operand_info &operand_lookup(unsigned n) const {
+    assert(n < m_operands.size());
+    return m_operands[n];
+  }
+  bool has_return() const { return m_return_var.is_valid(); }
+
+  memory_space_t get_space() const { return m_space_spec; }
+  unsigned get_vector() const { return m_vector_spec; }
+  unsigned get_atomic() const { return m_atomic_spec; }
+
+  int get_wmma_type() const { return m_wmma_type; }
+  int get_wmma_layout(int index) const {
+    return m_wmma_layout[index];  // 0->Matrix D,1->Matrix C
+  }
+  int get_type() const {
+    assert(!m_scalar_type.empty());
+    return m_scalar_type.front();
+  }
+
+  int get_type2() const {
+    assert(m_scalar_type.size() == 2);
+    return m_scalar_type.back();
+  }
+
+  void assign_bb(
+      basic_block_t *basic_block)  // assign instruction to a basic block
+  {
+    m_basic_block = basic_block;
+  }
+  basic_block_t *get_bb() { return m_basic_block; }
+  void set_m_instr_mem_index(unsigned index) { m_instr_mem_index = index; }
+  void set_PC(addr_t PC) { m_PC = PC; }
+  addr_t get_PC() const { return m_PC; }
+
+  unsigned get_m_instr_mem_index() { return m_instr_mem_index; }
+  unsigned get_cmpop() const { return m_compare_op; }
+  const symbol *get_label() const { return m_label; }
+  bool is_label() const {
+    if (m_label) {
+      assert(m_opcode == -1);
+      return true;
+    }
+    return false;
+  }
+  bool is_hi() const { return m_hi; }
+  bool is_lo() const { return m_lo; }
+  bool is_wide() const { return m_wide; }
+  bool is_uni() const { return m_uni; }
+  bool is_exit() const { return m_exit; }
+  bool is_abs() const { return m_abs; }
+  bool is_neg() const { return m_neg; }
+  bool is_to() const { return m_to_option; }
+  unsigned cache_option() const { return m_cache_option; }
+  unsigned rounding_mode() const { return m_rounding_mode; }
+  unsigned saturation_mode() const { return m_saturation_mode; }
+  unsigned dimension() const { return m_geom_spec; }
+  unsigned barrier_op() const { return m_barrier_op; }
+  unsigned shfl_op() const { return m_shfl_op; }
+  unsigned prmt_op() const { return m_prmt_op; }
+  enum vote_mode_t { vote_any, vote_all, vote_uni, vote_ballot };
+  enum vote_mode_t vote_mode() const { return m_vote_mode; }
+
+  int membar_level() const { return m_membar_level; }
+
+  bool has_memory_read() const {
+    if (m_opcode == LD_OP || m_opcode == LDU_OP || m_opcode == TEX_OP ||
+        m_opcode == MMA_LD_OP)
+      return true;
+    // Check PTXPlus operand type below
+    // Source operands are memory operands
+    ptx_instruction::const_iterator op = op_iter_begin();
+    for (int n = 0; op != op_iter_end(); op++, n++) {  // process operands
+      if (n > 0 && op->is_memory_operand2())           // source operands only
+        return true;
+    }
+    return false;
+  }
+  bool has_memory_write() const {
+    if (m_opcode == ST_OP || m_opcode == MMA_ST_OP) return true;
+    // Check PTXPlus operand type below
+    // Destination operand is a memory operand
+    ptx_instruction::const_iterator op = op_iter_begin();
+    for (int n = 0; (op != op_iter_end() && n < 1);
+         op++, n++) {                          // process operands
+      if (n == 0 && op->is_memory_operand2())  // source operands only
+        return true;
+    }
+    return false;
+  }
+
+ private:
+  void set_opcode_and_latency();
+  void set_bar_type();
+  void set_fp_or_int_archop();
+  void set_mul_div_or_other_archop();
+
+  basic_block_t *m_basic_block;
+  unsigned m_uid;
+  addr_t m_PC;
+  std::string m_source_file;
+  unsigned m_source_line;
+  std::string m_source;
+
+  const symbol *m_pred;
+  bool m_neg_pred;
+  int m_pred_mod;
+  int m_opcode;
+  const symbol *m_label;
+  std::vector<operand_info> m_operands;
+  operand_info m_return_var;
+
+  std::list<int> m_options;
+  std::list<int> m_wmma_options;
+  bool m_wide;
+  bool m_hi;
+  bool m_lo;
+  bool m_exit;
+  bool m_abs;
+  bool m_neg;
+  bool m_uni;  // if branch instruction, this evaluates to true for uniform
+               // branches (ie jumps)
+  bool m_to_option;
+  unsigned m_cache_option;
+  int m_wmma_type;
+  int m_wmma_layout[2];
+  int m_wmma_configuration;
+  unsigned m_rounding_mode;
+  unsigned m_compare_op;
+  unsigned m_saturation_mode;
+  unsigned m_barrier_op;
+  unsigned m_shfl_op;
+  unsigned m_prmt_op;
+
+  std::list<int> m_scalar_type;
+  memory_space_t m_space_spec;
+  int m_geom_spec;
+  int m_vector_spec;
+  int m_atomic_spec;
+  enum vote_mode_t m_vote_mode;
+  int m_membar_level;
+  int m_instr_mem_index;  // index into m_instr_mem array
+  unsigned m_inst_size;   // bytes
+
+  virtual void pre_decode();
+  friend class function_info;
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
 };
 
 class param_info {
-public:
-   param_info() { m_valid = false; m_value_set=false; m_size = 0; m_is_ptr = false; }
-   param_info( std::string name, int type, size_t size, bool is_ptr, memory_space_t ptr_space ) 
-   {
-      m_valid = true;
-      m_value_set = false;
-      m_name = name;
-      m_type = type;
-      m_size = size;
-      m_is_ptr = is_ptr; 
-      m_ptr_space = ptr_space; 
-   }
-   void add_data( param_t v ) { 
-      assert( (!m_value_set) || (m_value.size == v.size) ); // if this fails concurrent kernel launches might execute incorrectly
-      m_value_set = true;
-      m_value = v;
-   }
-   void add_offset( unsigned offset ) { m_offset = offset; }
-   unsigned get_offset() { assert(m_valid); return m_offset; }
-   std::string get_name() const { assert(m_valid); return m_name; }
-   int get_type() const { assert(m_valid);  return m_type; }
-   param_t get_value() const { assert(m_value_set); return m_value; }
-   size_t get_size() const { assert(m_valid); return m_size; }
-   bool is_ptr_shared() const { assert(m_valid); return (m_is_ptr and m_ptr_space == shared_space); }
-private:
-   bool m_valid;
-   std::string m_name;
-   int m_type;
-   size_t m_size;
-   bool m_value_set;
-   param_t m_value;
-   unsigned m_offset;
-   bool m_is_ptr; 
-   memory_space_t m_ptr_space; 
+ public:
+  param_info() {
+    m_valid = false;
+    m_value_set = false;
+    m_size = 0;
+    m_is_ptr = false;
+  }
+  param_info(std::string name, int type, size_t size, bool is_ptr,
+             memory_space_t ptr_space) {
+    m_valid = true;
+    m_value_set = false;
+    m_name = name;
+    m_type = type;
+    m_size = size;
+    m_is_ptr = is_ptr;
+    m_ptr_space = ptr_space;
+  }
+  void add_data(param_t v) {
+    assert((!m_value_set) ||
+           (m_value.size == v.size));  // if this fails concurrent kernel
+                                       // launches might execute incorrectly
+    m_value_set = true;
+    m_value = v;
+  }
+  void add_offset(unsigned offset) { m_offset = offset; }
+  unsigned get_offset() {
+    assert(m_valid);
+    return m_offset;
+  }
+  std::string get_name() const {
+    assert(m_valid);
+    return m_name;
+  }
+  int get_type() const {
+    assert(m_valid);
+    return m_type;
+  }
+  param_t get_value() const {
+    assert(m_value_set);
+    return m_value;
+  }
+  size_t get_size() const {
+    assert(m_valid);
+    return m_size;
+  }
+  bool is_ptr_shared() const {
+    assert(m_valid);
+    return (m_is_ptr and m_ptr_space == shared_space);
+  }
+
+ private:
+  bool m_valid;
+  std::string m_name;
+  int m_type;
+  size_t m_size;
+  bool m_value_set;
+  param_t m_value;
+  unsigned m_offset;
+  bool m_is_ptr;
+  memory_space_t m_ptr_space;
 };
 
 class function_info {
-public:
-   function_info(int entry_point );
-   const ptx_version &get_ptx_version() const { return m_symtab->get_ptx_version(); }
-   unsigned get_sm_target() const { return m_symtab->get_sm_target(); }
-   bool is_extern() const { return m_extern; }
-   void set_name(const char *name)
-   {
-      m_name = name;
-   }
-   void set_symtab(symbol_table *symtab )
-   {
-      m_symtab = symtab;
-   }
-   std::string get_name() const
-   {
-      return m_name;
-   }
-   unsigned print_insn( unsigned pc, FILE * fp ) const;
-    std::string get_insn_str( unsigned pc ) const;
-   void add_inst( const std::list<ptx_instruction*> &instructions )
-   {
-      m_instructions = instructions;
-   }
-   std::list<ptx_instruction*>::iterator find_next_real_instruction( std::list<ptx_instruction*>::iterator i );
-   void create_basic_blocks( );
-
-   void print_basic_blocks();
-
-   void print_basic_block_links();
-   void print_basic_block_dot();
-
-   operand_info* find_break_target( ptx_instruction * p_break_insn ); //find the target of a break instruction 
-   void connect_basic_blocks( ); //iterate across m_basic_blocks of function, connecting basic blocks together
-   bool connect_break_targets(); //connecting break instructions with proper targets
-
-   //iterate across m_basic_blocks of function, 
-   //finding dominator blocks, using algorithm of
-   //Muchnick's Adv. Compiler Design & Implemmntation Fig 7.14 
-   void find_dominators( );
-   void print_dominators();
-   void find_idominators();
-   void print_idominators();
-
-   //iterate across m_basic_blocks of function, 
-   //finding postdominator blocks, using algorithm of
-   //Muchnick's Adv. Compiler Design & Implemmntation Fig 7.14 
-   void find_postdominators( );
-   void print_postdominators();
-
-   //iterate across m_basic_blocks of function, 
-   //finding immediate postdominator blocks, using algorithm of
-   //Muchnick's Adv. Compiler Design & Implemmntation Fig 7.15 
-   void find_ipostdominators( );
-   void print_ipostdominators();
-   void do_pdom(); //function to call pdom analysis
-
-   unsigned get_num_reconvergence_pairs();
-
-   void get_reconvergence_pairs(gpgpu_recon_t* recon_points);
-
-   unsigned get_function_size() { return m_instructions.size();}
-
-   void ptx_assemble();
- 
-   unsigned ptx_get_inst_op( ptx_thread_info *thread );
-   void add_param( const char *name, struct param_t value )
-   {
-      m_kernel_params[ name ] = value;
-   }
-   void add_param_name_type_size( unsigned index, std::string name, int type, size_t size, bool ptr, memory_space_t space );
-   void add_param_data( unsigned argn, struct gpgpu_ptx_sim_arg *args );
-   void add_return_var( const symbol *rv )
-   {
-      m_return_var_sym = rv;
-   }
-   void add_arg( const symbol *arg )
-   {
-      assert( arg != NULL );
-      m_args.push_back(arg);
-   }
-   void remove_args()
-   {
-      m_args.clear();
-   }
-   unsigned num_args() const
-   {
-      return m_args.size();
-   }
-   unsigned get_args_aligned_size();
-
-   const symbol* get_arg( unsigned n ) const
-   {
-      assert( n < m_args.size() );
-      return m_args[n];
-   }
-   bool has_return() const
-   {
-      return m_return_var_sym != NULL;
-   }
-   const symbol *get_return_var() const
-   {
-      return m_return_var_sym;
-   }
-   const ptx_instruction *get_instruction( unsigned PC ) const
-   {
-      unsigned index = PC - m_start_PC;
-      if( index < m_instr_mem_size ) 
-         return m_instr_mem[index];
-      return NULL;
-   }
-   addr_t get_start_PC() const
-   {
-       return m_start_PC;
-   }
-
-   void finalize( memory_space *param_mem );
-   void param_to_shared( memory_space *shared_mem, symbol_table *symtab ); 
-   void list_param( FILE *fout ) const;
-   void ptx_jit_config(std::map<unsigned long long, size_t> mallocPtr_Size, memory_space *param_mem, gpgpu_t* gpu, dim3 gridDim, dim3 blockDim) ;
-
-   const struct gpgpu_ptx_sim_info* get_kernel_info () const
-   {
-      assert (m_kernel_info.maxthreads == maxnt_id);
-      return &m_kernel_info;
-   }
-
-   // TODO schi compile warn const void set_kernel_info (const struct gpgpu_ptx_sim_info &info) {
-   void set_kernel_info (const struct gpgpu_ptx_sim_info &info) {
-      m_kernel_info = info;
-      m_kernel_info.ptx_version = 10*get_ptx_version().ver();
-      m_kernel_info.sm_target = get_ptx_version().target();
-      // THIS DEPENDS ON ptxas being called after the PTX is parsed.
-      m_kernel_info.maxthreads = maxnt_id;
-   }
-   symbol_table *get_symtab()
-   {
-      return m_symtab;
-   }
-
-   static const ptx_instruction* pc_to_instruction(unsigned pc) 
-   {
-      if( pc < s_g_pc_to_insn.size() )
-          return s_g_pc_to_insn[pc];
-      else
-          return NULL;
-   }
-   unsigned local_mem_framesize() const 
-   { 
-      return m_local_mem_framesize; 
-   }
-   void set_framesize( unsigned sz )
-   {
-      m_local_mem_framesize = sz;
-   }
-   bool is_entry_point() const { return m_entry_point; }
-   bool is_pdom_set() const { return pdom_done; } //return pdom flag
-   void set_pdom() { pdom_done = true; } //set pdom flag
-
-   void add_config_param( size_t size, unsigned alignment ){
-      unsigned offset = 0;
-      if (m_param_configs.size()>0){
-          unsigned offset_nom = m_param_configs.back().first + m_param_configs.back().second;
-          //ensure offset matches alignment requirements
-          offset = offset_nom%alignment ? (offset_nom/alignment + 1) * alignment : offset_nom;
-      }
-      m_param_configs.push_back(std::pair<size_t,unsigned>(size, offset));
-   }
-
-   std::pair<size_t, unsigned> get_param_config(unsigned param_num) const { return m_param_configs[param_num]; }
-
-   void set_maxnt_id(unsigned maxthreads) { maxnt_id = maxthreads;}
-   unsigned get_maxnt_id() { return maxnt_id;}
-
-private:
-   unsigned maxnt_id;
-   unsigned m_uid;
-   unsigned m_local_mem_framesize;
-   bool m_entry_point;
-   bool m_extern;
-   bool m_assembled;
-   bool pdom_done; //flag to check whether pdom is completed or not
-   std::string m_name;
-   ptx_instruction **m_instr_mem;
-   unsigned m_start_PC;
-   unsigned m_instr_mem_size;
-   std::map<std::string,param_t> m_kernel_params;
-   std::map<unsigned,param_info> m_ptx_kernel_param_info;
-   std::vector< std::pair<size_t, unsigned> > m_param_configs;
-   const symbol *m_return_var_sym;
-   std::vector<const symbol*> m_args;
-   std::list<ptx_instruction*> m_instructions;
-   std::vector<basic_block_t*> m_basic_blocks;
-   std::list<std::pair<unsigned, unsigned> > m_back_edges;
-   std::map<std::string,unsigned> labels;
-   unsigned num_reconvergence_pairs;
-
-   //Registers/shmem/etc. used (from ptxas -v), loaded from ___.ptxinfo along with ___.ptx
-   struct gpgpu_ptx_sim_info m_kernel_info;
-
-   symbol_table *m_symtab;
-
-   static std::vector<ptx_instruction*> s_g_pc_to_insn; // a direct mapping from PC to instruction
-   static unsigned sm_next_uid;
-
-   //parameter size for device kernels
-   int m_args_aligned_size;
-   
-   addr_t m_n;  // offset in m_instr_mem (used in do_pdom)
+ public:
+  function_info(int entry_point, gpgpu_context *ctx);
+  const ptx_version &get_ptx_version() const {
+    return m_symtab->get_ptx_version();
+  }
+  unsigned get_sm_target() const { return m_symtab->get_sm_target(); }
+  bool is_extern() const { return m_extern; }
+  void set_name(const char *name) { m_name = name; }
+  void set_symtab(symbol_table *symtab) { m_symtab = symtab; }
+  std::string get_name() const { return m_name; }
+  unsigned print_insn(unsigned pc, FILE *fp) const;
+  std::string get_insn_str(unsigned pc) const;
+  void add_inst(const std::list<ptx_instruction *> &instructions) {
+    m_instructions = instructions;
+  }
+  std::list<ptx_instruction *>::iterator find_next_real_instruction(
+      std::list<ptx_instruction *>::iterator i);
+  void create_basic_blocks();
+
+  void print_basic_blocks();
+
+  void print_basic_block_links();
+  void print_basic_block_dot();
+
+  operand_info *find_break_target(
+      ptx_instruction *p_break_insn);  // find the target of a break instruction
+  void connect_basic_blocks();  // iterate across m_basic_blocks of function,
+                                // connecting basic blocks together
+  bool
+  connect_break_targets();  // connecting break instructions with proper targets
+
+  // iterate across m_basic_blocks of function,
+  // finding dominator blocks, using algorithm of
+  // Muchnick's Adv. Compiler Design & Implemmntation Fig 7.14
+  void find_dominators();
+  void print_dominators();
+  void find_idominators();
+  void print_idominators();
+
+  // iterate across m_basic_blocks of function,
+  // finding postdominator blocks, using algorithm of
+  // Muchnick's Adv. Compiler Design & Implemmntation Fig 7.14
+  void find_postdominators();
+  void print_postdominators();
+
+  // iterate across m_basic_blocks of function,
+  // finding immediate postdominator blocks, using algorithm of
+  // Muchnick's Adv. Compiler Design & Implemmntation Fig 7.15
+  void find_ipostdominators();
+  void print_ipostdominators();
+  void do_pdom();  // function to call pdom analysis
+  void gen_coasm(FILE* fp);  // function to convert ptx to coasm
+
+  unsigned get_num_reconvergence_pairs();
+
+  void get_reconvergence_pairs(gpgpu_recon_t *recon_points);
+
+  unsigned get_function_size() { return m_instructions.size(); }
+
+  void ptx_assemble();
+
+  unsigned ptx_get_inst_op(ptx_thread_info *thread);
+  void add_param(const char *name, struct param_t value) {
+    m_kernel_params[name] = value;
+  }
+  void add_param_name_type_size(unsigned index, std::string name, int type,
+                                size_t size, bool ptr, memory_space_t space);
+  void add_param_data(unsigned argn, struct gpgpu_ptx_sim_arg *args);
+  void add_return_var(const symbol *rv) { m_return_var_sym = rv; }
+  void add_arg(const symbol *arg) {
+    assert(arg != NULL);
+    m_args.push_back(arg);
+  }
+  void remove_args() { m_args.clear(); }
+  unsigned num_args() const { return m_args.size(); }
+  unsigned get_args_aligned_size();
+
+  const symbol *get_arg(unsigned n) const {
+    assert(n < m_args.size());
+    return m_args[n];
+  }
+  bool has_return() const { return m_return_var_sym != NULL; }
+  const symbol *get_return_var() const { return m_return_var_sym; }
+  const ptx_instruction *get_instruction(unsigned PC) const {
+    unsigned index = PC - m_start_PC;
+    if (index < m_instr_mem_size) return m_instr_mem[index];
+    return NULL;
+  }
+  addr_t get_start_PC() const { return m_start_PC; }
+
+  void finalize(memory_space *param_mem);
+  void param_to_shared(memory_space *shared_mem, symbol_table *symtab);
+  void list_param(FILE *fout) const;
+  void ptx_jit_config(std::map<unsigned long long, size_t> mallocPtr_Size,
+                      memory_space *param_mem, gpgpu_t *gpu, dim3 gridDim,
+                      dim3 blockDim);
+
+  virtual const struct gpgpu_ptx_sim_info *get_kernel_info() const {
+    assert(m_kernel_info.maxthreads == maxnt_id);
+    return &m_kernel_info;
+  }
+
+  virtual const void set_kernel_info(const struct gpgpu_ptx_sim_info &info) {
+    m_kernel_info = info;
+    m_kernel_info.ptx_version = 10 * get_ptx_version().ver();
+    m_kernel_info.sm_target = get_ptx_version().target();
+    // THIS DEPENDS ON ptxas being called after the PTX is parsed.
+    m_kernel_info.maxthreads = maxnt_id;
+  }
+  symbol_table *get_symtab() { return m_symtab; }
+
+  unsigned local_mem_framesize() const { return m_local_mem_framesize; }
+  void set_framesize(unsigned sz) { m_local_mem_framesize = sz; }
+  bool is_entry_point() const { return m_entry_point; }
+  bool is_pdom_set() const { return pdom_done; }  // return pdom flag
+  void set_pdom() { pdom_done = true; }           // set pdom flag
+
+  void add_config_param(size_t size, unsigned alignment) {
+    unsigned offset = 0;
+    if (m_param_configs.size() > 0) {
+      unsigned offset_nom =
+          m_param_configs.back().first + m_param_configs.back().second;
+      // ensure offset matches alignment requirements
+      offset = offset_nom % alignment ? (offset_nom / alignment + 1) * alignment
+                                      : offset_nom;
+    }
+    m_param_configs.push_back(std::pair<size_t, unsigned>(size, offset));
+  }
+
+  std::pair<size_t, unsigned> get_param_config(unsigned param_num) const {
+    return m_param_configs[param_num];
+  }
+
+  void set_maxnt_id(unsigned maxthreads) { maxnt_id = maxthreads; }
+  unsigned get_maxnt_id() { return maxnt_id; }
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+
+ protected:
+  // Registers/shmem/etc. used (from ptxas -v), loaded from ___.ptxinfo along
+  // with ___.ptx
+  struct gpgpu_ptx_sim_info m_kernel_info;
+
+ private:
+  unsigned maxnt_id;
+  unsigned m_uid;
+  unsigned m_local_mem_framesize;
+  bool m_entry_point;
+  bool m_extern;
+  bool m_assembled;
+  bool pdom_done;  // flag to check whether pdom is completed or not
+  std::string m_name;
+  ptx_instruction **m_instr_mem;
+  unsigned m_start_PC;
+  unsigned m_instr_mem_size;
+  std::map<std::string, param_t> m_kernel_params;
+  std::map<unsigned, param_info> m_ptx_kernel_param_info;
+  std::vector<std::pair<size_t, unsigned> > m_param_configs;
+  const symbol *m_return_var_sym;
+  std::vector<const symbol *> m_args;
+  std::list<ptx_instruction *> m_instructions;
+  std::vector<basic_block_t *> m_basic_blocks;
+  std::list<std::pair<unsigned, unsigned> > m_back_edges;
+  std::map<std::string, unsigned> labels;
+  unsigned num_reconvergence_pairs;
+
+  // Registers/shmem/etc. used (from ptxas -v), loaded from ___.ptxinfo along
+  // with ___.ptx
+  // with ___.ptx
+
+  symbol_table *m_symtab;
+
+  // parameter size for device kernels
+  int m_args_aligned_size;
+
+  addr_t m_n;  // offset in m_instr_mem (used in do_pdom)
+
+  std::map<std::string, unsigned> m_coasm_regs;
+  unsigned m_coasm_reg_max {3};
+
+  std::map<std::string, unsigned> m_coasm_tcc;
+  unsigned m_coasm_tcc_max {0};
+
+  // to be match with src/model/gpu/Compute.cpp
+  enum coasm_sregs {
+      GRID_DIM_X = 0,
+      GRID_DIM_Y,
+      GRID_DIM_Z,
+      BLOCK_DIM_X,
+      BLOCK_DIM_Y,
+      BLOCK_DIM_Z,
+      BLOCK_IDX_X,
+      BLOCK_IDX_Y,
+      BLOCK_IDX_Z,
+      SPECIAL_SREG_MAX
+  };
+
+  enum coasm_vregs {
+      THREAD_IDX_X = 0,
+      THREAD_IDX_Y,
+      THREAD_IDX_Z,
+      SPECIAL_VREG_MAX
+  };
+
+  std::vector<std::pair<std::string, int>> m_coasm_special_sregs {
+      {"grid_dim_x", 0},
+      {"grid_dim_y", 0},
+      {"grid_dim_z", 0},
+      {"block_dim_x", 0},
+      {"block_dim_y", 0},
+      {"block_dim_z", 0},
+      {"block_idx_x", 0},
+      {"block_idx_y", 0},
+      {"block_idx_z", 0}
+  };
+
+  std::vector<std::pair<std::string, int>> m_coasm_special_vregs {
+      {"thread_idx_x", 0},
+      {"thread_idx_y", 0},
+      {"thread_idx_z", 0}
+  };
+
+ public:
+  ptx_instruction* get_target_pI(addr_t addr) {
+      return m_instr_mem[addr];
+  }
+
+  unsigned get_label_addr(std::string label) {
+      return labels[label];
+  }
+
+  std::string get_coasm_reg(const operand_info &operand, int reg_size = 1) {
+      return "";
+  };
+
+  std::string get_coasm_tcc(const operand_info &operand) {
+      return "";
+  };
+
+  std::string get_coasm_buildin(int buildin_id, unsigned dim_mod) {
+      return "";
+  };
 };
 
 class arg_buffer_t {
-public:
-   arg_buffer_t()
-   {
-      m_is_reg=false;
-      m_is_param=false;
-      m_param_value=NULL;
-      m_reg_value=ptx_reg_t();
-   }
-   arg_buffer_t( const arg_buffer_t &another )
-   {
-      make_copy(another);
-   }
-   void make_copy( const arg_buffer_t &another )
-   {
-      m_dst = another.m_dst;
-      m_src_op = another.m_src_op;
-      m_is_reg = another.m_is_reg;
-      m_is_param = another.m_is_param;
-      m_reg_value = another.m_reg_value;
-      m_param_bytes = another.m_param_bytes;
-      if( m_is_param ) {
-         m_param_value = malloc(m_param_bytes);
-         memcpy(m_param_value,another.m_param_value,m_param_bytes);
+ public:
+  arg_buffer_t(gpgpu_context *ctx) : m_src_op(ctx) {
+    m_is_reg = false;
+    m_is_param = false;
+    m_param_value = NULL;
+    m_reg_value = ptx_reg_t();
+  }
+  arg_buffer_t(const arg_buffer_t &another, gpgpu_context *ctx)
+      : m_src_op(ctx) {
+    make_copy(another);
+  }
+  void make_copy(const arg_buffer_t &another) {
+    m_dst = another.m_dst;
+    m_src_op = another.m_src_op;
+    m_is_reg = another.m_is_reg;
+    m_is_param = another.m_is_param;
+    m_reg_value = another.m_reg_value;
+    m_param_bytes = another.m_param_bytes;
+    if (m_is_param) {
+      m_param_value = malloc(m_param_bytes);
+      memcpy(m_param_value, another.m_param_value, m_param_bytes);
+    }
+  }
+  void operator=(const arg_buffer_t &another) { make_copy(another); }
+  ~arg_buffer_t() {
+    if (m_is_param) free(m_param_value);
+  }
+  arg_buffer_t(const symbol *dst_sym, const operand_info &src_op,
+               ptx_reg_t source_value)
+      : m_src_op(src_op) {
+    m_dst = dst_sym;
+    m_reg_value = ptx_reg_t();
+    if (dst_sym->is_reg()) {
+      m_is_reg = true;
+      m_is_param = false;
+      assert(src_op.is_reg());
+      m_reg_value = source_value;
+    } else {
+      m_is_param = true;
+      m_is_reg = false;
+      m_param_value = calloc(sizeof(ptx_reg_t), 1);
+      // new (m_param_value) ptx_reg_t(source_value);
+      memcpy(m_param_value, &source_value, sizeof(ptx_reg_t));
+      m_param_bytes = sizeof(ptx_reg_t);
+    }
+  }
+  arg_buffer_t(const symbol *dst_sym, const operand_info &src_op,
+               void *source_param_value_array, unsigned array_size)
+      : m_src_op(src_op) {
+    m_dst = dst_sym;
+    if (dst_sym->is_reg()) {
+      m_is_reg = true;
+      m_is_param = false;
+      assert(src_op.is_param_local());
+      assert(dst_sym->get_size_in_bytes() == array_size);
+      switch (array_size) {
+        case 1:
+          m_reg_value.u8 = *(unsigned char *)source_param_value_array;
+          break;
+        case 2:
+          m_reg_value.u16 = *(unsigned short *)source_param_value_array;
+          break;
+        case 4:
+          m_reg_value.u32 = *(unsigned int *)source_param_value_array;
+          break;
+        case 8:
+          m_reg_value.u64 = *(unsigned long long *)source_param_value_array;
+          break;
+        default:
+          printf(
+              "GPGPU-Sim PTX: ERROR ** source param size does not match known "
+              "register sizes\n");
+          break;
       }
-   }
-   void operator=( const arg_buffer_t &another )
-   {
-      make_copy(another);
-   }
-   ~arg_buffer_t()
-   {
-      if( m_is_param ) 
-         free(m_param_value);
-   }
-   arg_buffer_t( const symbol *dst_sym, const operand_info &src_op, ptx_reg_t source_value ) : m_src_op(src_op)
-   {
-      m_dst = dst_sym;
-      m_reg_value=ptx_reg_t();
-      if( dst_sym->is_reg() ) {
-         m_is_reg = true;
-         m_is_param = false;
-         assert( src_op.is_reg() );
-         m_reg_value = source_value;
-      } else {
-         m_is_param = true;
-         m_is_reg = false;
-         m_param_value = calloc(sizeof(ptx_reg_t),1);
-         //new (m_param_value) ptx_reg_t(source_value);
-         memcpy(m_param_value,&source_value,sizeof(ptx_reg_t));
-         m_param_bytes = sizeof(ptx_reg_t);
-      }
-   }
-   arg_buffer_t( const symbol *dst_sym, const operand_info &src_op, void *source_param_value_array, unsigned array_size ) : m_src_op(src_op)
-   {
-      m_dst = dst_sym;
-      if( dst_sym->is_reg() ) {
-         m_is_reg = true;
-         m_is_param = false;
-         assert( src_op.is_param_local() );
-         assert( dst_sym->get_size_in_bytes() == array_size );
-         switch( array_size ) {
-         case 1: m_reg_value.u8 = *(unsigned char*)source_param_value_array; break;
-         case 2: m_reg_value.u16 = *(unsigned short*)source_param_value_array; break;
-         case 4: m_reg_value.u32 = *(unsigned int*)source_param_value_array; break;
-         case 8: m_reg_value.u64 = *(unsigned long long*)source_param_value_array; break;
-         default:
-            printf("GPGPU-Sim PTX: ERROR ** source param size does not match known register sizes\n");
-            break;
-         }
-      } else {
-         // param
-         m_is_param = true;
-         m_is_reg = false;
-         m_param_value = calloc(array_size,1);
-         m_param_bytes = array_size;
-         memcpy(m_param_value,source_param_value_array,array_size);
-      }
-   }
-
-   bool is_reg() const { return m_is_reg; }
-   ptx_reg_t get_reg() const 
-   { 
-      assert(m_is_reg); 
-      return m_reg_value; 
-   }
-
-   const void *get_param_buffer() const
-   {
-      assert(m_is_param);
-      return m_param_value;
-   }
-   size_t get_param_buffer_size() const
-   {
-      assert(m_is_param);
-      return m_param_bytes;
-   }
-
-   const symbol *get_dst() const { return m_dst; }
-
-private:
-   // destination of copy
-   const symbol *m_dst;
-
-   // source operand
-   operand_info m_src_op;
-
-   // source information
-   bool m_is_reg;
-   bool m_is_param;
-
-   // source is register
-   ptx_reg_t m_reg_value;
-
-   // source is param
-   void     *m_param_value;
-   unsigned  m_param_bytes;
-};
+    } else {
+      // param
+      m_is_param = true;
+      m_is_reg = false;
+      m_param_value = calloc(array_size, 1);
+      m_param_bytes = array_size;
+      memcpy(m_param_value, source_param_value_array, array_size);
+    }
+  }
+
+  bool is_reg() const { return m_is_reg; }
+  ptx_reg_t get_reg() const {
+    assert(m_is_reg);
+    return m_reg_value;
+  }
+
+  const void *get_param_buffer() const {
+    assert(m_is_param);
+    return m_param_value;
+  }
+  size_t get_param_buffer_size() const {
+    assert(m_is_param);
+    return m_param_bytes;
+  }
+
+  const symbol *get_dst() const { return m_dst; }
 
-typedef std::list< arg_buffer_t > arg_buffer_list_t;
-arg_buffer_t copy_arg_to_buffer(ptx_thread_info * thread, operand_info actual_param_op, const symbol * formal_param);
-void copy_args_into_buffer_list( const ptx_instruction * pI, 
-                                 ptx_thread_info * thread, 
-                                 const function_info * target_func, 
-                                 arg_buffer_list_t &arg_values ); 
-void copy_buffer_list_into_frame(ptx_thread_info * thread, arg_buffer_list_t &arg_values);
-void copy_buffer_to_frame(ptx_thread_info * thread, const arg_buffer_t &a);
+ private:
+  // destination of copy
+  const symbol *m_dst;
 
+  // source operand
+  operand_info m_src_op;
 
-struct textureInfo {
-   unsigned int texel_size; //size in bytes, e.g. (channelDesc.x+y+z+w)/8
-   unsigned int Tx,Ty; //tiling factor dimensions of layout of texels per 64B cache block
-   unsigned int Tx_numbits,Ty_numbits; //log2(T)
-   unsigned int texel_size_numbits; //log2(texel_size)
+  // source information
+  bool m_is_reg;
+  bool m_is_param;
+
+  // source is register
+  ptx_reg_t m_reg_value;
+
+  // source is param
+  void *m_param_value;
+  unsigned m_param_bytes;
 };
 
-extern std::map<std::string,symbol_table*> g_sym_name_to_symbol_table;
+typedef std::list<arg_buffer_t> arg_buffer_list_t;
+arg_buffer_t copy_arg_to_buffer(ptx_thread_info *thread,
+                                operand_info actual_param_op,
+                                const symbol *formal_param);
+void copy_args_into_buffer_list(const ptx_instruction *pI,
+                                ptx_thread_info *thread,
+                                const function_info *target_func,
+                                arg_buffer_list_t &arg_values);
+void copy_buffer_list_into_frame(ptx_thread_info *thread,
+                                 arg_buffer_list_t &arg_values);
+void copy_buffer_to_frame(ptx_thread_info *thread, const arg_buffer_t &a);
 
+struct textureInfo {
+  unsigned int texel_size;  // size in bytes, e.g. (channelDesc.x+y+z+w)/8
+  unsigned int Tx,
+      Ty;  // tiling factor dimensions of layout of texels per 64B cache block
+  unsigned int Tx_numbits, Ty_numbits;  // log2(T)
+  unsigned int texel_size_numbits;      // log2(texel_size)
+};
 
-extern bool g_keep_intermediate_files;
+extern std::map<std::string, symbol_table *> g_sym_name_to_symbol_table;
 
-void gpgpu_ptx_assemble( std::string kname, void *kinfo );
+void gpgpu_ptx_assemble(std::string kname, void *kinfo);
 #include "../option_parser.h"
-void ptx_reg_options(option_parser_t opp);
-unsigned ptx_kernel_shmem_size( void *kernel_impl );
-unsigned ptx_kernel_nregs( void *kernel_impl );
+unsigned ptx_kernel_shmem_size(void *kernel_impl);
+unsigned ptx_kernel_nregs(void *kernel_impl);
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.cc
index 67f1f942ab..0f2d2732fd 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.cc
@@ -26,493 +26,557 @@
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #include "ptx_loader.h"
-#include "ptx_ir.h"
-#include "cuda-sim.h"
-#include "ptx_parser.h"
-#include <unistd.h>
 #include <dirent.h>
+#include <unistd.h>
 #include <fstream>
 #include <sstream>
-
-/// globals
-
-memory_space *g_global_mem;
-memory_space *g_tex_mem;
-memory_space *g_surf_mem;
-memory_space *g_param_mem;
-bool g_override_embedded_ptx = false;
+#include "../../libcuda_sim/gpgpu_context.h"
+#include "cuda-sim.h"
+#include "ptx_ir.h"
+#include "ptx_parser.h"
 
 /// extern prototypes
 
-extern "C" int ptx_parse();
-extern "C" int ptx__scan_string(const char*);
+extern int ptx_error(yyscan_t yyscanner, ptx_recognizer *recognizer,
+                     const char *s);
+extern int ptx_lex_init(yyscan_t *scanner);
+extern void ptx_set_in(FILE *_in_str, yyscan_t yyscanner);
+extern int ptx_parse(yyscan_t scanner, ptx_recognizer *recognizer);
+extern int ptx_lex_destroy(yyscan_t scanner);
+extern int ptx__scan_string(const char *, yyscan_t scanner);
 
-extern std::map<unsigned,const char*> get_duplicate();
+extern std::map<unsigned, const char *> get_duplicate();
 
-const char *g_ptxinfo_filename;
-extern "C" int ptxinfo_parse();
-extern "C" int ptxinfo_debug;
-extern "C" FILE *ptxinfo_in;
+typedef void *yyscan_t;
+extern int ptxinfo_lex_init(yyscan_t *scanner);
+extern void ptxinfo_set_in(FILE *_in_str, yyscan_t yyscanner);
+extern int ptxinfo_parse(yyscan_t scanner, ptxinfo_data *ptxinfo);
+extern int ptxinfo_lex_destroy(yyscan_t scanner);
 
 static bool g_save_embedded_ptx;
 static int g_occupancy_sm_number;
-bool g_keep_intermediate_files;
-bool m_ptx_save_converted_ptxplus;
-
-bool keep_intermediate_files() {return g_keep_intermediate_files;}
-
-void ptx_reg_options(option_parser_t opp)
-{
-   option_parser_register(opp, "-save_embedded_ptx", OPT_BOOL, &g_save_embedded_ptx, 
-                "saves ptx files embedded in binary as <n>.ptx",
-                "0");
-   option_parser_register(opp, "-keep", OPT_BOOL, &g_keep_intermediate_files, 
-                "keep intermediate files created by GPGPU-Sim when interfacing with external programs",
-                "0");
-   option_parser_register(opp, "-gpgpu_ptx_save_converted_ptxplus", OPT_BOOL,
-                &m_ptx_save_converted_ptxplus,
-                "Saved converted ptxplus to a file",
-                "0");
-   option_parser_register(opp, "-gpgpu_occupancy_sm_number", OPT_INT32, &g_occupancy_sm_number,
-                "The SM number to pass to ptxas when getting register usage for computing GPU occupancy. "
-                "This parameter is required in the config.",
-                "0");
-}
 
-void print_ptx_file( const char *p, unsigned source_num, const char *filename )
-{
-   printf("\nGPGPU-Sim PTX: file _%u.ptx contents:\n\n", source_num );
-   char *s = strdup(p);
-   char *t = s;
-   unsigned n=1;
-   while ( *t != '\0'  ) {
-      char *u = t;
-      while ( (*u != '\n') && (*u != '\0') ) u++;
-      unsigned last = (*u == '\0');
-      *u = '\0';
-      const ptx_instruction *pI = ptx_instruction_lookup(filename,n);
-      char pc[64];
-      if( pI && pI->get_PC() )
-         snprintf(pc,64,"%4u", pI->get_PC() );
-      else 
-         snprintf(pc,64,"    ");
-      printf("    _%u.ptx  %4u (pc=%s):  %s\n", source_num, n, pc, t );
-      if ( last ) break;
-      t = u+1;
-      n++;
-   }
-   free(s);
-   fflush(stdout);
+bool ptxinfo_data::keep_intermediate_files() {
+  return g_keep_intermediate_files;
 }
 
-char* gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus(const std::string ptxfilename, const std::string elffilename, const std::string sassfilename)
-{
-
-	printf("GPGPU-Sim PTX: converting EMBEDDED .ptx file to ptxplus \n");
-
-    char fname_ptxplus[1024];
-    snprintf(fname_ptxplus,1024,"_ptxplus_XXXXXX");
-    int fd4=mkstemp(fname_ptxplus);
-    close(fd4);
-
-	// Run cuobjdump_to_ptxplus
-	char commandline[1024];
-	int result;
-	snprintf(commandline, 1024, "$GPGPUSIM_ROOT/build/$GPGPUSIM_CONFIG/cuobjdump_to_ptxplus/cuobjdump_to_ptxplus %s %s %s %s",
-			ptxfilename.c_str(),
-			sassfilename.c_str(),
-			elffilename.c_str(),
-			fname_ptxplus);
-	fflush(stdout);
-	printf("GPGPU-Sim PTX: calling cuobjdump_to_ptxplus\ncommandline: %s\n", commandline);
-	result = system(commandline);
-	if(result){fprintf(stderr, "GPGPU-Sim PTX: ERROR ** could not execute %s\n", commandline); exit(1);}
-
-
-	// Get ptxplus from file
-	std::ifstream fileStream(fname_ptxplus, std::ios::in);
-	std::string text, line;
-	while(getline(fileStream,line)) {
-		text += (line + "\n");
-	}
-	fileStream.close();
-
-	char* ptxplus_str = new char [strlen(text.c_str())+1];
-	strcpy(ptxplus_str, text.c_str());
-
-	if (!m_ptx_save_converted_ptxplus){
-		char rm_commandline[1024];
-
-		snprintf(rm_commandline,1024,"rm -f %s", fname_ptxplus);
-
-		printf("GPGPU-Sim PTX: removing temporary files using \"%s\"\n", rm_commandline);
-		int rm_result = system(rm_commandline);
-		if( rm_result != 0 ) {
-			fprintf(stderr, "GPGPU-Sim PTX: ERROR ** while removing temporary files %d\n", rm_result);
-			exit(1);
-		}
-	}
-	printf("GPGPU-Sim PTX: DONE converting EMBEDDED .ptx file to ptxplus \n");
-
-	return ptxplus_str;
+void gpgpu_context::ptx_reg_options(option_parser_t opp) {
+  option_parser_register(opp, "-save_embedded_ptx", OPT_BOOL,
+                         &g_save_embedded_ptx,
+                         "saves ptx files embedded in binary as <n>.ptx", "0");
+  option_parser_register(opp, "-keep", OPT_BOOL,
+                         &(ptxinfo->g_keep_intermediate_files),
+                         "keep intermediate files created by GPGPU-Sim when "
+                         "interfacing with external programs",
+                         "0");
+  option_parser_register(opp, "-gpgpu_ptx_save_converted_ptxplus", OPT_BOOL,
+                         &(ptxinfo->m_ptx_save_converted_ptxplus),
+                         "Saved converted ptxplus to a file", "0");
+  option_parser_register(opp, "-gpgpu_occupancy_sm_number", OPT_INT32,
+                         &g_occupancy_sm_number,
+                         "The SM number to pass to ptxas when getting register "
+                         "usage for computing GPU occupancy. "
+                         "This parameter is required in the config.",
+                         "0");
 }
 
+void gpgpu_context::print_ptx_file(const char *p, unsigned source_num,
+                                   const char *filename) {
+  printf("\nGPGPU-Sim PTX: file _%u.ptx contents:\n\n", source_num);
+  char *s = strdup(p);
+  char *t = s;
+  unsigned n = 1;
+  while (*t != '\0') {
+    char *u = t;
+    while ((*u != '\n') && (*u != '\0')) u++;
+    unsigned last = (*u == '\0');
+    *u = '\0';
+    const ptx_instruction *pI = ptx_parser->ptx_instruction_lookup(filename, n);
+    char pc[64];
+    if (pI && pI->get_PC())
+      snprintf(pc, 64, "%4u", pI->get_PC());
+    else
+      snprintf(pc, 64, "    ");
+    printf("    _%u.ptx  %4u (pc=%s):  %s\n", source_num, n, pc, t);
+    if (last) break;
+    t = u + 1;
+    n++;
+  }
+  free(s);
+  fflush(stdout);
+}
 
-symbol_table *gpgpu_ptx_sim_load_ptx_from_string( const char *p, unsigned source_num )
-{
-    char buf[1024];
-    snprintf(buf,1024,"_%u.ptx", source_num );
-    if( g_save_embedded_ptx ) {
-       FILE *fp = fopen(buf,"w");
-       fprintf(fp,"%s",p);
-       fclose(fp);
-    }
-    symbol_table *symtab=init_parser(buf);
-    ptx__scan_string(p);
-    int errors = ptx_parse ();
-    if ( errors ) {
-        char fname[1024];
-        snprintf(fname,1024,"_ptx_errors_XXXXXX");
-        int fd=mkstemp(fname); 
-        close(fd);
-        printf("GPGPU-Sim PTX: parser error detected, exiting... but first extracting .ptx to \"%s\"\n", fname);
-        FILE *ptxfile = fopen(fname,"w");
-        fprintf(ptxfile,"%s", p );
-        fclose(ptxfile);
-        abort();
-        exit(40);
+char *ptxinfo_data::gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus(
+    const std::string ptxfilename, const std::string elffilename,
+    const std::string sassfilename) {
+  printf("GPGPU-Sim PTX: converting EMBEDDED .ptx file to ptxplus \n");
+
+  char fname_ptxplus[1024];
+  snprintf(fname_ptxplus, 1024, "_ptxplus_XXXXXX");
+  int fd4 = mkstemp(fname_ptxplus);
+  close(fd4);
+
+  // Run cuobjdump_to_ptxplus
+  char commandline[1024];
+  int result;
+  snprintf(commandline, 1024,
+           "$GPGPUSIM_ROOT/build/$GPGPUSIM_CONFIG/cuobjdump_to_ptxplus/"
+           "cuobjdump_to_ptxplus %s %s %s %s",
+           ptxfilename.c_str(), sassfilename.c_str(), elffilename.c_str(),
+           fname_ptxplus);
+  fflush(stdout);
+  printf("GPGPU-Sim PTX: calling cuobjdump_to_ptxplus\ncommandline: %s\n",
+         commandline);
+  result = system(commandline);
+  if (result) {
+    fprintf(stderr, "GPGPU-Sim PTX: ERROR ** could not execute %s\n",
+            commandline);
+    exit(1);
+  }
+
+  // Get ptxplus from file
+  std::ifstream fileStream(fname_ptxplus, std::ios::in);
+  std::string text, line;
+  while (getline(fileStream, line)) {
+    text += (line + "\n");
+  }
+  fileStream.close();
+
+  char *ptxplus_str = new char[strlen(text.c_str()) + 1];
+  strcpy(ptxplus_str, text.c_str());
+
+  if (!m_ptx_save_converted_ptxplus) {
+    char rm_commandline[1024];
+
+    snprintf(rm_commandline, 1024, "rm -f %s", fname_ptxplus);
+
+    printf("GPGPU-Sim PTX: removing temporary files using \"%s\"\n",
+           rm_commandline);
+    int rm_result = system(rm_commandline);
+    if (rm_result != 0) {
+      fprintf(stderr,
+              "GPGPU-Sim PTX: ERROR ** while removing temporary files %d\n",
+              rm_result);
+      exit(1);
     }
+  }
+  printf("GPGPU-Sim PTX: DONE converting EMBEDDED .ptx file to ptxplus \n");
 
-    if ( g_debug_execution >= 100 ) 
-       print_ptx_file(p,source_num,buf);
+  return ptxplus_str;
+}
 
-    printf("GPGPU-Sim PTX: finished parsing EMBEDDED .ptx file %s\n",buf);
-    return symtab;
+symbol_table *gpgpu_context::gpgpu_ptx_sim_load_ptx_from_string(
+    const char *p, unsigned source_num) {
+  char buf[1024];
+  snprintf(buf, 1024, "_%u.ptx", source_num);
+  if (g_save_embedded_ptx) {
+    FILE *fp = fopen(buf, "w");
+    fprintf(fp, "%s", p);
+    fclose(fp);
+  }
+  symbol_table *symtab = init_parser(buf);
+  ptx_lex_init(&(ptx_parser->scanner));
+  ptx__scan_string(p, ptx_parser->scanner);
+  int errors = ptx_parse(ptx_parser->scanner, ptx_parser);
+  if (errors) {
+    char fname[1024];
+    snprintf(fname, 1024, "_ptx_errors_XXXXXX");
+    int fd = mkstemp(fname);
+    close(fd);
+    printf(
+        "GPGPU-Sim PTX: parser error detected, exiting... but first extracting "
+        ".ptx to \"%s\"\n",
+        fname);
+    FILE *ptxfile = fopen(fname, "w");
+    fprintf(ptxfile, "%s", p);
+    fclose(ptxfile);
+    abort();
+    exit(40);
+  }
+  ptx_lex_destroy(ptx_parser->scanner);
+
+  if (g_debug_execution >= 100) print_ptx_file(p, source_num, buf);
+
+  printf("GPGPU-Sim PTX: finished parsing EMBEDDED .ptx file %s\n", buf);
+  return symtab;
 }
 
-symbol_table *gpgpu_ptx_sim_load_ptx_from_filename( const char *filename )
-{
-    symbol_table *symtab=init_parser(filename);
-    printf("GPGPU-Sim PTX: finished parsing EMBEDDED .ptx file %s\n",filename);
-    return symtab;
+symbol_table *gpgpu_context::gpgpu_ptx_sim_load_ptx_from_filename(
+    const char *filename) {
+  symbol_table *symtab = init_parser(filename);
+  printf("GPGPU-Sim PTX: finished parsing EMBEDDED .ptx file %s\n", filename);
+  return symtab;
 }
 
 void fix_duplicate_errors(char fname2[1024]) {
-	char tempfile[1024] = "_temp_ptx";
-	char commandline[1024];
-
-	// change the name of the ptx file to _temp_ptx
-	snprintf(commandline,1024,"mv %s %s",fname2,tempfile);
-	printf("Running: %s\n", commandline);
-	int result = system(commandline);
-	if (result != 0) {
-		fprintf(stderr, "GPGPU-Sim PTX: ERROR ** while changing filename from %s to %s", fname2, tempfile);
-		exit(1);
-	}
-
-	// store all of the ptx into a char array
-	FILE *ptxsource = fopen(tempfile,"r");
-	fseek(ptxsource, 0, SEEK_END);
-	long filesize = ftell(ptxsource);
-	rewind(ptxsource);
-	char *ptxdata = (char*)malloc((filesize+1)*sizeof(char));
-	fread(ptxdata, filesize, 1, ptxsource);
-	fclose(ptxsource);
-
-	FILE *ptxdest = fopen(fname2,"w");
-	std::map<unsigned,const char*> duplicate = get_duplicate();
-	unsigned offset;
-	unsigned oldlinenum = 1;
-	unsigned linenum;
-	char *startptr = ptxdata;
-	char *funcptr;
-	char *tempptr = ptxdata - 1;
-	char *lineptr = ptxdata - 1;
-
-	// recreate the ptx file without duplications
-	for (	std::map<unsigned,const char*>::iterator iter = duplicate.begin();
-			iter != duplicate.end();
-			iter++){
-		// find the line of the next error
-		linenum = iter->first;
-		for (int i = oldlinenum; i < linenum; i++) {
-			lineptr = strchr(lineptr + 1, '\n');
-		}
-		
-		// find the end of the current section to be copied over
-		// then find the start of the next section that will be copied
-		if (strcmp("function", iter->second) == 0) {
-			// get location of most recent .func
-			while (tempptr < lineptr && tempptr != NULL) {
-				funcptr = tempptr;
-				tempptr = strstr(funcptr + 1, ".func");
-			}
-
-			// get the start of the previous line
-			offset = 0;
-			while (*(funcptr - offset) != '\n') offset++;
-
-			fwrite(startptr, sizeof(char), funcptr - offset + 1 - startptr, ptxdest);
-
-			//find next location of startptr
-			if (*(lineptr + 3) == ';') {
-				// for function definitions
-				startptr = lineptr + 5;
-			} else if (*(lineptr + 3) == '{') {
-				// for functions enclosed with curly brackets
-				offset = 5;
-				unsigned bracket = 1;
-				while (bracket != 0) {
-					if (*(lineptr + offset) == '{') bracket++;
-					else if (*(lineptr + offset) == '}') bracket--;
-					offset++;
-				}
-				startptr = lineptr + offset + 1;
-			} else {
-				printf("GPGPU-Sim PTX: ERROR ** Unrecognized function format\n");
-				abort();
-			}
-		} else if (strcmp("variable", iter->second) == 0) {
-			fwrite(startptr, sizeof(char), (int)(lineptr + 1 - startptr), ptxdest);
-			
-			//find next location of startptr
-			offset = 1;
-			while (*(lineptr + offset) != '\n') offset++;
-			startptr = lineptr + offset + 1;
-		} else {
-			printf("GPGPU-Sim PTX: ERROR ** Unsupported duplicate type: %s\n", iter->second);
-		}
-
-		oldlinenum = linenum;
-	}
-	// copy over the rest of the file
-	fwrite(startptr, sizeof(char), ptxdata + filesize - startptr, ptxdest);
-
-	// cleanup
-	free(ptxdata);
-	fclose(ptxdest);
-	snprintf(commandline,1024,"rm -f %s",tempfile);
-	printf("Running: %s\n", commandline);
-	result = system(commandline);
-	if (result != 0) {
-		fprintf(stderr, "GPGPU-Sim PTX: ERROR ** while deleting %s", tempfile);
-		exit(1);
-	}
-}
+  char tempfile[1024] = "_temp_ptx";
+  char commandline[1024];
+
+  // change the name of the ptx file to _temp_ptx
+  snprintf(commandline, 1024, "mv %s %s", fname2, tempfile);
+  printf("Running: %s\n", commandline);
+  int result = system(commandline);
+  if (result != 0) {
+    fprintf(stderr,
+            "GPGPU-Sim PTX: ERROR ** while changing filename from %s to %s",
+            fname2, tempfile);
+    exit(1);
+  }
+
+  // store all of the ptx into a char array
+  FILE *ptxsource = fopen(tempfile, "r");
+  fseek(ptxsource, 0, SEEK_END);
+  long filesize = ftell(ptxsource);
+  rewind(ptxsource);
+  char *ptxdata = (char *)malloc((filesize + 1) * sizeof(char));
+  // Fail if we do not read the file
+  assert(fread(ptxdata, filesize, 1, ptxsource) == 1);
+  fclose(ptxsource);
+
+  FILE *ptxdest = fopen(fname2, "w");
+  std::map<unsigned, const char *> duplicate = get_duplicate();
+  unsigned offset;
+  unsigned oldlinenum = 1;
+  unsigned linenum;
+  char *startptr = ptxdata;
+  char *funcptr;
+  char *tempptr = ptxdata - 1;
+  char *lineptr = ptxdata - 1;
+
+  // recreate the ptx file without duplications
+  for (std::map<unsigned, const char *>::iterator iter = duplicate.begin();
+       iter != duplicate.end(); iter++) {
+    // find the line of the next error
+    linenum = iter->first;
+    for (int i = oldlinenum; i < linenum; i++) {
+      lineptr = strchr(lineptr + 1, '\n');
+    }
 
+    // find the end of the current section to be copied over
+    // then find the start of the next section that will be copied
+    if (strcmp("function", iter->second) == 0) {
+      // get location of most recent .func
+      while (tempptr < lineptr && tempptr != NULL) {
+        funcptr = tempptr;
+        tempptr = strstr(funcptr + 1, ".func");
+      }
+
+      // get the start of the previous line
+      offset = 0;
+      while (*(funcptr - offset) != '\n') offset++;
+
+      fwrite(startptr, sizeof(char), funcptr - offset + 1 - startptr, ptxdest);
+
+      // find next location of startptr
+      if (*(lineptr + 3) == ';') {
+        // for function definitions
+        startptr = lineptr + 5;
+      } else if (*(lineptr + 3) == '{') {
+        // for functions enclosed with curly brackets
+        offset = 5;
+        unsigned bracket = 1;
+        while (bracket != 0) {
+          if (*(lineptr + offset) == '{')
+            bracket++;
+          else if (*(lineptr + offset) == '}')
+            bracket--;
+          offset++;
+        }
+        startptr = lineptr + offset + 1;
+      } else {
+        printf("GPGPU-Sim PTX: ERROR ** Unrecognized function format\n");
+        abort();
+      }
+    } else if (strcmp("variable", iter->second) == 0) {
+      fwrite(startptr, sizeof(char), (int)(lineptr + 1 - startptr), ptxdest);
+
+      // find next location of startptr
+      offset = 1;
+      while (*(lineptr + offset) != '\n') offset++;
+      startptr = lineptr + offset + 1;
+    } else {
+      printf("GPGPU-Sim PTX: ERROR ** Unsupported duplicate type: %s\n",
+             iter->second);
+    }
 
-//we need the application name here too.
-char* get_app_binary_name(){
-   char exe_path[1025];
-   char *self_exe_path;
+    oldlinenum = linenum;
+  }
+  // copy over the rest of the file
+  fwrite(startptr, sizeof(char), ptxdata + filesize - startptr, ptxdest);
+
+  // cleanup
+  free(ptxdata);
+  fclose(ptxdest);
+  snprintf(commandline, 1024, "rm -f %s", tempfile);
+  printf("Running: %s\n", commandline);
+  result = system(commandline);
+  if (result != 0) {
+    fprintf(stderr, "GPGPU-Sim PTX: ERROR ** while deleting %s", tempfile);
+    exit(1);
+  }
+}
+
+// we need the application name here too.
+char *get_app_binary_name() {
+  char exe_path[1025];
+  char *self_exe_path;
 #ifdef __APPLE__
-   //AMRUTH:  get apple device and check the result.
-   printf("WARNING: not tested for Apple-mac devices \n");
-   abort();
+  // AMRUTH:  get apple device and check the result.
+  printf("WARNING: not tested for Apple-mac devices \n");
+  abort();
 #else
-   std::stringstream exec_link;
-   exec_link << "/proc/self/exe";
-   ssize_t path_length = readlink(exec_link.str().c_str(), exe_path, 1024);
-   assert(path_length != -1);
-   exe_path[path_length] = '\0';
-
-   char *token = strtok(exe_path, "/");
-   while(token !=NULL){
-        self_exe_path = token;
-        token = strtok(NULL,"/");
-   }
+  std::stringstream exec_link;
+  exec_link << "/proc/self/exe";
+  ssize_t path_length = readlink(exec_link.str().c_str(), exe_path, 1024);
+  assert(path_length != -1);
+  exe_path[path_length] = '\0';
+
+  char *token = strtok(exe_path, "/");
+  while (token != NULL) {
+    self_exe_path = token;
+    token = strtok(NULL, "/");
+  }
 #endif
-   self_exe_path = strtok(self_exe_path, ".");
-   printf("self exe links to: %s\n", self_exe_path);
-   return self_exe_path;
+  self_exe_path = strtok(self_exe_path, ".");
+  printf("self exe links to: %s\n", self_exe_path);
+  return self_exe_path;
 }
 
-void gpgpu_ptx_info_load_from_filename( const char *filename, unsigned sm_version)
-{
-    std::string ptxas_filename(std::string(filename) + "as");
-    char buff[1024], extra_flags[1024];
-	extra_flags[0]=0;
-    extern bool g_cdp_enabled;
-	if(!g_cdp_enabled)
-	       	snprintf(extra_flags,1024,"--gpu-name=sm_%u",sm_version);
-	else
-	       	snprintf(extra_flags,1024,"--compile-only --gpu-name=sm_%u",sm_version);
-   	snprintf(buff,1024,"$CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  /dev/null 2> %s",
-         extra_flags, filename, ptxas_filename.c_str());
-	int result = system(buff);
-	if( result != 0 ) {
-		printf("GPGPU-Sim PTX: ERROR ** while loading PTX (b) %d\n", result);
-		printf("               Ensure ptxas is in your path.\n");
-		exit(1);
-	}
-
-	g_ptxinfo_filename = strdup(ptxas_filename.c_str());
-    ptxinfo_in = fopen(g_ptxinfo_filename,"r");
-    ptxinfo_parse();
-    fclose(ptxinfo_in);
+void gpgpu_context::gpgpu_ptx_info_load_from_filename(const char *filename,
+                                                      unsigned sm_version) {
+  std::string ptxas_filename(std::string(filename) + "as");
+  char buff[1024], extra_flags[1024];
+  extra_flags[0] = 0;
+  if (!device_runtime->g_cdp_enabled)
+    snprintf(extra_flags, 1024, "--gpu-name=sm_%u", sm_version);
+  else
+    snprintf(extra_flags, 1024, "--compile-only --gpu-name=sm_%u", sm_version);
+  snprintf(
+      buff, 1024,
+      "$CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  /dev/null 2> %s",
+      extra_flags, filename, ptxas_filename.c_str());
+  int result = system(buff);
+  if (result != 0) {
+    printf("GPGPU-Sim PTX: ERROR ** while loading PTX (b) %d\n", result);
+    printf("               Ensure ptxas is in your path.\n");
+    exit(1);
+  }
+
+  FILE *ptxinfo_in;
+  ptxinfo->g_ptxinfo_filename = strdup(ptxas_filename.c_str());
+  ptxinfo_in = fopen(ptxinfo->g_ptxinfo_filename, "r");
+  ptxinfo_lex_init(&(ptxinfo->scanner));
+  ptxinfo_set_in(ptxinfo_in, ptxinfo->scanner);
+  ptxinfo_parse(ptxinfo->scanner, ptxinfo);
+  ptxinfo_lex_destroy(ptxinfo->scanner);
+  fclose(ptxinfo_in);
 }
 
-void gpgpu_ptxinfo_load_from_string( const char *p_for_info, unsigned source_num, unsigned sm_version )
-{
-    //do ptxas for individual files instead of one big embedded ptx. This prevents the duplicate defs and declarations.
-    char ptx_file[1000];
-    char *name=get_app_binary_name();
-    char commandline[4096], fname[1024], fname2[1024], final_tempfile_ptxinfo[1024], tempfile_ptxinfo[1024];
-    for (int index=1; index <= no_of_ptx; index++){
-        snprintf(ptx_file, 1000, "%s.%d.sm_%u.ptx", name, index, sm_version);
-        snprintf(fname,1024,"_ptx_XXXXXX");
-        int fd=mkstemp(fname); 
-        close(fd);
-
-        printf("GPGPU-Sim PTX: extracting embedded .ptx to temporary file \"%s\"\n", fname);
-        snprintf(commandline,4096,"cat %s > %s",ptx_file, fname);
-        if (system(commandline) !=0) {
-            printf("ERROR: %s command failed\n", commandline);
-            exit(0);
-        }
-	
-        snprintf(fname2,1024,"_ptx2_XXXXXX");
-        fd=mkstemp(fname2); 
-        close(fd);
-        char commandline2[4096];
-        snprintf(commandline2,4096,"cat %s | sed 's/.version 1.5/.version 1.4/' | sed 's/, texmode_independent//' | sed 's/\\(\\.extern \\.const\\[1\\] .b8 \\w\\+\\)\\[\\]/\\1\\[1\\]/' | sed 's/const\\[.\\]/const\\[0\\]/g' > %s", fname, fname2);
-        printf("Running: %s\n", commandline2);
-        int result = system(commandline2);
-        if( result != 0 ) {
-       	    printf("GPGPU-Sim PTX: ERROR ** while loading PTX (a) %d\n", result);
-       	    printf("               Ensure you have write access to simulation directory\n");
-       	    printf("               and have \'cat\' and \'sed\' in your path.\n");
-       	    exit(1);
-    	}
-
-        snprintf(tempfile_ptxinfo,1024,"%sinfo",fname);
-        char extra_flags[1024];
-        extra_flags[0]=0;
+void gpgpu_context::gpgpu_ptxinfo_load_from_string(const char *p_for_info,
+                                                   unsigned source_num,
+                                                   unsigned sm_version,
+                                                   int no_of_ptx) {
+  // do ptxas for individual files instead of one big embedded ptx. This
+  // prevents the duplicate defs and declarations.
+  char ptx_file[1000];
+  char *name = get_app_binary_name();
+  char commandline[4096], fname[1024], fname2[1024],
+      final_tempfile_ptxinfo[1024], tempfile_ptxinfo[1024];
+  for (int index = 1; index <= no_of_ptx; index++) {
+    snprintf(ptx_file, 1000, "%s.%d.sm_%u.ptx", name, index, sm_version);
+    snprintf(fname, 1024, "_ptx_XXXXXX");
+    int fd = mkstemp(fname);
+    close(fd);
+
+    printf("GPGPU-Sim PTX: extracting embedded .ptx to temporary file \"%s\"\n",
+           fname);
+    snprintf(commandline, 4096, "cat %s > %s", ptx_file, fname);
+    if (system(commandline) != 0) {
+      printf("ERROR: %s command failed\n", commandline);
+      exit(0);
+    }
+
+    snprintf(fname2, 1024, "_ptx2_XXXXXX");
+    fd = mkstemp(fname2);
+    close(fd);
+    char commandline2[4096];
+    snprintf(commandline2, 4096,
+             "cat %s | sed 's/.version 1.5/.version 1.4/' | sed 's/, "
+             "texmode_independent//' | sed 's/\\(\\.extern \\.const\\[1\\] .b8 "
+             "\\w\\+\\)\\[\\]/\\1\\[1\\]/' | sed "
+             "'s/const\\[.\\]/const\\[0\\]/g' > %s",
+             fname, fname2);
+    printf("Running: %s\n", commandline2);
+    int result = system(commandline2);
+    if (result != 0) {
+      printf("GPGPU-Sim PTX: ERROR ** while loading PTX (a) %d\n", result);
+      printf(
+          "               Ensure you have write access to simulation "
+          "directory\n");
+      printf("               and have \'cat\' and \'sed\' in your path.\n");
+      exit(1);
+    }
+
+    snprintf(tempfile_ptxinfo, 1024, "%sinfo", fname);
+    char extra_flags[1024];
+    extra_flags[0] = 0;
 
 #if CUDART_VERSION >= 3000
-    if ( g_occupancy_sm_number == 0 ) {
-        fprintf( stderr, "gpgpusim.config must specify the sm version for the GPU that you use to compute occupancy \"-gpgpu_occupancy_sm_number XX\".\n"
-                         "The register file size is specifically tied to the sm version used to querry ptxas for register usage.\n"
-                         "A register size/SM mismatch may result in occupancy differences." );
-        exit(1);
+    if (g_occupancy_sm_number == 0) {
+      fprintf(
+          stderr,
+          "gpgpusim.config must specify the sm version for the GPU that you "
+          "use to compute occupancy \"-gpgpu_occupancy_sm_number XX\".\n"
+          "The register file size is specifically tied to the sm version used "
+          "to querry ptxas for register usage.\n"
+          "A register size/SM mismatch may result in occupancy differences.");
+      exit(1);
     }
-    extern bool g_cdp_enabled;
-    if(!g_cdp_enabled)
-        snprintf(extra_flags,1024,"--gpu-name=sm_%u", g_occupancy_sm_number);
+    if (!device_runtime->g_cdp_enabled)
+      snprintf(extra_flags, 1024, "--gpu-name=sm_%u", g_occupancy_sm_number);
     else
-        snprintf(extra_flags,1024,"--compile-only --gpu-name=sm_%u",g_occupancy_sm_number);
+      snprintf(extra_flags, 1024, "--compile-only --gpu-name=sm_%u",
+               g_occupancy_sm_number);
 #endif
 
-    snprintf(commandline,1024,"$PTXAS_CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  /dev/null 2> %s",
+    snprintf(commandline, 1024,
+             "$PTXAS_CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  "
+             "/dev/null 2> %s",
              extra_flags, fname2, tempfile_ptxinfo);
     printf("GPGPU-Sim PTX: generating ptxinfo using \"%s\"\n", commandline);
     result = system(commandline);
-    if( result != 0 ) {
-    	// 65280 = duplicate errors
-    	if (result == 65280) {
-    		ptxinfo_in = fopen(tempfile_ptxinfo,"r");
-		g_ptxinfo_filename = tempfile_ptxinfo;
-		ptxinfo_parse();
-
-    		fix_duplicate_errors(fname2);
-    		snprintf(commandline,1024,"$CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  /dev/null 2> %s",
-    			 extra_flags, fname2, tempfile_ptxinfo);
-    		printf("GPGPU-Sim PTX: regenerating ptxinfo using \"%s\"\n", commandline);
-    		result = system(commandline);
-	    }
-	    if (result != 0) {
-		printf("GPGPU-Sim PTX: ERROR ** while loading PTX (b) %d\n", result);
-		printf("               Ensure ptxas is in your path.\n");
-		exit(1);
-	    }
-    	}
+    if (result != 0) {
+      // 65280 = duplicate errors
+      if (result == 65280) {
+        FILE *ptxinfo_in;
+        ptxinfo_in = fopen(tempfile_ptxinfo, "r");
+        ptxinfo->g_ptxinfo_filename = tempfile_ptxinfo;
+        ptxinfo_lex_init(&(ptxinfo->scanner));
+        ptxinfo_set_in(ptxinfo_in, ptxinfo->scanner);
+        ptxinfo_parse(ptxinfo->scanner, ptxinfo);
+        ptxinfo_lex_destroy(ptxinfo->scanner);
+        fclose(ptxinfo_in);
+
+        fix_duplicate_errors(fname2);
+        snprintf(commandline, 1024,
+                 "$CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  "
+                 "/dev/null 2> %s",
+                 extra_flags, fname2, tempfile_ptxinfo);
+        printf("GPGPU-Sim PTX: regenerating ptxinfo using \"%s\"\n",
+               commandline);
+        result = system(commandline);
+      }
+      if (result != 0) {
+        printf("GPGPU-Sim PTX: ERROR ** while loading PTX (b) %d\n", result);
+        printf("               Ensure ptxas is in your path.\n");
+        exit(1);
+      }
     }
-
-    //TODO: duplicate code! move it into a function so that it can be reused!
-    if(no_of_ptx==0) {
-        //For CDP, we dump everything. So no_of_ptx will be 0.
-	snprintf(fname,1024,"_ptx_XXXXXX");
-	int fd=mkstemp(fname);
-	close(fd);
-
-	printf("GPGPU-Sim PTX: extracting embedded .ptx to temporary file \"%s\"\n", fname);
-	FILE *ptxfile = fopen(fname,"w");
-	fprintf(ptxfile,"%s", p_for_info);
-	fclose(ptxfile);
-
-	snprintf(fname2,1024,"_ptx2_XXXXXX");
-	fd=mkstemp(fname2);
-	close(fd);
-	char commandline2[4096];
-	snprintf(commandline2,4096,"cat %s | sed 's/.version 1.5/.version 1.4/' | sed 's/, texmode_independent//' | sed 's/\\(\\.extern \\.const\\[1\\] .b8 \\w\\+\\)\\[\\]/\\1\\[1\\]/' | sed 's/const\\[.\\]/const\\[0\\]/g' > %s", fname, fname2);
-	printf("Running: %s\n", commandline2);
-	int result = system(commandline2);
-	if( result != 0 ) {
-		printf("GPGPU-Sim PTX: ERROR ** while loading PTX (a) %d\n", result);
-		printf("               Ensure you have write access to simulation directory\n");
-		printf("               and have \'cat\' and \'sed\' in your path.\n");
-		exit(1);
-	}
-	//char tempfile_ptxinfo[1024];
-	snprintf(tempfile_ptxinfo,1024,"%sinfo",fname);
-	char extra_flags[1024];
-	extra_flags[0]=0;
-
-	#if CUDART_VERSION >= 3000
-	if (sm_version == 0) sm_version = 20;
-    	extern bool g_cdp_enabled;
-	if(!g_cdp_enabled)
-	       	snprintf(extra_flags,1024,"--gpu-name=sm_%u",sm_version);
-	else
-	       	snprintf(extra_flags,1024,"--compile-only --gpu-name=sm_%u",sm_version);
-	#endif
-
-	snprintf(commandline,1024,"$CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  /dev/null 2> %s",
-			            extra_flags, fname2, tempfile_ptxinfo);
-	printf("GPGPU-Sim PTX: generating ptxinfo using \"%s\"\n", commandline);
-	fflush(stdout);
-	result = system(commandline);
-	if( result != 0 ) {
-		printf("GPGPU-Sim PTX: ERROR ** while loading PTX (b) %d\n", result);
-		printf("               Ensure ptxas is in your path.\n");
-		exit(1);
-	}
+  }
+
+  // TODO: duplicate code! move it into a function so that it can be reused!
+  if (no_of_ptx == 0) {
+    // For CDP, we dump everything. So no_of_ptx will be 0.
+    snprintf(fname, 1024, "_ptx_XXXXXX");
+    int fd = mkstemp(fname);
+    close(fd);
+
+    printf("GPGPU-Sim PTX: extracting embedded .ptx to temporary file \"%s\"\n",
+           fname);
+    FILE *ptxfile = fopen(fname, "w");
+    fprintf(ptxfile, "%s", p_for_info);
+    fclose(ptxfile);
+
+    snprintf(fname2, 1024, "_ptx2_XXXXXX");
+    fd = mkstemp(fname2);
+    close(fd);
+    char commandline2[4096];
+    snprintf(commandline2, 4096,
+             "cat %s | sed 's/.version 1.5/.version 1.4/' | sed 's/, "
+             "texmode_independent//' | sed 's/\\(\\.extern \\.const\\[1\\] .b8 "
+             "\\w\\+\\)\\[\\]/\\1\\[1\\]/' | sed "
+             "'s/const\\[.\\]/const\\[0\\]/g' > %s",
+             fname, fname2);
+    printf("Running: %s\n", commandline2);
+    int result = system(commandline2);
+    if (result != 0) {
+      printf("GPGPU-Sim PTX: ERROR ** while loading PTX (a) %d\n", result);
+      printf(
+          "               Ensure you have write access to simulation "
+          "directory\n");
+      printf("               and have \'cat\' and \'sed\' in your path.\n");
+      exit(1);
     }
+    // char tempfile_ptxinfo[1024];
+    snprintf(tempfile_ptxinfo, 1024, "%sinfo", fname);
+    char extra_flags[1024];
+    extra_flags[0] = 0;
 
-    //Now that we got resource usage per kernel in a ptx file, we dump all into one file and pass it to rest of the code as usual.
-    if(no_of_ptx>0){
-        char commandline3[4096];
-        snprintf(final_tempfile_ptxinfo,1024,"f_tempfile_ptx");
-        snprintf(commandline3,4096, "cat *info > %s", final_tempfile_ptxinfo);
-        if (system(commandline3)!=0) {
-	    printf("ERROR: Either we dont have info files or cat is not working \n");
-            printf("ERROR: %s command failed\n",commandline3);
-	    exit(1);
-        }
-    }	
-
-    if(no_of_ptx>0)
-        g_ptxinfo_filename = final_tempfile_ptxinfo;
+#if CUDART_VERSION >= 3000
+    if (sm_version == 0) sm_version = 20;
+    if (!device_runtime->g_cdp_enabled)
+      snprintf(extra_flags, 1024, "--gpu-name=sm_%u", sm_version);
     else
-	g_ptxinfo_filename = tempfile_ptxinfo;
-    ptxinfo_in = fopen(g_ptxinfo_filename,"r");
-
-    ptxinfo_parse();
+      snprintf(extra_flags, 1024, "--compile-only --gpu-name=sm_%u",
+               sm_version);
+#endif
 
-    snprintf(commandline,1024,"rm -f *info");
-    if( system(commandline) != 0 ) {
-	    printf("GPGPU-Sim PTX: ERROR ** while removing temporary info files\n");
-	    exit(1);
+    snprintf(
+        commandline, 1024,
+        "$CUDA_INSTALL_PATH/bin/ptxas %s -v %s --output-file  /dev/null 2> %s",
+        extra_flags, fname2, tempfile_ptxinfo);
+    printf("GPGPU-Sim PTX: generating ptxinfo using \"%s\"\n", commandline);
+    fflush(stdout);
+    result = system(commandline);
+    if (result != 0) {
+      printf("GPGPU-Sim PTX: ERROR ** while loading PTX (b) %d\n", result);
+      printf("               Ensure ptxas is in your path.\n");
+      exit(1);
     }
-    if( ! g_save_embedded_ptx ) {
-	if(no_of_ptx>0)
-            snprintf(commandline,1024,"rm -f %s %s %s", fname, fname2, final_tempfile_ptxinfo);
-	else
-            snprintf(commandline,1024,"rm -f %s %s %s", fname, fname2, tempfile_ptxinfo);
-        printf("GPGPU-Sim PTX: removing ptxinfo using \"%s\"\n", commandline);
-        if( system(commandline) != 0 ) {
-    	    printf("GPGPU-Sim PTX: ERROR ** while removing temporary files\n");
-    	    exit(1);
-        }
+  }
+
+  // Now that we got resource usage per kernel in a ptx file, we dump all into
+  // one file and pass it to rest of the code as usual.
+  if (no_of_ptx > 0) {
+    char commandline3[4096];
+    snprintf(final_tempfile_ptxinfo, 1024, "f_tempfile_ptx");
+    snprintf(commandline3, 4096, "cat *info > %s", final_tempfile_ptxinfo);
+    if (system(commandline3) != 0) {
+      printf("ERROR: Either we dont have info files or cat is not working \n");
+      printf("ERROR: %s command failed\n", commandline3);
+      exit(1);
+    }
+  }
+
+  if (no_of_ptx > 0)
+    ptxinfo->g_ptxinfo_filename = final_tempfile_ptxinfo;
+  else
+    ptxinfo->g_ptxinfo_filename = tempfile_ptxinfo;
+  FILE *ptxinfo_in;
+  ptxinfo_in = fopen(ptxinfo->g_ptxinfo_filename, "r");
+
+  ptxinfo_lex_init(&(ptxinfo->scanner));
+  ptxinfo_set_in(ptxinfo_in, ptxinfo->scanner);
+  ptxinfo_parse(ptxinfo->scanner, ptxinfo);
+  ptxinfo_lex_destroy(ptxinfo->scanner);
+  fclose(ptxinfo_in);
+
+  snprintf(commandline, 1024, "rm -f *info");
+  if (system(commandline) != 0) {
+    printf("GPGPU-Sim PTX: ERROR ** while removing temporary info files\n");
+    exit(1);
+  }
+  if (!g_save_embedded_ptx) {
+    if (no_of_ptx > 0)
+      snprintf(commandline, 1024, "rm -f %s %s %s", fname, fname2,
+               final_tempfile_ptxinfo);
+    else
+      snprintf(commandline, 1024, "rm -f %s %s %s", fname, fname2,
+               tempfile_ptxinfo);
+    printf("GPGPU-Sim PTX: removing ptxinfo using \"%s\"\n", commandline);
+    if (system(commandline) != 0) {
+      printf("GPGPU-Sim PTX: ERROR ** while removing temporary files\n");
+      exit(1);
     }
+  }
 }
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.h
index 14b8984b01..db39befbcf 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_loader.h
@@ -29,14 +29,26 @@
 #define PTX_LOADER_H_INCLUDED
 #include <string>
 
-extern bool g_override_embedded_ptx;
-extern "C" int no_of_ptx; //counter to track number of ptx files to be extracted in an application.
- 
-class symbol_table *gpgpu_ptx_sim_load_ptx_from_string( const char *p, unsigned source_num );
-class symbol_table *gpgpu_ptx_sim_load_ptx_from_filename( const char *filename );
-void gpgpu_ptxinfo_load_from_string( const char *p_for_info, unsigned source_num, unsigned sm_version=20 );
-void gpgpu_ptx_info_load_from_filename( const char *filename, unsigned sm_version );
-char* gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus(const std::string ptx_str, const std::string sass_str, const std::string elf_str);
-bool keep_intermediate_files();
+extern "C" void gem5_ptxinfo_addinfo();
+
+#define PTXINFO_LINEBUF_SIZE 1024
+class gpgpu_context;
+typedef void* yyscan_t;
+class ptxinfo_data {
+ public:
+  ptxinfo_data(gpgpu_context* ctx) { gpgpu_ctx = ctx; }
+  yyscan_t scanner;
+  char linebuf[PTXINFO_LINEBUF_SIZE];
+  unsigned col;
+  const char* g_ptxinfo_filename;
+  class gpgpu_context* gpgpu_ctx;
+  bool g_keep_intermediate_files;
+  bool m_ptx_save_converted_ptxplus;
+  void ptxinfo_addinfo() { gem5_ptxinfo_addinfo(); };
+  bool keep_intermediate_files();
+  char* gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus(
+      const std::string ptx_str, const std::string sass_str,
+      const std::string elf_str);
+};
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.cc
index 411f76e558..9df7bd183d 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.cc
@@ -26,539 +26,469 @@
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #include "ptx_parser.h"
+#include "../../libcuda_sim/gpgpu_context.h"
 #include "ptx_ir.h"
 #include "ptx.tab.h"
 
-#ifndef LIBCUDA
 #include "gpu/gpgpu-sim/cuda_gpu.hh"
-#else
 
+#if 0
 namespace m5op {
     extern "C" {
 #include "../libgem5cuda/m5op.h"
     }
 }
-
 #endif
 
+
+typedef void *yyscan_t;
 #include <stdarg.h>
+#include "ptx.tab.h"
 
-extern gpgpu_sim *g_the_gpu;
-extern "C" int ptx_error( const char *s );
-extern int ptx_lineno;
-extern "C" int ptx_parse();
-extern FILE *ptx_in;
-
-static const struct core_config *g_shader_core_config;
-void set_ptx_warp_size(const struct core_config * warp_size)
-{
-   g_shader_core_config=warp_size;
-}
-
-static bool g_debug_ir_generation=false;
-const char *g_filename;
-unsigned g_max_regs_per_thread = 0;
-
-// the program intermediate representation...
-static symbol_table *g_global_allfiles_symbol_table = NULL;
-static symbol_table *g_global_symbol_table = NULL;
-std::map<std::string,symbol_table*> g_sym_name_to_symbol_table;
-static symbol_table *g_current_symbol_table = NULL;
-static std::list<ptx_instruction*> g_instructions;
-static symbol *g_last_symbol = NULL;
-
-int g_error_detected = 0;
-
-// type specifier stuff:
-memory_space_t g_space_spec = undefined_space;
-memory_space_t g_ptr_spec = undefined_space;
-int g_scalar_type_spec = -1;
-int g_vector_spec = -1;
-int g_alignment_spec = -1;
-int g_size = -1;
-int g_extern_spec = 0;
-
-// variable declaration stuff:
-type_info *g_var_type = NULL;
-
-// instruction definition stuff:
-const symbol *g_pred;
-int g_neg_pred;
-int g_pred_mod;
-symbol *g_label;
-int g_opcode = -1;
-std::list<operand_info> g_operands;
-std::list<int> g_options;
-std::list<int> g_wmma_options;
-std::list<int> g_scalar_type;
-
-#define PTX_PARSE_DPRINTF(...) \
-   if( g_debug_ir_generation ) { \
-      printf(" %s:%u => ",g_filename,ptx_lineno); \
-      printf("   (%s:%u) ", __FILE__, __LINE__); \
-      printf(__VA_ARGS__); \
-      printf("\n"); \
-      fflush(stdout); \
-   }
-
-static unsigned g_entry_func_param_index=0;
-static function_info *g_func_info = NULL;
-static std::map<unsigned,std::string> g_ptx_token_decode;
-static operand_info g_return_var;
-
-const char *decode_token( int type )
-{
-   return g_ptx_token_decode[type].c_str();
-}
-
-void read_parser_environment_variables() 
-{
-   g_filename = getenv("PTX_SIM_KERNELFILE"); 
-   char *dbg_level = getenv("PTX_SIM_DEBUG");
-   if ( dbg_level && strlen(dbg_level) ) {
-      int debug_execution=0;
-      sscanf(dbg_level,"%d", &debug_execution);
-      if ( debug_execution >= 30 ) 
-         g_debug_ir_generation=true;
-   }
-}
-
-void init_directive_state()
-{
-   PTX_PARSE_DPRINTF("init_directive_state");
-   g_space_spec=undefined_space;
-   g_ptr_spec=undefined_space;
-   g_scalar_type_spec=-1;
-   g_vector_spec=-1;
-   g_opcode=-1;
-   g_alignment_spec = -1;
-   g_size = -1;
-   g_extern_spec = 0;
-   g_scalar_type.clear();
-   g_operands.clear();
-   g_last_symbol = NULL;
-}
-
-void init_instruction_state()
-{
-   PTX_PARSE_DPRINTF("init_instruction_state");
-   g_pred = NULL;
-   g_neg_pred = 0;
-   g_pred_mod = -1;
-   g_label = NULL;
-   g_opcode = -1;
-   g_options.clear();
-   g_wmma_options.clear();
-   g_return_var = operand_info();
-   init_directive_state();
-}
-
-symbol_table *init_parser( const char *ptx_filename )
-{
-   g_filename = strdup(ptx_filename);
-   if  (g_global_allfiles_symbol_table == NULL) {
-       g_global_allfiles_symbol_table = new symbol_table("global_allfiles", 0, NULL);
-       g_global_symbol_table = g_current_symbol_table = g_global_allfiles_symbol_table;
-   }
-   /*else {
-       g_global_symbol_table = g_current_symbol_table = new symbol_table("global",0,g_global_allfiles_symbol_table);
-   }*/
-   ptx_lineno = 1;
-
-#define DEF(X,Y) g_ptx_token_decode[X] = Y;
+extern int ptx_get_lineno(yyscan_t yyscanner);
+extern YYSTYPE *ptx_get_lval(yyscan_t yyscanner);
+extern int ptx_error(yyscan_t yyscanner, ptx_recognizer *recognizer,
+                     const char *s);
+extern int ptx_lex_init(yyscan_t *scanner);
+extern void ptx_set_in(FILE *_in_str, yyscan_t yyscanner);
+extern FILE *ptx_get_in(yyscan_t yyscanner);
+extern int ptx_parse(yyscan_t scanner, ptx_recognizer *recognizer);
+extern int ptx_lex_destroy(yyscan_t scanner);
+
+void ptx_recognizer::set_ptx_warp_size(const struct core_config *warp_size) {
+  g_shader_core_config = warp_size;
+}
+
+#define PTX_PARSE_DPRINTF(...)                                            \
+  if (g_debug_ir_generation) {                                            \
+    printf(" %s:%u => ", gpgpu_ctx->g_filename, ptx_get_lineno(scanner)); \
+    printf("   (%s:%u) ", __FILE__, __LINE__);                            \
+    printf(__VA_ARGS__);                                                  \
+    printf("\n");                                                         \
+    fflush(stdout);                                                       \
+  }
+
+static std::map<unsigned, std::string> g_ptx_token_decode;
+
+const char *decode_token(int type) { return g_ptx_token_decode[type].c_str(); }
+
+void ptx_recognizer::read_parser_environment_variables() {
+  gpgpu_ctx->g_filename = getenv("PTX_SIM_KERNELFILE");
+  char *dbg_level = getenv("PTX_SIM_DEBUG");
+  if (dbg_level && strlen(dbg_level)) {
+    int debug_execution = 0;
+    sscanf(dbg_level, "%d", &debug_execution);
+    if (debug_execution >= 30) g_debug_ir_generation = true;
+  }
+}
+
+void ptx_recognizer::init_directive_state() {
+  PTX_PARSE_DPRINTF("init_directive_state");
+  g_space_spec = undefined_space;
+  g_ptr_spec = undefined_space;
+  g_scalar_type_spec = -1;
+  g_vector_spec = -1;
+  g_opcode = -1;
+  g_alignment_spec = -1;
+  g_size = -1;
+  g_extern_spec = 0;
+  g_scalar_type.clear();
+  g_operands.clear();
+  g_last_symbol = NULL;
+}
+
+void ptx_recognizer::init_instruction_state() {
+  PTX_PARSE_DPRINTF("init_instruction_state");
+  g_pred = NULL;
+  g_neg_pred = 0;
+  g_pred_mod = -1;
+  g_label = NULL;
+  g_opcode = -1;
+  g_options.clear();
+  g_wmma_options.clear();
+  g_return_var = operand_info(gpgpu_ctx);
+  init_directive_state();
+}
+
+symbol_table *gpgpu_context::init_parser(const char *ptx_filename) {
+  g_filename = strdup(ptx_filename);
+  if (g_global_allfiles_symbol_table == NULL) {
+    g_global_allfiles_symbol_table =
+        new symbol_table("global_allfiles", 0, NULL, this);
+    ptx_parser->g_global_symbol_table = ptx_parser->g_current_symbol_table =
+        g_global_allfiles_symbol_table;
+  }
+  /*else {
+      g_global_symbol_table = g_current_symbol_table = new
+  symbol_table("global",0,g_global_allfiles_symbol_table);
+  }*/
+
+#define DEF(X, Y) g_ptx_token_decode[X] = Y;
 #include "ptx_parser_decode.def"
 #undef DEF
-   g_ptx_token_decode[undefined_space] = "undefined_space";
-   g_ptx_token_decode[undefined_space] = "undefined_space=0";
-   g_ptx_token_decode[reg_space] = "reg_space";
-   g_ptx_token_decode[local_space] = "local_space";
-   g_ptx_token_decode[shared_space] = "shared_space";
-   g_ptx_token_decode[param_space_unclassified] = "param_space_unclassified";
-   g_ptx_token_decode[param_space_kernel] = "param_space_kernel";
-   g_ptx_token_decode[param_space_local] = "param_space_local";
-   g_ptx_token_decode[const_space] = "const_space";
-   g_ptx_token_decode[tex_space] = "tex_space";
-   g_ptx_token_decode[surf_space] = "surf_space";
-   g_ptx_token_decode[global_space] = "global_space";
-   g_ptx_token_decode[generic_space] = "generic_space";
-   g_ptx_token_decode[instruction_space] = "instruction_space";
-
-   init_directive_state();
-   init_instruction_state();
-
-   ptx_in = fopen(ptx_filename, "r");
-   ptx_parse();
-   fclose(ptx_in);
-   return g_global_symbol_table;
-}
-
-static int g_entry_point;
-
-void start_function( int entry_point ) 
-{
-   PTX_PARSE_DPRINTF("start_function");
-   init_directive_state();
-   init_instruction_state();
-   g_entry_point = entry_point;
-   g_func_info = NULL;
-   g_entry_func_param_index=0;
+  g_ptx_token_decode[undefined_space] = "undefined_space";
+  g_ptx_token_decode[undefined_space] = "undefined_space=0";
+  g_ptx_token_decode[reg_space] = "reg_space";
+  g_ptx_token_decode[local_space] = "local_space";
+  g_ptx_token_decode[shared_space] = "shared_space";
+  g_ptx_token_decode[param_space_unclassified] = "param_space_unclassified";
+  g_ptx_token_decode[param_space_kernel] = "param_space_kernel";
+  g_ptx_token_decode[param_space_local] = "param_space_local";
+  g_ptx_token_decode[const_space] = "const_space";
+  g_ptx_token_decode[tex_space] = "tex_space";
+  g_ptx_token_decode[surf_space] = "surf_space";
+  g_ptx_token_decode[global_space] = "global_space";
+  g_ptx_token_decode[generic_space] = "generic_space";
+  g_ptx_token_decode[instruction_space] = "instruction_space";
+
+  ptx_lex_init(&(ptx_parser->scanner));
+  ptx_parser->init_directive_state();
+  ptx_parser->init_instruction_state();
+
+  FILE *ptx_in;
+  ptx_in = fopen(ptx_filename, "r");
+  ptx_set_in(ptx_in, ptx_parser->scanner);
+  ptx_parse(ptx_parser->scanner, ptx_parser);
+  ptx_in = ptx_get_in(ptx_parser->scanner);
+  ptx_lex_destroy(ptx_parser->scanner);
+  fclose(ptx_in);
+  return ptx_parser->g_global_symbol_table;
+}
+
+void ptx_recognizer::start_function(int entry_point) {
+  PTX_PARSE_DPRINTF("start_function");
+  init_directive_state();
+  init_instruction_state();
+  g_entry_point = entry_point;
+  g_func_info = NULL;
+  g_entry_func_param_index = 0;
+}
+
+void ptx_recognizer::add_function_name(const char *name) {
+  PTX_PARSE_DPRINTF(
+      "add_function_name %s %s", name,
+      ((g_entry_point == 1) ? "(entrypoint)"
+                            : ((g_entry_point == 2) ? "(extern)" : "")));
+  bool prior_decl = g_global_symbol_table->add_function_decl(
+      name, g_entry_point, &g_func_info, &g_current_symbol_table);
+  if (g_add_identifier_cached__identifier) {
+    add_identifier(g_add_identifier_cached__identifier,
+                   g_add_identifier_cached__array_dim,
+                   g_add_identifier_cached__array_ident);
+    free(g_add_identifier_cached__identifier);
+    g_add_identifier_cached__identifier = NULL;
+    g_func_info->add_return_var(g_last_symbol);
+    init_directive_state();
+  }
+  if (prior_decl) {
+    g_func_info->remove_args();
+  }
+  g_global_symbol_table->add_function(g_func_info, gpgpu_ctx->g_filename,
+                                      ptx_get_lineno(scanner));
+}
+
+// Jin: handle instruction group for cdp
+void ptx_recognizer::start_inst_group() {
+  PTX_PARSE_DPRINTF("start_instruction_group");
+  g_current_symbol_table = g_current_symbol_table->start_inst_group();
+}
+
+void ptx_recognizer::end_inst_group() {
+  PTX_PARSE_DPRINTF("end_instruction_group");
+  g_current_symbol_table = g_current_symbol_table->end_inst_group();
+}
+
+void ptx_recognizer::add_directive() {
+  PTX_PARSE_DPRINTF("add_directive");
+  init_directive_state();
+}
+
+#define mymax(a, b) ((a) > (b) ? (a) : (b))
+
+void ptx_recognizer::end_function() {
+  PTX_PARSE_DPRINTF("end_function");
+
+  init_directive_state();
+  init_instruction_state();
+  g_max_regs_per_thread = mymax(g_max_regs_per_thread,
+                                (g_current_symbol_table->next_reg_num() - 1));
+  g_func_info->add_inst(g_instructions);
+  g_instructions.clear();
+  gpgpu_ptx_assemble(g_func_info->get_name(), g_func_info);
+  g_current_symbol_table = g_global_symbol_table;
+
+  PTX_PARSE_DPRINTF("function %s, PC = %d\n", g_func_info->get_name().c_str(),
+                    g_func_info->get_start_PC());
+}
+
+#define parse_error(msg, ...) \
+  parse_error_impl(__FILE__, __LINE__, msg, ##__VA_ARGS__)
+#define parse_assert(cond, msg, ...) \
+  parse_assert_impl((cond), __FILE__, __LINE__, msg, ##__VA_ARGS__)
+
+void ptx_recognizer::parse_error_impl(const char *file, unsigned line,
+                                      const char *msg, ...) {
+  va_list ap;
+  char buf[1024];
+  va_start(ap, msg);
+  vsnprintf(buf, 1024, msg, ap);
+  va_end(ap);
+
+  g_error_detected = 1;
+  printf("%s:%u: Parse error: %s (%s:%u)\n\n", gpgpu_ctx->g_filename,
+         ptx_get_lineno(scanner), buf, file, line);
+  ptx_error(scanner, this, NULL);
+  abort();
+  exit(1);
+}
+
+void ptx_recognizer::parse_assert_impl(int test_value, const char *file,
+                                       unsigned line, const char *msg, ...) {
+  va_list ap;
+  char buf[1024];
+  va_start(ap, msg);
+  vsnprintf(buf, 1024, msg, ap);
+  va_end(ap);
+
+  if (test_value == 0) parse_error_impl(file, line, msg);
+}
+
+void ptx_recognizer::set_return() {
+  parse_assert((g_opcode == CALL_OP || g_opcode == CALLP_OP),
+               "only call can have return value");
+  g_operands.front().set_return();
+  g_return_var = g_operands.front();
+}
+
+const ptx_instruction *ptx_recognizer::ptx_instruction_lookup(
+    const char *filename, unsigned linenumber) {
+  std::map<std::string, std::map<unsigned, const ptx_instruction *> >::iterator
+      f = g_inst_lookup.find(filename);
+  if (f == g_inst_lookup.end()) return NULL;
+  std::map<unsigned, const ptx_instruction *>::iterator l =
+      f->second.find(linenumber);
+  if (l == f->second.end()) return NULL;
+  return l->second;
+}
+
+void ptx_recognizer::add_instruction() {
+  PTX_PARSE_DPRINTF("add_instruction: %s",
+                    ((g_opcode > 0) ? g_opcode_string[g_opcode] : "<label>"));
+  assert(g_shader_core_config != 0);
+  ptx_instruction *i = new ptx_instruction(
+      g_opcode, g_pred, g_neg_pred, g_pred_mod, g_label, g_operands,
+      g_return_var, g_options, g_wmma_options, g_scalar_type, g_space_spec,
+      gpgpu_ctx->g_filename, ptx_get_lineno(scanner), linebuf,
+      g_shader_core_config, gpgpu_ctx);
+  g_instructions.push_back(i);
+  g_inst_lookup[gpgpu_ctx->g_filename][ptx_get_lineno(scanner)] = i;
+  init_instruction_state();
+}
+
+void ptx_recognizer::add_variables() {
+  PTX_PARSE_DPRINTF("add_variables");
+  if (!g_operands.empty()) {
+    assert(g_last_symbol != NULL);
+    g_last_symbol->add_initializer(g_operands);
+  }
+  init_directive_state();
+}
+
+void ptx_recognizer::set_variable_type() {
+  PTX_PARSE_DPRINTF("set_variable_type space_spec=%s scalar_type_spec=%s",
+                    g_ptx_token_decode[g_space_spec.get_type()].c_str(),
+                    g_ptx_token_decode[g_scalar_type_spec].c_str());
+  parse_assert(g_space_spec != undefined_space,
+               "variable has no space specification");
+  parse_assert(
+      g_scalar_type_spec != -1,
+      "variable has no type information");  // need to extend for structs?
+  g_var_type = g_current_symbol_table->add_type(
+      g_space_spec, g_scalar_type_spec, g_vector_spec, g_alignment_spec,
+      g_extern_spec);
+}
+
+bool ptx_recognizer::check_for_duplicates(const char *identifier) {
+  const symbol *s = g_current_symbol_table->lookup(identifier);
+  return (s != NULL);
 }
 
-char *g_add_identifier_cached__identifier = NULL;
-int g_add_identifier_cached__array_dim;
-int g_add_identifier_cached__array_ident;
-
-void add_function_name( const char *name ) 
-{
-   PTX_PARSE_DPRINTF("add_function_name %s %s", name,  ((g_entry_point==1)?"(entrypoint)":((g_entry_point==2)?"(extern)":"")));
-   bool prior_decl = g_global_symbol_table->add_function_decl( name, g_entry_point, &g_func_info, &g_current_symbol_table );
-   if( g_add_identifier_cached__identifier ) {
-      add_identifier( g_add_identifier_cached__identifier,
-                      g_add_identifier_cached__array_dim,
-                      g_add_identifier_cached__array_ident );
-      free( g_add_identifier_cached__identifier );
-      g_add_identifier_cached__identifier = NULL;
-      g_func_info->add_return_var( g_last_symbol );
-      init_directive_state();
-   }
-   if( prior_decl ) {
-      g_func_info->remove_args();
-   }
-   g_global_symbol_table->add_function( g_func_info, g_filename, ptx_lineno );
-}
-
-//Jin: handle instruction group for cdp
-void start_inst_group() {
-   PTX_PARSE_DPRINTF("start_instruction_group");
-   g_current_symbol_table = g_current_symbol_table->start_inst_group();
-}
-
-void end_inst_group() {
-   PTX_PARSE_DPRINTF("end_instruction_group");
-   g_current_symbol_table = g_current_symbol_table->end_inst_group();
-}
-
-void add_directive() 
-{
-   PTX_PARSE_DPRINTF("add_directive");
-   init_directive_state();
-}
-
-#define mymax(a,b) ((a)>(b)?(a):(b))
-
-void end_function() 
-{
-   PTX_PARSE_DPRINTF("end_function");
-
-   init_directive_state();
-   init_instruction_state();
-   g_max_regs_per_thread = mymax( g_max_regs_per_thread, (g_current_symbol_table->next_reg_num()-1)); 
-   g_func_info->add_inst( g_instructions );
-   g_instructions.clear();
-   gpgpu_ptx_assemble( g_func_info->get_name(), g_func_info );
-   g_current_symbol_table = g_global_symbol_table;
-
-   PTX_PARSE_DPRINTF("function %s, PC = %d\n", g_func_info->get_name().c_str(), g_func_info->get_start_PC());
-}
-
-#define parse_error(msg, ...) parse_error_impl(__FILE__,__LINE__, msg, ##__VA_ARGS__)
-#define parse_assert(cond,msg, ...) parse_assert_impl((cond),__FILE__,__LINE__, msg, ##__VA_ARGS__)
-
-void parse_error_impl( const char *file, unsigned line, const char *msg, ... )
-{
-   va_list ap;
-   char buf[1024];
-   va_start(ap,msg);
-   vsnprintf(buf,1024,msg,ap);
-   va_end(ap);
-
-   g_error_detected = 1;
-   printf("%s:%u: Parse error: %s (%s:%u)\n\n", g_filename, ptx_lineno, buf, file, line);
-   ptx_error(NULL);
-   abort();
-   exit(1);
-}
-
-void parse_assert_impl( int test_value, const char *file, unsigned line, const char *msg, ... )
-{
-   va_list ap;
-   char buf[1024];
-   va_start(ap,msg);
-   vsnprintf(buf,1024,msg,ap);
-   va_end(ap);
-
-   if ( test_value == 0 )
-      parse_error_impl(file,line, msg);
-}
-
-extern char linebuf[4096];
-
-
-void set_return()
-{
-   parse_assert( (g_opcode == CALL_OP || g_opcode == CALLP_OP), "only call can have return value");
-   g_operands.front().set_return();
-   g_return_var = g_operands.front();
-}
-
-std::map<std::string,std::map<unsigned,const ptx_instruction*> > g_inst_lookup;
-
-const ptx_instruction *ptx_instruction_lookup( const char *filename, unsigned linenumber )
-{
-   std::map<std::string,std::map<unsigned,const ptx_instruction*> >::iterator f=g_inst_lookup.find(filename);
-   if( f == g_inst_lookup.end() ) 
-      return NULL;
-   std::map<unsigned,const ptx_instruction*>::iterator l=f->second.find(linenumber);
-   if( l == f->second.end() ) 
-      return NULL;
-   return l->second; 
-}
-
-void add_instruction() 
-{
-   PTX_PARSE_DPRINTF("add_instruction: %s", ((g_opcode>0)?g_opcode_string[g_opcode]:"<label>") );
-   assert( g_shader_core_config != 0 );
-   ptx_instruction *i = new ptx_instruction( g_opcode, 
-                                             g_pred, 
-                                             g_neg_pred,
-                                             g_pred_mod, 
-                                             g_label, 
-                                             g_operands,
-                                             g_return_var,
-                                             g_options, 
-                                             g_wmma_options, 
-                                             g_scalar_type,
-                                             g_space_spec,
-                                             g_filename,
-                                             ptx_lineno,
-                                             linebuf,
-                                             g_shader_core_config );
-   g_instructions.push_back(i);
-   g_inst_lookup[g_filename][ptx_lineno] = i;
-   init_instruction_state();
-}
-
-void add_variables() 
-{
-   PTX_PARSE_DPRINTF("add_variables");
-   if ( !g_operands.empty() ) {
-      assert( g_last_symbol != NULL ); 
-      g_last_symbol->add_initializer(g_operands);
-   }
-   init_directive_state();
-}
-
-void set_variable_type()
-{
-   PTX_PARSE_DPRINTF("set_variable_type space_spec=%s scalar_type_spec=%s", 
-           g_ptx_token_decode[g_space_spec.get_type()].c_str(), 
-           g_ptx_token_decode[g_scalar_type_spec].c_str() );
-   parse_assert( g_space_spec != undefined_space, "variable has no space specification" );
-   parse_assert( g_scalar_type_spec != -1, "variable has no type information" ); // need to extend for structs?
-   g_var_type = g_current_symbol_table->add_type( g_space_spec, 
-                                                  g_scalar_type_spec, 
-                                                  g_vector_spec, 
-                                                  g_alignment_spec, 
-                                                  g_extern_spec );
-}
-
-bool check_for_duplicates( const char *identifier )
-{
-   const symbol *s = g_current_symbol_table->lookup(identifier);
-   return ( s != NULL );
-}
-
-extern std::set<std::string>   g_globals;
-extern std::set<std::string>   g_constants;
-
-int g_func_decl = 0;
-int g_ident_add_uid = 0;
-unsigned g_const_alloc = 1;
-
 // Returns padding that needs to be inserted ahead of address to make it aligned to min(size, maxalign)
 /*
  * @param address the address in bytes
  * @param size the size of the memory to be allocated in bytes
  * @param maximum alignment in bytes. i.e. if size is too big then align to this instead
  */
-int pad_address (new_addr_type address, unsigned size, unsigned maxalign) {
-    assert(size >= 0);
-    assert(maxalign > 0);
-    int alignto = maxalign;
-    if (size < maxalign &&
-            (size & (size-1)) == 0) { //size is a power of 2
-        alignto = size;
-    }
-    return alignto ? ((alignto - (address % alignto)) % alignto) : 0;
-}
-
-void add_identifier( const char *identifier, int array_dim, unsigned array_ident ) 
-{
-   if(array_ident==ARRAY_IDENTIFIER){
-       g_size *= array_dim;
-   }
-   if( g_func_decl && (g_func_info == NULL) ) {
-      // return variable decl...
-      assert( g_add_identifier_cached__identifier == NULL );
-      g_add_identifier_cached__identifier = strdup(identifier);
-      g_add_identifier_cached__array_dim = array_dim;
-      g_add_identifier_cached__array_ident = array_ident;
-      return;
-   }
-   PTX_PARSE_DPRINTF("add_identifier \"%s\" (%u)", identifier, g_ident_add_uid);
-   g_ident_add_uid++;
-   type_info *type = g_var_type;
-   type_info_key ti = type->get_key();
-   int basic_type;
-   int regnum;
-   size_t num_bits;
-   unsigned addr_pad;
-   new_addr_type addr;
-   ti.type_decode(num_bits,basic_type);
-
-   bool duplicates = check_for_duplicates( identifier );
-   if( duplicates ) {
-      symbol *s = g_current_symbol_table->lookup(identifier);
-      g_last_symbol = s;
-      if( g_func_decl ) 
-         return;
-      std::string msg = std::string(identifier) + " was declared previous at " + s->decl_location() + " skipping new declaration"; 
-      printf("GPGPU-Sim PTX: Warning %s\n", msg.c_str());
-      return;
-   }
-
-   assert( g_var_type != NULL );
-   switch ( array_ident ) {
-   case ARRAY_IDENTIFIER:
-      type = g_current_symbol_table->get_array_type(type,array_dim);
+int pad_address(new_addr_type address, unsigned size, unsigned maxalign) {
+  assert(size >= 0);
+  assert(maxalign > 0);
+  int alignto = maxalign;
+  if (size < maxalign && (size & (size - 1)) == 0) {  // size is a power of 2
+    alignto = size;
+  }
+  return alignto ? ((alignto - (address % alignto)) % alignto) : 0;
+}
+
+void ptx_recognizer::add_identifier(const char *identifier, int array_dim,
+                                    unsigned array_ident) {
+  if (array_ident == ARRAY_IDENTIFIER) {
+    g_size *= array_dim;
+  }
+  if (g_func_decl && (g_func_info == NULL)) {
+    // return variable decl...
+    assert(g_add_identifier_cached__identifier == NULL);
+    g_add_identifier_cached__identifier = strdup(identifier);
+    g_add_identifier_cached__array_dim = array_dim;
+    g_add_identifier_cached__array_ident = array_ident;
+    return;
+  }
+  PTX_PARSE_DPRINTF("add_identifier \"%s\" (%u)", identifier, g_ident_add_uid);
+  g_ident_add_uid++;
+  type_info *type = g_var_type;
+  type_info_key ti = type->get_key();
+  int basic_type;
+  int regnum;
+  size_t num_bits;
+  unsigned addr_pad;
+  new_addr_type addr;
+  ti.type_decode(num_bits, basic_type);
+
+  bool duplicates = check_for_duplicates(identifier);
+  if (duplicates) {
+    symbol *s = g_current_symbol_table->lookup(identifier);
+    g_last_symbol = s;
+    if (g_func_decl) return;
+    std::string msg = std::string(identifier) + " was declared previous at " +
+                      s->decl_location() + " skipping new declaration";
+    printf("GPGPU-Sim PTX: Warning %s\n", msg.c_str());
+    return;
+  }
+
+  assert(g_var_type != NULL);
+  switch (array_ident) {
+    case ARRAY_IDENTIFIER:
+      type = g_current_symbol_table->get_array_type(type, array_dim);
       num_bits = array_dim * num_bits;
       break;
-   case ARRAY_IDENTIFIER_NO_DIM:
-      type = g_current_symbol_table->get_array_type(type,(unsigned)-1);
+    case ARRAY_IDENTIFIER_NO_DIM:
+      type = g_current_symbol_table->get_array_type(type, (unsigned)-1);
       num_bits = 0;
       break;
-   default:
+    default:
       break;
-   }
-   g_last_symbol = g_current_symbol_table->add_variable(identifier,type,num_bits/8,g_filename,ptx_lineno);
-   switch ( ti.get_memory_space().get_type() ) {
-   case reg_space: {
+  }
+  g_last_symbol = g_current_symbol_table->add_variable(
+      identifier, type, num_bits / 8, gpgpu_ctx->g_filename,
+      ptx_get_lineno(scanner));
+  switch (ti.get_memory_space().get_type()) {
+    case reg_space: {
       regnum = g_current_symbol_table->next_reg_num();
       int arch_regnum = -1;
       for (int d = 0; d < strlen(identifier); d++) {
-         if (isdigit(identifier[d])) {
-            sscanf(identifier + d, "%d", &arch_regnum);
-            break;
-         }
+        if (isdigit(identifier[d])) {
+          sscanf(identifier + d, "%d", &arch_regnum);
+          break;
+        }
       }
       if (strcmp(identifier, "%sp") == 0) {
-         arch_regnum = 0;
+        arch_regnum = 0;
       }
       g_last_symbol->set_regno(regnum, arch_regnum);
-      } break;
-   case shared_space:
-      printf("GPGPU-Sim PTX: allocating shared region for \"%s\" ",
-             identifier);
+    } break;
+    case shared_space:
+      printf("GPGPU-Sim PTX: allocating shared region for \"%s\" ", identifier);
       fflush(stdout);
-      assert( (num_bits%8) == 0  );
+      assert((num_bits % 8) == 0);
       addr = g_current_symbol_table->get_shared_next();
-      addr_pad = pad_address(addr, num_bits/8, 128);
-      printf("from 0x%llx to 0x%llx (shared memory space)\n",
-              addr+addr_pad,
-              addr+addr_pad + num_bits/8);
-         fflush(stdout);
-      g_last_symbol->set_address( addr+addr_pad );
-      g_current_symbol_table->alloc_shared( num_bits/8 + addr_pad );
+      addr_pad = pad_address(addr, num_bits / 8, 128);
+      printf("from 0x%llx to 0x%llx (shared memory space)\n", addr + addr_pad,
+             addr + addr_pad + num_bits / 8);
+      fflush(stdout);
+      g_last_symbol->set_address(addr + addr_pad);
+      g_current_symbol_table->alloc_shared(num_bits / 8 + addr_pad);
+      break;
+    case sstarr_space:
+      printf("GPGPU-Sim PTX: allocating sstarr region for \"%s\" ", identifier);
+      fflush(stdout);
+      assert((num_bits % 8) == 0);
+      addr = g_current_symbol_table->get_sstarr_next();
+      addr_pad = pad_address(addr, num_bits / 8, 128);
+      printf("from 0x%llx to 0x%llx (sstarr memory space)\n", addr + addr_pad,
+             addr + addr_pad + num_bits / 8);
+      fflush(stdout);
+      g_last_symbol->set_address(addr + addr_pad);
+      g_current_symbol_table->alloc_sstarr(num_bits / 8 + addr_pad);
       break;
-   case sstarr_space:
-         printf("GPGPU-Sim PTX: allocating sstarr region for \"%s\" ",
-                identifier);
-         fflush(stdout);
-         assert( (num_bits%8) == 0  );
-         addr = g_current_symbol_table->get_sstarr_next();
-         addr_pad = pad_address(addr, num_bits/8, 128);
-         printf("from 0x%x to 0x%lx (sstarr memory space)\n",
-                 addr+addr_pad,
-                 addr+addr_pad + num_bits/8);
-            fflush(stdout);
-         g_last_symbol->set_address( addr+addr_pad );
-         g_current_symbol_table->alloc_sstarr( num_bits/8 + addr_pad );
-         break;
-   case const_space:
-      if( array_ident == ARRAY_IDENTIFIER_NO_DIM ) {
-         printf("GPGPU-Sim PTX: deferring allocation of constant region for \"%s\" (need size information)\n", identifier );
+    case const_space:
+      if (array_ident == ARRAY_IDENTIFIER_NO_DIM) {
+        printf(
+            "GPGPU-Sim PTX: deferring allocation of constant region for \"%s\" "
+            "(need size information)\n",
+            identifier);
       } else {
-         printf("GPGPU-Sim PTX: allocating constant region for \"%s\" ",
-                identifier);
-         fflush(stdout);
-         assert( (num_bits%8) == 0  ); 
-         addr = g_current_symbol_table->get_global_next();
-         addr_pad = pad_address(addr, num_bits/8, 128);
-         printf("from 0x%llx to 0x%llx (global memory space) %u\n",
-              addr+addr_pad,
-              addr+addr_pad + num_bits/8,
-              g_const_alloc++);
-         fflush(stdout);
-         g_last_symbol->set_address( addr + addr_pad );
-         // Do not alloc in gem5-gpu: g_current_symbol_table->alloc_global( num_bits/8 + addr_pad ); 
+        printf("GPGPU-Sim PTX: allocating constant region for \"%s\" ",
+               identifier);
+        fflush(stdout);
+        assert((num_bits % 8) == 0);
+        addr = g_current_symbol_table->get_global_next();
+        addr_pad = pad_address(addr, num_bits / 8, 128);
+        printf("from 0x%llx to 0x%llx (global memory space) %u\n",
+               addr + addr_pad, addr + addr_pad + num_bits / 8,
+               g_const_alloc++);
+        fflush(stdout);
+        g_last_symbol->set_address(addr + addr_pad);
+        // Do not alloc in gem5-gpu: g_current_symbol_table->alloc_global( num_bits/8 + addr_pad ); 
       }
-      if( g_current_symbol_table == g_global_symbol_table ) { 
-         g_constants.insert( identifier ); 
+      if (g_current_symbol_table == g_global_symbol_table) {
+        gpgpu_ctx->func_sim->g_constants.insert(identifier);
       }
-      assert( g_current_symbol_table != NULL );
-      g_sym_name_to_symbol_table[ identifier ] = g_current_symbol_table;
+      assert(g_current_symbol_table != NULL);
+      g_sym_name_to_symbol_table[identifier] = g_current_symbol_table;
       break;
-   case global_space:
-      printf("GPGPU-Sim PTX: allocating global region for \"%s\" ",
-             identifier);
+    case global_space:
+      printf("GPGPU-Sim PTX: allocating global region for \"%s\" ", identifier);
       fflush(stdout);
-      assert( (num_bits%8) == 0  );
+      assert((num_bits % 8) == 0);
       addr = g_current_symbol_table->get_global_next();
-      addr_pad = pad_address(addr, num_bits/8, 128);
-      printf("from 0x%llx to 0x%llx (global memory space)\n",
-              addr+addr_pad,
-              addr+addr_pad + num_bits/8);
+      addr_pad = pad_address(addr, num_bits / 8, 128);
+      printf("from 0x%llx to 0x%llx (global memory space)\n", addr + addr_pad,
+             addr + addr_pad + num_bits / 8);
       fflush(stdout);
-      g_last_symbol->set_address( addr+addr_pad );
+      g_last_symbol->set_address(addr + addr_pad);
       // Do not alloc in gem5-gpu: g_current_symbol_table->alloc_global( num_bits/8 + addr_pad );
-      g_globals.insert( identifier );
-      assert( g_current_symbol_table != NULL );
-      g_sym_name_to_symbol_table[ identifier ] = g_current_symbol_table;
+      gpgpu_ctx->func_sim->g_globals.insert(identifier);
+      assert(g_current_symbol_table != NULL);
+      g_sym_name_to_symbol_table[identifier] = g_current_symbol_table;
       break;
-   case local_space:
-      if( g_func_info == NULL ) {
-          printf("GPGPU-Sim PTX: allocating local region for \"%s\" ", identifier);
-         fflush(stdout);
-         assert( (num_bits%8) == 0  );
-         addr = g_current_symbol_table->get_local_next();
-         addr_pad = pad_address(addr, num_bits/8, 128);
-         printf("from 0x%llx to 0x%llx (local memory space)\n",
-                 addr+addr_pad,
-                 addr+addr_pad + num_bits/8);
-         fflush(stdout);
-         g_last_symbol->set_address( addr+addr_pad);
-         g_current_symbol_table->alloc_local( num_bits/8 + addr_pad);
-         // To signal gem5-gpu to allocate local memory for the GPU, add the
-         // local allocation to the global symbol table
-         g_global_symbol_table->alloc_local(num_bits/8 + addr_pad);
-#ifndef LIBCUDA
-         panic("gem5-gpu: This local memory path is untested!");
-#else
-         printf("gem5-gpu: This local memory path is untested!");
-         m5op::m5_panic();
-#endif
-      } else {
-        printf("GPGPU-Sim PTX: allocating stack frame region for .local \"%s\" ",
+    case local_space:
+      if (g_func_info == NULL) {
+        printf("GPGPU-Sim PTX: allocating local region for \"%s\" ",
                identifier);
         fflush(stdout);
-        assert( (num_bits%8) == 0 );
+        assert((num_bits % 8) == 0);
+        addr = g_current_symbol_table->get_local_next();
+        addr_pad = pad_address(addr, num_bits / 8, 128);
+        printf("from 0x%llx to 0x%llx (local memory space)\n", addr + addr_pad,
+               addr + addr_pad + num_bits / 8);
+        fflush(stdout);
+        g_last_symbol->set_address(addr + addr_pad);
+        g_current_symbol_table->alloc_local(num_bits / 8 + addr_pad);
+        // To signal gem5-gpu to allocate local memory for the GPU, add the
+        // local allocation to the global symbol table
+        g_global_symbol_table->alloc_local(num_bits/8 + addr_pad);
+        panic("gem5-gpu: This local memory path is untested!");
+      } else {
+        printf(
+            "GPGPU-Sim PTX: allocating stack frame region for .local \"%s\" ",
+            identifier);
+        fflush(stdout);
+        assert((num_bits % 8) == 0);
         addr = g_current_symbol_table->get_local_next();
-        addr_pad = pad_address(addr, num_bits/8, 128);
-        printf("from 0x%llx to 0x%llx\n",
-                addr+addr_pad,
-                addr+addr_pad + num_bits/8);
+        addr_pad = pad_address(addr, num_bits / 8, 128);
+        printf("from 0x%llx to 0x%llx\n", addr + addr_pad,
+               addr + addr_pad + num_bits / 8);
         fflush(stdout);
-        g_last_symbol->set_address( addr+addr_pad );
+        g_last_symbol->set_address(addr + addr_pad);
         assert(g_current_symbol_table != g_global_symbol_table);
         g_current_symbol_table->alloc_local( num_bits/8 + addr_pad);
         // To signal gem5-gpu to allocate local memory for the GPU, add the
@@ -567,17 +497,18 @@ void add_identifier( const char *identifier, int array_dim, unsigned array_ident
         g_func_info->set_framesize( g_current_symbol_table->get_local_next() );
       }
       break;
-   case tex_space:
+    case tex_space:
       printf("GPGPU-Sim PTX: encountered texture directive %s.\n", identifier);
       break;
-   case param_space_local:
-      printf("GPGPU-Sim PTX: allocating stack frame region for .param \"%s\" from 0x%x to 0x%lx\n",
-             identifier,
-             g_current_symbol_table->get_local_next(),
-             g_current_symbol_table->get_local_next() + num_bits/8 );
+    case param_space_local:
+      printf(
+          "GPGPU-Sim PTX: allocating stack frame region for .param \"%s\" from "
+          "0x%x to 0x%lx\n",
+          identifier, g_current_symbol_table->get_local_next(),
+          g_current_symbol_table->get_local_next() + num_bits / 8);
       fflush(stdout);
-      assert( (num_bits%8) == 0  );
-      g_last_symbol->set_address( g_current_symbol_table->get_local_next() );
+      assert((num_bits % 8) == 0);
+      g_last_symbol->set_address(g_current_symbol_table->get_local_next());
       g_current_symbol_table->alloc_local( num_bits/8 );
       // To signal gem5-gpu to allocate local memory for the GPU, add the
       // local allocation to the global symbol table
@@ -589,507 +520,500 @@ void add_identifier( const char *identifier, int array_dim, unsigned array_ident
    default:
       abort();
       break;
-   }
-
-   assert( !ti.is_param_unclassified() );
-   if ( ti.is_param_kernel() ) {
-      bool is_ptr = (g_ptr_spec != undefined_space); 
-      g_func_info->add_param_name_type_size(g_entry_func_param_index,identifier, ti.scalar_type(), num_bits, is_ptr, g_ptr_spec);
-      g_entry_func_param_index++;
-   }
-}
-
-void add_constptr(const char* identifier1, const char* identifier2, int offset)
-{
-   symbol *s1 = g_current_symbol_table->lookup(identifier1);
-   const symbol *s2 = g_current_symbol_table->lookup(identifier2);
-   parse_assert( s1 != NULL, "'from' constant identifier does not exist.");
-   parse_assert( s1 != NULL, "'to' constant identifier does not exist.");
-
-   unsigned addr = s2->get_address();
-
-   printf("GPGPU-Sim PTX: moving \"%s\" from 0x%x to 0x%x (%s+%x)\n",
-      identifier1, s1->get_address(), addr+offset, identifier2, offset);
-
-   s1->set_address( addr + offset );
-}
-
-void add_function_arg()
-{
-   assert(g_size>0);
-   if( g_func_info ) {
-      PTX_PARSE_DPRINTF("add_function_arg \"%s\"", g_last_symbol->name().c_str() );
-      g_func_info->add_arg(g_last_symbol);
-      unsigned alignment = (g_alignment_spec==-1) ? g_size : g_alignment_spec;
-      assert(alignment==1||alignment==2||alignment==4||alignment==8||alignment==16);//known valid alignment values
-      g_func_info->add_config_param( g_size,  alignment);
-   }
-
-}
-
-void add_extern_spec() 
-{
-   PTX_PARSE_DPRINTF("add_extern_spec");
-   g_extern_spec = 1;
-}
-
-void add_alignment_spec( int spec )
-{
-   PTX_PARSE_DPRINTF("add_alignment_spec");
-   parse_assert( g_alignment_spec == -1, "multiple .align specifiers per variable declaration not allowed." );
-   g_alignment_spec = spec;
-}
-
-void add_ptr_spec( enum _memory_space_t spec ) 
-{
-   PTX_PARSE_DPRINTF("add_ptr_spec \"%s\"", g_ptx_token_decode[spec].c_str() );
-   parse_assert( g_ptr_spec == undefined_space, "multiple ptr space specifiers not allowed." );
-   parse_assert( spec == global_space or spec == local_space or spec == shared_space, "invalid space for ptr directive." );
-   g_ptr_spec = spec; 
-}
-
-void add_space_spec( enum _memory_space_t spec, int value ) 
-{
-   PTX_PARSE_DPRINTF("add_space_spec \"%s\"", g_ptx_token_decode[spec].c_str() );
-   parse_assert( g_space_spec == undefined_space, "multiple space specifiers not allowed." );
-   if( spec == param_space_unclassified ) {
-      if( g_func_decl ) {
-         if( g_entry_point == 1) 
-            g_space_spec = param_space_kernel;
-         else 
-            g_space_spec = param_space_local;
-      } else
-         g_space_spec = param_space_unclassified;
-   } else {
-      g_space_spec = spec;
-      if( g_space_spec == const_space )
-         g_space_spec.set_bank((unsigned)value);
-   }
-}
-
-void add_vector_spec(int spec ) 
-{
-   PTX_PARSE_DPRINTF("add_vector_spec");
-   parse_assert( g_vector_spec == -1, "multiple vector specifiers not allowed." );
-   g_vector_spec = spec;
-}
-
-void add_scalar_type_spec( int type_spec ) 
-{
-   //save size of parameter
-   switch ( type_spec ) {
-      case B8_TYPE:
-      case S8_TYPE:
-      case U8_TYPE: 
-         g_size = 1; break;
-      case B16_TYPE:
-      case S16_TYPE:
-      case U16_TYPE:
-      case F16_TYPE: 
-         g_size = 2; break;
-      case B32_TYPE:
-      case S32_TYPE:
-      case U32_TYPE:
-      case F32_TYPE: 
-         g_size = 4; break;
-      case B64_TYPE:
-      case BB64_TYPE:
-      case S64_TYPE:
-      case U64_TYPE:
-      case F64_TYPE: 
-      case FF64_TYPE:
-         g_size = 8; break;
-      case BB128_TYPE: 
-         g_size = 16; break;
-   }
-   PTX_PARSE_DPRINTF("add_scalar_type_spec \"%s\"", g_ptx_token_decode[type_spec].c_str());
-   g_scalar_type.push_back( type_spec );
-   if ( g_scalar_type.size() > 1 ) {
-      parse_assert( (g_opcode == -1) || (g_opcode == CVT_OP) || (g_opcode == SET_OP) || (g_opcode == SLCT_OP)
-                    || (g_opcode == TEX_OP)|| (g_opcode==MMA_OP)|| (g_opcode == DP4A_OP),
-                    "only cvt, set, slct, tex and dp4a can have more than one type specifier.");
-   }
-   g_scalar_type_spec = type_spec;
-}
-
-void add_label( const char *identifier ) 
-{
-   PTX_PARSE_DPRINTF("add_label");
-   symbol *s = g_current_symbol_table->lookup(identifier);
-   if ( s != NULL ) {
-      g_label = s;
-   } else {
-      g_label = g_current_symbol_table->add_variable(identifier,NULL,0,g_filename,ptx_lineno);
-   }
-}
-
-void add_opcode( int opcode ) 
-{
-   g_opcode = opcode;
-}
-
-void add_pred( const char *identifier, int neg, int predModifier ) 
-{
-   PTX_PARSE_DPRINTF("add_pred");
-   const symbol *s = g_current_symbol_table->lookup(identifier);
-   if ( s == NULL ) {
-      std::string msg = std::string("predicate \"") + identifier + "\" has no declaration.";
-      parse_error( msg.c_str() );
-   }
-   g_pred = s;
-   g_neg_pred = neg;
-   g_pred_mod = predModifier;
-}
-
-void add_option( int option ) 
-{
-   PTX_PARSE_DPRINTF("add_option");
-   g_options.push_back( option );
-}
-void add_wmma_option( int option ) 
-{
-   PTX_PARSE_DPRINTF("add_option");
-   g_wmma_options.push_back( option );
-}
-void add_double_operand( const char *d1, const char *d2 )
-{
-   //operands that access two variables.
-   //eg. s[$ofs1+$r0], g[$ofs1+=$r0]
-   //TODO: Not sure if I'm going to use this for storing to two destinations or not.
-
-   PTX_PARSE_DPRINTF("add_double_operand");
-   const symbol *s1 = g_current_symbol_table->lookup(d1);
-   const symbol *s2 = g_current_symbol_table->lookup(d2);
-   parse_assert( s1 != NULL && s2 != NULL, "component(s) missing declarations.");
-   g_operands.push_back( operand_info(s1,s2) );
-}
-
-void add_1vector_operand( const char *d1 ) 
-{
-   // handles the single element vector operand ({%v1}) found in tex.1d instructions
-   PTX_PARSE_DPRINTF("add_1vector_operand");
-   const symbol *s1 = g_current_symbol_table->lookup(d1);
-   parse_assert( s1 != NULL, "component(s) missing declarations.");
-   g_operands.push_back( operand_info(s1,NULL,NULL,NULL) );
-}
-
-void add_2vector_operand( const char *d1, const char *d2 ) 
-{
-   PTX_PARSE_DPRINTF("add_2vector_operand");
-   const symbol *s1 = g_current_symbol_table->lookup(d1);
-   const symbol *s2 = g_current_symbol_table->lookup(d2);
-   parse_assert( s1 != NULL && s2 != NULL, "v2 component(s) missing declarations.");
-   g_operands.push_back( operand_info(s1,s2,NULL,NULL) );
-}
-
-void add_3vector_operand( const char *d1, const char *d2, const char *d3 ) 
-{
-   PTX_PARSE_DPRINTF("add_3vector_operand");
-   const symbol *s1 = g_current_symbol_table->lookup(d1);
-   const symbol *s2 = g_current_symbol_table->lookup(d2);
-   const symbol *s3 = g_current_symbol_table->lookup(d3);
-   parse_assert( s1 != NULL && s2 != NULL && s3 != NULL, "v3 component(s) missing declarations.");
-   g_operands.push_back( operand_info(s1,s2,s3,NULL) );
-}
-
-void add_4vector_operand( const char *d1, const char *d2, const char *d3, const char *d4 ) 
-{
-   PTX_PARSE_DPRINTF("add_4vector_operand");
-   const symbol *s1 = g_current_symbol_table->lookup(d1);
-   const symbol *s2 = g_current_symbol_table->lookup(d2);
-   const symbol *s3 = g_current_symbol_table->lookup(d3);
-   const symbol *s4 = g_current_symbol_table->lookup(d4);
-   parse_assert( s1 != NULL && s2 != NULL && s3 != NULL && s4 != NULL, "v4 component(s) missing declarations.");
-   const symbol *null_op = g_current_symbol_table->lookup("_");
-   if ( s2 == null_op ) s2 = NULL;
-   if ( s3 == null_op ) s3 = NULL;
-   if ( s4 == null_op ) s4 = NULL;
-   g_operands.push_back( operand_info(s1,s2,s3,s4) );
-}
-void add_8vector_operand( const char *d1, const char *d2, const char *d3, const char *d4,const char *d5,const char *d6,const char *d7,const char *d8 ) 
-{
-   PTX_PARSE_DPRINTF("add_8vector_operand");
-   const symbol *s1 = g_current_symbol_table->lookup(d1);
-   const symbol *s2 = g_current_symbol_table->lookup(d2);
-   const symbol *s3 = g_current_symbol_table->lookup(d3);
-   const symbol *s4 = g_current_symbol_table->lookup(d4);
-   const symbol *s5 = g_current_symbol_table->lookup(d5);
-   const symbol *s6 = g_current_symbol_table->lookup(d6);
-   const symbol *s7 = g_current_symbol_table->lookup(d7);
-   const symbol *s8 = g_current_symbol_table->lookup(d8);
-   parse_assert( s1 != NULL && s2 != NULL && s3 != NULL && s4 != NULL && s5 !=NULL && s6 !=NULL && s7 !=NULL && s8 !=NULL, "v4 component(s) missing declarations.");
-   const symbol *null_op = g_current_symbol_table->lookup("_");
-   if ( s2 == null_op ) s2 = NULL;
-   if ( s3 == null_op ) s3 = NULL;
-   if ( s4 == null_op ) s4 = NULL;
-   if ( s5 == null_op ) s5 = NULL;
-   if ( s6 == null_op ) s6 = NULL;
-   if ( s7 == null_op ) s7 = NULL;
-   if ( s8 == null_op ) s8 = NULL;
-   g_operands.push_back( operand_info(s1,s2,s3,s4,s5,s6,s7,s8) );
-}
-
-void add_builtin_operand( int builtin, int dim_modifier ) 
-{
-   PTX_PARSE_DPRINTF("add_builtin_operand");
-   g_operands.push_back( operand_info(builtin,dim_modifier) );
-}
-
-void add_memory_operand() 
-{
-   PTX_PARSE_DPRINTF("add_memory_operand");
-   assert( !g_operands.empty() );
-   g_operands.back().make_memory_operand();
+  }
+
+  assert(!ti.is_param_unclassified());
+  if (ti.is_param_kernel()) {
+    bool is_ptr = (g_ptr_spec != undefined_space);
+    g_func_info->add_param_name_type_size(g_entry_func_param_index, identifier,
+                                          ti.scalar_type(), num_bits, is_ptr,
+                                          g_ptr_spec);
+    g_entry_func_param_index++;
+  }
+}
+
+void ptx_recognizer::add_constptr(const char *identifier1,
+                                  const char *identifier2, int offset) {
+  symbol *s1 = g_current_symbol_table->lookup(identifier1);
+  const symbol *s2 = g_current_symbol_table->lookup(identifier2);
+  parse_assert(s1 != NULL, "'from' constant identifier does not exist.");
+  parse_assert(s1 != NULL, "'to' constant identifier does not exist.");
+
+  unsigned addr = s2->get_address();
+
+  printf("GPGPU-Sim PTX: moving \"%s\" from 0x%x to 0x%x (%s+%x)\n",
+         identifier1, s1->get_address(), addr + offset, identifier2, offset);
+
+  s1->set_address(addr + offset);
+}
+
+void ptx_recognizer::add_function_arg() {
+  assert(g_size > 0);
+  if (g_func_info) {
+    PTX_PARSE_DPRINTF("add_function_arg \"%s\"", g_last_symbol->name().c_str());
+    g_func_info->add_arg(g_last_symbol);
+    unsigned alignment = (g_alignment_spec == -1) ? g_size : g_alignment_spec;
+    assert(alignment == 1 || alignment == 2 || alignment == 4 ||
+           alignment == 8 || alignment == 16);  // known valid alignment values
+    g_func_info->add_config_param(g_size, alignment);
+  }
+}
+
+void ptx_recognizer::add_extern_spec() {
+  PTX_PARSE_DPRINTF("add_extern_spec");
+  g_extern_spec = 1;
+}
+
+void ptx_recognizer::add_alignment_spec(int spec) {
+  PTX_PARSE_DPRINTF("add_alignment_spec");
+  parse_assert(
+      g_alignment_spec == -1,
+      "multiple .align specifiers per variable declaration not allowed.");
+  g_alignment_spec = spec;
+}
+
+void ptx_recognizer::add_ptr_spec(enum _memory_space_t spec) {
+  PTX_PARSE_DPRINTF("add_ptr_spec \"%s\"", g_ptx_token_decode[spec].c_str());
+  parse_assert(g_ptr_spec == undefined_space,
+               "multiple ptr space specifiers not allowed.");
+  parse_assert(
+      spec == global_space or spec == local_space or spec == shared_space,
+      "invalid space for ptr directive.");
+  g_ptr_spec = spec;
+}
+
+void ptx_recognizer::add_space_spec(enum _memory_space_t spec, int value) {
+  PTX_PARSE_DPRINTF("add_space_spec \"%s\"", g_ptx_token_decode[spec].c_str());
+  parse_assert(g_space_spec == undefined_space,
+               "multiple space specifiers not allowed.");
+  if (spec == param_space_unclassified) {
+    if (g_func_decl) {
+      if (g_entry_point == 1)
+        g_space_spec = param_space_kernel;
+      else
+        g_space_spec = param_space_local;
+    } else
+      g_space_spec = param_space_unclassified;
+  } else {
+    g_space_spec = spec;
+    if (g_space_spec == const_space) g_space_spec.set_bank((unsigned)value);
+  }
+}
+
+void ptx_recognizer::add_vector_spec(int spec) {
+  PTX_PARSE_DPRINTF("add_vector_spec");
+  parse_assert(g_vector_spec == -1, "multiple vector specifiers not allowed.");
+  g_vector_spec = spec;
+}
+
+void ptx_recognizer::add_scalar_type_spec(int type_spec) {
+  // save size of parameter
+  switch (type_spec) {
+    case B8_TYPE:
+    case S8_TYPE:
+    case U8_TYPE:
+      g_size = 1;
+      break;
+    case B16_TYPE:
+    case S16_TYPE:
+    case U16_TYPE:
+    case F16_TYPE:
+      g_size = 2;
+      break;
+    case B32_TYPE:
+    case S32_TYPE:
+    case U32_TYPE:
+    case F32_TYPE:
+      g_size = 4;
+      break;
+    case B64_TYPE:
+    case BB64_TYPE:
+    case S64_TYPE:
+    case U64_TYPE:
+    case F64_TYPE:
+    case FF64_TYPE:
+      g_size = 8;
+      break;
+    case BB128_TYPE:
+      g_size = 16;
+      break;
+  }
+  PTX_PARSE_DPRINTF("add_scalar_type_spec \"%s\"",
+                    g_ptx_token_decode[type_spec].c_str());
+  g_scalar_type.push_back(type_spec);
+  if (g_scalar_type.size() > 1) {
+    parse_assert(
+        (g_opcode == -1) || (g_opcode == CVT_OP) || (g_opcode == SET_OP) ||
+            (g_opcode == SLCT_OP) || (g_opcode == TEX_OP) ||
+            (g_opcode == MMA_OP) || (g_opcode == DP4A_OP) ||
+            (g_opcode == VMIN_OP) || (g_opcode == VMAX_OP),
+        "only cvt, set, slct, tex, vmin, vmax and dp4a can have more than one "
+        "type specifier.");
+  }
+  g_scalar_type_spec = type_spec;
+}
+
+void ptx_recognizer::add_label(const char *identifier) {
+  PTX_PARSE_DPRINTF("add_label");
+  symbol *s = g_current_symbol_table->lookup(identifier);
+  if (s != NULL) {
+    g_label = s;
+  } else {
+    g_label = g_current_symbol_table->add_variable(
+        identifier, NULL, 0, gpgpu_ctx->g_filename, ptx_get_lineno(scanner));
+  }
+}
+
+void ptx_recognizer::add_opcode(int opcode) { g_opcode = opcode; }
+
+void ptx_recognizer::add_pred(const char *identifier, int neg,
+                              int predModifier) {
+  PTX_PARSE_DPRINTF("add_pred");
+  const symbol *s = g_current_symbol_table->lookup(identifier);
+  if (s == NULL) {
+    std::string msg =
+        std::string("predicate \"") + identifier + "\" has no declaration.";
+    parse_error(msg.c_str());
+  }
+  g_pred = s;
+  g_neg_pred = neg;
+  g_pred_mod = predModifier;
+}
+
+void ptx_recognizer::add_option(int option) {
+  PTX_PARSE_DPRINTF("add_option");
+  g_options.push_back(option);
+}
+void ptx_recognizer::add_wmma_option(int option) {
+  PTX_PARSE_DPRINTF("add_option");
+  g_wmma_options.push_back(option);
+}
+void ptx_recognizer::add_double_operand(const char *d1, const char *d2) {
+  // operands that access two variables.
+  // eg. s[$ofs1+$r0], g[$ofs1+=$r0]
+  // TODO: Not sure if I'm going to use this for storing to two destinations or
+  // not.
+
+  PTX_PARSE_DPRINTF("add_double_operand");
+  const symbol *s1 = g_current_symbol_table->lookup(d1);
+  const symbol *s2 = g_current_symbol_table->lookup(d2);
+  parse_assert(s1 != NULL && s2 != NULL, "component(s) missing declarations.");
+  g_operands.push_back(operand_info(s1, s2, gpgpu_ctx));
+}
+
+void ptx_recognizer::add_1vector_operand(const char *d1) {
+  // handles the single element vector operand ({%v1}) found in tex.1d
+  // instructions
+  PTX_PARSE_DPRINTF("add_1vector_operand");
+  const symbol *s1 = g_current_symbol_table->lookup(d1);
+  parse_assert(s1 != NULL, "component(s) missing declarations.");
+  g_operands.push_back(operand_info(s1, NULL, NULL, NULL, gpgpu_ctx));
+}
+
+void ptx_recognizer::add_2vector_operand(const char *d1, const char *d2) {
+  PTX_PARSE_DPRINTF("add_2vector_operand");
+  const symbol *s1 = g_current_symbol_table->lookup(d1);
+  const symbol *s2 = g_current_symbol_table->lookup(d2);
+  parse_assert(s1 != NULL && s2 != NULL,
+               "v2 component(s) missing declarations.");
+  g_operands.push_back(operand_info(s1, s2, NULL, NULL, gpgpu_ctx));
+}
+
+void ptx_recognizer::add_3vector_operand(const char *d1, const char *d2,
+                                         const char *d3) {
+  PTX_PARSE_DPRINTF("add_3vector_operand");
+  const symbol *s1 = g_current_symbol_table->lookup(d1);
+  const symbol *s2 = g_current_symbol_table->lookup(d2);
+  const symbol *s3 = g_current_symbol_table->lookup(d3);
+  parse_assert(s1 != NULL && s2 != NULL && s3 != NULL,
+               "v3 component(s) missing declarations.");
+  g_operands.push_back(operand_info(s1, s2, s3, NULL, gpgpu_ctx));
+}
+
+void ptx_recognizer::add_4vector_operand(const char *d1, const char *d2,
+                                         const char *d3, const char *d4) {
+  PTX_PARSE_DPRINTF("add_4vector_operand");
+  const symbol *s1 = g_current_symbol_table->lookup(d1);
+  const symbol *s2 = g_current_symbol_table->lookup(d2);
+  const symbol *s3 = g_current_symbol_table->lookup(d3);
+  const symbol *s4 = g_current_symbol_table->lookup(d4);
+  parse_assert(s1 != NULL && s2 != NULL && s3 != NULL && s4 != NULL,
+               "v4 component(s) missing declarations.");
+  const symbol *null_op = g_current_symbol_table->lookup("_");
+  if (s2 == null_op) s2 = NULL;
+  if (s3 == null_op) s3 = NULL;
+  if (s4 == null_op) s4 = NULL;
+  g_operands.push_back(operand_info(s1, s2, s3, s4, gpgpu_ctx));
+}
+void ptx_recognizer::add_8vector_operand(const char *d1, const char *d2,
+                                         const char *d3, const char *d4,
+                                         const char *d5, const char *d6,
+                                         const char *d7, const char *d8) {
+  PTX_PARSE_DPRINTF("add_8vector_operand");
+  const symbol *s1 = g_current_symbol_table->lookup(d1);
+  const symbol *s2 = g_current_symbol_table->lookup(d2);
+  const symbol *s3 = g_current_symbol_table->lookup(d3);
+  const symbol *s4 = g_current_symbol_table->lookup(d4);
+  const symbol *s5 = g_current_symbol_table->lookup(d5);
+  const symbol *s6 = g_current_symbol_table->lookup(d6);
+  const symbol *s7 = g_current_symbol_table->lookup(d7);
+  const symbol *s8 = g_current_symbol_table->lookup(d8);
+  parse_assert(s1 != NULL && s2 != NULL && s3 != NULL && s4 != NULL &&
+                   s5 != NULL && s6 != NULL && s7 != NULL && s8 != NULL,
+               "v4 component(s) missing declarations.");
+  const symbol *null_op = g_current_symbol_table->lookup("_");
+  if (s2 == null_op) s2 = NULL;
+  if (s3 == null_op) s3 = NULL;
+  if (s4 == null_op) s4 = NULL;
+  if (s5 == null_op) s5 = NULL;
+  if (s6 == null_op) s6 = NULL;
+  if (s7 == null_op) s7 = NULL;
+  if (s8 == null_op) s8 = NULL;
+  g_operands.push_back(operand_info(s1, s2, s3, s4, s5, s6, s7, s8, gpgpu_ctx));
+}
+
+void ptx_recognizer::add_builtin_operand(int builtin, int dim_modifier) {
+  PTX_PARSE_DPRINTF("add_builtin_operand");
+  g_operands.push_back(operand_info(builtin, dim_modifier, gpgpu_ctx));
+}
+
+void ptx_recognizer::add_memory_operand() {
+  PTX_PARSE_DPRINTF("add_memory_operand");
+  assert(!g_operands.empty());
+  g_operands.back().make_memory_operand();
 }
 
 /*TODO: add other memory locations*/
-void change_memory_addr_space(const char *identifier) 
-{
-   /*0 = N/A, not reading from memory
-    *1 = global memory
-    *2 = shared memory
-    *3 = const memory segment
-    *4 = local memory segment
-    */
-
-   bool recognizedType = false;
-
-   PTX_PARSE_DPRINTF("change_memory_addr_space");
-   assert( !g_operands.empty() );
-   if(!strcmp(identifier, "g"))
-   {
-       g_operands.back().set_addr_space(global_space);
-       recognizedType = true;
-   }
-   if(!strcmp(identifier, "s"))
-   {
-       g_operands.back().set_addr_space(shared_space);
-       recognizedType = true;
-   }
-   // For constants, check if the first character is 'c'
-   char c[2];
-   strncpy(c, identifier, 1); c[1] = '\0';
-   if(!strcmp(c, "c"))
-   {
-       g_operands.back().set_addr_space(const_space);
-       parse_assert(g_current_symbol_table->lookup(identifier) != NULL, "Constant was not defined.");
-       g_operands.back().set_const_mem_offset(g_current_symbol_table->lookup(identifier)->get_address());
-       recognizedType = true;
-   }
-   // For local memory, check if the first character is 'l'
-   char l[2];
-   strncpy(l, identifier, 1); l[1] = '\0';
-   if(!strcmp(l, "l"))
-   {
-       g_operands.back().set_addr_space(local_space);
-       //parse_assert(g_current_symbol_table->lookup(identifier) != NULL, "Local memory segment was not defined.");
-       //g_operands.back().set_const_mem_offset(g_current_symbol_table->lookup(identifier)->get_address());
-       recognizedType = true;
-   }
-
-   parse_assert(recognizedType, "Error: unrecognized memory type.");
-}
-
-void change_operand_lohi( int lohi )
-{
-   /*0 = N/A, read entire operand
-    *1 = lo, reading from lowest bits
-    *2 = hi, reading from highest bits
-    */
-
-   PTX_PARSE_DPRINTF("change_operand_lohi");
-   assert( !g_operands.empty() );
-
-   g_operands.back().set_operand_lohi(lohi);
-
-}
-
-void set_immediate_operand_type ()
-{
-     PTX_PARSE_DPRINTF("set_immediate_operand_type");
-     assert( !g_operands.empty() );
-     g_operands.back().set_immediate_addr();
-}
-
-void change_double_operand_type( int operand_type )
-{
-   /*
-    *-3 = reg / reg (set instruction, but both get same value)
-    *-2 = reg | reg (cvt instruction)
-    *-1 = reg | reg (set instruction)
-    *0 = N/A, default
-    *1 = reg + reg
-    *2 = reg += reg
-    *3 = reg += immediate
-    */
-
-   PTX_PARSE_DPRINTF("change_double_operand_type");
-   assert( !g_operands.empty() );
-
-   // For double destination operands, ensure valid instruction
-   if( operand_type == -1 || operand_type == -2 ) {
-      if((g_opcode == SET_OP)||(g_opcode == SETP_OP))
-         g_operands.back().set_double_operand_type(-1);
-      else
-         g_operands.back().set_double_operand_type(-2);
-   } else if( operand_type == -3 ) {
-      if(g_opcode == SET_OP || g_opcode == MAD_OP)
-         g_operands.back().set_double_operand_type(operand_type);
-      else
-         parse_assert(0, "Error: Unsupported use of double destination operand.");
-   } else {
+void ptx_recognizer::change_memory_addr_space(const char *identifier) {
+  /*0 = N/A, not reading from memory
+   *1 = global memory
+   *2 = shared memory
+   *3 = const memory segment
+   *4 = local memory segment
+   */
+
+  bool recognizedType = false;
+
+  PTX_PARSE_DPRINTF("change_memory_addr_space");
+  assert(!g_operands.empty());
+  if (!strcmp(identifier, "g")) {
+    g_operands.back().set_addr_space(global_space);
+    recognizedType = true;
+  }
+  if (!strcmp(identifier, "s")) {
+    g_operands.back().set_addr_space(shared_space);
+    recognizedType = true;
+  }
+  // For constants, check if the first character is 'c'
+  char c[2];
+  strncpy(c, identifier, 1);
+  c[1] = '\0';
+  if (!strcmp(c, "c")) {
+    g_operands.back().set_addr_space(const_space);
+    parse_assert(g_current_symbol_table->lookup(identifier) != NULL,
+                 "Constant was not defined.");
+    g_operands.back().set_const_mem_offset(
+        g_current_symbol_table->lookup(identifier)->get_address());
+    recognizedType = true;
+  }
+  // For local memory, check if the first character is 'l'
+  char l[2];
+  strncpy(l, identifier, 1);
+  l[1] = '\0';
+  if (!strcmp(l, "l")) {
+    g_operands.back().set_addr_space(local_space);
+    // parse_assert(g_current_symbol_table->lookup(identifier) != NULL, "Local
+    // memory segment was not defined.");
+    // g_operands.back().set_const_mem_offset(g_current_symbol_table->lookup(identifier)->get_address());
+    recognizedType = true;
+  }
+
+  parse_assert(recognizedType, "Error: unrecognized memory type.");
+}
+
+void ptx_recognizer::change_operand_lohi(int lohi) {
+  /*0 = N/A, read entire operand
+   *1 = lo, reading from lowest bits
+   *2 = hi, reading from highest bits
+   */
+
+  PTX_PARSE_DPRINTF("change_operand_lohi");
+  assert(!g_operands.empty());
+
+  g_operands.back().set_operand_lohi(lohi);
+}
+
+void ptx_recognizer::set_immediate_operand_type() {
+  PTX_PARSE_DPRINTF("set_immediate_operand_type");
+  assert(!g_operands.empty());
+  g_operands.back().set_immediate_addr();
+}
+
+void ptx_recognizer::change_double_operand_type(int operand_type) {
+  /*
+   *-3 = reg / reg (set instruction, but both get same value)
+   *-2 = reg | reg (cvt instruction)
+   *-1 = reg | reg (set instruction)
+   *0 = N/A, default
+   *1 = reg + reg
+   *2 = reg += reg
+   *3 = reg += immediate
+   */
+
+  PTX_PARSE_DPRINTF("change_double_operand_type");
+  assert(!g_operands.empty());
+
+  // For double destination operands, ensure valid instruction
+  if (operand_type == -1 || operand_type == -2) {
+    if ((g_opcode == SET_OP) || (g_opcode == SETP_OP))
+      g_operands.back().set_double_operand_type(-1);
+    else
+      g_operands.back().set_double_operand_type(-2);
+  } else if (operand_type == -3) {
+    if (g_opcode == SET_OP || g_opcode == MAD_OP)
       g_operands.back().set_double_operand_type(operand_type);
-   }
+    else
+      parse_assert(0, "Error: Unsupported use of double destination operand.");
+  } else {
+    g_operands.back().set_double_operand_type(operand_type);
+  }
+}
+
+void ptx_recognizer::change_operand_neg() {
+  PTX_PARSE_DPRINTF("change_operand_neg");
+  assert(!g_operands.empty());
+
+  g_operands.back().set_operand_neg();
+}
+
+void ptx_recognizer::add_literal_int(int value) {
+  PTX_PARSE_DPRINTF("add_literal_int");
+  g_operands.push_back(operand_info(value, gpgpu_ctx));
+}
 
+void ptx_recognizer::add_literal_float(float value) {
+  PTX_PARSE_DPRINTF("add_literal_float");
+  g_operands.push_back(operand_info(value, gpgpu_ctx));
 }
 
-void change_operand_neg( )
-{
-   PTX_PARSE_DPRINTF("change_operand_neg");
-   assert( !g_operands.empty() );
+void ptx_recognizer::add_literal_double(double value) {
+  PTX_PARSE_DPRINTF("add_literal_double");
+  g_operands.push_back(operand_info(value, gpgpu_ctx));
+}
 
-   g_operands.back().set_operand_neg();
+void ptx_recognizer::add_scalar_operand(const char *identifier) {
+  PTX_PARSE_DPRINTF("add_scalar_operand");
+  const symbol *s = g_current_symbol_table->lookup(identifier);
+  if (s == NULL) {
+    if (g_opcode == BRA_OP || g_opcode == CALLP_OP) {
+      // forward branch target...
+      s = g_current_symbol_table->add_variable(
+          identifier, NULL, 0, gpgpu_ctx->g_filename, ptx_get_lineno(scanner));
+    } else {
+      std::string msg =
+          std::string("operand \"") + identifier + "\" has no declaration.";
+      parse_error(msg.c_str());
+    }
+  }
+  g_operands.push_back(operand_info(s, gpgpu_ctx));
+}
 
+void ptx_recognizer::add_neg_pred_operand(const char *identifier) {
+  PTX_PARSE_DPRINTF("add_neg_pred_operand");
+  const symbol *s = g_current_symbol_table->lookup(identifier);
+  if (s == NULL) {
+    s = g_current_symbol_table->add_variable(
+        identifier, NULL, 1, gpgpu_ctx->g_filename, ptx_get_lineno(scanner));
+  }
+  operand_info op(s, gpgpu_ctx);
+  op.set_neg_pred();
+  g_operands.push_back(op);
 }
 
-void add_literal_int( int value ) 
-{
-   PTX_PARSE_DPRINTF("add_literal_int");
-   g_operands.push_back( operand_info(value) );
+void ptx_recognizer::add_address_operand(const char *identifier, int offset) {
+  PTX_PARSE_DPRINTF("add_address_operand");
+  const symbol *s = g_current_symbol_table->lookup(identifier);
+  if (s == NULL) {
+    std::string msg =
+        std::string("operand \"") + identifier + "\" has no declaration.";
+    parse_error(msg.c_str());
+  }
+  g_operands.push_back(operand_info(s, offset, gpgpu_ctx));
 }
 
-void add_literal_float( float value ) 
-{
-   PTX_PARSE_DPRINTF("add_literal_float");
-   g_operands.push_back( operand_info(value) );
+void ptx_recognizer::add_address_operand2(int offset) {
+  PTX_PARSE_DPRINTF("add_address_operand");
+  g_operands.push_back(operand_info((unsigned)offset, gpgpu_ctx));
 }
 
-void add_literal_double( double value ) 
-{
-   PTX_PARSE_DPRINTF("add_literal_double");
-   g_operands.push_back( operand_info(value) );
+void ptx_recognizer::add_array_initializer() {
+  g_last_symbol->add_initializer(g_operands);
 }
 
-void add_scalar_operand( const char *identifier ) 
-{
-   PTX_PARSE_DPRINTF("add_scalar_operand");
-   const symbol *s = g_current_symbol_table->lookup(identifier);
-   if ( s == NULL ) {
-      if ( g_opcode == BRA_OP || g_opcode == CALLP_OP) {
-         // forward branch target...
-         s = g_current_symbol_table->add_variable(identifier,NULL,0,g_filename,ptx_lineno);
-      } else {
-         std::string msg = std::string("operand \"") + identifier + "\" has no declaration.";
-         parse_error( msg.c_str() );
-      }
-   }
-   g_operands.push_back( operand_info(s) );
-}
-
-void add_neg_pred_operand( const char *identifier ) 
-{
-   PTX_PARSE_DPRINTF("add_neg_pred_operand");
-   const symbol *s = g_current_symbol_table->lookup(identifier);
-   if ( s == NULL ) {
-       s = g_current_symbol_table->add_variable(identifier,NULL,1,g_filename,ptx_lineno);
-   }
-   operand_info op(s);
-   op.set_neg_pred();
-   g_operands.push_back( op );
-}
-
-void add_address_operand( const char *identifier, int offset ) 
-{
-   PTX_PARSE_DPRINTF("add_address_operand");
-   const symbol *s = g_current_symbol_table->lookup(identifier);
-   if ( s == NULL ) {
-      std::string msg = std::string("operand \"") + identifier + "\" has no declaration.";
-      parse_error( msg.c_str() );
-   }
-   g_operands.push_back( operand_info(s,offset) );
-}
-
-void add_address_operand2( int offset )
-{
-   PTX_PARSE_DPRINTF("add_address_operand");
-   g_operands.push_back( operand_info((unsigned)offset) );
-}
-
-void add_array_initializer()
-{
-   g_last_symbol->add_initializer(g_operands);
-}
-
-void add_version_info( float ver, unsigned ext )
-{
-   g_global_symbol_table->set_ptx_version(ver,ext);
-}
-
-void add_file( unsigned num, const char *filename )
-{
-   if( g_filename == NULL ) {
-      char *b = strdup(filename);
-      char *l=b;
-      char *n=b;
-      while( *n != '\0' ) {
-          if( *n == '/' ) 
-              l = n+1;
-          n++;
-      }
+void ptx_recognizer::add_version_info(float ver, unsigned ext) {
+  g_global_symbol_table->set_ptx_version(ver, ext);
+}
+
+void ptx_recognizer::add_file(unsigned num, const char *filename) {
+  if (gpgpu_ctx->g_filename == NULL) {
+    char *b = strdup(filename);
+    char *l = b;
+    char *n = b;
+    while (*n != '\0') {
+      if (*n == '/') l = n + 1;
+      n++;
+    }
 
-      char *p = strtok(l,".");
-      char buf[1024];
-      snprintf(buf,1024,"%s.ptx",p);
+    char *p = strtok(l, ".");
+    char buf[1024];
+    snprintf(buf, 1024, "%s.ptx", p);
 
-      char *q = strtok(NULL,".");
-      if( q && !strcmp(q,"cu") ) {
-          g_filename = strdup(buf);
-      }
+    char *q = strtok(NULL, ".");
+    if (q && !strcmp(q, "cu")) {
+      gpgpu_ctx->g_filename = strdup(buf);
+    }
 
-      free( b );
-   }
+    free(b);
+  }
 
-   g_current_symbol_table = g_global_symbol_table;
+  g_current_symbol_table = g_global_symbol_table;
 }
 
-void *reset_symtab()
-{
-   void *result = g_current_symbol_table;
-   g_current_symbol_table = g_global_symbol_table;
-   return result;
+void *ptx_recognizer::reset_symtab() {
+  void *result = g_current_symbol_table;
+  g_current_symbol_table = g_global_symbol_table;
+  return result;
 }
 
-void set_symtab(void*symtab)
-{
-   g_current_symbol_table = (symbol_table*)symtab;
+void ptx_recognizer::set_symtab(void *symtab) {
+  g_current_symbol_table = (symbol_table *)symtab;
 }
 
-void add_pragma( const char *str )
-{
-   printf("GPGPU-Sim PTX: Warning -- ignoring pragma '%s'\n", str );
+void ptx_recognizer::add_pragma(const char *str) {
+  printf("GPGPU-Sim PTX: Warning -- ignoring pragma '%s'\n", str);
 }
 
-void version_header(double a) {}  //intentional dummy function
+void ptx_recognizer::version_header(double a) {}  // intentional dummy function
 
-void target_header(char* a) 
-{
-   g_global_symbol_table->set_sm_target(a,NULL,NULL);
+void ptx_recognizer::target_header(char *a) {
+  g_global_symbol_table->set_sm_target(a, NULL, NULL);
 }
 
-void target_header2(char* a, char* b) 
-{
-   g_global_symbol_table->set_sm_target(a,b,NULL);
+void ptx_recognizer::target_header2(char *a, char *b) {
+  g_global_symbol_table->set_sm_target(a, b, NULL);
 }
 
-void target_header3(char* a, char* b, char* c) 
-{
-   g_global_symbol_table->set_sm_target(a,b,c);
+void ptx_recognizer::target_header3(char *a, char *b, char *c) {
+  g_global_symbol_table->set_sm_target(a, b, c);
 }
 
-void maxnt_id(int x, int y, int z) {
+void ptx_recognizer::maxnt_id(int x, int y, int z) {
   g_func_info->set_maxnt_id(x * y * z);
 }
 
-void func_header(const char* a) {} //intentional dummy function
-void func_header_info(const char* a) {} //intentional dummy function
-void func_header_info_int(const char* a, int b) {} //intentional dummy function
+void ptx_recognizer::func_header(const char *a) {}  // intentional dummy
+                                                    // function
+void ptx_recognizer::func_header_info(const char *a) {
+}  // intentional dummy function
+void ptx_recognizer::func_header_info_int(const char *a, int b) {
+}  // intentional dummy function
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.h
index ca0d36e841..2778ceca84 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_parser.h
@@ -28,86 +28,166 @@
 #ifndef ptx_parser_INCLUDED
 #define ptx_parser_INCLUDED
 
-#ifndef LIBCUDA
-#include "abstract_hardware_model.h"
-#else
-#include "../libcuda/abstract_hardware_model.h"
-#endif
+#include "../abstract_hardware_model.h"
+#include "ptx_ir.h"
 
-extern const char *g_filename;
-extern int g_error_detected;
+class gpgpu_context;
+typedef void *yyscan_t;
+class ptx_recognizer {
+ public:
+  ptx_recognizer(gpgpu_context *ctx) : g_return_var(ctx) {
+    scanner = NULL;
+    g_size = -1;
+    g_add_identifier_cached__identifier = NULL;
+    g_alignment_spec = -1;
+    g_var_type = NULL;
+    g_opcode = -1;
+    g_space_spec = undefined_space;
+    g_ptr_spec = undefined_space;
+    g_scalar_type_spec = -1;
+    g_vector_spec = -1;
+    g_extern_spec = 0;
+    g_func_decl = 0;
+    g_ident_add_uid = 0;
+    g_const_alloc = 1;
+    g_max_regs_per_thread = 0;
+    g_global_symbol_table = NULL;
+    g_current_symbol_table = NULL;
+    g_last_symbol = NULL;
+    g_error_detected = 0;
+    g_entry_func_param_index = 0;
+    g_func_info = NULL;
+    g_debug_ir_generation = false;
+    gpgpu_ctx = ctx;
+  }
+  // global list
+  yyscan_t scanner;
+#define PTX_LINEBUF_SIZE (4 * 1024)
+  char linebuf[PTX_LINEBUF_SIZE];
+  unsigned col;
+  int g_size;
+  char *g_add_identifier_cached__identifier;
+  int g_add_identifier_cached__array_dim;
+  int g_add_identifier_cached__array_ident;
+  int g_alignment_spec;
+  // variable declaration stuff:
+  type_info *g_var_type;
+  // instruction definition stuff:
+  const symbol *g_pred;
+  int g_neg_pred;
+  int g_pred_mod;
+  symbol *g_label;
+  int g_opcode;
+  std::list<operand_info> g_operands;
+  std::list<int> g_options;
+  std::list<int> g_wmma_options;
+  std::list<int> g_scalar_type;
+  // type specifier stuff:
+  memory_space_t g_space_spec;
+  memory_space_t g_ptr_spec;
+  int g_scalar_type_spec;
+  int g_vector_spec;
+  int g_extern_spec;
+  int g_func_decl;
+  int g_ident_add_uid;
+  unsigned g_const_alloc;
+  unsigned g_max_regs_per_thread;
+  symbol_table *g_global_symbol_table;
+  symbol_table *g_current_symbol_table;
+  symbol *g_last_symbol;
+  std::list<ptx_instruction *> g_instructions;
+  int g_error_detected;
+  unsigned g_entry_func_param_index;
+  function_info *g_func_info;
+  operand_info g_return_var;
+  bool g_debug_ir_generation;
+  int g_entry_point;
+  const struct core_config *g_shader_core_config;
+  std::map<std::string, std::map<unsigned, const ptx_instruction *> >
+      g_inst_lookup;
+  // the program intermediate representation...
+  std::map<std::string, symbol_table *> g_sym_name_to_symbol_table;
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
 
-#ifdef __cplusplus 
-class symbol_table* init_parser(const char*);
-const class ptx_instruction *ptx_instruction_lookup( const char *filename, unsigned linenumber );
-extern "C" {
-#endif
+  // member function list
+  void init_directive_state();
+  void init_instruction_state();
+  void start_function(int entry_point);
+  void add_function_name(const char *fname);
+  void add_directive();
+  void end_function();
+  void add_identifier(const char *s, int array_dim, unsigned array_ident);
+  void add_function_arg();
+  void add_scalar_type_spec(int type_spec);
+  void add_scalar_operand(const char *identifier);
+  void add_neg_pred_operand(const char *identifier);
+  void add_variables();
+  void set_variable_type();
+  void add_opcode(int opcode);
+  void add_pred(const char *identifier, int negate, int predModifier);
+  void add_1vector_operand(const char *d1);
+  void add_2vector_operand(const char *d1, const char *d2);
+  void add_3vector_operand(const char *d1, const char *d2, const char *d3);
+  void add_4vector_operand(const char *d1, const char *d2, const char *d3,
+                           const char *d4);
+  void add_8vector_operand(const char *d1, const char *d2, const char *d3,
+                           const char *d4, const char *d5, const char *d6,
+                           const char *d7, const char *d8);
+  void add_option(int option);
+  void add_wmma_option(int option);
+  void add_builtin_operand(int builtin, int dim_modifier);
+  void add_memory_operand();
+  void add_literal_int(int value);
+  void add_literal_float(float value);
+  void add_literal_double(double value);
+  void add_address_operand(const char *identifier, int offset);
+  void add_address_operand2(int offset);
+  void add_label(const char *idenfiier);
+  void add_vector_spec(int spec);
+  void add_space_spec(enum _memory_space_t spec, int value);
+  void add_ptr_spec(enum _memory_space_t spec);
+  void add_extern_spec();
+  void add_instruction();
+  void set_return();
+  void add_alignment_spec(int spec);
+  void add_array_initializer();
+  void add_file(unsigned num, const char *filename);
+  void add_version_info(float ver, unsigned ext);
+  void *reset_symtab();
+  void set_symtab(void *);
+  void add_pragma(const char *str);
+  void func_header(const char *a);
+  void func_header_info(const char *a);
+  void func_header_info_int(const char *a, int b);
+  void add_constptr(const char *identifier1, const char *identifier2,
+                    int offset);
+  void target_header(char *a);
+  void target_header2(char *a, char *b);
+  void target_header3(char *a, char *b, char *c);
+  void add_double_operand(const char *d1, const char *d2);
+  void change_memory_addr_space(const char *identifier);
+  void change_operand_lohi(int lohi);
+  void change_double_operand_type(int addr_type);
+  void change_operand_neg();
+  void set_immediate_operand_type();
+  void version_header(double a);
+  void maxnt_id(int x, int y, int z);
+  void parse_error_impl(const char *file, unsigned line, const char *msg, ...);
+  void parse_assert_impl(int test_value, const char *file, unsigned line,
+                         const char *msg, ...);
+  // Jin: handle instructino group for cdp
+  void start_inst_group();
+  void end_inst_group();
+  bool check_for_duplicates(const char *identifier);
+  void read_parser_environment_variables();
+  void set_ptx_warp_size(const struct core_config *warp_size);
+  const class ptx_instruction *ptx_instruction_lookup(const char *filename,
+                                                      unsigned linenumber);
+};
 
-const char *decode_token( int type );
+const char *decode_token(int type);
 void read_parser_environment_variables();
-void start_function( int entry_point );
-void add_function_name( const char *fname );
-void init_directive_state();
-void add_directive(); 
-void end_function();
-void add_identifier( const char *s, int array_dim, unsigned array_ident );
-void add_function_arg();
-void add_scalar_type_spec( int type_spec );
-void add_scalar_operand( const char *identifier );
-void add_neg_pred_operand( const char *identifier );
-void add_variables();
-void set_variable_type();
-void add_opcode( int opcode );
-void add_pred( const char *identifier, int negate, int predModifier );
-void add_1vector_operand( const char *d1 );
-void add_2vector_operand( const char *d1, const char *d2 );
-void add_3vector_operand( const char *d1, const char *d2, const char *d3 );
-void add_4vector_operand( const char *d1, const char *d2, const char *d3, const char *d4 );
-void add_8vector_operand( const char *d1, const char *d2, const char *d3, const char *d4 ,const char *d5,const char *d6,const char *d7,const char *d8);
-void add_option(int option );
-void add_wmma_option(int option );
-void add_builtin_operand( int builtin, int dim_modifier );
-void add_memory_operand( );
-void add_literal_int( int value );
-void add_literal_float( float value );
-void add_literal_double( double value );
-void add_address_operand( const char *identifier, int offset );
-void add_address_operand2( int offset );
-void add_label( const char *idenfiier );
-void add_vector_spec(int spec );
-void add_space_spec( enum _memory_space_t spec, int value );
-void add_ptr_spec( enum _memory_space_t spec ); 
-void add_extern_spec();
-void add_instruction();
-void set_return();
-void add_alignment_spec( int spec );
-void add_array_initializer();
-void add_file( unsigned num, const char *filename );
-void add_version_info( float ver, unsigned ext);
-void *reset_symtab();
-void set_symtab(void*);
-void add_pragma( const char *str );
-void func_header(const char* a);
-void func_header_info(const char* a);
-void func_header_info_int(const char* a, int b);
-void add_constptr(const char* identifier1, const char* identifier2, int offset);
-void target_header(char* a);
-void target_header2(char* a, char* b);
-void target_header3(char* a, char* b, char* c);
-void add_double_operand( const char *d1, const char *d2 );
-void change_memory_addr_space( const char *identifier );
-void change_operand_lohi( int lohi );
-void change_double_operand_type( int addr_type );
-void change_operand_neg( );
-void set_immediate_operand_type( );
-void version_header(double a);
-void maxnt_id(int x, int y, int z);
-//Jin: handle instructino group for cdp
-void start_inst_group();
-void end_inst_group();
-#ifdef __cplusplus
-}
-#endif
 
 #define NON_ARRAY_IDENTIFIER 1
 #define ARRAY_IDENTIFIER_NO_DIM 2
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.cc b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.cc
index 6e23d3b6af..de8efb689b 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.cc
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.cc
@@ -28,579 +28,589 @@
 #include "ptx_sim.h"
 #include <string>
 #include "ptx_ir.h"
-#include "ptx.tab.h"
-
-#ifndef LIBCUDA
+class ptx_recognizer;
+typedef void *yyscan_t;
+#include "../../libcuda_sim/gpgpu_context.h"
 #include "../gpgpu-sim/gpu-sim.h"
 #include "../gpgpu-sim/shader.h"
-#else
-#include "gpu-sim.h"
-#endif
-
-void feature_not_implemented( const char *f );
-
-std::set<unsigned long long> g_ptx_cta_info_sm_idx_used;
-unsigned long long g_ptx_cta_info_uid = 1;
+#include "ptx.tab.h"
 
-ptx_cta_info::ptx_cta_info( unsigned sm_idx )
-{
-   assert( g_ptx_cta_info_sm_idx_used.find(sm_idx) == g_ptx_cta_info_sm_idx_used.end() );
-   g_ptx_cta_info_sm_idx_used.insert(sm_idx);
+void feature_not_implemented(const char *f);
 
-   m_sm_idx = sm_idx;
-   m_uid = g_ptx_cta_info_uid++;
-   m_bar_threads = 0;
-}
+ptx_cta_info::ptx_cta_info(unsigned sm_idx, gpgpu_context *ctx) {
+  assert(ctx->func_sim->g_ptx_cta_info_sm_idx_used.find(sm_idx) ==
+         ctx->func_sim->g_ptx_cta_info_sm_idx_used.end());
+  ctx->func_sim->g_ptx_cta_info_sm_idx_used.insert(sm_idx);
 
-void ptx_cta_info::add_thread( ptx_thread_info *thd )
-{
-   m_threads_in_cta.insert(thd);
+  m_sm_idx = sm_idx;
+  m_uid = (ctx->g_ptx_cta_info_uid)++;
+  m_bar_threads = 0;
+  gpgpu_ctx = ctx;
 }
 
-unsigned ptx_cta_info::num_threads() const
-{
-   return m_threads_in_cta.size();
+void ptx_cta_info::add_thread(ptx_thread_info *thd) {
+  m_threads_in_cta.insert(thd);
 }
 
-void ptx_cta_info::check_cta_thread_status_and_reset()
-{
-   bool fail = false;
-   if ( m_threads_that_have_exited.size() != m_threads_in_cta.size() ) {
-      printf("\n\n");
-      printf("Execution error: Some threads still running in CTA during CTA reallocation! (1)\n");
-      printf("   CTA uid = %Lu (sm_idx = %u) : %lu running out of %zu total\n", 
-             m_uid, 
-             m_sm_idx,
-             (m_threads_in_cta.size() - m_threads_that_have_exited.size()), m_threads_in_cta.size() );
-      printf("   These are the threads that are still running:\n");
-      std::set<ptx_thread_info*>::iterator t_iter;
-      for ( t_iter=m_threads_in_cta.begin(); t_iter != m_threads_in_cta.end(); ++t_iter ) {
-         ptx_thread_info *t = *t_iter;
-         if ( m_threads_that_have_exited.find(t) == m_threads_that_have_exited.end() ) {
-            if ( m_dangling_pointers.find(t) != m_dangling_pointers.end() ) {
-               printf("       <thread deleted>\n");
-            } else {
-               printf("       [done=%c] : ", (t->is_done()?'Y':'N') );
-               t->print_insn( t->get_pc(), stdout );
-               printf("\n");
-            }
-         }
-      }
-      printf("\n\n");
-      fail = true;
-   }
-   if ( fail ) {
-      abort();
-   }
+unsigned ptx_cta_info::num_threads() const { return m_threads_in_cta.size(); }
 
-   bool fail2 = false;
-   std::set<ptx_thread_info*>::iterator t_iter;
-   for ( t_iter=m_threads_in_cta.begin(); t_iter != m_threads_in_cta.end(); ++t_iter ) {
+void ptx_cta_info::check_cta_thread_status_and_reset() {
+  bool fail = false;
+  if (m_threads_that_have_exited.size() != m_threads_in_cta.size()) {
+    printf("\n\n");
+    printf(
+        "Execution error: Some threads still running in CTA during CTA "
+        "reallocation! (1)\n");
+    printf("   CTA uid = %Lu (sm_idx = %u) : %lu running out of %zu total\n",
+           m_uid, m_sm_idx,
+           (m_threads_in_cta.size() - m_threads_that_have_exited.size()),
+           m_threads_in_cta.size());
+    printf("   These are the threads that are still running:\n");
+    std::set<ptx_thread_info *>::iterator t_iter;
+    for (t_iter = m_threads_in_cta.begin(); t_iter != m_threads_in_cta.end();
+         ++t_iter) {
       ptx_thread_info *t = *t_iter;
-      if ( m_dangling_pointers.find(t) == m_dangling_pointers.end() ) {
-         if ( !t->is_done() ) {
-            if ( !fail2 ) {
-               printf("Execution error: Some threads still running in CTA during CTA reallocation! (2)\n");
-               printf("   CTA uid = %Lu (sm_idx = %u) :\n", m_uid, m_sm_idx );
-               fail2 = true;
-            }
-            printf("       ");
-            t->print_insn( t->get_pc(), stdout );
-            printf("\n");
-         }
+      if (m_threads_that_have_exited.find(t) ==
+          m_threads_that_have_exited.end()) {
+        if (m_dangling_pointers.find(t) != m_dangling_pointers.end()) {
+          printf("       <thread deleted>\n");
+        } else {
+          printf("       [done=%c] : ", (t->is_done() ? 'Y' : 'N'));
+          t->print_insn(t->get_pc(), stdout);
+          printf("\n");
+        }
       }
-   }
-   if ( fail2 ) {
-      abort();
-   }
-   m_threads_in_cta.clear();
-   m_threads_that_have_exited.clear();
-   m_dangling_pointers.clear();
-}
-
-void ptx_cta_info::register_thread_exit( ptx_thread_info *thd )
-{
-   assert( m_threads_that_have_exited.find(thd) == m_threads_that_have_exited.end() );
-   m_threads_that_have_exited.insert(thd);
-}
-
-void ptx_cta_info::register_deleted_thread( ptx_thread_info *thd )
-{
-   m_dangling_pointers.insert(thd);
-}
-
-unsigned ptx_cta_info::get_sm_idx() const
-{
-   return m_sm_idx;
-}
-
-unsigned ptx_cta_info::get_bar_threads() const
-{
-   return m_bar_threads;
-}
-
-void ptx_cta_info::inc_bar_threads()
-{
-	m_bar_threads++;
-}
-
-void ptx_cta_info::reset_bar_threads()
-{
-	m_bar_threads = 0;
-}
-
-ptx_warp_info::ptx_warp_info()
-{
-	reset_done_threads();
-}
-
-unsigned ptx_warp_info::get_done_threads() const
-{
-	return m_done_threads;
-}
-
-void ptx_warp_info::inc_done_threads()
-{
-	m_done_threads++;
-}
-
-void ptx_warp_info::reset_done_threads()
-{
-	m_done_threads = 0;
-}
-
-unsigned g_ptx_thread_info_uid_next=1;
-unsigned g_ptx_thread_info_delete_count=0;
-
-ptx_thread_info::~ptx_thread_info()
-{
-   g_ptx_thread_info_delete_count++;
-}
-
-ptx_thread_info::ptx_thread_info( kernel_info_t &kernel )
-    : m_kernel(kernel)
-{
-   m_uid = g_ptx_thread_info_uid_next++;
-   m_core = NULL;
-   m_barrier_num = -1;
-   m_at_barrier = false;
-   m_valid = false;
-   m_gridid = 0;
-   m_thread_done = false;
-   m_cycle_done = 0;
-   m_PC=0;
-   m_icount = 0;
-   m_last_effective_address = 0;
-   m_last_memory_space = undefined_space; 
-   m_branch_taken = 0;
-   m_shared_mem = NULL;
-   m_sstarr_mem = NULL;
-   m_warp_info = NULL;
-   m_cta_info = NULL;
-   m_local_mem = NULL;
-   m_symbol_table = NULL;
-   m_func_info = NULL;
-   m_hw_tid = -1;
-   m_hw_wid = -1;
-   m_hw_sid = -1;
-   m_last_dram_callback.function = NULL;
-   m_last_dram_callback.instruction = NULL;
-   m_regs.push_back( reg_map_t() );
-   m_debug_trace_regs_modified.push_back( reg_map_t() );
-   m_debug_trace_regs_read.push_back( reg_map_t() );
-   m_callstack.push_back( stack_entry() );
-   m_RPC = -1;
-   m_RPC_updated = false;
-   m_last_was_call = false;
-   m_enable_debug_trace = false;
-   m_local_mem_stack_pointer = 0;
-   m_gpu = NULL;
-   m_last_set_operand_value=ptx_reg_t();
-}
-
-const ptx_version &ptx_thread_info::get_ptx_version() const 
-{ 
-   return m_func_info->get_ptx_version(); 
-}
-
-void ptx_thread_info::set_done() 
-{
-   assert( !m_at_barrier );
-   m_thread_done = true;
-   m_cycle_done = gpu_sim_cycle; 
-}
-
-unsigned ptx_thread_info::get_builtin( int builtin_id, unsigned dim_mod ) 
-{
-   assert( m_valid );
-   switch ((builtin_id&0xFFFF)) {
-   case CLOCK_REG:
-      return (unsigned)(gpu_sim_cycle + gpu_tot_sim_cycle);
-   case CLOCK64_REG:
-      abort(); // change return value to unsigned long long?
-	  // GPGPUSim clock is 4 times slower - multiply by 4
-	   return (gpu_sim_cycle + gpu_tot_sim_cycle)*4;
-   case HALFCLOCK_ID:
+    }
+    printf("\n\n");
+    fail = true;
+  }
+  if (fail) {
+    abort();
+  }
+
+  bool fail2 = false;
+  std::set<ptx_thread_info *>::iterator t_iter;
+  for (t_iter = m_threads_in_cta.begin(); t_iter != m_threads_in_cta.end();
+       ++t_iter) {
+    ptx_thread_info *t = *t_iter;
+    if (m_dangling_pointers.find(t) == m_dangling_pointers.end()) {
+      if (!t->is_done()) {
+        if (!fail2) {
+          printf(
+              "Execution error: Some threads still running in CTA during CTA "
+              "reallocation! (2)\n");
+          printf("   CTA uid = %Lu (sm_idx = %u) :\n", m_uid, m_sm_idx);
+          fail2 = true;
+        }
+        printf("       ");
+        t->print_insn(t->get_pc(), stdout);
+        printf("\n");
+      }
+    }
+  }
+  if (fail2) {
+    abort();
+  }
+  m_threads_in_cta.clear();
+  m_threads_that_have_exited.clear();
+  m_dangling_pointers.clear();
+}
+
+void ptx_cta_info::register_thread_exit(ptx_thread_info *thd) {
+  assert(m_threads_that_have_exited.find(thd) ==
+         m_threads_that_have_exited.end());
+  m_threads_that_have_exited.insert(thd);
+}
+
+void ptx_cta_info::register_deleted_thread(ptx_thread_info *thd) {
+  m_dangling_pointers.insert(thd);
+}
+
+unsigned ptx_cta_info::get_sm_idx() const { return m_sm_idx; }
+
+unsigned ptx_cta_info::get_bar_threads() const { return m_bar_threads; }
+
+void ptx_cta_info::inc_bar_threads() { m_bar_threads++; }
+
+void ptx_cta_info::reset_bar_threads() { m_bar_threads = 0; }
+
+ptx_warp_info::ptx_warp_info() { reset_done_threads(); }
+
+unsigned ptx_warp_info::get_done_threads() const { return m_done_threads; }
+
+void ptx_warp_info::inc_done_threads() { m_done_threads++; }
+
+void ptx_warp_info::reset_done_threads() { m_done_threads = 0; }
+
+ptx_thread_info::~ptx_thread_info() {
+  m_gpu->gpgpu_ctx->func_sim->g_ptx_thread_info_delete_count++;
+}
+
+ptx_thread_info::ptx_thread_info(kernel_info_t &kernel) : m_kernel(kernel) {
+  m_uid = kernel.entry()->gpgpu_ctx->func_sim->g_ptx_thread_info_uid_next++;
+  m_core = NULL;
+  m_barrier_num = -1;
+  m_at_barrier = false;
+  m_valid = false;
+  m_gridid = 0;
+  m_thread_done = false;
+  m_cycle_done = 0;
+  m_PC = 0;
+  m_icount = 0;
+  m_last_effective_address = 0;
+  m_last_memory_space = undefined_space;
+  m_branch_taken = 0;
+  m_shared_mem = NULL;
+  m_sstarr_mem = NULL;
+  m_warp_info = NULL;
+  m_cta_info = NULL;
+  m_local_mem = NULL;
+  m_symbol_table = NULL;
+  m_func_info = NULL;
+  m_hw_tid = -1;
+  m_hw_wid = -1;
+  m_hw_sid = -1;
+  m_last_dram_callback.function = NULL;
+  m_last_dram_callback.instruction = NULL;
+  m_regs.push_back(reg_map_t());
+  m_debug_trace_regs_modified.push_back(reg_map_t());
+  m_debug_trace_regs_read.push_back(reg_map_t());
+  m_callstack.push_back(stack_entry());
+  m_RPC = -1;
+  m_RPC_updated = false;
+  m_last_was_call = false;
+  m_enable_debug_trace = false;
+  m_local_mem_stack_pointer = 0;
+  m_gpu = NULL;
+  m_last_set_operand_value = ptx_reg_t();
+}
+
+const ptx_version &ptx_thread_info::get_ptx_version() const {
+  return m_func_info->get_ptx_version();
+}
+
+void ptx_thread_info::set_done() {
+  assert(!m_at_barrier);
+  m_thread_done = true;
+  m_cycle_done = m_gpu->gpu_sim_cycle;
+}
+
+unsigned ptx_thread_info::get_builtin(int builtin_id, unsigned dim_mod) {
+  assert(m_valid);
+  switch ((builtin_id & 0xFFFF)) {
+    case CLOCK_REG:
+      return (unsigned)(m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+    case CLOCK64_REG:
+      abort();  // change return value to unsigned long long?
+                // GPGPUSim clock is 4 times slower - multiply by 4
+      return (m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle) * 4;
+    case HALFCLOCK_ID:
       // GPGPUSim clock is 4 times slower - multiply by 4
-	  // Hardware clock counter is incremented at half the shader clock frequency - divide by 2 (Henry '10)
-      return (gpu_sim_cycle + gpu_tot_sim_cycle)*2;
-   case CTAID_REG:
-      assert( dim_mod < 3 );
-      if( dim_mod == 0 ) return m_ctaid.x;
-      if( dim_mod == 1 ) return m_ctaid.y;
-      if( dim_mod == 2 ) return m_ctaid.z;
+      // Hardware clock counter is incremented at half the shader clock
+      // frequency - divide by 2 (Henry '10)
+      return (m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle) * 2;
+    case CTAID_REG:
+      assert(dim_mod < 3);
+      if (dim_mod == 0) return m_ctaid.x;
+      if (dim_mod == 1) return m_ctaid.y;
+      if (dim_mod == 2) return m_ctaid.z;
       abort();
       break;
-   case ENVREG_REG:{
-	int index = builtin_id >> 16;
-	dim3 gdim = this->get_core()->get_kernel_info()->get_grid_dim();
-		switch(index){
-		case 0:
-		case 1:
-		case 2:
-		case 3:
-		case 4:
-		case 5:
-			return 0;
-			break;
-		case 6:
-			return gdim.x;
-		case 7:
-			return gdim.y;
-		case 8:
-			return gdim.z;
-		case 9:
-			if(gdim.z == 1 && gdim.y == 1)
-				return 1;
-			else if(gdim.z == 1)
-				return 2;
-			else
-				return 3;
-			break;
-		default:
-			break;
-		}
-   }
-   case GRIDID_REG:
+    case ENVREG_REG: {
+      int index = builtin_id >> 16;
+      dim3 gdim = this->get_core()->get_kernel_info()->get_grid_dim();
+      switch (index) {
+        case 0:
+        case 1:
+        case 2:
+        case 3:
+        case 4:
+        case 5:
+          return 0;
+          break;
+        case 6:
+          return gdim.x;
+        case 7:
+          return gdim.y;
+        case 8:
+          return gdim.z;
+        case 9:
+          if (gdim.z == 1 && gdim.y == 1)
+            return 1;
+          else if (gdim.z == 1)
+            return 2;
+          else
+            return 3;
+          break;
+        default:
+          break;
+      }
+    }
+    case GRIDID_REG:
       return m_gridid;
-   case LANEID_REG: return get_hw_tid() % m_core->get_warp_size();
-   case LANEMASK_EQ_REG: feature_not_implemented( "%lanemask_eq" ); return 0;
-   case LANEMASK_LE_REG: feature_not_implemented( "%lanemask_le" ); return 0;
-   case LANEMASK_LT_REG: feature_not_implemented( "%lanemask_lt" ); return 0;
-   case LANEMASK_GE_REG: feature_not_implemented( "%lanemask_ge" ); return 0;
-   case LANEMASK_GT_REG: feature_not_implemented( "%lanemask_gt" ); return 0;
-   case NCTAID_REG:
-      assert( dim_mod < 3 );
-      if( dim_mod == 0 ) return m_nctaid.x;
-      if( dim_mod == 1 ) return m_nctaid.y;
-      if( dim_mod == 2 ) return m_nctaid.z;
+    case LANEID_REG:
+      return get_hw_tid() % m_core->get_warp_size();
+    case LANEMASK_EQ_REG:
+      feature_not_implemented("%lanemask_eq");
+      return 0;
+    case LANEMASK_LE_REG:
+      feature_not_implemented("%lanemask_le");
+      return 0;
+    case LANEMASK_LT_REG:
+      feature_not_implemented("%lanemask_lt");
+      return 0;
+    case LANEMASK_GE_REG:
+      feature_not_implemented("%lanemask_ge");
+      return 0;
+    case LANEMASK_GT_REG:
+      feature_not_implemented("%lanemask_gt");
+      return 0;
+    case NCTAID_REG:
+      assert(dim_mod < 3);
+      if (dim_mod == 0) return m_nctaid.x;
+      if (dim_mod == 1) return m_nctaid.y;
+      if (dim_mod == 2) return m_nctaid.z;
       abort();
       break;
-   case NTID_REG:
-      assert( dim_mod < 3 );
-      if( dim_mod == 0 ) return m_ntid.x;
-      if( dim_mod == 1 ) return m_ntid.y;
-      if( dim_mod == 2 ) return m_ntid.z;
+    case NTID_REG:
+      assert(dim_mod < 3);
+      if (dim_mod == 0) return m_ntid.x;
+      if (dim_mod == 1) return m_ntid.y;
+      if (dim_mod == 2) return m_ntid.z;
       abort();
       break;
-   case NWARPID_REG: feature_not_implemented( "%nwarpid" ); return 0;
-   case PM_REG: feature_not_implemented( "%pm" ); return 0;
-   case SMID_REG: feature_not_implemented( "%smid" ); return 0;
-   case TID_REG:
-      assert( dim_mod < 3 );
-      if( dim_mod == 0 ) return m_tid.x;
-      if( dim_mod == 1 ) return m_tid.y;
-      if( dim_mod == 2 ) return m_tid.z;
+    case NWARPID_REG:
+      feature_not_implemented("%nwarpid");
+      return 0;
+    case PM_REG:
+      feature_not_implemented("%pm");
+      return 0;
+    case SMID_REG:
+      feature_not_implemented("%smid");
+      return 0;
+    case TID_REG:
+      assert(dim_mod < 3);
+      if (dim_mod == 0) return m_tid.x;
+      if (dim_mod == 1) return m_tid.y;
+      if (dim_mod == 2) return m_tid.z;
       abort();
       break;
-   case WARPSZ_REG: return m_core->get_warp_size() ;
-   default:
+    case WARPSZ_REG:
+      return m_core->get_warp_size();
+    default:
       assert(0);
-   }
-   return 0;
+  }
+  return 0;
 }
 
-void ptx_thread_info::set_info( function_info *func ) 
-{
+void ptx_thread_info::set_info(function_info *func) {
   m_symbol_table = func->get_symtab();
   m_func_info = func;
   m_PC = func->get_start_PC();
 }
 
-void ptx_thread_info::cpy_tid_to_reg( dim3 tid )
-{
-   //copies %tid.x, %tid.y and %tid.z into $r0
-   ptx_reg_t data;
-   data.s64=0;
-
-   data.u32=(tid.x + (tid.y<<16) + (tid.z<<26));
-
-   const symbol *r0 = m_symbol_table->lookup("$r0");
-   if (r0){
-	   //No need to set pid if kernel doesn't use it
-	   set_reg(r0,data);
-   }
-}
-
-void ptx_thread_info::print_insn( unsigned pc, FILE * fp ) const
-{
-   m_func_info->print_insn(pc,fp);
-}
-
-static void print_reg( FILE *fp, std::string name, ptx_reg_t value, symbol_table *symtab )
-{
-   const symbol *sym = symtab->lookup(name.c_str());
-   fprintf(fp,"  %8s   ", name.c_str() );
-   if( sym == NULL ) {
-      fprintf(fp,"<unknown type> 0x%llx\n", (unsigned long long ) value.u64 );
-      return;
-   }
-   const type_info *t = sym->type();
-   if( t == NULL ) {
-      fprintf(fp,"<unknown type> 0x%llx\n", (unsigned long long ) value.u64 );
-      return;
-   }
-   type_info_key ti = t->get_key();
-
-   switch ( ti.scalar_type() ) {
-   case S8_TYPE:  fprintf(fp,".s8  %d\n", value.s8 );  break;
-   case S16_TYPE: fprintf(fp,".s16 %d\n", value.s16 ); break;
-   case S32_TYPE: fprintf(fp,".s32 %d\n", value.s32 ); break;
-   case S64_TYPE: fprintf(fp,".s64 %Ld\n", value.s64 ); break;
-   case U8_TYPE:  fprintf(fp,".u8  %u [0x%02x]\n", value.u8, (unsigned) value.u8 );  break;
-   case U16_TYPE: fprintf(fp,".u16 %u [0x%04x]\n", value.u16, (unsigned) value.u16 ); break;
-   case U32_TYPE: fprintf(fp,".u32 %u [0x%08x]\n", value.u32, (unsigned) value.u32 ); break;
-   case U64_TYPE: fprintf(fp,".u64 %llu [0x%llx]\n", value.u64, value.u64 ); break;
-   case F16_TYPE: fprintf(fp,".f16 %f [0x%04x]\n",  (float)value.f16, (unsigned) value.u16 ); break;
-   case F32_TYPE: fprintf(fp,".f32 %.15lf [0x%08x]\n",  value.f32, value.u32 ); break;
-   case F64_TYPE: fprintf(fp,".f64 %.15le [0x%016llx]\n", value.f64, value.u64 ); break;
-   case B8_TYPE:  fprintf(fp,".b8  0x%02x\n",   (unsigned) value.u8 );  break;
-   case B16_TYPE: fprintf(fp,".b16 0x%04x\n",   (unsigned) value.u16 ); break;
-   case B32_TYPE: fprintf(fp,".b32 0x%08x\n", (unsigned) value.u32 ); break;
-   case B64_TYPE: fprintf(fp,".b64 0x%llx\n",    (unsigned long long ) value.u64 ); break;
-   case PRED_TYPE: fprintf(fp,".pred %u\n",     (unsigned) value.pred ); break;
-   default: 
-      fprintf( fp, "non-scalar type\n" );
-      break;
-   }
-   fflush(fp);
-}
-
-static void print_reg( std::string name, ptx_reg_t value, symbol_table *symtab )
-{
-   print_reg(stdout,name,value,symtab);
-}
+void ptx_thread_info::cpy_tid_to_reg(dim3 tid) {
+  // copies %tid.x, %tid.y and %tid.z into $r0
+  ptx_reg_t data;
+  data.s64 = 0;
 
-void ptx_thread_info::callstack_push( unsigned pc, unsigned rpc, const symbol *return_var_src, const symbol *return_var_dst, unsigned call_uid )
-{
-   m_RPC = -1;
-   m_RPC_updated = true;
-   m_last_was_call = true;
-   assert( m_func_info != NULL );
-   m_callstack.push_back( stack_entry(m_symbol_table,m_func_info,pc,rpc,return_var_src,return_var_dst,call_uid) );
-   m_regs.push_back( reg_map_t() );
-   m_debug_trace_regs_modified.push_back( reg_map_t() );
-   m_debug_trace_regs_read.push_back( reg_map_t() );
-   m_local_mem_stack_pointer += m_func_info->local_mem_framesize(); 
-}
+  data.u32 = (tid.x + (tid.y << 16) + (tid.z << 26));
 
-//ptxplus version of callstack_push.
-void ptx_thread_info::callstack_push_plus( unsigned pc, unsigned rpc, const symbol *return_var_src, const symbol *return_var_dst, unsigned call_uid )
-{
-   m_RPC = -1;
-   m_RPC_updated = true;
-   m_last_was_call = true;
-   assert( m_func_info != NULL );
-   m_callstack.push_back( stack_entry(m_symbol_table,m_func_info,pc,rpc,return_var_src,return_var_dst,call_uid) );
-   //m_regs.push_back( reg_map_t() );
-   //m_debug_trace_regs_modified.push_back( reg_map_t() );
-   //m_debug_trace_regs_read.push_back( reg_map_t() );
-   m_local_mem_stack_pointer += m_func_info->local_mem_framesize();
+  const symbol *r0 = m_symbol_table->lookup("$r0");
+  if (r0) {
+    // No need to set pid if kernel doesn't use it
+    set_reg(r0, data);
+  }
 }
 
-
-bool ptx_thread_info::callstack_pop()
-{
-   const symbol *rv_src = m_callstack.back().m_return_var_src;
-   const symbol *rv_dst = m_callstack.back().m_return_var_dst;
-   assert( !((rv_src != NULL) ^ (rv_dst != NULL)) ); // ensure caller and callee agree on whether there is a return value
-
-   // read return value from callee frame
-   arg_buffer_t buffer;
-   if( rv_src != NULL ) 
-      buffer = copy_arg_to_buffer(this, operand_info(rv_src), rv_dst );
-
-   m_symbol_table = m_callstack.back().m_symbol_table;
-   m_NPC = m_callstack.back().m_PC;
-   m_RPC_updated = true;
-   m_last_was_call = false;
-   m_RPC = m_callstack.back().m_RPC;
-   m_func_info = m_callstack.back().m_func_info;
-   if( m_func_info ) {
-      assert( m_local_mem_stack_pointer >= m_func_info->local_mem_framesize() );
-      m_local_mem_stack_pointer -= m_func_info->local_mem_framesize(); 
-   }
-   m_callstack.pop_back();
-   m_regs.pop_back();
-   m_debug_trace_regs_modified.pop_back();
-   m_debug_trace_regs_read.pop_back();
-
-   // write return value into caller frame
-   if( rv_dst != NULL ) 
-      copy_buffer_to_frame(this, buffer);
-
-   return m_callstack.empty();
+void ptx_thread_info::print_insn(unsigned pc, FILE *fp) const {
+  m_func_info->print_insn(pc, fp);
 }
 
-//ptxplus version of callstack_pop
-bool ptx_thread_info::callstack_pop_plus()
-{
-   const symbol *rv_src = m_callstack.back().m_return_var_src;
-   const symbol *rv_dst = m_callstack.back().m_return_var_dst;
-   assert( !((rv_src != NULL) ^ (rv_dst != NULL)) ); // ensure caller and callee agree on whether there is a return value
-
-   // read return value from callee frame
-   arg_buffer_t buffer;
-   if( rv_src != NULL )
-      buffer = copy_arg_to_buffer(this, operand_info(rv_src), rv_dst );
-
-   m_symbol_table = m_callstack.back().m_symbol_table;
-   m_NPC = m_callstack.back().m_PC;
-   m_RPC_updated = true;
-   m_last_was_call = false;
-   m_RPC = m_callstack.back().m_RPC;
-   m_func_info = m_callstack.back().m_func_info;
-   if( m_func_info ) {
-      assert( m_local_mem_stack_pointer >= m_func_info->local_mem_framesize() );
-      m_local_mem_stack_pointer -= m_func_info->local_mem_framesize();
-   }
-   m_callstack.pop_back();
-   //m_regs.pop_back();
-   //m_debug_trace_regs_modified.pop_back();
-   //m_debug_trace_regs_read.pop_back();
-
-   // write return value into caller frame
-   if( rv_dst != NULL )
-      copy_buffer_to_frame(this, buffer);
-
-   return m_callstack.empty();
-}
+static void print_reg(FILE *fp, std::string name, ptx_reg_t value,
+                      symbol_table *symtab) {
+  const symbol *sym = symtab->lookup(name.c_str());
+  fprintf(fp, "  %8s   ", name.c_str());
+  if (sym == NULL) {
+    fprintf(fp, "<unknown type> 0x%llx\n", (unsigned long long)value.u64);
+    return;
+  }
+  const type_info *t = sym->type();
+  if (t == NULL) {
+    fprintf(fp, "<unknown type> 0x%llx\n", (unsigned long long)value.u64);
+    return;
+  }
+  type_info_key ti = t->get_key();
 
-void ptx_thread_info::dump_callstack() const
-{
-   std::list<stack_entry>::const_iterator c=m_callstack.begin();
-   std::list<reg_map_t>::const_iterator r=m_regs.begin();
-
-   printf("\n\n");
-   printf("Call stack for thread uid = %u (sc=%u, hwtid=%u)\n", m_uid, m_hw_sid, m_hw_tid );
-   while( c != m_callstack.end() && r != m_regs.end() ) {
-      const stack_entry &c_e = *c;
-      const reg_map_t &regs = *r;
-      if( !c_e.m_valid ) {
-         printf("  <entry>                              #regs = %zu\n", regs.size() );
-      } else {
-         printf("  %20s  PC=%3u RV= (callee=\'%s\',caller=\'%s\') #regs = %zu\n", 
-                c_e.m_func_info->get_name().c_str(), c_e.m_PC, 
-                c_e.m_return_var_src->name().c_str(), 
-                c_e.m_return_var_dst->name().c_str(), 
-                regs.size() );
-      }
-      c++;
-      r++;
-   }
-   if( c != m_callstack.end() || r != m_regs.end() ) {
-      printf("  *** mismatch in m_regs and m_callstack sizes ***\n" );
-   }
-   printf("\n\n");
-}
-
-std::string ptx_thread_info::get_location() const
-{
-   const ptx_instruction *pI = m_func_info->get_instruction(m_PC);
-   char buf[1024];
-   snprintf(buf,1024,"%s:%u", pI->source_file(), pI->source_line() );
-   return std::string(buf);
-}
-
-const ptx_instruction *ptx_thread_info::get_inst() const
-{
-   return m_func_info->get_instruction(m_PC);
-}
-
-const ptx_instruction *ptx_thread_info::get_inst( addr_t pc ) const
-{
-   return m_func_info->get_instruction(pc);
-}
-
-void ptx_thread_info::dump_regs( FILE *fp )
-{
-   if(m_regs.empty()) return;
-   if(m_regs.back().empty()) return;
-   fprintf(fp,"Register File Contents:\n");
-   fflush(fp);
-   reg_map_t::const_iterator r;
-   for ( r=m_regs.back().begin(); r != m_regs.back().end(); ++r ) {
+  switch (ti.scalar_type()) {
+    case S8_TYPE:
+      fprintf(fp, ".s8  %d\n", value.s8);
+      break;
+    case S16_TYPE:
+      fprintf(fp, ".s16 %d\n", value.s16);
+      break;
+    case S32_TYPE:
+      fprintf(fp, ".s32 %d\n", value.s32);
+      break;
+    case S64_TYPE:
+      fprintf(fp, ".s64 %Ld\n", value.s64);
+      break;
+    case U8_TYPE:
+      fprintf(fp, ".u8  %u [0x%02x]\n", value.u8, (unsigned)value.u8);
+      break;
+    case U16_TYPE:
+      fprintf(fp, ".u16 %u [0x%04x]\n", value.u16, (unsigned)value.u16);
+      break;
+    case U32_TYPE:
+      fprintf(fp, ".u32 %u [0x%08x]\n", value.u32, (unsigned)value.u32);
+      break;
+    case U64_TYPE:
+      fprintf(fp, ".u64 %llu [0x%llx]\n", value.u64, value.u64);
+      break;
+    case F16_TYPE:
+      fprintf(fp, ".f16 %f [0x%04x]\n", value.f16, (unsigned)value.u16);
+      break;
+    case F32_TYPE:
+      fprintf(fp, ".f32 %.15lf [0x%08x]\n", value.f32, value.u32);
+      break;
+    case F64_TYPE:
+      fprintf(fp, ".f64 %.15le [0x%016llx]\n", value.f64, value.u64);
+      break;
+    case B8_TYPE:
+      fprintf(fp, ".b8  0x%02x\n", (unsigned)value.u8);
+      break;
+    case B16_TYPE:
+      fprintf(fp, ".b16 0x%04x\n", (unsigned)value.u16);
+      break;
+    case B32_TYPE:
+      fprintf(fp, ".b32 0x%08x\n", (unsigned)value.u32);
+      break;
+    case B64_TYPE:
+      fprintf(fp, ".b64 0x%llx\n", (unsigned long long)value.u64);
+      break;
+    case PRED_TYPE:
+      fprintf(fp, ".pred %u\n", (unsigned)value.pred);
+      break;
+    default:
+      fprintf(fp, "non-scalar type\n");
+      break;
+  }
+  fflush(fp);
+}
+
+static void print_reg(std::string name, ptx_reg_t value, symbol_table *symtab) {
+  print_reg(stdout, name, value, symtab);
+}
+
+void ptx_thread_info::callstack_push(unsigned pc, unsigned rpc,
+                                     const symbol *return_var_src,
+                                     const symbol *return_var_dst,
+                                     unsigned call_uid) {
+  m_RPC = -1;
+  m_RPC_updated = true;
+  m_last_was_call = true;
+  assert(m_func_info != NULL);
+  m_callstack.push_back(stack_entry(m_symbol_table, m_func_info, pc, rpc,
+                                    return_var_src, return_var_dst, call_uid));
+  m_regs.push_back(reg_map_t());
+  m_debug_trace_regs_modified.push_back(reg_map_t());
+  m_debug_trace_regs_read.push_back(reg_map_t());
+  m_local_mem_stack_pointer += m_func_info->local_mem_framesize();
+}
+
+// ptxplus version of callstack_push.
+void ptx_thread_info::callstack_push_plus(unsigned pc, unsigned rpc,
+                                          const symbol *return_var_src,
+                                          const symbol *return_var_dst,
+                                          unsigned call_uid) {
+  m_RPC = -1;
+  m_RPC_updated = true;
+  m_last_was_call = true;
+  assert(m_func_info != NULL);
+  m_callstack.push_back(stack_entry(m_symbol_table, m_func_info, pc, rpc,
+                                    return_var_src, return_var_dst, call_uid));
+  // m_regs.push_back( reg_map_t() );
+  // m_debug_trace_regs_modified.push_back( reg_map_t() );
+  // m_debug_trace_regs_read.push_back( reg_map_t() );
+  m_local_mem_stack_pointer += m_func_info->local_mem_framesize();
+}
+
+bool ptx_thread_info::callstack_pop() {
+  const symbol *rv_src = m_callstack.back().m_return_var_src;
+  const symbol *rv_dst = m_callstack.back().m_return_var_dst;
+  assert(!((rv_src != NULL) ^
+           (rv_dst != NULL)));  // ensure caller and callee agree on whether
+                                // there is a return value
+
+  // read return value from callee frame
+  arg_buffer_t buffer(m_gpu->gpgpu_ctx);
+  if (rv_src != NULL)
+    buffer = copy_arg_to_buffer(this, operand_info(rv_src, m_gpu->gpgpu_ctx),
+                                rv_dst);
+
+  m_symbol_table = m_callstack.back().m_symbol_table;
+  m_NPC = m_callstack.back().m_PC;
+  m_RPC_updated = true;
+  m_last_was_call = false;
+  m_RPC = m_callstack.back().m_RPC;
+  m_func_info = m_callstack.back().m_func_info;
+  if (m_func_info) {
+    assert(m_local_mem_stack_pointer >= m_func_info->local_mem_framesize());
+    m_local_mem_stack_pointer -= m_func_info->local_mem_framesize();
+  }
+  m_callstack.pop_back();
+  m_regs.pop_back();
+  m_debug_trace_regs_modified.pop_back();
+  m_debug_trace_regs_read.pop_back();
+
+  // write return value into caller frame
+  if (rv_dst != NULL) copy_buffer_to_frame(this, buffer);
+
+  return m_callstack.empty();
+}
+
+// ptxplus version of callstack_pop
+bool ptx_thread_info::callstack_pop_plus() {
+  const symbol *rv_src = m_callstack.back().m_return_var_src;
+  const symbol *rv_dst = m_callstack.back().m_return_var_dst;
+  assert(!((rv_src != NULL) ^
+           (rv_dst != NULL)));  // ensure caller and callee agree on whether
+                                // there is a return value
+
+  // read return value from callee frame
+  arg_buffer_t buffer(m_gpu->gpgpu_ctx);
+  if (rv_src != NULL)
+    buffer = copy_arg_to_buffer(this, operand_info(rv_src, m_gpu->gpgpu_ctx),
+                                rv_dst);
+
+  m_symbol_table = m_callstack.back().m_symbol_table;
+  m_NPC = m_callstack.back().m_PC;
+  m_RPC_updated = true;
+  m_last_was_call = false;
+  m_RPC = m_callstack.back().m_RPC;
+  m_func_info = m_callstack.back().m_func_info;
+  if (m_func_info) {
+    assert(m_local_mem_stack_pointer >= m_func_info->local_mem_framesize());
+    m_local_mem_stack_pointer -= m_func_info->local_mem_framesize();
+  }
+  m_callstack.pop_back();
+  // m_regs.pop_back();
+  // m_debug_trace_regs_modified.pop_back();
+  // m_debug_trace_regs_read.pop_back();
+
+  // write return value into caller frame
+  if (rv_dst != NULL) copy_buffer_to_frame(this, buffer);
+
+  return m_callstack.empty();
+}
+
+void ptx_thread_info::dump_callstack() const {
+  std::list<stack_entry>::const_iterator c = m_callstack.begin();
+  std::list<reg_map_t>::const_iterator r = m_regs.begin();
+
+  printf("\n\n");
+  printf("Call stack for thread uid = %u (sc=%u, hwtid=%u)\n", m_uid, m_hw_sid,
+         m_hw_tid);
+  while (c != m_callstack.end() && r != m_regs.end()) {
+    const stack_entry &c_e = *c;
+    const reg_map_t &regs = *r;
+    if (!c_e.m_valid) {
+      printf("  <entry>                              #regs = %zu\n",
+             regs.size());
+    } else {
+      printf("  %20s  PC=%3u RV= (callee=\'%s\',caller=\'%s\') #regs = %zu\n",
+             c_e.m_func_info->get_name().c_str(), c_e.m_PC,
+             c_e.m_return_var_src->name().c_str(),
+             c_e.m_return_var_dst->name().c_str(), regs.size());
+    }
+    c++;
+    r++;
+  }
+  if (c != m_callstack.end() || r != m_regs.end()) {
+    printf("  *** mismatch in m_regs and m_callstack sizes ***\n");
+  }
+  printf("\n\n");
+}
+
+std::string ptx_thread_info::get_location() const {
+  const ptx_instruction *pI = m_func_info->get_instruction(m_PC);
+  char buf[1024];
+  snprintf(buf, 1024, "%s:%u", pI->source_file(), pI->source_line());
+  return std::string(buf);
+}
+
+const ptx_instruction *ptx_thread_info::get_inst() const {
+  return m_func_info->get_instruction(m_PC);
+}
+
+const ptx_instruction *ptx_thread_info::get_inst(addr_t pc) const {
+  return m_func_info->get_instruction(pc);
+}
+
+void ptx_thread_info::dump_regs(FILE *fp) {
+  if (m_regs.empty()) return;
+  if (m_regs.back().empty()) return;
+  fprintf(fp, "Register File Contents:\n");
+  fflush(fp);
+  reg_map_t::const_iterator r;
+  for (r = m_regs.back().begin(); r != m_regs.back().end(); ++r) {
+    const symbol *sym = r->first;
+    ptx_reg_t value = r->second;
+    std::string name = sym->name();
+    print_reg(fp, name, value, m_symbol_table);
+  }
+}
+
+void ptx_thread_info::dump_modifiedregs(FILE *fp) {
+  if (!(m_debug_trace_regs_modified.empty() ||
+        m_debug_trace_regs_modified.back().empty())) {
+    fprintf(fp, "Output Registers:\n");
+    fflush(fp);
+    reg_map_t::iterator r;
+    for (r = m_debug_trace_regs_modified.back().begin();
+         r != m_debug_trace_regs_modified.back().end(); ++r) {
       const symbol *sym = r->first;
+      std::string name = sym->name();
       ptx_reg_t value = r->second;
+      print_reg(fp, name, value, m_symbol_table);
+    }
+  }
+  if (!(m_debug_trace_regs_read.empty() ||
+        m_debug_trace_regs_read.back().empty())) {
+    fprintf(fp, "Input Registers:\n");
+    fflush(fp);
+    reg_map_t::iterator r;
+    for (r = m_debug_trace_regs_read.back().begin();
+         r != m_debug_trace_regs_read.back().end(); ++r) {
+      const symbol *sym = r->first;
       std::string name = sym->name();
-      print_reg(fp,name,value,m_symbol_table);
-   }
-}
-
-void ptx_thread_info::dump_modifiedregs(FILE *fp)
-{
-   if( !(m_debug_trace_regs_modified.empty() || 
-         m_debug_trace_regs_modified.back().empty()) ) { 
-      fprintf(fp,"Output Registers:\n");
-      fflush(fp);
-      reg_map_t::iterator r;
-      for ( r=m_debug_trace_regs_modified.back().begin(); r != m_debug_trace_regs_modified.back().end(); ++r ) {
-         const symbol *sym = r->first;
-         std::string name = sym->name();
-         ptx_reg_t value = r->second;
-         print_reg(fp,name,value,m_symbol_table);
-      }
-   }
-   if( !(m_debug_trace_regs_read.empty() ||
-         m_debug_trace_regs_read.back().empty()) ) { 
-      fprintf(fp,"Input Registers:\n");
-      fflush(fp);
-      reg_map_t::iterator r;
-      for ( r=m_debug_trace_regs_read.back().begin(); r != m_debug_trace_regs_read.back().end(); ++r ) {
-         const symbol *sym = r->first;
-         std::string name = sym->name();
-         ptx_reg_t value = r->second;
-         print_reg(fp,name,value,m_symbol_table);
-      }
-   }
+      ptx_reg_t value = r->second;
+      print_reg(fp, name, value, m_symbol_table);
+    }
+  }
 }
 
-void ptx_thread_info::push_breakaddr(const operand_info &breakaddr) 
-{
-   m_breakaddrs.push(breakaddr);
+void ptx_thread_info::push_breakaddr(const operand_info &breakaddr) {
+  m_breakaddrs.push(breakaddr);
 }
 
-const operand_info& ptx_thread_info::pop_breakaddr() 
-{
-   if(m_breakaddrs.empty()) {
-      printf("empty breakaddrs stack");
-      assert(0);
-   }
-   operand_info& breakaddr = m_breakaddrs.top();
-   m_breakaddrs.pop();
-   return breakaddr;
+const operand_info &ptx_thread_info::pop_breakaddr() {
+  if (m_breakaddrs.empty()) {
+    printf("empty breakaddrs stack");
+    assert(0);
+  }
+  operand_info &breakaddr = m_breakaddrs.top();
+  m_breakaddrs.pop();
+  return breakaddr;
 }
 
-void ptx_thread_info::set_npc( const function_info *f )
-{
-   m_NPC = f->get_start_PC();
-   m_func_info = const_cast<function_info*>( f );
-   m_symbol_table = m_func_info->get_symtab();
+void ptx_thread_info::set_npc(const function_info *f) {
+  m_NPC = f->get_start_PC();
+  m_func_info = const_cast<function_info *>(f);
+  m_symbol_table = m_func_info->get_symtab();
 }
 
-
-void feature_not_implemented( const char *f ) 
-{
-   printf("GPGPU-Sim: feature '%s' not supported\n", f );
-   abort();
+void feature_not_implemented(const char *f) {
+  printf("GPGPU-Sim: feature '%s' not supported\n", f);
+  abort();
 }
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.h
index 8db7414e51..90ec095f18 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptx_sim.h
@@ -28,37 +28,28 @@
 #define ptx_sim_h_INCLUDED
 
 #include <stdlib.h>
-#include "half.h"
-
-#ifndef LIBCUDA
 #include "../abstract_hardware_model.h"
-#else
-#include "../libcuda/abstract_hardware_model.h"
-#endif
-
 #include "../tr1_hash_map.h"
+#include "half.h"
 
 #include <assert.h>
 #include "opcodes.h"
 
-#include <string>
+#include <list>
 #include <map>
 #include <set>
-#include <list>
+#include <string>
 
 #include "memory.h"
 
-#define GCC_VERSION (__GNUC__ * 10000 \
-                     + __GNUC_MINOR__ * 100 \
-                     + __GNUC_PATCHLEVEL__)
-
-
+#define GCC_VERSION \
+  (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__)
 
 struct param_t {
-   const void *pdata;
-   int type;
-   size_t size;
-   size_t offset;
+  const void *pdata;
+  int type;
+  size_t size;
+  size_t offset;
 };
 
 #include <stack>
@@ -68,98 +59,93 @@ struct param_t {
 using half_float::half;
 
 union ptx_reg_t {
-   ptx_reg_t() {
-      bits.ms = 0;
-      bits.ls = 0;
-      u128.low=0;
-      u128.lowest=0;
-      u128.highest=0;
-      u128.high=0;
-      s8=0;
-      s16=0;
-      s32=0;
-      s64=0;
-      u8=0;
-      u16=0;
-      u64=0;
-      f16=0;
-      f32=0;
-      f64=0;
-      pred=0;
-   }
-   ptx_reg_t(unsigned x) 
-   {
-      bits.ms = 0;
-      bits.ls = 0;
-      u128.low=0;
-      u128.lowest=0;
-      u128.highest=0;
-      u128.high=0;
-      s8=0;
-      s16=0;
-      s32=0;
-      s64=0;
-      u8=0;
-      u16=0;
-      u64=0;
-      f16=0;
-      f32=0;
-      f64=0;
-      pred=0;
-      u32 = x;
-   }
-   operator unsigned int() { return u32;}
-   operator unsigned short() { return u16;}
-   operator unsigned char() { return u8;}
-   operator unsigned long long() { return u64;}
-
-   void mask_and( unsigned ms, unsigned ls )
-   {
-      bits.ms &= ms;
-      bits.ls &= ls;
-   }
-
-   void mask_or( unsigned ms, unsigned ls )
-   {
-      bits.ms |= ms;
-      bits.ls |= ls;
-   }
-   int get_bit( unsigned bit )
-   {
-      if ( bit < 32 )
-         return(bits.ls >> bit) & 1;
-      else
-         return(bits.ms >> (bit-32)) & 1;
-   }
-
-   signed char       s8;
-   signed short      s16;
-   signed int        s32;
-   signed long long  s64;
-   unsigned char     u8;
-   unsigned short    u16;
-   unsigned int      u32;
-   unsigned long long   u64;
-   //gcc 4.7.0
-   #if GCC_VERSION >= 40700
-   half 		f16; 
-   #else
-   float 		f16; 
-   #endif
-   float          f32;
-   double            f64;
-   struct {
-      unsigned ls;
-      unsigned ms;
-   } bits;
-   struct {
-       unsigned int lowest;
-       unsigned int low;
-       unsigned int high;
-       unsigned int highest;
-   } u128;
-   unsigned       pred : 4;
-
+  ptx_reg_t() {
+    bits.ms = 0;
+    bits.ls = 0;
+    u128.low = 0;
+    u128.lowest = 0;
+    u128.highest = 0;
+    u128.high = 0;
+    s8 = 0;
+    s16 = 0;
+    s32 = 0;
+    s64 = 0;
+    u8 = 0;
+    u16 = 0;
+    u64 = 0;
+    f16 = 0;
+    f32 = 0;
+    f64 = 0;
+    pred = 0;
+  }
+  ptx_reg_t(unsigned x) {
+    bits.ms = 0;
+    bits.ls = 0;
+    u128.low = 0;
+    u128.lowest = 0;
+    u128.highest = 0;
+    u128.high = 0;
+    s8 = 0;
+    s16 = 0;
+    s32 = 0;
+    s64 = 0;
+    u8 = 0;
+    u16 = 0;
+    u64 = 0;
+    f16 = 0;
+    f32 = 0;
+    f64 = 0;
+    pred = 0;
+    u32 = x;
+  }
+  operator unsigned int() { return u32; }
+  operator unsigned short() { return u16; }
+  operator unsigned char() { return u8; }
+  operator unsigned long long() { return u64; }
+
+  void mask_and(unsigned ms, unsigned ls) {
+    bits.ms &= ms;
+    bits.ls &= ls;
+  }
+
+  void mask_or(unsigned ms, unsigned ls) {
+    bits.ms |= ms;
+    bits.ls |= ls;
+  }
+  int get_bit(unsigned bit) {
+    if (bit < 32)
+      return (bits.ls >> bit) & 1;
+    else
+      return (bits.ms >> (bit - 32)) & 1;
+  }
+
+  signed char s8;
+  signed short s16;
+  signed int s32;
+  signed long long s64;
+  unsigned char u8;
+  unsigned short u16;
+  unsigned int u32;
+  unsigned long long u64;
+// gcc 4.7.0
+#if GCC_VERSION >= 40700
+  half f16;
+#else
+  float f16;
+#endif
+  float f32;
+  double f64;
+  struct {
+    unsigned ls;
+    unsigned ms;
+  } bits;
+  struct {
+    unsigned int lowest;
+    unsigned int low;
+    unsigned int high;
+    unsigned int highest;
+  } u128;
+  unsigned pred : 4;
 };
 
 class ptx_instruction;
@@ -167,377 +153,381 @@ class operand_info;
 class symbol_table;
 class function_info;
 class ptx_thread_info;
+class gpgpu_context;
 
 class ptx_cta_info {
-public:
-   ptx_cta_info( unsigned sm_idx );
-   void add_thread( ptx_thread_info *thd );
-   unsigned num_threads() const;
-   void check_cta_thread_status_and_reset();
-   void register_thread_exit( ptx_thread_info *thd );
-   void register_deleted_thread( ptx_thread_info *thd );
-   unsigned get_sm_idx() const;
-   unsigned get_bar_threads() const;
-   void inc_bar_threads();
-   void reset_bar_threads();
-
-private:
-   unsigned           m_bar_threads;
-   unsigned long long         m_uid;
-   unsigned                m_sm_idx;
-   std::set<ptx_thread_info*>    m_threads_in_cta;
-   std::set<ptx_thread_info*>  m_threads_that_have_exited;
-   std::set<ptx_thread_info*>  m_dangling_pointers;
+ public:
+  ptx_cta_info(unsigned sm_idx, gpgpu_context *ctx);
+  void add_thread(ptx_thread_info *thd);
+  unsigned num_threads() const;
+  void check_cta_thread_status_and_reset();
+  void register_thread_exit(ptx_thread_info *thd);
+  void register_deleted_thread(ptx_thread_info *thd);
+  unsigned get_sm_idx() const;
+  unsigned get_bar_threads() const;
+  void inc_bar_threads();
+  void reset_bar_threads();
+
+ private:
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+  unsigned m_bar_threads;
+  unsigned long long m_uid;
+  unsigned m_sm_idx;
+  std::set<ptx_thread_info *> m_threads_in_cta;
+  std::set<ptx_thread_info *> m_threads_that_have_exited;
+  std::set<ptx_thread_info *> m_dangling_pointers;
 };
 
 class ptx_warp_info {
-public:
-	ptx_warp_info(); // add get_core or something, or threads?
-	unsigned get_done_threads() const;
-	void inc_done_threads();
-	void reset_done_threads();
-
-private:
-	unsigned m_done_threads;
+ public:
+  ptx_warp_info();  // add get_core or something, or threads?
+  unsigned get_done_threads() const;
+  void inc_done_threads();
+  void reset_done_threads();
+
+ private:
+  unsigned m_done_threads;
 };
 
 class symbol;
 
 struct stack_entry {
-   stack_entry() {
-      m_symbol_table=NULL;
-      m_func_info=NULL;
-      m_PC=0;
-      m_RPC=-1;
-      m_return_var_src = NULL;
-      m_return_var_dst = NULL;
-      m_call_uid = 0;
-      m_valid = false;
-   }
-   stack_entry( symbol_table *s, function_info *f, unsigned pc, unsigned rpc, const symbol *return_var_src, const symbol *return_var_dst, unsigned call_uid )
-   {
-      m_symbol_table=s;
-      m_func_info=f;
-      m_PC=pc;
-      m_RPC=rpc;
-      m_return_var_src = return_var_src;
-      m_return_var_dst = return_var_dst;
-      m_call_uid = call_uid;
-      m_valid = true;
-   }
-
-   bool m_valid;
-   symbol_table  *m_symbol_table;
-   function_info *m_func_info;
-   unsigned       m_PC;
-   unsigned       m_RPC;
-   const symbol  *m_return_var_src;
-   const symbol  *m_return_var_dst;
-   unsigned       m_call_uid;
+  stack_entry() {
+    m_symbol_table = NULL;
+    m_func_info = NULL;
+    m_PC = 0;
+    m_RPC = -1;
+    m_return_var_src = NULL;
+    m_return_var_dst = NULL;
+    m_call_uid = 0;
+    m_valid = false;
+  }
+  stack_entry(symbol_table *s, function_info *f, unsigned pc, unsigned rpc,
+              const symbol *return_var_src, const symbol *return_var_dst,
+              unsigned call_uid) {
+    m_symbol_table = s;
+    m_func_info = f;
+    m_PC = pc;
+    m_RPC = rpc;
+    m_return_var_src = return_var_src;
+    m_return_var_dst = return_var_dst;
+    m_call_uid = call_uid;
+    m_valid = true;
+  }
+
+  bool m_valid;
+  symbol_table *m_symbol_table;
+  function_info *m_func_info;
+  unsigned m_PC;
+  unsigned m_RPC;
+  const symbol *m_return_var_src;
+  const symbol *m_return_var_dst;
+  unsigned m_call_uid;
 };
 
 class ptx_version {
-public:
-      ptx_version()
-      {
-         m_valid = false;
-         m_ptx_version = 0;
-         m_ptx_extensions = 0;
-         m_sm_version_valid=false;
-         m_texmode_unified=true;
-         m_map_f64_to_f32 = true; 
-      }
-      ptx_version(float ver, unsigned extensions)
-      {
-         m_valid = true;
-         m_ptx_version = ver;
-         m_ptx_extensions = extensions;
-         m_sm_version_valid=false;
-         m_texmode_unified=true;
-      }
-      void set_target( const char *sm_ver, const char *ext, const char *ext2 ) 
-      { 
-         assert( m_valid );
-         m_sm_version_str = sm_ver;
-         check_target_extension(ext); 
-         check_target_extension(ext2); 
-         sscanf(sm_ver,"%u",&m_sm_version);
-         m_sm_version_valid=true; 
-      }
-      float    ver() const { assert(m_valid); return m_ptx_version; }
-      unsigned target() const { assert(m_valid&&m_sm_version_valid); return m_sm_version; }
-      unsigned extensions() const { assert(m_valid); return m_ptx_extensions; }
-private:
-      void check_target_extension( const char *ext ) 
-      {
-         if( ext ) {
-            if( !strcmp(ext,"texmode_independent") ) 
-               m_texmode_unified=false;
-            else if( !strcmp(ext,"texmode_unified") ) 
-               m_texmode_unified=true;
-            else if( !strcmp(ext,"map_f64_to_f32") ) 
-               m_map_f64_to_f32 = true; 
-            // FIXME schi else abort();
-         }
-      }
-
-      bool     m_valid;
-      float    m_ptx_version;
-      unsigned m_sm_version_valid;
-      std::string m_sm_version_str;
-      bool     m_texmode_unified;
-      bool     m_map_f64_to_f32; 
-      unsigned m_sm_version;
-      unsigned m_ptx_extensions;
+ public:
+  ptx_version() {
+    m_valid = false;
+    m_ptx_version = 0;
+    m_ptx_extensions = 0;
+    m_sm_version_valid = false;
+    m_texmode_unified = true;
+    m_map_f64_to_f32 = true;
+  }
+  ptx_version(float ver, unsigned extensions) {
+    m_valid = true;
+    m_ptx_version = ver;
+    m_ptx_extensions = extensions;
+    m_sm_version_valid = false;
+    m_texmode_unified = true;
+  }
+  void set_target(const char *sm_ver, const char *ext, const char *ext2) {
+    assert(m_valid);
+    m_sm_version_str = sm_ver;
+    check_target_extension(ext);
+    check_target_extension(ext2);
+    sscanf(sm_ver, "%u", &m_sm_version);
+    m_sm_version_valid = true;
+  }
+  float ver() const {
+    assert(m_valid);
+    return m_ptx_version;
+  }
+  unsigned target() const {
+    assert(m_valid && m_sm_version_valid);
+    return m_sm_version;
+  }
+  unsigned extensions() const {
+    assert(m_valid);
+    return m_ptx_extensions;
+  }
+
+ private:
+  void check_target_extension(const char *ext) {
+    if (ext) {
+      if (!strcmp(ext, "texmode_independent"))
+        m_texmode_unified = false;
+      else if (!strcmp(ext, "texmode_unified"))
+        m_texmode_unified = true;
+      else if (!strcmp(ext, "map_f64_to_f32"))
+        m_map_f64_to_f32 = true;
+      else
+        abort();
+    }
+  }
+
+  bool m_valid;
+  float m_ptx_version;
+  unsigned m_sm_version_valid;
+  std::string m_sm_version_str;
+  bool m_texmode_unified;
+  bool m_map_f64_to_f32;
+  unsigned m_sm_version;
+  unsigned m_ptx_extensions;
 };
 
 class ptx_thread_info {
-public:
-   ~ptx_thread_info();
-   ptx_thread_info( kernel_info_t &kernel );
-
-   void init(gpgpu_t *gpu, core_t *core, unsigned sid, unsigned cta_id, unsigned wid, unsigned tid, bool fsim) 
-   { 
-      m_gpu = gpu;
-      m_core = core; 
-      m_hw_sid=sid;
-      m_hw_ctaid=cta_id;
-      m_hw_wid=wid;
-      m_hw_tid=tid;
-      m_functionalSimulationMode = fsim;
-   }
-
-   void ptx_fetch_inst( inst_t &inst ) const;
-
-   // TODO schi add
-   int readRegister(const warp_inst_t &inst, unsigned lane_id, char *data, unsigned id = 1);
-   void writeRegister(const warp_inst_t &inst, unsigned lane_id, char *data);
-
-   void ptx_exec_inst( warp_inst_t &inst, unsigned lane_id );
-
-   const ptx_version &get_ptx_version() const;
-   void set_reg( const symbol *reg, const ptx_reg_t &value );
-   void print_reg_thread (char * fname);
-   void resume_reg_thread(char * fname,  symbol_table * symtab);
-   ptx_reg_t get_reg( const symbol *reg );
-   ptx_reg_t get_operand_value( const operand_info &op, operand_info dstInfo, unsigned opType, ptx_thread_info *thread, int derefFlag );
-   void set_operand_value( const operand_info &dst, const ptx_reg_t &data, unsigned type, ptx_thread_info *thread, const ptx_instruction *pI );
-   void set_operand_value( const operand_info &dst, const ptx_reg_t &data, unsigned type, ptx_thread_info *thread, const ptx_instruction *pI, int overflow, int carry );
-   void get_vector_operand_values( const operand_info &op, ptx_reg_t* ptx_regs, unsigned num_elements );
-   void set_vector_operand_values( const operand_info &dst, 
-                                   const ptx_reg_t &data1, 
-                                   const ptx_reg_t &data2, 
-                                   const ptx_reg_t &data3, 
-                                   const ptx_reg_t &data4 );
-   void set_wmma_vector_operand_values( const operand_info &dst, 
-                                        const ptx_reg_t &data1, 
-                                        const ptx_reg_t &data2, 
-                                        const ptx_reg_t &data3, 
-                                        const ptx_reg_t &data4, 
-                                        const ptx_reg_t &data5, 
-                                        const ptx_reg_t &data6, 
-                                        const ptx_reg_t &data7, 
-                                        const ptx_reg_t &data8 );
-
-   function_info *func_info()
-   {
-      return m_func_info; 
-   }
-   void print_insn( unsigned pc, FILE * fp ) const;
-   void set_info( function_info *func );
-   unsigned get_uid() const
-   {
-      return m_uid;
-   }
-
-   dim3 get_ctaid() const { return m_ctaid; }
-   dim3 get_tid() const { return m_tid; }
-   class gpgpu_sim *get_gpu() { return (gpgpu_sim*)m_gpu;}
-   unsigned get_hw_tid() const { return m_hw_tid;}
-   unsigned get_hw_ctaid() const { return m_hw_ctaid;}
-   unsigned get_hw_wid() const { return m_hw_wid;}
-   unsigned get_hw_sid() const { return m_hw_sid;}
-   core_t *get_core() { return m_core; }
-
-   unsigned get_icount() const { return m_icount;}
-   void set_valid() { m_valid = true;}
-   addr_t last_eaddr() const { return m_last_effective_address;}
-   memory_space_t last_space() const { return m_last_memory_space;}
-   dram_callback_t last_callback() const { return m_last_dram_callback;}
-   unsigned long long get_cta_uid() { return m_cta_info->get_sm_idx();}
-
-   void set_single_thread_single_block()
-   {
-      m_ntid.x = 1;
-      m_ntid.y = 1;
-      m_ntid.z = 1;
-      m_ctaid.x = 0;
-      m_ctaid.y = 0;
-      m_ctaid.z = 0;
-      m_tid.x = 0;
-      m_tid.y = 0;
-      m_tid.z = 0;
-      m_nctaid.x = 1;
-      m_nctaid.y = 1;
-      m_nctaid.z = 1;
-      m_gridid = 0;
-      m_valid = true;
-   }
-   void set_tid( dim3 tid ) { m_tid = tid; }
-   void cpy_tid_to_reg( dim3 tid );
-   void set_ctaid( dim3 ctaid ) { m_ctaid = ctaid; }
-   void set_ntid( dim3 tid ) { m_ntid = tid; }
-   void set_nctaid( dim3 cta_size ) { m_nctaid = cta_size; }
-
-   unsigned get_builtin( int builtin_id, unsigned dim_mod ); 
-
-   void set_done();
-   bool is_done() { return m_thread_done;}
-   unsigned donecycle() const { return m_cycle_done; }
-
-   unsigned next_instr()
-   {
-      m_icount++;
-      m_branch_taken = false;
-      return m_PC;
-   }
-   bool branch_taken() const
-   {
-      return m_branch_taken;
-   }
-   unsigned get_pc() const
-   {
-      return m_PC;
-   }
-   void set_npc( unsigned npc )
-   {
-      m_NPC = npc;
-   }
-   void set_npc( const function_info *f );
-   void callstack_push( unsigned npc, unsigned rpc, const symbol *return_var_src, const symbol *return_var_dst, unsigned call_uid );
-   bool callstack_pop();
-   void callstack_push_plus( unsigned npc, unsigned rpc, const symbol *return_var_src, const symbol *return_var_dst, unsigned call_uid );
-   bool callstack_pop_plus();
-   void dump_callstack() const;
-   std::string get_location() const;
-   const ptx_instruction *get_inst() const;
-   const ptx_instruction *get_inst( addr_t pc ) const;
-   bool rpc_updated() const { return m_RPC_updated; }
-   bool last_was_call() const { return m_last_was_call; }
-   unsigned get_rpc() const { return m_RPC; }
-   void clearRPC()
-   {
-      m_RPC = -1;
-      m_RPC_updated = false;
-      m_last_was_call = false;
-   }
-   unsigned get_return_PC()
-   {
-       return m_callstack.back().m_PC;
-   }
-   void update_pc( )
-   {
-      m_PC = m_NPC;
-   }
-   void dump_regs(FILE * fp);
-   void dump_modifiedregs(FILE *fp);
-   void clear_modifiedregs() { m_debug_trace_regs_modified.back().clear(); m_debug_trace_regs_read.back().clear(); }
-   function_info *get_finfo() { return m_func_info;   }
-   const function_info *get_finfo() const { return m_func_info;   }
-   void push_breakaddr(const operand_info &breakaddr);
-   const operand_info& pop_breakaddr();
-   void enable_debug_trace() { m_enable_debug_trace = true; }
-   unsigned get_local_mem_stack_pointer() const { return m_local_mem_stack_pointer; }
-
-   memory_space *get_global_memory() { return m_gpu->get_global_memory(); }
-   memory_space *get_tex_memory() { return m_gpu->get_tex_memory(); }
-   memory_space *get_surf_memory() { return m_gpu->get_surf_memory(); }
-   memory_space *get_param_memory() { return m_kernel.get_param_memory(); }
-   const gpgpu_functional_sim_config &get_config() const { return m_gpu->get_config(); }
-   bool isInFunctionalSimulationMode(){ return m_functionalSimulationMode;}
-   void exitCore()
-   {
-       //m_core is not used in case of functional simulation mode
-       if(!m_functionalSimulationMode)
-           m_core->warp_exit(m_hw_wid);
-   }
-   
-   void registerExit(){m_cta_info->register_thread_exit(this);}
-   unsigned get_reduction_value(unsigned ctaid, unsigned barid) {return m_core->get_reduction_value(ctaid,barid);}
-   void and_reduction(unsigned ctaid, unsigned barid, bool value) {m_core->and_reduction(ctaid,barid,value);}
-   void or_reduction(unsigned ctaid, unsigned barid, bool value) {m_core->or_reduction(ctaid,barid,value);}
-   void popc_reduction(unsigned ctaid, unsigned barid, bool value) {m_core->popc_reduction(ctaid,barid,value);}
-
-   //Jin: get corresponding kernel grid for CDP purpose
-   kernel_info_t & get_kernel() { return m_kernel; }
-
-public:
-   addr_t         m_last_effective_address;
-   bool        m_branch_taken;
-   memory_space_t m_last_memory_space;
-   dram_callback_t   m_last_dram_callback; 
-   memory_space   *m_shared_mem;
-   memory_space   *m_sstarr_mem;
-   memory_space   *m_local_mem;
-   ptx_warp_info  *m_warp_info;
-   ptx_cta_info   *m_cta_info;
-   ptx_reg_t m_last_set_operand_value;
-
-private:
-
-   bool m_functionalSimulationMode; 
-   unsigned m_uid;
-   kernel_info_t &m_kernel;
-   core_t *m_core;
-   gpgpu_t *m_gpu;
-   bool   m_valid;
-   dim3   m_ntid;
-   dim3   m_tid;
-   dim3   m_nctaid;
-   dim3   m_ctaid;
-   unsigned m_gridid;
-   bool m_thread_done;
-   unsigned m_hw_sid;
-   unsigned m_hw_tid;
-   unsigned m_hw_wid;
-   unsigned m_hw_ctaid;
-
-   unsigned m_icount;
-   unsigned m_PC;
-   unsigned m_NPC;
-   unsigned m_RPC;
-   bool m_RPC_updated;
-   bool m_last_was_call;
-   unsigned m_cycle_done;
-
-   int m_barrier_num;
-   bool m_at_barrier;
-
-   symbol_table  *m_symbol_table;
-   function_info *m_func_info;
-
-   std::list<stack_entry> m_callstack;
-   unsigned m_local_mem_stack_pointer;
-
-   typedef tr1_hash_map<const symbol*,ptx_reg_t> reg_map_t;
-   std::list<reg_map_t> m_regs;
-   std::list<reg_map_t> m_debug_trace_regs_modified;
-   std::list<reg_map_t> m_debug_trace_regs_read;
-   bool m_enable_debug_trace;
-
-   std::stack<class operand_info, std::vector<operand_info> > m_breakaddrs;
+ public:
+  ~ptx_thread_info();
+  ptx_thread_info(kernel_info_t &kernel);
+
+  void init(gpgpu_t *gpu, core_t *core, unsigned sid, unsigned cta_id,
+            unsigned wid, unsigned tid, bool fsim) {
+    m_gpu = gpu;
+    m_core = core;
+    m_hw_sid = sid;
+    m_hw_ctaid = cta_id;
+    m_hw_wid = wid;
+    m_hw_tid = tid;
+    m_functionalSimulationMode = fsim;
+  }
+
+  int readRegister(const warp_inst_t &inst, unsigned lane_id, char *data, unsigned id = 1);
+  void writeRegister(const warp_inst_t &inst, unsigned lane_id, char *data);
+
+  void ptx_fetch_inst(inst_t &inst) const;
+  void ptx_exec_inst(warp_inst_t &inst, unsigned lane_id);
+
+  const ptx_version &get_ptx_version() const;
+  void set_reg(const symbol *reg, const ptx_reg_t &value);
+  void print_reg_thread(char *fname);
+  void resume_reg_thread(char *fname, symbol_table *symtab);
+  ptx_reg_t get_reg(const symbol *reg);
+  ptx_reg_t get_operand_value(const operand_info &op, operand_info dstInfo,
+                              unsigned opType, ptx_thread_info *thread,
+                              int derefFlag);
+  void set_operand_value(const operand_info &dst, const ptx_reg_t &data,
+                         unsigned type, ptx_thread_info *thread,
+                         const ptx_instruction *pI);
+  void set_operand_value(const operand_info &dst, const ptx_reg_t &data,
+                         unsigned type, ptx_thread_info *thread,
+                         const ptx_instruction *pI, int overflow, int carry);
+  void get_vector_operand_values(const operand_info &op, ptx_reg_t *ptx_regs,
+                                 unsigned num_elements);
+  void set_vector_operand_values(const operand_info &dst,
+                                 const ptx_reg_t &data1, const ptx_reg_t &data2,
+                                 const ptx_reg_t &data3,
+                                 const ptx_reg_t &data4);
+  void set_wmma_vector_operand_values(
+      const operand_info &dst, const ptx_reg_t &data1, const ptx_reg_t &data2,
+      const ptx_reg_t &data3, const ptx_reg_t &data4, const ptx_reg_t &data5,
+      const ptx_reg_t &data6, const ptx_reg_t &data7, const ptx_reg_t &data8);
+
+  function_info *func_info() { return m_func_info; }
+  void print_insn(unsigned pc, FILE *fp) const;
+  void set_info(function_info *func);
+  unsigned get_uid() const { return m_uid; }
+
+  dim3 get_ctaid() const { return m_ctaid; }
+  dim3 get_tid() const { return m_tid; }
+  dim3 get_ntid() const { return m_ntid; }
+  class gpgpu_sim *get_gpu() {
+    return (gpgpu_sim *)m_gpu;
+  }
+  unsigned get_hw_tid() const { return m_hw_tid; }
+  unsigned get_hw_ctaid() const { return m_hw_ctaid; }
+  unsigned get_hw_wid() const { return m_hw_wid; }
+  unsigned get_hw_sid() const { return m_hw_sid; }
+  core_t *get_core() { return m_core; }
+
+  unsigned get_icount() const { return m_icount; }
+  void set_valid() { m_valid = true; }
+  addr_t last_eaddr() const { return m_last_effective_address; }
+  memory_space_t last_space() const { return m_last_memory_space; }
+  dram_callback_t last_callback() const { return m_last_dram_callback; }
+  unsigned long long get_cta_uid() { return m_cta_info->get_sm_idx(); }
+
+  void set_single_thread_single_block() {
+    m_ntid.x = 1;
+    m_ntid.y = 1;
+    m_ntid.z = 1;
+    m_ctaid.x = 0;
+    m_ctaid.y = 0;
+    m_ctaid.z = 0;
+    m_tid.x = 0;
+    m_tid.y = 0;
+    m_tid.z = 0;
+    m_nctaid.x = 1;
+    m_nctaid.y = 1;
+    m_nctaid.z = 1;
+    m_gridid = 0;
+    m_valid = true;
+  }
+  void set_tid(dim3 tid) { m_tid = tid; }
+  void cpy_tid_to_reg(dim3 tid);
+  void set_ctaid(dim3 ctaid) { m_ctaid = ctaid; }
+  void set_ntid(dim3 tid) { m_ntid = tid; }
+  void set_nctaid(dim3 cta_size) { m_nctaid = cta_size; }
+
+  unsigned get_builtin(int builtin_id, unsigned dim_mod);
+
+  void set_done();
+  bool is_done() { return m_thread_done; }
+  unsigned donecycle() const { return m_cycle_done; }
+
+  unsigned next_instr() {
+    m_icount++;
+    m_branch_taken = false;
+    return m_PC;
+  }
+  bool branch_taken() const { return m_branch_taken; }
+  unsigned get_pc() const { return m_PC; }
+  void set_npc(unsigned npc) { m_NPC = npc; }
+  void set_npc(const function_info *f);
+  void callstack_push(unsigned npc, unsigned rpc, const symbol *return_var_src,
+                      const symbol *return_var_dst, unsigned call_uid);
+  bool callstack_pop();
+  void callstack_push_plus(unsigned npc, unsigned rpc,
+                           const symbol *return_var_src,
+                           const symbol *return_var_dst, unsigned call_uid);
+  bool callstack_pop_plus();
+  void dump_callstack() const;
+  std::string get_location() const;
+  const ptx_instruction *get_inst() const;
+  const ptx_instruction *get_inst(addr_t pc) const;
+  bool rpc_updated() const { return m_RPC_updated; }
+  bool last_was_call() const { return m_last_was_call; }
+  unsigned get_rpc() const { return m_RPC; }
+  void clearRPC() {
+    m_RPC = -1;
+    m_RPC_updated = false;
+    m_last_was_call = false;
+  }
+  unsigned get_return_PC() { return m_callstack.back().m_PC; }
+  void update_pc() { m_PC = m_NPC; }
+  void dump_regs(FILE *fp);
+  void dump_modifiedregs(FILE *fp);
+  void clear_modifiedregs() {
+    m_debug_trace_regs_modified.back().clear();
+    m_debug_trace_regs_read.back().clear();
+  }
+  function_info *get_finfo() { return m_func_info; }
+  const function_info *get_finfo() const { return m_func_info; }
+  void push_breakaddr(const operand_info &breakaddr);
+  const operand_info &pop_breakaddr();
+  void enable_debug_trace() { m_enable_debug_trace = true; }
+  unsigned get_local_mem_stack_pointer() const {
+    return m_local_mem_stack_pointer;
+  }
+
+  memory_space *get_global_memory() { return m_gpu->get_global_memory(); }
+  memory_space *get_tex_memory() { return m_gpu->get_tex_memory(); }
+  memory_space *get_surf_memory() { return m_gpu->get_surf_memory(); }
+  memory_space *get_param_memory() { return m_kernel.get_param_memory(); }
+  const gpgpu_functional_sim_config &get_config() const {
+    return m_gpu->get_config();
+  }
+  bool isInFunctionalSimulationMode() { return m_functionalSimulationMode; }
+  void exitCore() {
+    // m_core is not used in case of functional simulation mode
+    if (!m_functionalSimulationMode) m_core->warp_exit(m_hw_wid);
+  }
+
+  void registerExit() { m_cta_info->register_thread_exit(this); }
+  unsigned get_reduction_value(unsigned ctaid, unsigned barid) {
+    return m_core->get_reduction_value(ctaid, barid);
+  }
+  void and_reduction(unsigned ctaid, unsigned barid, bool value) {
+    m_core->and_reduction(ctaid, barid, value);
+  }
+  void or_reduction(unsigned ctaid, unsigned barid, bool value) {
+    m_core->or_reduction(ctaid, barid, value);
+  }
+  void popc_reduction(unsigned ctaid, unsigned barid, bool value) {
+    m_core->popc_reduction(ctaid, barid, value);
+  }
+
+  // Jin: get corresponding kernel grid for CDP purpose
+  kernel_info_t &get_kernel() { return m_kernel; }
+
+ public:
+  addr_t m_last_effective_address;
+  bool m_branch_taken;
+  memory_space_t m_last_memory_space;
+  dram_callback_t m_last_dram_callback;
+  memory_space *m_shared_mem;
+  memory_space *m_sstarr_mem;
+  memory_space *m_local_mem;
+  ptx_warp_info *m_warp_info;
+  ptx_cta_info *m_cta_info;
+  ptx_reg_t m_last_set_operand_value;
+
+ private:
+  bool m_functionalSimulationMode;
+  unsigned m_uid;
+  kernel_info_t &m_kernel;
+  core_t *m_core;
+  gpgpu_t *m_gpu;
+  bool m_valid;
+  dim3 m_ntid;
+  dim3 m_tid;
+  dim3 m_nctaid;
+  dim3 m_ctaid;
+  unsigned m_gridid;
+  bool m_thread_done;
+  unsigned m_hw_sid;
+  unsigned m_hw_tid;
+  unsigned m_hw_wid;
+  unsigned m_hw_ctaid;
+
+  unsigned m_icount;
+  unsigned m_PC;
+  unsigned m_NPC;
+  unsigned m_RPC;
+  bool m_RPC_updated;
+  bool m_last_was_call;
+  unsigned m_cycle_done;
+
+  int m_barrier_num;
+  bool m_at_barrier;
+
+  symbol_table *m_symbol_table;
+  function_info *m_func_info;
+
+  std::list<stack_entry> m_callstack;
+  unsigned m_local_mem_stack_pointer;
+
+  typedef tr1_hash_map<const symbol *, ptx_reg_t> reg_map_t;
+  std::list<reg_map_t> m_regs;
+  std::list<reg_map_t> m_debug_trace_regs_modified;
+  std::list<reg_map_t> m_debug_trace_regs_read;
+  bool m_enable_debug_trace;
+
+  std::stack<class operand_info, std::vector<operand_info> > m_breakaddrs;
 };
 
-addr_t generic_to_local( unsigned smid, unsigned hwtid, addr_t addr );
-addr_t generic_to_shared( unsigned smid, addr_t addr );
-addr_t generic_to_global( addr_t addr );
-addr_t local_to_generic( unsigned smid, unsigned hwtid, addr_t addr );
-addr_t shared_to_generic( unsigned smid, addr_t addr );
-addr_t global_to_generic( addr_t addr );
-bool isspace_local( unsigned smid, unsigned hwtid, addr_t addr );
-bool isspace_shared( unsigned smid, addr_t addr );
-bool isspace_global( addr_t addr );
-memory_space_t whichspace( addr_t addr );
+addr_t generic_to_local(unsigned smid, unsigned hwtid, addr_t addr);
+addr_t generic_to_shared(unsigned smid, addr_t addr);
+addr_t generic_to_global(addr_t addr);
+addr_t local_to_generic(unsigned smid, unsigned hwtid, addr_t addr);
+addr_t shared_to_generic(unsigned smid, addr_t addr);
+addr_t global_to_generic(addr_t addr);
+bool isspace_local(unsigned smid, unsigned hwtid, addr_t addr);
+bool isspace_shared(unsigned smid, addr_t addr);
+bool isspace_global(addr_t addr);
+memory_space_t whichspace(addr_t addr);
 
 extern unsigned g_ptx_thread_info_uid_next;
 
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.l b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.l
index 33c2748a15..a7f91edac6 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.l
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.l
@@ -31,17 +31,23 @@ OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 %option noyywrap
 %option yylineno
 %option prefix="ptxinfo_"
+
+%option bison-bridge
+%option reentrant
+
 %{
+#include "ptx_loader.h"
 #include "ptxinfo.tab.h"
 #include <string.h>
+#include "../../libcuda_sim/gpgpu_context.h"
 
 #define LINEBUF_SIZE 1024
-char ptxinfo_linebuf[LINEBUF_SIZE];
-unsigned ptxinfo_col = 0;
-#define TC if( (ptxinfo_lineno == 1) && ((ptxinfo_col + strlen(ptxinfo_text)) < LINEBUF_SIZE) ) { \
-		strncpy(ptxinfo_linebuf+ptxinfo_col,ptxinfo_text,strlen(ptxinfo_text)); \
+#define TC if( (yylineno == 1) && (ptxinfo->col + strlen(yytext) < LINEBUF_SIZE) ) { \
+		strncpy(ptxinfo->linebuf+ptxinfo->col,yytext,strlen(yytext)); \
 	   } \
-	   ptxinfo_col+=strlen(ptxinfo_text); 
+	   ptxinfo->col+=strlen(yytext);
+#define YY_DECL int ptxinfo_lex \
+	       (YYSTYPE * yylval_param , yyscan_t yyscanner, ptxinfo_data* ptxinfo)
 %}
 
 %%
@@ -61,12 +67,12 @@ unsigned ptxinfo_col = 0;
 "for"		TC; return FOR;
 "textures"	TC; return TEXTURES;
 "error   : Duplicate definition of"	TC; return DUPLICATE;
-"function"	TC; ptxinfo_lval.string_value = strdup(yytext); return FUNCTION;
-"variable"	TC; ptxinfo_lval.string_value = strdup(yytext); return VARIABLE;
-"fatal   : Ptx assembly aborted due to errors"	TC; return FATAL;
+"function"	TC; yylval->string_value = strdup(yytext); return FUNCTION;
+"variable"	TC; yylval->string_value = strdup(yytext); return VARIABLE;
+"fatal   : Ptx assembly aborted due to errors"	TC; return FATAL_GPGPU;
 
-[_A-Za-z$%][_0-9A-Za-z$]*  TC; ptxinfo_lval.string_value = strdup(yytext); return IDENTIFIER;
-[-]{0,1}[0-9]+	 TC; ptxinfo_lval.int_value =  atoi(yytext); return INT_OPERAND;
+[_A-Za-z$%][_0-9A-Za-z$]*  TC; yylval->string_value = strdup(yytext); return IDENTIFIER;
+[-]{0,1}[0-9]+	 TC; yylval->int_value =  atoi(yytext); return INT_OPERAND;
 
 "+"	TC; return PLUS;
 ","     TC; return COMMA;
@@ -78,26 +84,23 @@ unsigned ptxinfo_col = 0;
 " " TC;
 "\t" TC;
 
-\n.*  ptxinfo_col=0; strncpy(ptxinfo_linebuf, yytext + 1, 1024); yyless( 1 );
+\n.*  ptxinfo->col=0; strncpy(ptxinfo->linebuf, yytext + 1, 1024); yyless( 1 );
 
 %%
 
-extern int g_ptxinfo_error_detected;
-extern const char *g_filename;
-extern const char *g_ptxinfo_filename;
-
-int ptxinfo_error( const char *s )
+int ptxinfo_error(yyscan_t yyscanner, ptxinfo_data* ptxinfo, const char* msg)
 {
+    struct yyguts_t * yyg = (struct yyguts_t*)yyscanner;
 	int i;
-	g_ptxinfo_error_detected = 1;
+	ptxinfo->gpgpu_ctx->func_sim->g_ptxinfo_error_detected = 1;
 	fflush(stdout);
 	printf("GPGPU-Sim: ERROR while parsing output of ptxas (used to capture resource usage information)\n");
-	if( s != NULL )
-		printf("GPGPU-Sim:     %s (%s:%u) Syntax error:\n\n", g_filename, g_ptxinfo_filename, ptxinfo_lineno );
-	printf("   %s\n", ptxinfo_linebuf );
+	if( msg != NULL )
+		printf("GPGPU-Sim:     %s (%s:%u) Syntax error:\n\n", ptxinfo->gpgpu_ctx->g_filename, ptxinfo->g_ptxinfo_filename, yylineno );
+	printf("   %s\n", ptxinfo->linebuf );
 	printf("   ");
-	for( i=0; i < ptxinfo_col-1; i++ ) {
-		if( ptxinfo_linebuf[i] == '\t' ) printf("\t");
+	for( i=0; i < ptxinfo->col-1; i++ ) {
+		if( ptxinfo->linebuf[i] == '\t' ) printf("\t");
 		else printf(" ");
 	}
 			
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.c b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.c
index 4795290399..89a4a89600 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.c
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.c
@@ -1,8 +1,9 @@
-/* A Bison parser, made by GNU Bison 3.0.4.  */
+/* A Bison parser, made by GNU Bison 3.5.1.  */
 
 /* Bison implementation for Yacc-like parsers in C
 
-   Copyright (C) 1984, 1989-1990, 2000-2015 Free Software Foundation, Inc.
+   Copyright (C) 1984, 1989-1990, 2000-2015, 2018-2020 Free Software Foundation,
+   Inc.
 
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
@@ -40,17 +41,20 @@
    define necessary library symbols; they are noted "INFRINGES ON
    USER NAME SPACE" below.  */
 
+/* Undocumented macros, especially those whose name start with YY_,
+   are private implementation details.  Do not rely on them.  */
+
 /* Identify Bison output.  */
 #define YYBISON 1
 
 /* Bison version.  */
-#define YYBISON_VERSION "3.0.4"
+#define YYBISON_VERSION "3.5.1"
 
 /* Skeleton name.  */
 #define YYSKELETON_NAME "yacc.c"
 
 /* Pure parsers.  */
-#define YYPURE 0
+#define YYPURE 2
 
 /* Push parsers.  */
 #define YYPUSH 0
@@ -66,18 +70,32 @@
 #define yydebug         ptxinfo_debug
 #define yynerrs         ptxinfo_nerrs
 
-#define yylval          ptxinfo_lval
-#define yychar          ptxinfo_char
+/* First part of user prologue.  */
+#line 30 "ptxinfo.y"
 
-/* Copy the first part of user declarations.  */
+typedef void * yyscan_t;
+#include "ptx_loader.h"
 
-#line 75 "ptxinfo.tab.c" /* yacc.c:339  */
+#line 80 "ptxinfo.tab.c"
 
+# ifndef YY_CAST
+#  ifdef __cplusplus
+#   define YY_CAST(Type, Val) static_cast<Type> (Val)
+#   define YY_REINTERPRET_CAST(Type, Val) reinterpret_cast<Type> (Val)
+#  else
+#   define YY_CAST(Type, Val) ((Type) (Val))
+#   define YY_REINTERPRET_CAST(Type, Val) ((Type) (Val))
+#  endif
+# endif
 # ifndef YY_NULLPTR
-#  if defined __cplusplus && 201103L <= __cplusplus
-#   define YY_NULLPTR nullptr
+#  if defined __cplusplus
+#   if 201103L <= __cplusplus
+#    define YY_NULLPTR nullptr
+#   else
+#    define YY_NULLPTR 0
+#   endif
 #  else
-#   define YY_NULLPTR 0
+#   define YY_NULLPTR ((void*)0)
 #  endif
 # endif
 
@@ -89,10 +107,10 @@
 # define YYERROR_VERBOSE 0
 #endif
 
-/* In a future release of Bison, this section will be replaced
-   by #include "ptxinfo.tab.h".  */
-#ifndef YY_PTXINFO_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTXINFO_TAB_H_INCLUDED
-# define YY_PTXINFO_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTXINFO_TAB_H_INCLUDED
+/* Use api.header.include to #include this header
+   instead of duplicating it here.  */
+#ifndef YY_PTXINFO_PTXINFO_TAB_H_INCLUDED
+# define YY_PTXINFO_PTXINFO_TAB_H_INCLUDED
 /* Debug traces.  */
 #ifndef YYDEBUG
 # define YYDEBUG 0
@@ -108,7 +126,7 @@ extern int ptxinfo_debug;
   {
     INT_OPERAND = 258,
     HEADER = 259,
-    INFO = 260,
+    INFO_GPGPU = 260,
     FUNC = 261,
     USED = 262,
     REGS = 263,
@@ -132,83 +150,156 @@ extern int ptxinfo_debug;
     DUPLICATE = 281,
     FUNCTION = 282,
     VARIABLE = 283,
-    FATAL = 284
+    FATAL_GPGPU = 284
   };
 #endif
+/* Tokens.  */
+#define INT_OPERAND 258
+#define HEADER 259
+#define INFO_GPGPU 260
+#define FUNC 261
+#define USED 262
+#define REGS 263
+#define BYTES 264
+#define LMEM 265
+#define SMEM 266
+#define CMEM 267
+#define GMEM 268
+#define IDENTIFIER 269
+#define PLUS 270
+#define COMMA 271
+#define LEFT_SQUARE_BRACKET 272
+#define RIGHT_SQUARE_BRACKET 273
+#define COLON 274
+#define SEMICOLON 275
+#define QUOTE 276
+#define LINE 277
+#define WARNING 278
+#define FOR 279
+#define TEXTURES 280
+#define DUPLICATE 281
+#define FUNCTION 282
+#define VARIABLE 283
+#define FATAL_GPGPU 284
 
 /* Value type.  */
 #if ! defined YYSTYPE && ! defined YYSTYPE_IS_DECLARED
-
 union YYSTYPE
 {
-#line 30 "ptxinfo.y" /* yacc.c:355  */
+#line 41 "ptxinfo.y"
 
   int    int_value;
   char * string_value;
 
-#line 150 "ptxinfo.tab.c" /* yacc.c:355  */
-};
+#line 195 "ptxinfo.tab.c"
 
+};
 typedef union YYSTYPE YYSTYPE;
 # define YYSTYPE_IS_TRIVIAL 1
 # define YYSTYPE_IS_DECLARED 1
 #endif
 
 
-extern YYSTYPE ptxinfo_lval;
 
-int ptxinfo_parse (void);
+int ptxinfo_parse (yyscan_t scanner, ptxinfo_data* ptxinfo);
 
-#endif /* !YY_PTXINFO_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTXINFO_TAB_H_INCLUDED  */
+#endif /* !YY_PTXINFO_PTXINFO_TAB_H_INCLUDED  */
 
-/* Copy the second part of user declarations.  */
-#line 63 "ptxinfo.y" /* yacc.c:358  */
+/* Second part of user prologue.  */
+#line 74 "ptxinfo.y"
 
 	#include <stdlib.h>
 	#include <string.h>
 	
 	static unsigned g_declared;
 	static unsigned g_system;
-	int ptxinfo_lex(void);
-	void ptxinfo_addinfo();
+	int ptxinfo_lex(YYSTYPE * yylval_param, yyscan_t yyscanner, ptxinfo_data* ptxinfo);
+	void yyerror(yyscan_t yyscanner, ptxinfo_data* ptxinfo, const char* msg);
 	void ptxinfo_function(const char *fname );
 	void ptxinfo_regs( unsigned nregs );
 	void ptxinfo_lmem( unsigned declared, unsigned system );
 	void ptxinfo_gmem( unsigned declared, unsigned system );
 	void ptxinfo_smem( unsigned declared, unsigned system );
 	void ptxinfo_cmem( unsigned nbytes, unsigned bank );
-	int ptxinfo_error(const char*);
 	void ptxinfo_linenum( unsigned );
 	void ptxinfo_dup_type( const char* );
 
-#line 185 "ptxinfo.tab.c" /* yacc.c:358  */
+#line 228 "ptxinfo.tab.c"
+
 
 #ifdef short
 # undef short
 #endif
 
-#ifdef YYTYPE_UINT8
-typedef YYTYPE_UINT8 yytype_uint8;
-#else
-typedef unsigned char yytype_uint8;
+/* On compilers that do not define __PTRDIFF_MAX__ etc., make sure
+   <limits.h> and (if available) <stdint.h> are included
+   so that the code can choose integer types of a good width.  */
+
+#ifndef __PTRDIFF_MAX__
+# include <limits.h> /* INFRINGES ON USER NAME SPACE */
+# if defined __STDC_VERSION__ && 199901 <= __STDC_VERSION__
+#  include <stdint.h> /* INFRINGES ON USER NAME SPACE */
+#  define YY_STDINT_H
+# endif
 #endif
 
-#ifdef YYTYPE_INT8
-typedef YYTYPE_INT8 yytype_int8;
+/* Narrow types that promote to a signed type and that can represent a
+   signed or unsigned integer of at least N bits.  In tables they can
+   save space and decrease cache pressure.  Promoting to a signed type
+   helps avoid bugs in integer arithmetic.  */
+
+#ifdef __INT_LEAST8_MAX__
+typedef __INT_LEAST8_TYPE__ yytype_int8;
+#elif defined YY_STDINT_H
+typedef int_least8_t yytype_int8;
 #else
 typedef signed char yytype_int8;
 #endif
 
-#ifdef YYTYPE_UINT16
-typedef YYTYPE_UINT16 yytype_uint16;
+#ifdef __INT_LEAST16_MAX__
+typedef __INT_LEAST16_TYPE__ yytype_int16;
+#elif defined YY_STDINT_H
+typedef int_least16_t yytype_int16;
 #else
-typedef unsigned short int yytype_uint16;
+typedef short yytype_int16;
 #endif
 
-#ifdef YYTYPE_INT16
-typedef YYTYPE_INT16 yytype_int16;
+#if defined __UINT_LEAST8_MAX__ && __UINT_LEAST8_MAX__ <= __INT_MAX__
+typedef __UINT_LEAST8_TYPE__ yytype_uint8;
+#elif (!defined __UINT_LEAST8_MAX__ && defined YY_STDINT_H \
+       && UINT_LEAST8_MAX <= INT_MAX)
+typedef uint_least8_t yytype_uint8;
+#elif !defined __UINT_LEAST8_MAX__ && UCHAR_MAX <= INT_MAX
+typedef unsigned char yytype_uint8;
 #else
-typedef short int yytype_int16;
+typedef short yytype_uint8;
+#endif
+
+#if defined __UINT_LEAST16_MAX__ && __UINT_LEAST16_MAX__ <= __INT_MAX__
+typedef __UINT_LEAST16_TYPE__ yytype_uint16;
+#elif (!defined __UINT_LEAST16_MAX__ && defined YY_STDINT_H \
+       && UINT_LEAST16_MAX <= INT_MAX)
+typedef uint_least16_t yytype_uint16;
+#elif !defined __UINT_LEAST16_MAX__ && USHRT_MAX <= INT_MAX
+typedef unsigned short yytype_uint16;
+#else
+typedef int yytype_uint16;
+#endif
+
+#ifndef YYPTRDIFF_T
+# if defined __PTRDIFF_TYPE__ && defined __PTRDIFF_MAX__
+#  define YYPTRDIFF_T __PTRDIFF_TYPE__
+#  define YYPTRDIFF_MAXIMUM __PTRDIFF_MAX__
+# elif defined PTRDIFF_MAX
+#  ifndef ptrdiff_t
+#   include <stddef.h> /* INFRINGES ON USER NAME SPACE */
+#  endif
+#  define YYPTRDIFF_T ptrdiff_t
+#  define YYPTRDIFF_MAXIMUM PTRDIFF_MAX
+# else
+#  define YYPTRDIFF_T long
+#  define YYPTRDIFF_MAXIMUM LONG_MAX
+# endif
 #endif
 
 #ifndef YYSIZE_T
@@ -216,15 +307,27 @@ typedef short int yytype_int16;
 #  define YYSIZE_T __SIZE_TYPE__
 # elif defined size_t
 #  define YYSIZE_T size_t
-# elif ! defined YYSIZE_T
+# elif defined __STDC_VERSION__ && 199901 <= __STDC_VERSION__
 #  include <stddef.h> /* INFRINGES ON USER NAME SPACE */
 #  define YYSIZE_T size_t
 # else
-#  define YYSIZE_T unsigned int
+#  define YYSIZE_T unsigned
 # endif
 #endif
 
-#define YYSIZE_MAXIMUM ((YYSIZE_T) -1)
+#define YYSIZE_MAXIMUM                                  \
+  YY_CAST (YYPTRDIFF_T,                                 \
+           (YYPTRDIFF_MAXIMUM < YY_CAST (YYSIZE_T, -1)  \
+            ? YYPTRDIFF_MAXIMUM                         \
+            : YY_CAST (YYSIZE_T, -1)))
+
+#define YYSIZEOF(X) YY_CAST (YYPTRDIFF_T, sizeof (X))
+
+/* Stored state numbers (used for stacks). */
+typedef yytype_int8 yy_state_t;
+
+/* State numbers in computations.  */
+typedef int yy_state_fast_t;
 
 #ifndef YY_
 # if defined YYENABLE_NLS && YYENABLE_NLS
@@ -238,30 +341,19 @@ typedef short int yytype_int16;
 # endif
 #endif
 
-#ifndef YY_ATTRIBUTE
-# if (defined __GNUC__                                               \
-      && (2 < __GNUC__ || (__GNUC__ == 2 && 96 <= __GNUC_MINOR__)))  \
-     || defined __SUNPRO_C && 0x5110 <= __SUNPRO_C
-#  define YY_ATTRIBUTE(Spec) __attribute__(Spec)
+#ifndef YY_ATTRIBUTE_PURE
+# if defined __GNUC__ && 2 < __GNUC__ + (96 <= __GNUC_MINOR__)
+#  define YY_ATTRIBUTE_PURE __attribute__ ((__pure__))
 # else
-#  define YY_ATTRIBUTE(Spec) /* empty */
+#  define YY_ATTRIBUTE_PURE
 # endif
 #endif
 
-#ifndef YY_ATTRIBUTE_PURE
-# define YY_ATTRIBUTE_PURE   YY_ATTRIBUTE ((__pure__))
-#endif
-
 #ifndef YY_ATTRIBUTE_UNUSED
-# define YY_ATTRIBUTE_UNUSED YY_ATTRIBUTE ((__unused__))
-#endif
-
-#if !defined _Noreturn \
-     && (!defined __STDC_VERSION__ || __STDC_VERSION__ < 201112)
-# if defined _MSC_VER && 1200 <= _MSC_VER
-#  define _Noreturn __declspec (noreturn)
+# if defined __GNUC__ && 2 < __GNUC__ + (7 <= __GNUC_MINOR__)
+#  define YY_ATTRIBUTE_UNUSED __attribute__ ((__unused__))
 # else
-#  define _Noreturn YY_ATTRIBUTE ((__noreturn__))
+#  define YY_ATTRIBUTE_UNUSED
 # endif
 #endif
 
@@ -272,13 +364,13 @@ typedef short int yytype_int16;
 # define YYUSE(E) /* empty */
 #endif
 
-#if defined __GNUC__ && 407 <= __GNUC__ * 100 + __GNUC_MINOR__
+#if defined __GNUC__ && ! defined __ICC && 407 <= __GNUC__ * 100 + __GNUC_MINOR__
 /* Suppress an incorrect diagnostic about yylval being uninitialized.  */
-# define YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN \
-    _Pragma ("GCC diagnostic push") \
-    _Pragma ("GCC diagnostic ignored \"-Wuninitialized\"")\
+# define YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN                            \
+    _Pragma ("GCC diagnostic push")                                     \
+    _Pragma ("GCC diagnostic ignored \"-Wuninitialized\"")              \
     _Pragma ("GCC diagnostic ignored \"-Wmaybe-uninitialized\"")
-# define YY_IGNORE_MAYBE_UNINITIALIZED_END \
+# define YY_IGNORE_MAYBE_UNINITIALIZED_END      \
     _Pragma ("GCC diagnostic pop")
 #else
 # define YY_INITIAL_VALUE(Value) Value
@@ -291,6 +383,20 @@ typedef short int yytype_int16;
 # define YY_INITIAL_VALUE(Value) /* Nothing. */
 #endif
 
+#if defined __cplusplus && defined __GNUC__ && ! defined __ICC && 6 <= __GNUC__
+# define YY_IGNORE_USELESS_CAST_BEGIN                          \
+    _Pragma ("GCC diagnostic push")                            \
+    _Pragma ("GCC diagnostic ignored \"-Wuseless-cast\"")
+# define YY_IGNORE_USELESS_CAST_END            \
+    _Pragma ("GCC diagnostic pop")
+#endif
+#ifndef YY_IGNORE_USELESS_CAST_BEGIN
+# define YY_IGNORE_USELESS_CAST_BEGIN
+# define YY_IGNORE_USELESS_CAST_END
+#endif
+
+
+#define YY_ASSERT(E) ((void) (0 && (E)))
 
 #if ! defined yyoverflow || YYERROR_VERBOSE
 
@@ -367,17 +473,17 @@ void free (void *); /* INFRINGES ON USER NAME SPACE */
 /* A type that is properly aligned for any stack member.  */
 union yyalloc
 {
-  yytype_int16 yyss_alloc;
+  yy_state_t yyss_alloc;
   YYSTYPE yyvs_alloc;
 };
 
 /* The size of the maximum gap between one aligned stack and the next.  */
-# define YYSTACK_GAP_MAXIMUM (sizeof (union yyalloc) - 1)
+# define YYSTACK_GAP_MAXIMUM (YYSIZEOF (union yyalloc) - 1)
 
 /* The size of an array large to enough to hold all stacks, each with
    N elements.  */
 # define YYSTACK_BYTES(N) \
-     ((N) * (sizeof (yytype_int16) + sizeof (YYSTYPE)) \
+     ((N) * (YYSIZEOF (yy_state_t) + YYSIZEOF (YYSTYPE)) \
       + YYSTACK_GAP_MAXIMUM)
 
 # define YYCOPY_NEEDED 1
@@ -390,11 +496,11 @@ union yyalloc
 # define YYSTACK_RELOCATE(Stack_alloc, Stack)                           \
     do                                                                  \
       {                                                                 \
-        YYSIZE_T yynewbytes;                                            \
+        YYPTRDIFF_T yynewbytes;                                         \
         YYCOPY (&yyptr->Stack_alloc, Stack, yysize);                    \
         Stack = &yyptr->Stack_alloc;                                    \
-        yynewbytes = yystacksize * sizeof (*Stack) + YYSTACK_GAP_MAXIMUM; \
-        yyptr += yynewbytes / sizeof (*yyptr);                          \
+        yynewbytes = yystacksize * YYSIZEOF (*Stack) + YYSTACK_GAP_MAXIMUM; \
+        yyptr += yynewbytes / YYSIZEOF (*yyptr);                        \
       }                                                                 \
     while (0)
 
@@ -406,12 +512,12 @@ union yyalloc
 # ifndef YYCOPY
 #  if defined __GNUC__ && 1 < __GNUC__
 #   define YYCOPY(Dst, Src, Count) \
-      __builtin_memcpy (Dst, Src, (Count) * sizeof (*(Src)))
+      __builtin_memcpy (Dst, Src, YY_CAST (YYSIZE_T, (Count)) * sizeof (*(Src)))
 #  else
 #   define YYCOPY(Dst, Src, Count)              \
       do                                        \
         {                                       \
-          YYSIZE_T yyi;                         \
+          YYPTRDIFF_T yyi;                      \
           for (yyi = 0; yyi < (Count); yyi++)   \
             (Dst)[yyi] = (Src)[yyi];            \
         }                                       \
@@ -434,17 +540,18 @@ union yyalloc
 /* YYNSTATES -- Number of states.  */
 #define YYNSTATES  63
 
-/* YYTRANSLATE[YYX] -- Symbol number corresponding to YYX as returned
-   by yylex, with out-of-bounds checking.  */
 #define YYUNDEFTOK  2
 #define YYMAXUTOK   284
 
+
+/* YYTRANSLATE(TOKEN-NUM) -- Symbol number corresponding to TOKEN-NUM
+   as returned by yylex, with out-of-bounds checking.  */
 #define YYTRANSLATE(YYX)                                                \
-  ((unsigned int) (YYX) <= YYMAXUTOK ? yytranslate[YYX] : YYUNDEFTOK)
+  (0 <= (YYX) && (YYX) <= YYMAXUTOK ? yytranslate[YYX] : YYUNDEFTOK)
 
 /* YYTRANSLATE[TOKEN-NUM] -- Symbol number corresponding to TOKEN-NUM
-   as returned by yylex, without out-of-bounds checking.  */
-static const yytype_uint8 yytranslate[] =
+   as returned by yylex.  */
+static const yytype_int8 yytranslate[] =
 {
        0,     2,     2,     2,     2,     2,     2,     2,     2,     2,
        2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
@@ -481,9 +588,9 @@ static const yytype_uint8 yytranslate[] =
   /* YYRLINE[YYN] -- Source line where rule number YYN was defined.  */
 static const yytype_uint8 yyrline[] =
 {
-       0,    84,    84,    85,    88,    89,    90,    91,    92,    95,
-      96,    97,   100,   101,   104,   105,   108,   111,   112,   113,
-     114,   115,   116,   117,   118,   119,   120,   123,   125,   126
+       0,    94,    94,    95,    98,    99,   100,   101,   102,   105,
+     106,   107,   110,   111,   114,   115,   118,   121,   122,   123,
+     124,   125,   126,   127,   128,   129,   130,   133,   135,   136
 };
 #endif
 
@@ -492,11 +599,11 @@ static const yytype_uint8 yyrline[] =
    First, the terminals, then, starting at YYNTOKENS, nonterminals.  */
 static const char *const yytname[] =
 {
-  "$end", "error", "$undefined", "INT_OPERAND", "HEADER", "INFO", "FUNC",
+  "$end", "error", "$undefined", "INT_OPERAND", "HEADER", "INFO_GPGPU", "FUNC",
   "USED", "REGS", "BYTES", "LMEM", "SMEM", "CMEM", "GMEM", "IDENTIFIER",
   "PLUS", "COMMA", "LEFT_SQUARE_BRACKET", "RIGHT_SQUARE_BRACKET", "COLON",
   "SEMICOLON", "QUOTE", "LINE", "WARNING", "FOR", "TEXTURES", "DUPLICATE",
-  "FUNCTION", "VARIABLE", "FATAL", "$accept", "input", "line", "line_info",
+  "FUNCTION", "VARIABLE", "FATAL_GPGPU", "$accept", "input", "line", "line_info",
   "function_name", "function_info", "gmem_info", "info", "tuple",
   "duplicate", YY_NULLPTR
 };
@@ -505,7 +612,7 @@ static const char *const yytname[] =
 # ifdef YYPRINT
 /* YYTOKNUM[NUM] -- (External) token number corresponding to the
    (internal) symbol number NUM (which must be that of a token).  */
-static const yytype_uint16 yytoknum[] =
+static const yytype_int16 yytoknum[] =
 {
        0,   256,   257,   258,   259,   260,   261,   262,   263,   264,
      265,   266,   267,   268,   269,   270,   271,   272,   273,   274,
@@ -513,14 +620,14 @@ static const yytype_uint16 yytoknum[] =
 };
 # endif
 
-#define YYPACT_NINF -17
+#define YYPACT_NINF (-17)
 
-#define yypact_value_is_default(Yystate) \
-  (!!((Yystate) == (-17)))
+#define yypact_value_is_default(Yyn) \
+  ((Yyn) == YYPACT_NINF)
 
-#define YYTABLE_NINF -22
+#define YYTABLE_NINF (-22)
 
-#define yytable_value_is_error(Yytable_value) \
+#define yytable_value_is_error(Yyn) \
   0
 
   /* YYPACT[STATE-NUM] -- Index in YYTABLE of the portion describing
@@ -539,7 +646,7 @@ static const yytype_int8 yypact[] =
   /* YYDEFACT[STATE-NUM] -- Default reduction number in state STATE-NUM.
      Performed when YYTABLE does not specify something else to do.  Zero
      means the default is an error.  */
-static const yytype_uint8 yydefact[] =
+static const yytype_int8 yydefact[] =
 {
        2,     0,     1,     0,     3,     0,     0,     6,     8,     0,
        0,     0,     0,     0,     4,     9,    10,    11,    14,     0,
@@ -591,7 +698,7 @@ static const yytype_int8 yycheck[] =
 
   /* YYSTOS[STATE-NUM] -- The (internal number of the) accessing
      symbol of state STATE-NUM.  */
-static const yytype_uint8 yystos[] =
+static const yytype_int8 yystos[] =
 {
        0,    31,     0,     4,    32,     5,    14,    23,    29,    19,
       16,     3,     6,     7,    33,    34,    35,    36,    37,    38,
@@ -603,7 +710,7 @@ static const yytype_uint8 yystos[] =
 };
 
   /* YYR1[YYN] -- Symbol number of symbol that rule YYN derives.  */
-static const yytype_uint8 yyr1[] =
+static const yytype_int8 yyr1[] =
 {
        0,    30,    31,    31,    32,    32,    32,    32,    32,    33,
       33,    33,    34,    34,    35,    35,    36,    37,    37,    37,
@@ -611,7 +718,7 @@ static const yytype_uint8 yyr1[] =
 };
 
   /* YYR2[YYN] -- Number of symbols on the right hand side of rule YYN.  */
-static const yytype_uint8 yyr2[] =
+static const yytype_int8 yyr2[] =
 {
        0,     2,     0,     2,     4,     7,     2,     8,     2,     1,
        1,     1,     4,     8,     1,     3,     3,     3,     2,     2,
@@ -631,22 +738,22 @@ static const yytype_uint8 yyr2[] =
 
 #define YYRECOVERING()  (!!yyerrstatus)
 
-#define YYBACKUP(Token, Value)                                  \
-do                                                              \
-  if (yychar == YYEMPTY)                                        \
-    {                                                           \
-      yychar = (Token);                                         \
-      yylval = (Value);                                         \
-      YYPOPSTACK (yylen);                                       \
-      yystate = *yyssp;                                         \
-      goto yybackup;                                            \
-    }                                                           \
-  else                                                          \
-    {                                                           \
-      yyerror (YY_("syntax error: cannot back up")); \
-      YYERROR;                                                  \
-    }                                                           \
-while (0)
+#define YYBACKUP(Token, Value)                                    \
+  do                                                              \
+    if (yychar == YYEMPTY)                                        \
+      {                                                           \
+        yychar = (Token);                                         \
+        yylval = (Value);                                         \
+        YYPOPSTACK (yylen);                                       \
+        yystate = *yyssp;                                         \
+        goto yybackup;                                            \
+      }                                                           \
+    else                                                          \
+      {                                                           \
+        yyerror (scanner, ptxinfo, YY_("syntax error: cannot back up")); \
+        YYERROR;                                                  \
+      }                                                           \
+  while (0)
 
 /* Error token number */
 #define YYTERROR        1
@@ -680,43 +787,47 @@ do {                                                                      \
     {                                                                     \
       YYFPRINTF (stderr, "%s ", Title);                                   \
       yy_symbol_print (stderr,                                            \
-                  Type, Value); \
+                  Type, Value, scanner, ptxinfo); \
       YYFPRINTF (stderr, "\n");                                           \
     }                                                                     \
 } while (0)
 
 
-/*----------------------------------------.
-| Print this symbol's value on YYOUTPUT.  |
-`----------------------------------------*/
+/*-----------------------------------.
+| Print this symbol's value on YYO.  |
+`-----------------------------------*/
 
 static void
-yy_symbol_value_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
+yy_symbol_value_print (FILE *yyo, int yytype, YYSTYPE const * const yyvaluep, yyscan_t scanner, ptxinfo_data* ptxinfo)
 {
-  FILE *yyo = yyoutput;
-  YYUSE (yyo);
+  FILE *yyoutput = yyo;
+  YYUSE (yyoutput);
+  YYUSE (scanner);
+  YYUSE (ptxinfo);
   if (!yyvaluep)
     return;
 # ifdef YYPRINT
   if (yytype < YYNTOKENS)
-    YYPRINT (yyoutput, yytoknum[yytype], *yyvaluep);
+    YYPRINT (yyo, yytoknum[yytype], *yyvaluep);
 # endif
+  YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN
   YYUSE (yytype);
+  YY_IGNORE_MAYBE_UNINITIALIZED_END
 }
 
 
-/*--------------------------------.
-| Print this symbol on YYOUTPUT.  |
-`--------------------------------*/
+/*---------------------------.
+| Print this symbol on YYO.  |
+`---------------------------*/
 
 static void
-yy_symbol_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
+yy_symbol_print (FILE *yyo, int yytype, YYSTYPE const * const yyvaluep, yyscan_t scanner, ptxinfo_data* ptxinfo)
 {
-  YYFPRINTF (yyoutput, "%s %s (",
+  YYFPRINTF (yyo, "%s %s (",
              yytype < YYNTOKENS ? "token" : "nterm", yytname[yytype]);
 
-  yy_symbol_value_print (yyoutput, yytype, yyvaluep);
-  YYFPRINTF (yyoutput, ")");
+  yy_symbol_value_print (yyo, yytype, yyvaluep, scanner, ptxinfo);
+  YYFPRINTF (yyo, ")");
 }
 
 /*------------------------------------------------------------------.
@@ -725,7 +836,7 @@ yy_symbol_print (FILE *yyoutput, int yytype, YYSTYPE const * const yyvaluep)
 `------------------------------------------------------------------*/
 
 static void
-yy_stack_print (yytype_int16 *yybottom, yytype_int16 *yytop)
+yy_stack_print (yy_state_t *yybottom, yy_state_t *yytop)
 {
   YYFPRINTF (stderr, "Stack now");
   for (; yybottom <= yytop; yybottom++)
@@ -748,21 +859,21 @@ do {                                                            \
 `------------------------------------------------*/
 
 static void
-yy_reduce_print (yytype_int16 *yyssp, YYSTYPE *yyvsp, int yyrule)
+yy_reduce_print (yy_state_t *yyssp, YYSTYPE *yyvsp, int yyrule, yyscan_t scanner, ptxinfo_data* ptxinfo)
 {
-  unsigned long int yylno = yyrline[yyrule];
+  int yylno = yyrline[yyrule];
   int yynrhs = yyr2[yyrule];
   int yyi;
-  YYFPRINTF (stderr, "Reducing stack by rule %d (line %lu):\n",
+  YYFPRINTF (stderr, "Reducing stack by rule %d (line %d):\n",
              yyrule - 1, yylno);
   /* The symbols being reduced.  */
   for (yyi = 0; yyi < yynrhs; yyi++)
     {
       YYFPRINTF (stderr, "   $%d = ", yyi + 1);
       yy_symbol_print (stderr,
-                       yystos[yyssp[yyi + 1 - yynrhs]],
-                       &(yyvsp[(yyi + 1) - (yynrhs)])
-                                              );
+                       yystos[+yyssp[yyi + 1 - yynrhs]],
+                       &yyvsp[(yyi + 1) - (yynrhs)]
+                                              , scanner, ptxinfo);
       YYFPRINTF (stderr, "\n");
     }
 }
@@ -770,7 +881,7 @@ yy_reduce_print (yytype_int16 *yyssp, YYSTYPE *yyvsp, int yyrule)
 # define YY_REDUCE_PRINT(Rule)          \
 do {                                    \
   if (yydebug)                          \
-    yy_reduce_print (yyssp, yyvsp, Rule); \
+    yy_reduce_print (yyssp, yyvsp, Rule, scanner, ptxinfo); \
 } while (0)
 
 /* Nonzero means print parse trace.  It is left uninitialized so that
@@ -805,13 +916,13 @@ int yydebug;
 
 # ifndef yystrlen
 #  if defined __GLIBC__ && defined _STRING_H
-#   define yystrlen strlen
+#   define yystrlen(S) (YY_CAST (YYPTRDIFF_T, strlen (S)))
 #  else
 /* Return the length of YYSTR.  */
-static YYSIZE_T
+static YYPTRDIFF_T
 yystrlen (const char *yystr)
 {
-  YYSIZE_T yylen;
+  YYPTRDIFF_T yylen;
   for (yylen = 0; yystr[yylen]; yylen++)
     continue;
   return yylen;
@@ -847,12 +958,12 @@ yystpcpy (char *yydest, const char *yysrc)
    backslash-backslash).  YYSTR is taken from yytname.  If YYRES is
    null, do not copy; instead, return the length of what the result
    would have been.  */
-static YYSIZE_T
+static YYPTRDIFF_T
 yytnamerr (char *yyres, const char *yystr)
 {
   if (*yystr == '"')
     {
-      YYSIZE_T yyn = 0;
+      YYPTRDIFF_T yyn = 0;
       char const *yyp = yystr;
 
       for (;;)
@@ -865,7 +976,10 @@ yytnamerr (char *yyres, const char *yystr)
           case '\\':
             if (*++yyp != '\\')
               goto do_not_strip_quotes;
-            /* Fall through.  */
+            else
+              goto append;
+
+          append:
           default:
             if (yyres)
               yyres[yyn] = *yyp;
@@ -880,10 +994,10 @@ yytnamerr (char *yyres, const char *yystr)
     do_not_strip_quotes: ;
     }
 
-  if (! yyres)
+  if (yyres)
+    return yystpcpy (yyres, yystr) - yyres;
+  else
     return yystrlen (yystr);
-
-  return yystpcpy (yyres, yystr) - yyres;
 }
 # endif
 
@@ -896,19 +1010,19 @@ yytnamerr (char *yyres, const char *yystr)
    *YYMSG_ALLOC to the required number of bytes.  Return 2 if the
    required number of bytes is too large to store.  */
 static int
-yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
-                yytype_int16 *yyssp, int yytoken)
+yysyntax_error (YYPTRDIFF_T *yymsg_alloc, char **yymsg,
+                yy_state_t *yyssp, int yytoken)
 {
-  YYSIZE_T yysize0 = yytnamerr (YY_NULLPTR, yytname[yytoken]);
-  YYSIZE_T yysize = yysize0;
   enum { YYERROR_VERBOSE_ARGS_MAXIMUM = 5 };
   /* Internationalized format string. */
   const char *yyformat = YY_NULLPTR;
-  /* Arguments of yyformat. */
+  /* Arguments of yyformat: reported tokens (one for the "unexpected",
+     one per "expected"). */
   char const *yyarg[YYERROR_VERBOSE_ARGS_MAXIMUM];
-  /* Number of reported tokens (one for the "unexpected", one per
-     "expected"). */
+  /* Actual size of YYARG. */
   int yycount = 0;
+  /* Cumulated lengths of YYARG.  */
+  YYPTRDIFF_T yysize = 0;
 
   /* There are many possibilities here to consider:
      - If this state is a consistent state with a default action, then
@@ -935,7 +1049,9 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
   */
   if (yytoken != YYEMPTY)
     {
-      int yyn = yypact[*yyssp];
+      int yyn = yypact[+*yyssp];
+      YYPTRDIFF_T yysize0 = yytnamerr (YY_NULLPTR, yytname[yytoken]);
+      yysize = yysize0;
       yyarg[yycount++] = yytname[yytoken];
       if (!yypact_value_is_default (yyn))
         {
@@ -960,11 +1076,12 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
                   }
                 yyarg[yycount++] = yytname[yyx];
                 {
-                  YYSIZE_T yysize1 = yysize + yytnamerr (YY_NULLPTR, yytname[yyx]);
-                  if (! (yysize <= yysize1
-                         && yysize1 <= YYSTACK_ALLOC_MAXIMUM))
+                  YYPTRDIFF_T yysize1
+                    = yysize + yytnamerr (YY_NULLPTR, yytname[yyx]);
+                  if (yysize <= yysize1 && yysize1 <= YYSTACK_ALLOC_MAXIMUM)
+                    yysize = yysize1;
+                  else
                     return 2;
-                  yysize = yysize1;
                 }
               }
         }
@@ -976,6 +1093,7 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
       case N:                               \
         yyformat = S;                       \
       break
+    default: /* Avoid compiler warnings. */
       YYCASE_(0, YY_("syntax error"));
       YYCASE_(1, YY_("syntax error, unexpected %s"));
       YYCASE_(2, YY_("syntax error, unexpected %s, expecting %s"));
@@ -986,10 +1104,13 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
     }
 
   {
-    YYSIZE_T yysize1 = yysize + yystrlen (yyformat);
-    if (! (yysize <= yysize1 && yysize1 <= YYSTACK_ALLOC_MAXIMUM))
+    /* Don't count the "%s"s in the final size, but reserve room for
+       the terminator.  */
+    YYPTRDIFF_T yysize1 = yysize + (yystrlen (yyformat) - 2 * yycount) + 1;
+    if (yysize <= yysize1 && yysize1 <= YYSTACK_ALLOC_MAXIMUM)
+      yysize = yysize1;
+    else
       return 2;
-    yysize = yysize1;
   }
 
   if (*yymsg_alloc < yysize)
@@ -1015,8 +1136,8 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
         }
       else
         {
-          yyp++;
-          yyformat++;
+          ++yyp;
+          ++yyformat;
         }
   }
   return 0;
@@ -1028,9 +1149,11 @@ yysyntax_error (YYSIZE_T *yymsg_alloc, char **yymsg,
 `-----------------------------------------------*/
 
 static void
-yydestruct (const char *yymsg, int yytype, YYSTYPE *yyvaluep)
+yydestruct (const char *yymsg, int yytype, YYSTYPE *yyvaluep, yyscan_t scanner, ptxinfo_data* ptxinfo)
 {
   YYUSE (yyvaluep);
+  YYUSE (scanner);
+  YYUSE (ptxinfo);
   if (!yymsg)
     yymsg = "Deleting";
   YY_SYMBOL_PRINT (yymsg, yytype, yyvaluep, yylocationp);
@@ -1043,23 +1166,27 @@ yydestruct (const char *yymsg, int yytype, YYSTYPE *yyvaluep)
 
 
 
-/* The lookahead symbol.  */
-int yychar;
-
-/* The semantic value of the lookahead symbol.  */
-YYSTYPE yylval;
-/* Number of syntax errors so far.  */
-int yynerrs;
-
-
 /*----------.
 | yyparse.  |
 `----------*/
 
 int
-yyparse (void)
+yyparse (yyscan_t scanner, ptxinfo_data* ptxinfo)
 {
-    int yystate;
+/* The lookahead symbol.  */
+int yychar;
+
+
+/* The semantic value of the lookahead symbol.  */
+/* Default value used for initialization, for pacifying older GCCs
+   or non-GCC compilers.  */
+YY_INITIAL_VALUE (static YYSTYPE yyval_default;)
+YYSTYPE yylval YY_INITIAL_VALUE (= yyval_default);
+
+    /* Number of syntax errors so far.  */
+    int yynerrs;
+
+    yy_state_fast_t yystate;
     /* Number of tokens to shift before error messages enabled.  */
     int yyerrstatus;
 
@@ -1071,16 +1198,16 @@ yyparse (void)
        to reallocate them elsewhere.  */
 
     /* The state stack.  */
-    yytype_int16 yyssa[YYINITDEPTH];
-    yytype_int16 *yyss;
-    yytype_int16 *yyssp;
+    yy_state_t yyssa[YYINITDEPTH];
+    yy_state_t *yyss;
+    yy_state_t *yyssp;
 
     /* The semantic value stack.  */
     YYSTYPE yyvsa[YYINITDEPTH];
     YYSTYPE *yyvs;
     YYSTYPE *yyvsp;
 
-    YYSIZE_T yystacksize;
+    YYPTRDIFF_T yystacksize;
 
   int yyn;
   int yyresult;
@@ -1094,7 +1221,7 @@ yyparse (void)
   /* Buffer for error messages, and its allocated size.  */
   char yymsgbuf[128];
   char *yymsg = yymsgbuf;
-  YYSIZE_T yymsg_alloc = sizeof yymsgbuf;
+  YYPTRDIFF_T yymsg_alloc = sizeof yymsgbuf;
 #endif
 
 #define YYPOPSTACK(N)   (yyvsp -= (N), yyssp -= (N))
@@ -1115,46 +1242,54 @@ yyparse (void)
   yychar = YYEMPTY; /* Cause a token to be read.  */
   goto yysetstate;
 
+
 /*------------------------------------------------------------.
-| yynewstate -- Push a new state, which is found in yystate.  |
+| yynewstate -- push a new state, which is found in yystate.  |
 `------------------------------------------------------------*/
- yynewstate:
+yynewstate:
   /* In all cases, when you get here, the value and location stacks
      have just been pushed.  So pushing a state here evens the stacks.  */
   yyssp++;
 
- yysetstate:
-  *yyssp = yystate;
+
+/*--------------------------------------------------------------------.
+| yysetstate -- set current state (the top of the stack) to yystate.  |
+`--------------------------------------------------------------------*/
+yysetstate:
+  YYDPRINTF ((stderr, "Entering state %d\n", yystate));
+  YY_ASSERT (0 <= yystate && yystate < YYNSTATES);
+  YY_IGNORE_USELESS_CAST_BEGIN
+  *yyssp = YY_CAST (yy_state_t, yystate);
+  YY_IGNORE_USELESS_CAST_END
 
   if (yyss + yystacksize - 1 <= yyssp)
+#if !defined yyoverflow && !defined YYSTACK_RELOCATE
+    goto yyexhaustedlab;
+#else
     {
       /* Get the current used size of the three stacks, in elements.  */
-      YYSIZE_T yysize = yyssp - yyss + 1;
+      YYPTRDIFF_T yysize = yyssp - yyss + 1;
 
-#ifdef yyoverflow
+# if defined yyoverflow
       {
         /* Give user a chance to reallocate the stack.  Use copies of
            these so that the &'s don't force the real ones into
            memory.  */
+        yy_state_t *yyss1 = yyss;
         YYSTYPE *yyvs1 = yyvs;
-        yytype_int16 *yyss1 = yyss;
 
         /* Each stack pointer address is followed by the size of the
            data in use in that stack, in bytes.  This used to be a
            conditional around just the two extra args, but that might
            be undefined if yyoverflow is a macro.  */
         yyoverflow (YY_("memory exhausted"),
-                    &yyss1, yysize * sizeof (*yyssp),
-                    &yyvs1, yysize * sizeof (*yyvsp),
+                    &yyss1, yysize * YYSIZEOF (*yyssp),
+                    &yyvs1, yysize * YYSIZEOF (*yyvsp),
                     &yystacksize);
-
         yyss = yyss1;
         yyvs = yyvs1;
       }
-#else /* no yyoverflow */
-# ifndef YYSTACK_RELOCATE
-      goto yyexhaustedlab;
-# else
+# else /* defined YYSTACK_RELOCATE */
       /* Extend the stack our own way.  */
       if (YYMAXDEPTH <= yystacksize)
         goto yyexhaustedlab;
@@ -1163,42 +1298,43 @@ yyparse (void)
         yystacksize = YYMAXDEPTH;
 
       {
-        yytype_int16 *yyss1 = yyss;
+        yy_state_t *yyss1 = yyss;
         union yyalloc *yyptr =
-          (union yyalloc *) YYSTACK_ALLOC (YYSTACK_BYTES (yystacksize));
+          YY_CAST (union yyalloc *,
+                   YYSTACK_ALLOC (YY_CAST (YYSIZE_T, YYSTACK_BYTES (yystacksize))));
         if (! yyptr)
           goto yyexhaustedlab;
         YYSTACK_RELOCATE (yyss_alloc, yyss);
         YYSTACK_RELOCATE (yyvs_alloc, yyvs);
-#  undef YYSTACK_RELOCATE
+# undef YYSTACK_RELOCATE
         if (yyss1 != yyssa)
           YYSTACK_FREE (yyss1);
       }
 # endif
-#endif /* no yyoverflow */
 
       yyssp = yyss + yysize - 1;
       yyvsp = yyvs + yysize - 1;
 
-      YYDPRINTF ((stderr, "Stack size increased to %lu\n",
-                  (unsigned long int) yystacksize));
+      YY_IGNORE_USELESS_CAST_BEGIN
+      YYDPRINTF ((stderr, "Stack size increased to %ld\n",
+                  YY_CAST (long, yystacksize)));
+      YY_IGNORE_USELESS_CAST_END
 
       if (yyss + yystacksize - 1 <= yyssp)
         YYABORT;
     }
-
-  YYDPRINTF ((stderr, "Entering state %d\n", yystate));
+#endif /* !defined yyoverflow && !defined YYSTACK_RELOCATE */
 
   if (yystate == YYFINAL)
     YYACCEPT;
 
   goto yybackup;
 
+
 /*-----------.
 | yybackup.  |
 `-----------*/
 yybackup:
-
   /* Do appropriate processing given the current state.  Read a
      lookahead token if we need one and don't already have one.  */
 
@@ -1213,7 +1349,7 @@ yybackup:
   if (yychar == YYEMPTY)
     {
       YYDPRINTF ((stderr, "Reading a token: "));
-      yychar = yylex ();
+      yychar = yylex (&yylval, scanner, ptxinfo);
     }
 
   if (yychar <= YYEOF)
@@ -1248,15 +1384,13 @@ yybackup:
 
   /* Shift the lookahead token.  */
   YY_SYMBOL_PRINT ("Shifting", yytoken, &yylval, &yylloc);
-
-  /* Discard the shifted token.  */
-  yychar = YYEMPTY;
-
   yystate = yyn;
   YY_IGNORE_MAYBE_UNINITIALIZED_BEGIN
   *++yyvsp = yylval;
   YY_IGNORE_MAYBE_UNINITIALIZED_END
 
+  /* Discard the shifted token.  */
+  yychar = YYEMPTY;
   goto yynewstate;
 
 
@@ -1271,7 +1405,7 @@ yydefault:
 
 
 /*-----------------------------.
-| yyreduce -- Do a reduction.  |
+| yyreduce -- do a reduction.  |
 `-----------------------------*/
 yyreduce:
   /* yyn is the number of a rule to reduce with.  */
@@ -1291,116 +1425,117 @@ yyreduce:
   YY_REDUCE_PRINT (yyn);
   switch (yyn)
     {
-        case 6:
-#line 90 "ptxinfo.y" /* yacc.c:1646  */
-    { printf("GPGPU-Sim: ptxas %s\n", (yyvsp[0].string_value)); }
-#line 1298 "ptxinfo.tab.c" /* yacc.c:1646  */
+  case 6:
+#line 100 "ptxinfo.y"
+                         { printf("GPGPU-Sim: ptxas %s\n", (yyvsp[0].string_value)); }
+#line 1432 "ptxinfo.tab.c"
     break;
 
   case 7:
-#line 91 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_linenum((yyvsp[-3].int_value)); }
-#line 1304 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 101 "ptxinfo.y"
+                                                                                 { ptxinfo_linenum((yyvsp[-3].int_value)); }
+#line 1438 "ptxinfo.tab.c"
     break;
 
   case 10:
-#line 96 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_addinfo(); }
-#line 1310 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 106 "ptxinfo.y"
+                        { ptxinfo->ptxinfo_addinfo(); }
+#line 1444 "ptxinfo.tab.c"
     break;
 
   case 12:
-#line 100 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_function((yyvsp[-1].string_value)); }
-#line 1316 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 110 "ptxinfo.y"
+                                            { ptxinfo_function((yyvsp[-1].string_value)); }
+#line 1450 "ptxinfo.tab.c"
     break;
 
   case 13:
-#line 101 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_function((yyvsp[-5].string_value)); }
-#line 1322 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 111 "ptxinfo.y"
+                                                                  { ptxinfo_function((yyvsp[-5].string_value)); }
+#line 1456 "ptxinfo.tab.c"
     break;
 
   case 17:
-#line 111 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_regs((yyvsp[-1].int_value)); }
-#line 1328 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 121 "ptxinfo.y"
+                                { ptxinfo_regs((yyvsp[-1].int_value)); }
+#line 1462 "ptxinfo.tab.c"
     break;
 
   case 18:
-#line 112 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_lmem(g_declared,g_system); }
-#line 1334 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 122 "ptxinfo.y"
+                     { ptxinfo_lmem(g_declared,g_system); }
+#line 1468 "ptxinfo.tab.c"
     break;
 
   case 19:
-#line 113 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_smem(g_declared,g_system); }
-#line 1340 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 123 "ptxinfo.y"
+                     { ptxinfo_smem(g_declared,g_system); }
+#line 1474 "ptxinfo.tab.c"
     break;
 
   case 20:
-#line 114 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_cmem((yyvsp[-5].int_value),(yyvsp[-1].int_value)); }
-#line 1346 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 124 "ptxinfo.y"
+                                                                                      { ptxinfo_cmem((yyvsp[-5].int_value),(yyvsp[-1].int_value)); }
+#line 1480 "ptxinfo.tab.c"
     break;
 
   case 21:
-#line 115 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_gmem((yyvsp[-2].int_value),0); }
-#line 1352 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 125 "ptxinfo.y"
+                                 { ptxinfo_gmem((yyvsp[-2].int_value),0); }
+#line 1486 "ptxinfo.tab.c"
     break;
 
   case 22:
-#line 116 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_lmem((yyvsp[-2].int_value),0); }
-#line 1358 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 126 "ptxinfo.y"
+                                 { ptxinfo_lmem((yyvsp[-2].int_value),0); }
+#line 1492 "ptxinfo.tab.c"
     break;
 
   case 23:
-#line 117 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_smem((yyvsp[-2].int_value),0); }
-#line 1364 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 127 "ptxinfo.y"
+                                 { ptxinfo_smem((yyvsp[-2].int_value),0); }
+#line 1498 "ptxinfo.tab.c"
     break;
 
   case 24:
-#line 118 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_cmem((yyvsp[-2].int_value),0); }
-#line 1370 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 128 "ptxinfo.y"
+                                 { ptxinfo_cmem((yyvsp[-2].int_value),0); }
+#line 1504 "ptxinfo.tab.c"
     break;
 
   case 25:
-#line 119 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_regs((yyvsp[-1].int_value)); }
-#line 1376 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 129 "ptxinfo.y"
+                           { ptxinfo_regs((yyvsp[-1].int_value)); }
+#line 1510 "ptxinfo.tab.c"
     break;
 
   case 26:
-#line 120 "ptxinfo.y" /* yacc.c:1646  */
-    {}
-#line 1382 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 130 "ptxinfo.y"
+                               {}
+#line 1516 "ptxinfo.tab.c"
     break;
 
   case 27:
-#line 123 "ptxinfo.y" /* yacc.c:1646  */
-    { g_declared=(yyvsp[-3].int_value); g_system=(yyvsp[-1].int_value); }
-#line 1388 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 133 "ptxinfo.y"
+                                          { g_declared=(yyvsp[-3].int_value); g_system=(yyvsp[-1].int_value); }
+#line 1522 "ptxinfo.tab.c"
     break;
 
   case 28:
-#line 125 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_dup_type((yyvsp[-3].string_value)); }
-#line 1394 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 135 "ptxinfo.y"
+                                                { ptxinfo_dup_type((yyvsp[-3].string_value)); }
+#line 1528 "ptxinfo.tab.c"
     break;
 
   case 29:
-#line 126 "ptxinfo.y" /* yacc.c:1646  */
-    { ptxinfo_dup_type((yyvsp[-3].string_value)); }
-#line 1400 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 136 "ptxinfo.y"
+                                          { ptxinfo_dup_type((yyvsp[-3].string_value)); }
+#line 1534 "ptxinfo.tab.c"
     break;
 
 
-#line 1404 "ptxinfo.tab.c" /* yacc.c:1646  */
+#line 1538 "ptxinfo.tab.c"
+
       default: break;
     }
   /* User semantic actions sometimes alter yychar, and that requires
@@ -1425,14 +1560,13 @@ yyreduce:
   /* Now 'shift' the result of the reduction.  Determine what state
      that goes to, based on the state we popped back to and the rule
      number reduced by.  */
-
-  yyn = yyr1[yyn];
-
-  yystate = yypgoto[yyn - YYNTOKENS] + *yyssp;
-  if (0 <= yystate && yystate <= YYLAST && yycheck[yystate] == *yyssp)
-    yystate = yytable[yystate];
-  else
-    yystate = yydefgoto[yyn - YYNTOKENS];
+  {
+    const int yylhs = yyr1[yyn] - YYNTOKENS;
+    const int yyi = yypgoto[yylhs] + *yyssp;
+    yystate = (0 <= yyi && yyi <= YYLAST && yycheck[yyi] == *yyssp
+               ? yytable[yyi]
+               : yydefgoto[yylhs]);
+  }
 
   goto yynewstate;
 
@@ -1450,7 +1584,7 @@ yyerrlab:
     {
       ++yynerrs;
 #if ! YYERROR_VERBOSE
-      yyerror (YY_("syntax error"));
+      yyerror (scanner, ptxinfo, YY_("syntax error"));
 #else
 # define YYSYNTAX_ERROR yysyntax_error (&yymsg_alloc, &yymsg, \
                                         yyssp, yytoken)
@@ -1464,7 +1598,7 @@ yyerrlab:
           {
             if (yymsg != yymsgbuf)
               YYSTACK_FREE (yymsg);
-            yymsg = (char *) YYSTACK_ALLOC (yymsg_alloc);
+            yymsg = YY_CAST (char *, YYSTACK_ALLOC (YY_CAST (YYSIZE_T, yymsg_alloc)));
             if (!yymsg)
               {
                 yymsg = yymsgbuf;
@@ -1477,7 +1611,7 @@ yyerrlab:
                 yymsgp = yymsg;
               }
           }
-        yyerror (yymsgp);
+        yyerror (scanner, ptxinfo, yymsgp);
         if (yysyntax_error_status == 2)
           goto yyexhaustedlab;
       }
@@ -1501,7 +1635,7 @@ yyerrlab:
       else
         {
           yydestruct ("Error: discarding",
-                      yytoken, &yylval);
+                      yytoken, &yylval, scanner, ptxinfo);
           yychar = YYEMPTY;
         }
     }
@@ -1515,12 +1649,10 @@ yyerrlab:
 | yyerrorlab -- error raised explicitly by YYERROR.  |
 `---------------------------------------------------*/
 yyerrorlab:
-
-  /* Pacify compilers like GCC when the user code never invokes
-     YYERROR and the label yyerrorlab therefore never appears in user
-     code.  */
-  if (/*CONSTCOND*/ 0)
-     goto yyerrorlab;
+  /* Pacify compilers when the user code never invokes YYERROR and the
+     label yyerrorlab therefore never appears in user code.  */
+  if (0)
+    YYERROR;
 
   /* Do not reclaim the symbols of the rule whose action triggered
      this YYERROR.  */
@@ -1557,7 +1689,7 @@ yyerrlab1:
 
 
       yydestruct ("Error: popping",
-                  yystos[yystate], yyvsp);
+                  yystos[yystate], yyvsp, scanner, ptxinfo);
       YYPOPSTACK (1);
       yystate = *yyssp;
       YY_STACK_PRINT (yyss, yyssp);
@@ -1582,6 +1714,7 @@ yyacceptlab:
   yyresult = 0;
   goto yyreturn;
 
+
 /*-----------------------------------.
 | yyabortlab -- YYABORT comes here.  |
 `-----------------------------------*/
@@ -1589,16 +1722,21 @@ yyabortlab:
   yyresult = 1;
   goto yyreturn;
 
+
 #if !defined yyoverflow || YYERROR_VERBOSE
 /*-------------------------------------------------.
 | yyexhaustedlab -- memory exhaustion comes here.  |
 `-------------------------------------------------*/
 yyexhaustedlab:
-  yyerror (YY_("memory exhausted"));
+  yyerror (scanner, ptxinfo, YY_("memory exhausted"));
   yyresult = 2;
   /* Fall through.  */
 #endif
 
+
+/*-----------------------------------------------------.
+| yyreturn -- parsing is finished, return the result.  |
+`-----------------------------------------------------*/
 yyreturn:
   if (yychar != YYEMPTY)
     {
@@ -1606,7 +1744,7 @@ yyreturn:
          user semantic actions for why this is necessary.  */
       yytoken = YYTRANSLATE (yychar);
       yydestruct ("Cleanup: discarding lookahead",
-                  yytoken, &yylval);
+                  yytoken, &yylval, scanner, ptxinfo);
     }
   /* Do not reclaim the symbols of the rule whose action triggered
      this YYABORT or YYACCEPT.  */
@@ -1615,7 +1753,7 @@ yyreturn:
   while (yyssp != yyss)
     {
       yydestruct ("Cleanup: popping",
-                  yystos[*yyssp], yyvsp);
+                  yystos[+*yyssp], yyvsp, scanner, ptxinfo);
       YYPOPSTACK (1);
     }
 #ifndef yyoverflow
@@ -1628,7 +1766,7 @@ yyreturn:
 #endif
   return yyresult;
 }
-#line 129 "ptxinfo.y" /* yacc.c:1906  */
+#line 139 "ptxinfo.y"
 
 
 
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.h b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.h
index 123b6235c1..7e19e5ed7d 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.h
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.tab.h
@@ -1,8 +1,9 @@
-/* A Bison parser, made by GNU Bison 3.0.4.  */
+/* A Bison parser, made by GNU Bison 3.5.1.  */
 
 /* Bison interface for Yacc-like parsers in C
 
-   Copyright (C) 1984, 1989-1990, 2000-2015 Free Software Foundation, Inc.
+   Copyright (C) 1984, 1989-1990, 2000-2015, 2018-2020 Free Software Foundation,
+   Inc.
 
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
@@ -30,8 +31,11 @@
    This special exception was added by the Free Software Foundation in
    version 2.2 of Bison.  */
 
-#ifndef YY_PTXINFO_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTXINFO_TAB_H_INCLUDED
-# define YY_PTXINFO_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTXINFO_TAB_H_INCLUDED
+/* Undocumented macros, especially those whose name start with YY_,
+   are private implementation details.  Do not rely on them.  */
+
+#ifndef YY_PTXINFO_PTXINFO_TAB_H_INCLUDED
+# define YY_PTXINFO_PTXINFO_TAB_H_INCLUDED
 /* Debug traces.  */
 #ifndef YYDEBUG
 # define YYDEBUG 0
@@ -47,7 +51,7 @@ extern int ptxinfo_debug;
   {
     INT_OPERAND = 258,
     HEADER = 259,
-    INFO = 260,
+    INFO_GPGPU = 260,
     FUNC = 261,
     USED = 262,
     REGS = 263,
@@ -71,31 +75,57 @@ extern int ptxinfo_debug;
     DUPLICATE = 281,
     FUNCTION = 282,
     VARIABLE = 283,
-    FATAL = 284
+    FATAL_GPGPU = 284
   };
 #endif
+/* Tokens.  */
+#define INT_OPERAND 258
+#define HEADER 259
+#define INFO_GPGPU 260
+#define FUNC 261
+#define USED 262
+#define REGS 263
+#define BYTES 264
+#define LMEM 265
+#define SMEM 266
+#define CMEM 267
+#define GMEM 268
+#define IDENTIFIER 269
+#define PLUS 270
+#define COMMA 271
+#define LEFT_SQUARE_BRACKET 272
+#define RIGHT_SQUARE_BRACKET 273
+#define COLON 274
+#define SEMICOLON 275
+#define QUOTE 276
+#define LINE 277
+#define WARNING 278
+#define FOR 279
+#define TEXTURES 280
+#define DUPLICATE 281
+#define FUNCTION 282
+#define VARIABLE 283
+#define FATAL_GPGPU 284
 
 /* Value type.  */
 #if ! defined YYSTYPE && ! defined YYSTYPE_IS_DECLARED
-
 union YYSTYPE
 {
-#line 30 "ptxinfo.y" /* yacc.c:1909  */
+#line 41 "ptxinfo.y"
 
   int    int_value;
   char * string_value;
 
-#line 89 "/mnt/d/source/github/sim/gpgpu-sim_distribution-1/build/gcc-7.4.0/cuda-10000/debug/cuda-sim/ptxinfo.tab.h" /* yacc.c:1909  */
-};
+#line 120 "ptxinfo.tab.h"
 
+};
 typedef union YYSTYPE YYSTYPE;
 # define YYSTYPE_IS_TRIVIAL 1
 # define YYSTYPE_IS_DECLARED 1
 #endif
 
 
-extern YYSTYPE ptxinfo_lval;
 
-int ptxinfo_parse (void);
+int ptxinfo_parse (yyscan_t scanner, ptxinfo_data* ptxinfo);
 
-#endif /* !YY_PTXINFO_MNT_D_SOURCE_GITHUB_SIM_GPGPU_SIM_DISTRIBUTION_1_BUILD_GCC_7_4_0_CUDA_10000_DEBUG_CUDA_SIM_PTXINFO_TAB_H_INCLUDED  */
+#endif /* !YY_PTXINFO_PTXINFO_TAB_H_INCLUDED  */
diff --git a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.y b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.y
index d241d8c9aa..4626cd6460 100644
--- a/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.y
+++ b/design/gpgpu/gpgpu-sim/src/cuda-sim/ptxinfo.y
@@ -27,6 +27,17 @@ OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
 
+%{
+typedef void * yyscan_t;
+#include "ptx_loader.h"
+%}
+
+%define api.pure full
+%parse-param {yyscan_t scanner}
+%parse-param {ptxinfo_data* ptxinfo}
+%lex-param {yyscan_t scanner}
+%lex-param {ptxinfo_data* ptxinfo}
+
 %union {
   int    int_value;
   char * string_value;
@@ -58,7 +69,7 @@ OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 %token DUPLICATE
 %token <string_value> FUNCTION
 %token <string_value> VARIABLE
-%token FATAL
+%token FATAL_GPGPU
 
 %{
 	#include <stdlib.h>
@@ -66,15 +77,14 @@ OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 	
 	static unsigned g_declared;
 	static unsigned g_system;
-	int ptxinfo_lex(void);
-	void ptxinfo_addinfo();
+	int ptxinfo_lex(YYSTYPE * yylval_param, yyscan_t yyscanner, ptxinfo_data* ptxinfo);
+	void yyerror(yyscan_t yyscanner, ptxinfo_data* ptxinfo, const char* msg);
 	void ptxinfo_function(const char *fname );
 	void ptxinfo_regs( unsigned nregs );
 	void ptxinfo_lmem( unsigned declared, unsigned system );
 	void ptxinfo_gmem( unsigned declared, unsigned system );
 	void ptxinfo_smem( unsigned declared, unsigned system );
 	void ptxinfo_cmem( unsigned nbytes, unsigned bank );
-	int ptxinfo_error(const char*);
 	void ptxinfo_linenum( unsigned );
 	void ptxinfo_dup_type( const char* );
 %}
@@ -89,11 +99,11 @@ line: 	HEADER INFO COLON line_info
 	| HEADER IDENTIFIER COMMA LINE INT_OPERAND SEMICOLON WARNING
 	| HEADER WARNING { printf("GPGPU-Sim: ptxas %s\n", $2); }
 	| HEADER IDENTIFIER COMMA LINE INT_OPERAND SEMICOLON DUPLICATE duplicate { ptxinfo_linenum($5); }
-	| HEADER FATAL
+	| HEADER FATAL_GPGPU
 	;
 
 line_info: function_name
-	| function_info { ptxinfo_addinfo(); }
+	| function_info { ptxinfo->ptxinfo_addinfo(); }
 	| gmem_info
 	;
 
diff --git a/design/gpgpu/gpgpu-sim/src/debug.cc b/design/gpgpu/gpgpu-sim/src/debug.cc
index 0c20fa736d..29506bd75d 100644
--- a/design/gpgpu/gpgpu-sim/src/debug.cc
+++ b/design/gpgpu/gpgpu-sim/src/debug.cc
@@ -7,225 +7,212 @@
 //
 // Redistributions of source code must retain the above copyright notice, this
 // list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// Redistributions in binary form must reproduce the above copyright notice,
+// this list of conditions and the following disclaimer in the documentation
+// and/or other materials provided with the distribution. Neither the name of
+// The University of British Columbia nor the names of its contributors may be
+// used to endorse or promote products derived from this software without
+// specific prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
 
 #include "debug.h"
-#include "gpgpu-sim/shader.h"
-#include "gpgpu-sim/gpu-sim.h"
-#include "cuda-sim/ptx_sim.h"
 #include "cuda-sim/cuda-sim.h"
 #include "cuda-sim/ptx_ir.h"
+#include "cuda-sim/ptx_sim.h"
+#include "gpgpu-sim/gpu-sim.h"
+#include "gpgpu-sim/shader.h"
 
-#include <map>
 #include <stdio.h>
 #include <string.h>
+#include <map>
 
-class watchpoint_event {
-public:
-   watchpoint_event()
-   {
-      m_thread=NULL;
-      m_inst=NULL;
-   }
-   watchpoint_event(const ptx_thread_info *thd, const ptx_instruction *pI) 
-   {
-      m_thread=thd;
-      m_inst = pI;
-   }
-   const ptx_thread_info *thread() const { return m_thread; }
-   const ptx_instruction *inst() const { return m_inst; }
-private:
-   const ptx_thread_info *m_thread;
-   const ptx_instruction *m_inst;
-};
-
-std::map<unsigned,watchpoint_event> g_watchpoint_hits;
-
-void hit_watchpoint( unsigned watchpoint_num, ptx_thread_info *thd, const ptx_instruction *pI )
-{
-   g_watchpoint_hits[watchpoint_num]=watchpoint_event(thd,pI);
+void gpgpu_sim::hit_watchpoint(unsigned watchpoint_num, ptx_thread_info *thd,
+                               const ptx_instruction *pI) {
+  g_watchpoint_hits[watchpoint_num] = watchpoint_event(thd, pI);
 }
 
-/// interactive debugger 
-
-void gpgpu_sim::gpgpu_debug()
-{
-   bool done=true;
-
-   static bool single_step=true;
-   static unsigned next_brkpt=1;
-   static std::map<unsigned,brk_pt> breakpoints;
-
-   /// if single stepping, go to interactive debugger
-
-   if( single_step ) 
-      done=false;
-
-#ifndef LIBCUDA
-   /// check if we've reached a breakpoint
-   const ptx_thread_info *brk_thd = NULL;
-   const ptx_instruction *brk_inst = NULL;
-
-   for( std::map<unsigned,brk_pt>::iterator i=breakpoints.begin(); i!=breakpoints.end(); i++) {
-      unsigned num=i->first;
-      brk_pt &b=i->second;
-      if( b.is_watchpoint() ) {
-         unsigned addr = b.get_addr();
-         unsigned new_value; 
-         m_global_mem->read(addr,4,&new_value);
-         if( new_value != b.get_value() || g_watchpoint_hits.find(num) != g_watchpoint_hits.end() ) {
-            printf( "GPGPU-Sim PTX DBG: watch point %u triggered (old value=%x, new value=%x)\n",
-                     num,b.get_value(),new_value );
-            std::map<unsigned,watchpoint_event>::iterator w=g_watchpoint_hits.find(num);
-            if( w==g_watchpoint_hits.end() ) 
-               printf( "GPGPU-Sim PTX DBG: memory transfer modified value\n");
-            else {
-               watchpoint_event wa = w->second;
-               brk_thd = wa.thread();
-               brk_inst = wa.inst();
-               printf( "GPGPU-Sim PTX DBG: modified by thread uid=%u, sid=%u, hwtid=%u\n",
-                       brk_thd->get_uid(),brk_thd->get_hw_sid(), brk_thd->get_hw_tid() );
-               printf( "GPGPU-Sim PTX DBG: ");
-               brk_inst->print_insn(stdout);
-               printf( "\n" );
-               g_watchpoint_hits.erase(w);
-            }
-            b.set_value(new_value);
-            done = false; 
-         }
-      } else {
-          /*
-         for( unsigned sid=0; sid < m_n_shader; sid++ ) { 
-            unsigned hw_thread_id = -1;
-            abort();
-            ptx_thread_info *thread = m_sc[sid]->get_functional_thread(hw_thread_id);
-            if( thread_at_brkpt(thread, b) ) {
-               done = false;
-               printf("GPGPU-Sim PTX DBG: reached breakpoint %u at %s (sm=%u, hwtid=%u)\n", 
-                      num, b.location().c_str(), sid, hw_thread_id );
-               brk_thd = thread;
-               brk_inst = brk_thd->get_inst();
-               printf( "GPGPU-Sim PTX DBG: reached by thread uid=%u, sid=%u, hwtid=%u\n",
-                       brk_thd->get_uid(),brk_thd->get_hw_sid(), brk_thd->get_hw_tid() );
-               printf( "GPGPU-Sim PTX DBG: ");
-               brk_inst->print_insn(stdout);
-               printf( "\n" );
-            }
-         }
-         */
+/// interactive debugger
+
+void gpgpu_sim::gpgpu_debug() {
+  bool done = true;
+
+  static bool single_step = true;
+  static unsigned next_brkpt = 1;
+  static std::map<unsigned, brk_pt> breakpoints;
+
+  /// if single stepping, go to interactive debugger
+
+  if (single_step) done = false;
+
+  /// check if we've reached a breakpoint
+  const ptx_thread_info *brk_thd = NULL;
+  const ptx_instruction *brk_inst = NULL;
+
+  for (std::map<unsigned, brk_pt>::iterator i = breakpoints.begin();
+       i != breakpoints.end(); i++) {
+    unsigned num = i->first;
+    brk_pt &b = i->second;
+    if (b.is_watchpoint()) {
+      unsigned addr = b.get_addr();
+      unsigned new_value;
+      m_global_mem->read(addr, 4, &new_value);
+      if (new_value != b.get_value() ||
+          g_watchpoint_hits.find(num) != g_watchpoint_hits.end()) {
+        printf(
+            "GPGPU-Sim PTX DBG: watch point %u triggered (old value=%x, new "
+            "value=%x)\n",
+            num, b.get_value(), new_value);
+        std::map<unsigned, watchpoint_event>::iterator w =
+            g_watchpoint_hits.find(num);
+        if (w == g_watchpoint_hits.end())
+          printf("GPGPU-Sim PTX DBG: memory transfer modified value\n");
+        else {
+          watchpoint_event wa = w->second;
+          brk_thd = wa.thread();
+          brk_inst = wa.inst();
+          printf(
+              "GPGPU-Sim PTX DBG: modified by thread uid=%u, sid=%u, "
+              "hwtid=%u\n",
+              brk_thd->get_uid(), brk_thd->get_hw_sid(), brk_thd->get_hw_tid());
+          printf("GPGPU-Sim PTX DBG: ");
+          brk_inst->print_insn(stdout);
+          printf("\n");
+          g_watchpoint_hits.erase(w);
+        }
+        b.set_value(new_value);
+        done = false;
       }
-   }
-
-   if( done ) 
-      assert( g_watchpoint_hits.empty() );
-
-   /// enter interactive debugger loop
-
-   while (!done) {
-      printf("(ptx debugger) ");
+    } else {
+      /*
+     for( unsigned sid=0; sid < m_n_shader; sid++ ) {
+        unsigned hw_thread_id = -1;
+        abort();
+        ptx_thread_info *thread =
+     m_sc[sid]->get_functional_thread(hw_thread_id); if( thread_at_brkpt(thread,
+     b) ) { done = false; printf("GPGPU-Sim PTX DBG: reached breakpoint %u at %s
+     (sm=%u, hwtid=%u)\n", num, b.location().c_str(), sid, hw_thread_id );
+           brk_thd = thread;
+           brk_inst = brk_thd->get_inst();
+           printf( "GPGPU-Sim PTX DBG: reached by thread uid=%u, sid=%u,
+     hwtid=%u\n", brk_thd->get_uid(),brk_thd->get_hw_sid(),
+     brk_thd->get_hw_tid() ); printf( "GPGPU-Sim PTX DBG: ");
+           brk_inst->print_insn(stdout);
+           printf( "\n" );
+        }
+     }
+     */
+    }
+  }
+
+  if (done) assert(g_watchpoint_hits.empty());
+
+  /// enter interactive debugger loop
+
+  while (!done) {
+    printf("(ptx debugger) ");
+    fflush(stdout);
+
+    char line[1024];
+    fgets(line, 1024, stdin);
+
+    char *tok = strtok(line, " \t\n");
+    if (!strcmp(tok, "dp")) {
+      int shader_num = 0;
+      tok = strtok(NULL, " \t\n");
+      sscanf(tok, "%d", &shader_num);
+      dump_pipeline((0x40 | 0x4 | 0x1), shader_num, 0);
+      printf("\n");
       fflush(stdout);
-      
-      char line[1024];
-      fgets(line,1024,stdin);
-
-      char *tok = strtok(line," \t\n");
-      if( !strcmp(tok,"dp") ) {
-         int shader_num = 0;
-         tok = strtok(NULL," \t\n");
-         sscanf(tok,"%d",&shader_num);
-         dump_pipeline((0x40|0x4|0x1),shader_num,0);
-         printf("\n");
-         fflush(stdout);
-      } else if( !strcmp(tok,"q") || !strcmp(tok,"quit") ) {
-         printf("\nreally quit GPGPU-Sim (y/n)?\n");
-         fgets(line,1024,stdin);
-         tok = strtok(line," \t\n");
-         if( !strcmp(tok,"y") ) {
-            exit(0);
-         } else {
-            printf("not quiting.\n");
-         }
-      } else if( !strcmp(tok,"b") ) {
-         tok = strtok(NULL," \t\n");
-         char brkpt[1024];
-         sscanf(tok,"%s",brkpt);
-         tok = strtok(NULL," \t\n");
-         unsigned uid;
-         sscanf(tok,"%u",&uid);
-         breakpoints[next_brkpt++] = brk_pt(brkpt,uid);
-      } else if( !strcmp(tok,"d") ) {
-         tok = strtok(NULL," \t\n");
-         unsigned uid;
-         sscanf(tok,"%u",&uid);
-         breakpoints.erase(uid);
-      } else if( !strcmp(tok,"s") ) {
-         done = true;
-      } else if( !strcmp(tok,"c") ) {
-         single_step=false;
-         done = true;
-      } else if( !strcmp(tok,"w") ) {
-         tok = strtok(NULL," \t\n");
-         unsigned addr;
-         sscanf(tok,"%x",&addr);
-         unsigned value; 
-         m_global_mem->read(addr,4,&value);
-         m_global_mem->set_watch(addr,next_brkpt); 
-         breakpoints[next_brkpt++] = brk_pt(addr,value);
-      } else if( !strcmp(tok,"l") ) {
-         if( brk_thd == NULL  ) {
-            printf("no thread selected\n");
-         } else {
-            addr_t pc = brk_thd->get_pc();
-            addr_t start_pc = (pc<5)?0:(pc-5);
-            for( addr_t p=start_pc;  p <= pc+5; p++ ) {
-               const ptx_instruction *i = brk_thd->get_inst(p);
-               if( i ) {
-                  if( p != pc )
-                     printf( "    " );
-                  else
-                     printf( "==> " );
-                      i->print_insn(stdout);
-                  printf( "\n" );
-               }
-            }
-         }
-      } else if( !strcmp(tok,"h") ) {
-         printf("commands:\n");
-         printf("  q                           - quit GPGPU-Sim\n");
-         printf("  b <file>:<line> <thead uid> - set breakpoint\n");
-         printf("  w <global address>          - set watchpoint\n");
-         printf("  del <n>                     - delete breakpoint\n");
-         printf("  s                           - single step one shader cycle (all cores)\n");
-         printf("  c                           - continue simulation without single stepping\n");
-         printf("  l                           - list PTX around current breakpoint\n");
-         printf("  dp <n>                      - display pipeline contents on SM <n>\n");
-         printf("  h                           - print this message\n");
+    } else if (!strcmp(tok, "q") || !strcmp(tok, "quit")) {
+      printf("\nreally quit GPGPU-Sim (y/n)?\n");
+      fgets(line, 1024, stdin);
+      tok = strtok(line, " \t\n");
+      if (!strcmp(tok, "y")) {
+        exit(0);
       } else {
-         printf("\ncommand not understood.\n");
+        printf("not quiting.\n");
       }
-      fflush(stdout);
-   }
-#endif
+    } else if (!strcmp(tok, "b")) {
+      tok = strtok(NULL, " \t\n");
+      char brkpt[1024];
+      sscanf(tok, "%s", brkpt);
+      tok = strtok(NULL, " \t\n");
+      unsigned uid;
+      sscanf(tok, "%u", &uid);
+      breakpoints[next_brkpt++] = brk_pt(brkpt, uid);
+    } else if (!strcmp(tok, "d")) {
+      tok = strtok(NULL, " \t\n");
+      unsigned uid;
+      sscanf(tok, "%u", &uid);
+      breakpoints.erase(uid);
+    } else if (!strcmp(tok, "s")) {
+      done = true;
+    } else if (!strcmp(tok, "c")) {
+      single_step = false;
+      done = true;
+    } else if (!strcmp(tok, "w")) {
+      tok = strtok(NULL, " \t\n");
+      unsigned addr;
+      sscanf(tok, "%x", &addr);
+      unsigned value;
+      m_global_mem->read(addr, 4, &value);
+      m_global_mem->set_watch(addr, next_brkpt);
+      breakpoints[next_brkpt++] = brk_pt(addr, value);
+    } else if (!strcmp(tok, "l")) {
+      if (brk_thd == NULL) {
+        printf("no thread selected\n");
+      } else {
+        addr_t pc = brk_thd->get_pc();
+        addr_t start_pc = (pc < 5) ? 0 : (pc - 5);
+        for (addr_t p = start_pc; p <= pc + 5; p++) {
+          const ptx_instruction *i = brk_thd->get_inst(p);
+          if (i) {
+            if (p != pc)
+              printf("    ");
+            else
+              printf("==> ");
+            i->print_insn(stdout);
+            printf("\n");
+          }
+        }
+      }
+    } else if (!strcmp(tok, "h")) {
+      printf("commands:\n");
+      printf("  q                           - quit GPGPU-Sim\n");
+      printf("  b <file>:<line> <thead uid> - set breakpoint\n");
+      printf("  w <global address>          - set watchpoint\n");
+      printf("  del <n>                     - delete breakpoint\n");
+      printf(
+          "  s                           - single step one shader cycle (all "
+          "cores)\n");
+      printf(
+          "  c                           - continue simulation without single "
+          "stepping\n");
+      printf(
+          "  l                           - list PTX around current "
+          "breakpoint\n");
+      printf(
+          "  dp <n>                      - display pipeline contents on SM "
+          "<n>\n");
+      printf("  h                           - print this message\n");
+    } else {
+      printf("\ncommand not understood.\n");
+    }
+    fflush(stdout);
+  }
 }
 
-bool thread_at_brkpt( ptx_thread_info *thread, const class brk_pt &b )
-{
-   return b.is_equal(thread->get_location(),thread->get_uid());
+bool thread_at_brkpt(ptx_thread_info *thread, const class brk_pt &b) {
+  return b.is_equal(thread->get_location(), thread->get_uid());
 }
-
diff --git a/design/gpgpu/gpgpu-sim/src/debug.h b/design/gpgpu/gpgpu-sim/src/debug.h
index 1277494b80..0c4dea7b5e 100644
--- a/design/gpgpu/gpgpu-sim/src/debug.h
+++ b/design/gpgpu/gpgpu-sim/src/debug.h
@@ -7,23 +7,24 @@
 //
 // Redistributions of source code must retain the above copyright notice, this
 // list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// Redistributions in binary form must reproduce the above copyright notice,
+// this list of conditions and the following disclaimer in the documentation
+// and/or other materials provided with the distribution. Neither the name of
+// The University of British Columbia nor the names of its contributors may be
+// used to endorse or promote products derived from this software without
+// specific prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
 
 #ifndef PTX_DEBUG_INCLUDED
 #define PTX_DEBUG_INCLUDED
@@ -33,61 +34,53 @@
 #include <string>
 
 class brk_pt {
-public:
-   brk_pt() { m_valid=false; }
-   brk_pt( const char *fileline, unsigned uid )
-   {
-      m_valid = true;
-      m_watch = false;
-      m_fileline = std::string(fileline);
-      m_thread_uid=uid;
-   }
-   brk_pt( unsigned addr, unsigned value )
-   {
-      m_valid = true;
-      m_watch = true;
-      m_addr = addr;
-      m_value = value;
-   }
+ public:
+  brk_pt() { m_valid = false; }
+  brk_pt(const char *fileline, unsigned uid) {
+    m_valid = true;
+    m_watch = false;
+    m_fileline = std::string(fileline);
+    m_thread_uid = uid;
+  }
+  brk_pt(unsigned addr, unsigned value) {
+    m_valid = true;
+    m_watch = true;
+    m_addr = addr;
+    m_value = value;
+  }
 
-   unsigned get_value() const { return m_value; }
-   addr_t get_addr() const { return m_addr; }
-   bool is_valid() const { return m_valid; }
-   bool is_watchpoint() const { return m_watch; }
-   bool is_equal( const std::string &fileline, unsigned uid ) const
-   {
-      if( m_watch ) 
-         return false; 
-      if( (m_thread_uid != (unsigned)-1) && (uid != m_thread_uid) ) 
-         return false;
-      return m_fileline == fileline;
-   }
-   std::string location() const
-   {
-      char buffer[1024];
-      sprintf(buffer,"%s thread uid = %u", m_fileline.c_str(), m_thread_uid);
-      return buffer;
-   }
+  unsigned get_value() const { return m_value; }
+  addr_t get_addr() const { return m_addr; }
+  bool is_valid() const { return m_valid; }
+  bool is_watchpoint() const { return m_watch; }
+  bool is_equal(const std::string &fileline, unsigned uid) const {
+    if (m_watch) return false;
+    if ((m_thread_uid != (unsigned)-1) && (uid != m_thread_uid)) return false;
+    return m_fileline == fileline;
+  }
+  std::string location() const {
+    char buffer[1024];
+    sprintf(buffer, "%s thread uid = %u", m_fileline.c_str(), m_thread_uid);
+    return buffer;
+  }
 
-   unsigned set_value( unsigned val ) { return m_value=val; }
-private:
-   bool         m_valid;
-   bool         m_watch;
+  unsigned set_value(unsigned val) { return m_value = val; }
 
-   // break point
-   std::string  m_fileline;
-   unsigned     m_thread_uid;
+ private:
+  bool m_valid;
+  bool m_watch;
 
-   // watch point
-   unsigned     m_addr;
-   unsigned     m_value;
-};
+  // break point
+  std::string m_fileline;
+  unsigned m_thread_uid;
 
-extern int gpgpu_ptx_instruction_classification ;
+  // watch point
+  unsigned m_addr;
+  unsigned m_value;
+};
 
 class ptx_thread_info;
 class ptx_instruction;
-bool thread_at_brkpt( ptx_thread_info *thd_info, const class brk_pt &b );
-void hit_watchpoint( unsigned watchpoint_num, ptx_thread_info *thd, const ptx_instruction *pI );
+bool thread_at_brkpt(ptx_thread_info *thd_info, const class brk_pt &b);
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/SConscript b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/SConscript
index c105ebb586..b8eddab369 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/SConscript
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/SConscript
@@ -54,6 +54,7 @@ Source('stack.cc')
 Source('stat-tool.cc', append={'CXXFLAGS': '-Wno-error=format'})
 Source('traffic_breakdown.cc')
 Source('visualizer.cc')
+Source('hashing.cc')
 
 #Source('fq_push_m5.cc')
 
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.cc
index 192cb657de..371ed1d7ba 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.cc
@@ -26,13 +26,14 @@
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-#include "gpu-sim.h"
-#include "gpu-misc.h"
 #include "dram.h"
-#include "mem_latency_stat.h"
 #include "dram_sched.h"
-#include "mem_fetch.h"
+#include "gpu-misc.h"
+#include "gpu-sim.h"
+#include "hashing.h"
 #include "l2cache.h"
+#include "mem_fetch.h"
+#include "mem_latency_stat.h"
 
 #ifdef DRAM_VERIFY
 int PRINT_CYCLE = 0;
@@ -41,868 +42,839 @@ int PRINT_CYCLE = 0;
 template class fifo_pipeline<mem_fetch>;
 template class fifo_pipeline<dram_req_t>;
 
-dram_t::dram_t( unsigned int partition_id, const struct memory_config *config, memory_stats_t *stats,
-                memory_partition_unit *mp )
-{
-   id = partition_id;
-   m_memory_partition_unit = mp;
-   m_stats = stats;
-   m_config = config;
-
-   //rowblp
-   access_num=0;
-   hits_num=0;
-   read_num=0;
-   write_num=0;
-   hits_read_num=0;
-   hits_write_num=0;
-   banks_1time=0;
-   banks_acess_total=0;
-   banks_acess_total_after=0;
-   banks_time_ready=0;
-   banks_access_ready_total=0;
-   issued_two=0;
-   issued_total=0;
-   issued_total_row=0;
-   issued_total_col=0;
-
-   CCDc = 0;
-   RRDc = 0;
-   RTWc = 0;
-   WTRc = 0;
-
-   wasted_bw_row=0;
-   wasted_bw_col=0;
-   util_bw=0;
-   idle_bw=0;
-   RCDc_limit=0;
-   CCDLc_limit=0;
-   CCDLc_limit_alone=0;
-   CCDc_limit=0;
-   WTRc_limit=0;
-   WTRc_limit_alone=0;
-   RCDWRc_limit=0;
-   RTWc_limit=0;
-   RTWc_limit_alone=0;
-   rwq_limit=0;
-   write_to_read_ratio_blp_rw_average=0;
-   bkgrp_parallsim_rw=0;
-
-   rw = READ; //read mode is default
-
-	bkgrp = (bankgrp_t**) calloc(sizeof(bankgrp_t*), m_config->nbkgrp);
-	bkgrp[0] = (bankgrp_t*) calloc(sizeof(bank_t), m_config->nbkgrp);
-	for (unsigned i=1; i<m_config->nbkgrp; i++) {
-		bkgrp[i] = bkgrp[0] + i;
-	}
-	for (unsigned i=0; i<m_config->nbkgrp; i++) {
-		bkgrp[i]->CCDLc = 0;
-		bkgrp[i]->RTPLc = 0;
-	}
-
-   bk = (bank_t**) calloc(sizeof(bank_t*),m_config->nbk);
-   bk[0] = (bank_t*) calloc(sizeof(bank_t),m_config->nbk);
-   for (unsigned i=1;i<m_config->nbk;i++) 
-      bk[i] = bk[0] + i;
-   for (unsigned i=0;i<m_config->nbk;i++) {
-      bk[i]->state = BANK_IDLE;
-      bk[i]->bkgrpindex = i/(m_config->nbk/m_config->nbkgrp);
-   }
-   prio = 0;
-
-   rwq = new fifo_pipeline<dram_req_t>("rwq",m_config->CL,m_config->CL+1);
-   mrqq = new fifo_pipeline<dram_req_t>("mrqq",0,2);
-   returnq = new fifo_pipeline<mem_fetch>("dramreturnq",0,m_config->gpgpu_dram_return_queue_size==0?1024:m_config->gpgpu_dram_return_queue_size); 
-   m_frfcfs_scheduler = NULL;
-   if ( m_config->scheduler_type == DRAM_FRFCFS)
-      m_frfcfs_scheduler = new frfcfs_scheduler(m_config,this,stats);
-   n_cmd = 0;
-   n_activity = 0;
-   n_nop = 0; 
-   n_act = 0; 
-   n_pre = 0; 
-   n_rd = 0;
-   n_wr = 0;
-   n_wr_WB=0;
-   n_rd_L2_A=0;
-   n_req = 0;
-   max_mrqs_temp = 0;
-   bwutil = 0;
-   max_mrqs = 0;
-   ave_mrqs = 0;
-
-   for (unsigned i=0;i<10;i++) {
-      dram_util_bins[i]=0;
-      dram_eff_bins[i]=0;
-   }
-   last_n_cmd = last_n_activity = last_bwutil = 0;
-
-   n_cmd_partial = 0;
-   n_activity_partial = 0;
-   n_nop_partial = 0;  
-   n_act_partial = 0;  
-   n_pre_partial = 0;  
-   n_req_partial = 0;
-   ave_mrqs_partial = 0;
-   bwutil_partial = 0;
-
-   if ( queue_limit() )
-      mrqq_Dist = StatCreate("mrqq_length",1, queue_limit());
-   else //queue length is unlimited; 
-      mrqq_Dist = StatCreate("mrqq_length",1,64); //track up to 64 entries
-
+dram_t::dram_t(unsigned int partition_id, const memory_config *config,
+               memory_stats_t *stats, memory_partition_unit *mp,
+               gpgpu_sim *gpu) {
+  id = partition_id;
+  m_memory_partition_unit = mp;
+  m_stats = stats;
+  m_config = config;
+  m_gpu = gpu;
+
+  // rowblp
+  access_num = 0;
+  hits_num = 0;
+  read_num = 0;
+  write_num = 0;
+  hits_read_num = 0;
+  hits_write_num = 0;
+  banks_1time = 0;
+  banks_acess_total = 0;
+  banks_acess_total_after = 0;
+  banks_time_ready = 0;
+  banks_access_ready_total = 0;
+  issued_two = 0;
+  issued_total = 0;
+  issued_total_row = 0;
+  issued_total_col = 0;
+
+  CCDc = 0;
+  RRDc = 0;
+  RTWc = 0;
+  WTRc = 0;
+
+  wasted_bw_row = 0;
+  wasted_bw_col = 0;
+  util_bw = 0;
+  idle_bw = 0;
+  RCDc_limit = 0;
+  CCDLc_limit = 0;
+  CCDLc_limit_alone = 0;
+  CCDc_limit = 0;
+  WTRc_limit = 0;
+  WTRc_limit_alone = 0;
+  RCDWRc_limit = 0;
+  RTWc_limit = 0;
+  RTWc_limit_alone = 0;
+  rwq_limit = 0;
+  write_to_read_ratio_blp_rw_average = 0;
+  bkgrp_parallsim_rw = 0;
+
+  rw = READ;  // read mode is default
+
+  bkgrp = (bankgrp_t **)calloc(sizeof(bankgrp_t *), m_config->nbkgrp);
+  bkgrp[0] = (bankgrp_t *)calloc(sizeof(bank_t), m_config->nbkgrp);
+  for (unsigned i = 1; i < m_config->nbkgrp; i++) {
+    bkgrp[i] = bkgrp[0] + i;
+  }
+  for (unsigned i = 0; i < m_config->nbkgrp; i++) {
+    bkgrp[i]->CCDLc = 0;
+    bkgrp[i]->RTPLc = 0;
+  }
+
+  bk = (bank_t **)calloc(sizeof(bank_t *), m_config->nbk);
+  bk[0] = (bank_t *)calloc(sizeof(bank_t), m_config->nbk);
+  for (unsigned i = 1; i < m_config->nbk; i++) bk[i] = bk[0] + i;
+  for (unsigned i = 0; i < m_config->nbk; i++) {
+    bk[i]->state = BANK_IDLE;
+    bk[i]->bkgrpindex = i / (m_config->nbk / m_config->nbkgrp);
+  }
+  prio = 0;
+
+  rwq = new fifo_pipeline<dram_req_t>("rwq", m_config->CL, m_config->CL + 1);
+  mrqq = new fifo_pipeline<dram_req_t>("mrqq", 0, 2);
+  returnq = new fifo_pipeline<mem_fetch>(
+      "dramreturnq", 0,
+      m_config->gpgpu_dram_return_queue_size == 0
+          ? 1024
+          : m_config->gpgpu_dram_return_queue_size);
+  m_frfcfs_scheduler = NULL;
+  if (m_config->scheduler_type == DRAM_FRFCFS)
+    m_frfcfs_scheduler = new frfcfs_scheduler(m_config, this, stats);
+  n_cmd = 0;
+  n_activity = 0;
+  n_nop = 0;
+  n_act = 0;
+  n_pre = 0;
+  n_rd = 0;
+  n_wr = 0;
+  n_wr_WB = 0;
+  n_rd_L2_A = 0;
+  n_req = 0;
+  max_mrqs_temp = 0;
+  bwutil = 0;
+  max_mrqs = 0;
+  ave_mrqs = 0;
+
+  for (unsigned i = 0; i < 10; i++) {
+    dram_util_bins[i] = 0;
+    dram_eff_bins[i] = 0;
+  }
+  last_n_cmd = last_n_activity = last_bwutil = 0;
+
+  n_cmd_partial = 0;
+  n_activity_partial = 0;
+  n_nop_partial = 0;
+  n_act_partial = 0;
+  n_pre_partial = 0;
+  n_req_partial = 0;
+  ave_mrqs_partial = 0;
+  bwutil_partial = 0;
+
+  if (queue_limit())
+    mrqq_Dist = StatCreate("mrqq_length", 1, queue_limit());
+  else                                             // queue length is unlimited;
+    mrqq_Dist = StatCreate("mrqq_length", 1, 64);  // track up to 64 entries
 }
 
-bool dram_t::full(bool is_write) const
-{
-    if(m_config->scheduler_type == DRAM_FRFCFS){
-        if(m_config->gpgpu_frfcfs_dram_sched_queue_size == 0 ) return false;
-        if(m_config->seperate_write_queue_enabled){
-        	if(is_write)
-        		return m_frfcfs_scheduler->num_write_pending() >= m_config->gpgpu_frfcfs_dram_write_queue_size;
-        	else
-        		return m_frfcfs_scheduler->num_pending() >= m_config->gpgpu_frfcfs_dram_sched_queue_size;
-        }
-        else
-        	return m_frfcfs_scheduler->num_pending() >= m_config->gpgpu_frfcfs_dram_sched_queue_size;
-    }
-   else return mrqq->full();
+bool dram_t::full(bool is_write) const {
+  if (m_config->scheduler_type == DRAM_FRFCFS) {
+    if (m_config->gpgpu_frfcfs_dram_sched_queue_size == 0) return false;
+    if (m_config->seperate_write_queue_enabled) {
+      if (is_write)
+        return m_frfcfs_scheduler->num_write_pending() >=
+               m_config->gpgpu_frfcfs_dram_write_queue_size;
+      else
+        return m_frfcfs_scheduler->num_pending() >=
+               m_config->gpgpu_frfcfs_dram_sched_queue_size;
+    } else
+      return m_frfcfs_scheduler->num_pending() >=
+             m_config->gpgpu_frfcfs_dram_sched_queue_size;
+  } else
+    return mrqq->full();
 }
 
-unsigned dram_t::que_length() const
-{
-   unsigned nreqs = 0;
-   if (m_config->scheduler_type == DRAM_FRFCFS) {
-      nreqs = m_frfcfs_scheduler->num_pending();
-   } else {
-      nreqs = mrqq->get_length();
-   }
-   return nreqs;
+unsigned dram_t::que_length() const {
+  unsigned nreqs = 0;
+  if (m_config->scheduler_type == DRAM_FRFCFS) {
+    nreqs = m_frfcfs_scheduler->num_pending();
+  } else {
+    nreqs = mrqq->get_length();
+  }
+  return nreqs;
 }
 
-bool dram_t::returnq_full() const
-{
-   return returnq->full();
-}
+bool dram_t::returnq_full() const { return returnq->full(); }
 
-unsigned int dram_t::queue_limit() const 
-{ 
-   return m_config->gpgpu_frfcfs_dram_sched_queue_size; 
+unsigned int dram_t::queue_limit() const {
+  return m_config->gpgpu_frfcfs_dram_sched_queue_size;
 }
 
+dram_req_t::dram_req_t(class mem_fetch *mf, unsigned banks,
+                       unsigned dram_bnk_indexing_policy,
+                       class gpgpu_sim *gpu) {
+  txbytes = 0;
+  dqbytes = 0;
+  data = mf;
+  m_gpu = gpu;
 
-dram_req_t::dram_req_t( class mem_fetch *mf, unsigned banks, unsigned dram_bnk_indexing_policy)
-{
-   txbytes = 0;
-   dqbytes = 0;
-   data = mf;
-
-   const addrdec_t &tlx = mf->get_tlx_addr();
-
-    switch(dram_bnk_indexing_policy){
-		case LINEAR_BK_INDEX:
-		{
-			bk = tlx.bk;
-			break;
-		}
-		case BITWISE_XORING_BK_INDEX:
-		{
-			//xoring bank bits with lower bits of the page
-			int lbank = log2(banks);
-			bk  = tlx.bk ^ (tlx.row & ((1<<lbank)-1));
-			break;
-		}
-		case CUSTOM_BK_INDEX:
-	        /* No custom set function implemented */
-			//Do you custom index here
-			break;
-		default:
-			 assert("\nUndefined bank index function.\n" && 0);
-			 break;
-	}
-
-
-   row = tlx.row; 
-   col = tlx.col; 
-   nbytes = mf->get_data_size();
-
-   timestamp = gpu_tot_sim_cycle + gpu_sim_cycle;
-   addr = mf->get_addr();
-   insertion_time = (unsigned) gpu_sim_cycle;
-   rw = data->get_is_write()?WRITE:READ;
-}
+  const addrdec_t &tlx = mf->get_tlx_addr();
 
-void dram_t::push( class mem_fetch *data ) 
-{
-   assert(id == data->get_tlx_addr().chip); // Ensure request is in correct memory partition
-
-   dram_req_t *mrq = new dram_req_t(data,m_config->nbk,m_config->dram_bnk_indexing_policy);
-
-   data->set_status(IN_PARTITION_MC_INTERFACE_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-	   mrqq->push(mrq);
-
-   // stats...
-   n_req += 1;
-   n_req_partial += 1;
-   if ( m_config->scheduler_type == DRAM_FRFCFS) {
-      unsigned nreqs = m_frfcfs_scheduler->num_pending();
-      if ( nreqs > max_mrqs_temp)
-         max_mrqs_temp = nreqs;
-   } else {
-      max_mrqs_temp = (max_mrqs_temp > mrqq->get_length())? max_mrqs_temp : mrqq->get_length();
-   }
-   m_stats->memlatstat_dram_access(data);
+  switch (dram_bnk_indexing_policy) {
+    case LINEAR_BK_INDEX: {
+      bk = tlx.bk;
+      break;
+    }
+    case BITWISE_XORING_BK_INDEX: {
+      // xoring bank bits with lower bits of the page
+      bk = bitwise_hash_function(tlx.row, tlx.bk, banks);
+      assert(bk < banks);
+      break;
+    }
+    case IPOLY_BK_INDEX: {
+      /*IPOLY for bank indexing function from "Pseudo-randomly interleaved
+       * memory." Rau, B. R et al. ISCA 1991
+       * http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=348DEA37A3E440473B3C075EAABC63B6?doi=10.1.1.12.7149&rep=rep1&type=pdf
+       */
+      // xoring bank bits with lower bits of the page
+      bk = ipoly_hash_function(tlx.row, tlx.bk, banks);
+      assert(bk < banks);
+      break;
+    }
+    case CUSTOM_BK_INDEX:
+      /* No custom set function implemented */
+      // Do you custom index here
+      break;
+    default:
+      assert("\nUndefined bank index function.\n" && 0);
+      break;
+  }
+
+  row = tlx.row;
+  col = tlx.col;
+  nbytes = mf->get_data_size();
+
+  timestamp = m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle;
+  addr = mf->get_addr();
+  insertion_time = (unsigned)m_gpu->gpu_sim_cycle;
+  rw = data->get_is_write() ? WRITE : READ;
 }
 
-void dram_t::scheduler_fifo()
-{
-   if (!mrqq->empty()) {
-      unsigned int bkn;
-      dram_req_t *head_mrqq = mrqq->top();
-      head_mrqq->data->set_status(IN_PARTITION_MC_BANK_ARB_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-      bkn = head_mrqq->bk;
-      if (!bk[bkn]->mrq) 
-         bk[bkn]->mrq = mrqq->pop();
-   }
+void dram_t::push(class mem_fetch *data) {
+  assert(id == data->get_tlx_addr()
+                   .chip);  // Ensure request is in correct memory partition
+
+  dram_req_t *mrq =
+      new dram_req_t(data, m_config->nbk, m_config->dram_bnk_indexing_policy,
+                     m_memory_partition_unit->get_mgpu());
+
+  data->set_status(IN_PARTITION_MC_INTERFACE_QUEUE,
+                   m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+  mrqq->push(mrq);
+
+  // stats...
+  n_req += 1;
+  n_req_partial += 1;
+  if (m_config->scheduler_type == DRAM_FRFCFS) {
+    unsigned nreqs = m_frfcfs_scheduler->num_pending();
+    if (nreqs > max_mrqs_temp) max_mrqs_temp = nreqs;
+  } else {
+    max_mrqs_temp = (max_mrqs_temp > mrqq->get_length()) ? max_mrqs_temp
+                                                         : mrqq->get_length();
+  }
+  m_stats->memlatstat_dram_access(data);
 }
 
+void dram_t::scheduler_fifo() {
+  if (!mrqq->empty()) {
+    unsigned int bkn;
+    dram_req_t *head_mrqq = mrqq->top();
+    head_mrqq->data->set_status(
+        IN_PARTITION_MC_BANK_ARB_QUEUE,
+        m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+    bkn = head_mrqq->bk;
+    if (!bk[bkn]->mrq) bk[bkn]->mrq = mrqq->pop();
+  }
+}
 
-#define DEC2ZERO(x) x = (x)? (x-1) : 0;
-#define SWAP(a,b) a ^= b; b ^= a; a ^= b;
-
-void dram_t::cycle()
-{
+#define DEC2ZERO(x) x = (x) ? (x - 1) : 0;
+#define SWAP(a, b) \
+  a ^= b;          \
+  b ^= a;          \
+  a ^= b;
 
-   if( !returnq->full() ) {
-       dram_req_t *cmd = rwq->pop();
-       if( cmd ) {
-#ifdef DRAM_VIEWCMD 
-           printf("\tDQ: BK%d Row:%03x Col:%03x", cmd->bk, cmd->row, cmd->col + cmd->dqbytes);
+void dram_t::cycle() {
+  if (!returnq->full()) {
+    dram_req_t *cmd = rwq->pop();
+    if (cmd) {
+#ifdef DRAM_VIEWCMD
+      printf("\tDQ: BK%d Row:%03x Col:%03x", cmd->bk, cmd->row,
+             cmd->col + cmd->dqbytes);
 #endif
-           cmd->dqbytes += m_config->dram_atom_size; 
-
-           if (cmd->dqbytes >= cmd->nbytes) {
-              mem_fetch *data = cmd->data; 
-              data->set_status(IN_PARTITION_MC_RETURNQ,gpu_sim_cycle+gpu_tot_sim_cycle); 
-              if( data->get_access_type() != L1_WRBK_ACC && data->get_access_type() != L2_WRBK_ACC ) {
-                 data->set_reply();
-                 returnq->push(data);
-              } else {
-                 m_memory_partition_unit->set_done(data);
-                 delete data;
-              }
-              delete cmd;
-           }
-#ifdef DRAM_VIEWCMD 
-           printf("\n");
+      cmd->dqbytes += m_config->dram_atom_size;
+
+      if (cmd->dqbytes >= cmd->nbytes) {
+        mem_fetch *data = cmd->data;
+        data->set_status(IN_PARTITION_MC_RETURNQ,
+                         m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+        if (data->get_access_type() != L1_WRBK_ACC &&
+            data->get_access_type() != L2_WRBK_ACC) {
+          data->set_reply();
+          returnq->push(data);
+        } else {
+          m_memory_partition_unit->set_done(data);
+          delete data;
+        }
+        delete cmd;
+      }
+#ifdef DRAM_VIEWCMD
+      printf("\n");
 #endif
-       }
-   }
-
-   /* check if the upcoming request is on an idle bank */
-   /* Should we modify this so that multiple requests are checked? */
-
-   switch (m_config->scheduler_type) {
-   case DRAM_FIFO: scheduler_fifo(); break;
-   case DRAM_FRFCFS: scheduler_frfcfs(); break;
-	default:
-		printf("Error: Unknown DRAM scheduler type\n");
-		assert(0);
-   }
-   if ( m_config->scheduler_type == DRAM_FRFCFS) {
-      unsigned nreqs = m_frfcfs_scheduler->num_pending();
-      if ( nreqs > max_mrqs) {
-         max_mrqs = nreqs;
+    }
+  }
+
+  /* check if the upcoming request is on an idle bank */
+  /* Should we modify this so that multiple requests are checked? */
+
+  switch (m_config->scheduler_type) {
+    case DRAM_FIFO:
+      scheduler_fifo();
+      break;
+    case DRAM_FRFCFS:
+      scheduler_frfcfs();
+      break;
+    default:
+      printf("Error: Unknown DRAM scheduler type\n");
+      assert(0);
+  }
+  if (m_config->scheduler_type == DRAM_FRFCFS) {
+    unsigned nreqs = m_frfcfs_scheduler->num_pending();
+    if (nreqs > max_mrqs) {
+      max_mrqs = nreqs;
+    }
+    ave_mrqs += nreqs;
+    ave_mrqs_partial += nreqs;
+  } else {
+    if (mrqq->get_length() > max_mrqs) {
+      max_mrqs = mrqq->get_length();
+    }
+    ave_mrqs += mrqq->get_length();
+    ave_mrqs_partial += mrqq->get_length();
+  }
+
+  unsigned k = m_config->nbk;
+  bool issued = false;
+
+  // collect row buffer locality, BLP and other statistics
+  /////////////////////////////////////////////////////////////////////////
+  unsigned int memory_pending = 0;
+  for (unsigned i = 0; i < m_config->nbk; i++) {
+    if (bk[i]->mrq) memory_pending++;
+  }
+  banks_1time += memory_pending;
+  if (memory_pending > 0) banks_acess_total++;
+
+  unsigned int memory_pending_rw = 0;
+  unsigned read_blp_rw = 0;
+  unsigned write_blp_rw = 0;
+  std::bitset<8> bnkgrp_rw_found;  // assume max we have 8 bank groups
+
+  for (unsigned j = 0; j < m_config->nbk; j++) {
+    unsigned grp = get_bankgrp_number(j);
+    if (bk[j]->mrq &&
+        (((bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == READ) &&
+          (bk[j]->state == BANK_ACTIVE)))) {
+      memory_pending_rw++;
+      read_blp_rw++;
+      bnkgrp_rw_found.set(grp);
+    } else if (bk[j]->mrq &&
+               (((bk[j]->curr_row == bk[j]->mrq->row) &&
+                 (bk[j]->mrq->rw == WRITE) && (bk[j]->state == BANK_ACTIVE)))) {
+      memory_pending_rw++;
+      write_blp_rw++;
+      bnkgrp_rw_found.set(grp);
+    }
+  }
+  banks_time_rw += memory_pending_rw;
+  bkgrp_parallsim_rw += bnkgrp_rw_found.count();
+  if (memory_pending_rw > 0) {
+    write_to_read_ratio_blp_rw_average +=
+        (double)write_blp_rw / (write_blp_rw + read_blp_rw);
+    banks_access_rw_total++;
+  }
+
+  unsigned int memory_Pending_ready = 0;
+  for (unsigned j = 0; j < m_config->nbk; j++) {
+    unsigned grp = get_bankgrp_number(j);
+    if (bk[j]->mrq &&
+        ((!CCDc && !bk[j]->RCDc && !(bkgrp[grp]->CCDLc) &&
+          (bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == READ) &&
+          (WTRc == 0) && (bk[j]->state == BANK_ACTIVE) && !rwq->full()) ||
+         (!CCDc && !bk[j]->RCDWRc && !(bkgrp[grp]->CCDLc) &&
+          (bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == WRITE) &&
+          (RTWc == 0) && (bk[j]->state == BANK_ACTIVE) && !rwq->full()))) {
+      memory_Pending_ready++;
+    }
+  }
+  banks_time_ready += memory_Pending_ready;
+  if (memory_Pending_ready > 0) banks_access_ready_total++;
+  ///////////////////////////////////////////////////////////////////////////////////
+
+  bool issued_col_cmd = false;
+  bool issued_row_cmd = false;
+
+  if (m_config->dual_bus_interface) {
+    // dual bus interface
+    // issue one row command and one column command
+    for (unsigned i = 0; i < m_config->nbk; i++) {
+      unsigned j = (i + prio) % m_config->nbk;
+      issued_col_cmd = issue_col_command(j);
+      if (issued_col_cmd) break;
+    }
+    for (unsigned i = 0; i < m_config->nbk; i++) {
+      unsigned j = (i + prio) % m_config->nbk;
+      issued_row_cmd = issue_row_command(j);
+      if (issued_row_cmd) break;
+    }
+    for (unsigned i = 0; i < m_config->nbk; i++) {
+      unsigned j = (i + prio) % m_config->nbk;
+      if (!bk[j]->mrq) {
+        if (!CCDc && !RRDc && !RTWc && !WTRc && !bk[j]->RCDc && !bk[j]->RASc &&
+            !bk[j]->RCc && !bk[j]->RPc && !bk[j]->RCDWRc)
+          k--;
+        bk[j]->n_idle++;
       }
-      ave_mrqs += nreqs;
-      ave_mrqs_partial += nreqs;
-   } else {
-      if (mrqq->get_length() > max_mrqs) {
-         max_mrqs = mrqq->get_length();
+    }
+  } else {
+    // single bus interface
+    // issue only one row/column command
+    for (unsigned i = 0; i < m_config->nbk; i++) {
+      unsigned j = (i + prio) % m_config->nbk;
+      if (!issued_col_cmd) issued_col_cmd = issue_col_command(j);
+
+      if (!issued_col_cmd && !issued_row_cmd)
+        issued_row_cmd = issue_row_command(j);
+
+      if (!bk[j]->mrq) {
+        if (!CCDc && !RRDc && !RTWc && !WTRc && !bk[j]->RCDc && !bk[j]->RASc &&
+            !bk[j]->RCc && !bk[j]->RPc && !bk[j]->RCDWRc)
+          k--;
+        bk[j]->n_idle++;
       }
-      ave_mrqs += mrqq->get_length();
-      ave_mrqs_partial +=  mrqq->get_length();
-   }
-
-   unsigned k=m_config->nbk;
-   bool issued = false;
-
-   //collect row buffer locality, BLP and other statistics
-   /////////////////////////////////////////////////////////////////////////
-   unsigned int memory_pending=0;
-   for (unsigned i=0;i<m_config->nbk;i++) {
-	   if (bk[i]->mrq)
-		   memory_pending++;
-   }
-   banks_1time += memory_pending;
-   if(memory_pending >0)
-	   banks_acess_total++;
-
-   unsigned int memory_pending_rw=0;
-   unsigned read_blp_rw=0;
-   unsigned write_blp_rw=0;
-   std::bitset<8> bnkgrp_rw_found;  //assume max we have 8 bank groups
-
-   for (unsigned j=0;j<m_config->nbk;j++) {
-  	   unsigned grp = get_bankgrp_number(j);
-  	   if (bk[j]->mrq && (((bk[j]->curr_row == bk[j]->mrq->row) &&
-  		  (bk[j]->mrq->rw == READ)  &&
-  		  (bk[j]->state == BANK_ACTIVE))))
-  	   {
-  		    memory_pending_rw++;
-  		    read_blp_rw++;
-  		    bnkgrp_rw_found.set(grp);
-  	   }
-  	   else if
-  	        (bk[j]->mrq && (((bk[j]->curr_row == bk[j]->mrq->row)  &&
-  			 (bk[j]->mrq->rw == WRITE) &&
-  			 (bk[j]->state == BANK_ACTIVE))))
-  	   {
-  		     memory_pending_rw++;
-  		     write_blp_rw++;
-  		     bnkgrp_rw_found.set(grp);
-  	   }
-     }
-     banks_time_rw += memory_pending_rw;
-     bkgrp_parallsim_rw += bnkgrp_rw_found.count();
-     if(memory_pending_rw >0)
-     {
-    	 write_to_read_ratio_blp_rw_average += (double)write_blp_rw/(write_blp_rw+read_blp_rw);
-  	     banks_access_rw_total++;
-     }
-
-   unsigned int memory_Pending_ready=0;
-   for (unsigned j=0;j<m_config->nbk;j++) {
-	   unsigned grp = get_bankgrp_number(j);
-	   if (bk[j]->mrq && ((!CCDc && !bk[j]->RCDc &&
-		  !(bkgrp[grp]->CCDLc) &&
-		  (bk[j]->curr_row == bk[j]->mrq->row) &&
-		  (bk[j]->mrq->rw == READ) && (WTRc == 0 )  &&
-		  (bk[j]->state == BANK_ACTIVE) &&
-		  !rwq->full())
-		   ||
-		   (!CCDc && !bk[j]->RCDWRc &&
-			 !(bkgrp[grp]->CCDLc) &&
-			 (bk[j]->curr_row == bk[j]->mrq->row)  &&
-			 (bk[j]->mrq->rw == WRITE) && (RTWc == 0 )  &&
-			 (bk[j]->state == BANK_ACTIVE) &&
-			 !rwq->full())))
-	   {
-		   memory_Pending_ready++;
-	   }
-   }
-   banks_time_ready += memory_Pending_ready;
-   if(memory_Pending_ready >0)
-	   banks_access_ready_total++;
-   ///////////////////////////////////////////////////////////////////////////////////
-
-   bool issued_col_cmd = false;
-   bool issued_row_cmd = false;
-
-   if(m_config->dual_bus_interface)
-   {
-	   //dual bus interface
-	   //issue one row command and one column command
-	     for (unsigned i=0;i<m_config->nbk;i++) {
-	       unsigned j = (i + prio) % m_config->nbk;
-	  	   issued_col_cmd  = issue_col_command(j);
-	  	   if(issued_col_cmd) break;
-	     }
-	     for (unsigned i=0;i<m_config->nbk;i++) {
-	    	unsigned j = (i + prio) % m_config->nbk;
-	  	    issued_row_cmd = issue_row_command(j);
-	  	    if(issued_row_cmd) break;
-	     }
-	     for (unsigned i=0;i<m_config->nbk;i++) {
-	    	 unsigned j = (i + prio) % m_config->nbk;
-	    	 if(!bk[j]->mrq) {
-				  if (!CCDc && !RRDc && !RTWc && !WTRc && !bk[j]->RCDc && !bk[j]->RASc
-					  && !bk[j]->RCc && !bk[j]->RPc  && !bk[j]->RCDWRc) k--;
-				  bk[j]->n_idle++;
-			}
-	     }
-   }
-   else
-   {
-	   //single bus interface
-	   //issue only one row/column command
-	   for (unsigned i=0;i<m_config->nbk;i++) {
-		   unsigned j = (i + prio) % m_config->nbk;
-		   if(!issued_col_cmd)
-		       issued_col_cmd  = issue_col_command(j);
-
-		   if(!issued_col_cmd && !issued_row_cmd)
-		       issued_row_cmd = issue_row_command(j);
-
-		   if(!bk[j]->mrq) {
-		          if (!CCDc && !RRDc && !RTWc && !WTRc && !bk[j]->RCDc && !bk[j]->RASc
-		              && !bk[j]->RCc && !bk[j]->RPc  && !bk[j]->RCDWRc) k--;
-		          bk[j]->n_idle++;
-		    }
-
-	   }
-   }
-
-   issued = issued_row_cmd || issued_col_cmd;
-   if (!issued) {
-      n_nop++;
-      n_nop_partial++;
+    }
+  }
+
+  issued = issued_row_cmd || issued_col_cmd;
+  if (!issued) {
+    n_nop++;
+    n_nop_partial++;
 #ifdef DRAM_VIEWCMD
-      printf("\tNOP                        ");
+    printf("\tNOP                        ");
 #endif
-   }
-   if (k) {
-      n_activity++;
-      n_activity_partial++;
-   }
-   n_cmd++;
-   n_cmd_partial++;
-   if(issued)
-   {
-	   issued_total++;
-	   if(issued_col_cmd && issued_row_cmd)
-		   issued_two++;
-   }
-   if(issued_col_cmd)  issued_total_col++;
-   if(issued_row_cmd)  issued_total_row++;
-
-
-   //Collect some statistics
-   //check the limitation, see where BW is wasted?
-   /////////////////////////////////////////////////////////
-   unsigned int memory_pending_found=0;
-      for (unsigned i=0;i<m_config->nbk;i++) {
-   	   if (bk[i]->mrq)
-   		memory_pending_found++;
+  }
+  if (k) {
+    n_activity++;
+    n_activity_partial++;
+  }
+  n_cmd++;
+  n_cmd_partial++;
+  if (issued) {
+    issued_total++;
+    if (issued_col_cmd && issued_row_cmd) issued_two++;
+  }
+  if (issued_col_cmd) issued_total_col++;
+  if (issued_row_cmd) issued_total_row++;
+
+  // Collect some statistics
+  // check the limitation, see where BW is wasted?
+  /////////////////////////////////////////////////////////
+  unsigned int memory_pending_found = 0;
+  for (unsigned i = 0; i < m_config->nbk; i++) {
+    if (bk[i]->mrq) memory_pending_found++;
+  }
+  if (memory_pending_found > 0) banks_acess_total_after++;
+
+  bool memory_pending_rw_found = false;
+  for (unsigned j = 0; j < m_config->nbk; j++) {
+    if (bk[j]->mrq &&
+        (((bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == READ) &&
+          (bk[j]->state == BANK_ACTIVE)) ||
+         ((bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == WRITE) &&
+          (bk[j]->state == BANK_ACTIVE))))
+      memory_pending_rw_found = true;
+  }
+
+  if (issued_col_cmd || CCDc)
+    util_bw++;
+  else if (memory_pending_rw_found) {
+    wasted_bw_col++;
+    for (unsigned j = 0; j < m_config->nbk; j++) {
+      unsigned grp = get_bankgrp_number(j);
+      // read
+      if (bk[j]->mrq &&
+          (((bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == READ) &&
+            (bk[j]->state == BANK_ACTIVE)))) {
+        if (bk[j]->RCDc) RCDc_limit++;
+        if (bkgrp[grp]->CCDLc) CCDLc_limit++;
+        if (WTRc) WTRc_limit++;
+        if (CCDc) CCDc_limit++;
+        if (rwq->full()) rwq_limit++;
+        if (bkgrp[grp]->CCDLc && !WTRc) CCDLc_limit_alone++;
+        if (!bkgrp[grp]->CCDLc && WTRc) WTRc_limit_alone++;
       }
-      if(memory_pending_found>0)
-    	  banks_acess_total_after++;
-
-        bool memory_pending_rw_found=false;
-        for (unsigned j=0;j<m_config->nbk;j++) {
-     	   unsigned grp = get_bankgrp_number(j);
-     	   if (bk[j]->mrq && (((bk[j]->curr_row == bk[j]->mrq->row) &&
-     		  (bk[j]->mrq->rw == READ)  &&
-     		  (bk[j]->state == BANK_ACTIVE))
-     		   ||
-     		   (
-     			 (bk[j]->curr_row == bk[j]->mrq->row)  &&
-     			 (bk[j]->mrq->rw == WRITE) &&
-     			 (bk[j]->state == BANK_ACTIVE))))
-     		  memory_pending_rw_found=true;
-        }
-
-
-   if(issued_col_cmd  || CCDc)
-	   util_bw++;
-   else if (memory_pending_rw_found)
-   {
-	   wasted_bw_col++;
-	   for (unsigned j=0;j<m_config->nbk;j++) {
-		   unsigned grp = get_bankgrp_number(j);
-		   //read
-		   if (bk[j]->mrq && (((bk[j]->curr_row == bk[j]->mrq->row) &&
-			  (bk[j]->mrq->rw == READ)  &&
-			  (bk[j]->state == BANK_ACTIVE))))
-		   {
-			   if(bk[j]->RCDc) RCDc_limit++;
-			   if(bkgrp[grp]->CCDLc) CCDLc_limit++;
-			   if(WTRc) WTRc_limit++;
-			   if(CCDc) CCDc_limit++;
-			   if(rwq->full()) rwq_limit++;
-			   if(bkgrp[grp]->CCDLc && !WTRc) CCDLc_limit_alone++;
-			   if(!bkgrp[grp]->CCDLc && WTRc) WTRc_limit_alone++;
-		   }
-		   //write
-		   else if (bk[j]->mrq && ((bk[j]->curr_row == bk[j]->mrq->row)  &&
-				 (bk[j]->mrq->rw == WRITE) &&
-				 (bk[j]->state == BANK_ACTIVE)))
-		   {
-			   if(bk[j]->RCDWRc) RCDWRc_limit++;
-			   if(bkgrp[grp]->CCDLc) CCDLc_limit++;
-			   if(RTWc) RTWc_limit++;
-			   if(CCDc) CCDc_limit++;
-			   if(rwq->full()) rwq_limit++;
-			   if(bkgrp[grp]->CCDLc && !RTWc) CCDLc_limit_alone++;
-			   if(!bkgrp[grp]->CCDLc && RTWc) RTWc_limit_alone++;
-		   }
-	     }
-   }
-   else if (memory_pending_found)
-	   wasted_bw_row++;
-   else if (!memory_pending_found)
-  	   idle_bw++;
-   else
-	   assert(1);
-
-   /////////////////////////////////////////////////////////
-
-   // decrements counters once for each time dram_issueCMD is called
-   DEC2ZERO(RRDc);
-   DEC2ZERO(CCDc);
-   DEC2ZERO(RTWc);
-   DEC2ZERO(WTRc);
-   for (unsigned j=0;j<m_config->nbk;j++) {
-      DEC2ZERO(bk[j]->RCDc);
-      DEC2ZERO(bk[j]->RASc);
-      DEC2ZERO(bk[j]->RCc);
-      DEC2ZERO(bk[j]->RPc);
-      DEC2ZERO(bk[j]->RCDWRc);
-      DEC2ZERO(bk[j]->WTPc);
-      DEC2ZERO(bk[j]->RTPc);
-   }
-   for (unsigned j=0; j<m_config->nbkgrp; j++) {
-	   DEC2ZERO(bkgrp[j]->CCDLc);
-	   DEC2ZERO(bkgrp[j]->RTPLc);
-   }
+      // write
+      else if (bk[j]->mrq &&
+               ((bk[j]->curr_row == bk[j]->mrq->row) &&
+                (bk[j]->mrq->rw == WRITE) && (bk[j]->state == BANK_ACTIVE))) {
+        if (bk[j]->RCDWRc) RCDWRc_limit++;
+        if (bkgrp[grp]->CCDLc) CCDLc_limit++;
+        if (RTWc) RTWc_limit++;
+        if (CCDc) CCDc_limit++;
+        if (rwq->full()) rwq_limit++;
+        if (bkgrp[grp]->CCDLc && !RTWc) CCDLc_limit_alone++;
+        if (!bkgrp[grp]->CCDLc && RTWc) RTWc_limit_alone++;
+      }
+    }
+  } else if (memory_pending_found)
+    wasted_bw_row++;
+  else if (!memory_pending_found)
+    idle_bw++;
+  else
+    assert(1);
+
+  /////////////////////////////////////////////////////////
+
+  // decrements counters once for each time dram_issueCMD is called
+  DEC2ZERO(RRDc);
+  DEC2ZERO(CCDc);
+  DEC2ZERO(RTWc);
+  DEC2ZERO(WTRc);
+  for (unsigned j = 0; j < m_config->nbk; j++) {
+    DEC2ZERO(bk[j]->RCDc);
+    DEC2ZERO(bk[j]->RASc);
+    DEC2ZERO(bk[j]->RCc);
+    DEC2ZERO(bk[j]->RPc);
+    DEC2ZERO(bk[j]->RCDWRc);
+    DEC2ZERO(bk[j]->WTPc);
+    DEC2ZERO(bk[j]->RTPc);
+  }
+  for (unsigned j = 0; j < m_config->nbkgrp; j++) {
+    DEC2ZERO(bkgrp[j]->CCDLc);
+    DEC2ZERO(bkgrp[j]->RTPLc);
+  }
 
 #ifdef DRAM_VISUALIZE
-   visualize();
+  visualize();
 #endif
 }
 
-bool dram_t::issue_col_command(int j)
-{
-	bool issued = false;
-	unsigned grp = get_bankgrp_number(j);
-    if (bk[j]->mrq) { //if currently servicing a memory request
-        bk[j]->mrq->data->set_status(IN_PARTITION_DRAM,gpu_sim_cycle+gpu_tot_sim_cycle);
-       // correct row activated for a READ
-       if ( !issued && !CCDc && !bk[j]->RCDc &&
-            !(bkgrp[grp]->CCDLc) &&
-            (bk[j]->curr_row == bk[j]->mrq->row) &&
-            (bk[j]->mrq->rw == READ) && (WTRc == 0 )  &&
-            (bk[j]->state == BANK_ACTIVE) &&
-            !rwq->full() ) {
-          if (rw==WRITE) {
-             rw=READ;
-             rwq->set_min_length(m_config->CL);
-          }
-          rwq->push(bk[j]->mrq);
-          bk[j]->mrq->txbytes += m_config->dram_atom_size;
-          CCDc = m_config->tCCD;
-          bkgrp[grp]->CCDLc = m_config->tCCDL;
-          RTWc = m_config->tRTW;
-          bk[j]->RTPc = m_config->BL/m_config->data_command_freq_ratio;
-          bkgrp[grp]->RTPLc = m_config->tRTPL;
-          issued = true;
-          if(bk[j]->mrq->data->get_access_type() == L2_WR_ALLOC_R)
-        	  n_rd_L2_A++;
-          else
-                n_rd++;
-
-          bwutil += m_config->BL/m_config->data_command_freq_ratio;
-          bwutil_partial += m_config->BL/m_config->data_command_freq_ratio;
-          bk[j]->n_access++;
+bool dram_t::issue_col_command(int j) {
+  bool issued = false;
+  unsigned grp = get_bankgrp_number(j);
+  if (bk[j]->mrq) {  // if currently servicing a memory request
+    bk[j]->mrq->data->set_status(
+        IN_PARTITION_DRAM, m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+    // correct row activated for a READ
+    if (!issued && !CCDc && !bk[j]->RCDc && !(bkgrp[grp]->CCDLc) &&
+        (bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == READ) &&
+        (WTRc == 0) && (bk[j]->state == BANK_ACTIVE) && !rwq->full()) {
+      if (rw == WRITE) {
+        rw = READ;
+        rwq->set_min_length(m_config->CL);
+      }
+      rwq->push(bk[j]->mrq);
+      bk[j]->mrq->txbytes += m_config->dram_atom_size;
+      CCDc = m_config->tCCD;
+      bkgrp[grp]->CCDLc = m_config->tCCDL;
+      RTWc = m_config->tRTW;
+      bk[j]->RTPc = m_config->BL / m_config->data_command_freq_ratio;
+      bkgrp[grp]->RTPLc = m_config->tRTPL;
+      issued = true;
+      if (bk[j]->mrq->data->get_access_type() == L2_WR_ALLOC_R)
+        n_rd_L2_A++;
+      else
+        n_rd++;
+
+      bwutil += m_config->BL / m_config->data_command_freq_ratio;
+      bwutil_partial += m_config->BL / m_config->data_command_freq_ratio;
+      bk[j]->n_access++;
 
 #ifdef DRAM_VERIFY
-          PRINT_CYCLE=1;
-          printf("\tRD  Bk:%d Row:%03x Col:%03x \n",
-                 j, bk[j]->curr_row,
-                 bk[j]->mrq->col + bk[j]->mrq->txbytes - m_config->dram_atom_size);
+      PRINT_CYCLE = 1;
+      printf("\tRD  Bk:%d Row:%03x Col:%03x \n", j, bk[j]->curr_row,
+             bk[j]->mrq->col + bk[j]->mrq->txbytes - m_config->dram_atom_size);
 #endif
-          // transfer done
-          if ( !(bk[j]->mrq->txbytes < bk[j]->mrq->nbytes) ) {
-             bk[j]->mrq = NULL;
-          }
-       } else
-          // correct row activated for a WRITE
-          if ( !issued && !CCDc && !bk[j]->RCDWRc &&
-               !(bkgrp[grp]->CCDLc) &&
-               (bk[j]->curr_row == bk[j]->mrq->row)  &&
-               (bk[j]->mrq->rw == WRITE) && (RTWc == 0 )  &&
-               (bk[j]->state == BANK_ACTIVE) &&
-               !rwq->full() ) {
-          if (rw==READ) {
-             rw=WRITE;
-             rwq->set_min_length(m_config->WL);
-          }
-          rwq->push(bk[j]->mrq);
-
-          bk[j]->mrq->txbytes += m_config->dram_atom_size;
-          CCDc = m_config->tCCD;
-          bkgrp[grp]->CCDLc = m_config->tCCDL;
-          WTRc = m_config->tWTR;
-          bk[j]->WTPc = m_config->tWTP;
-          issued = true;
-
-          if(bk[j]->mrq->data->get_access_type() == L2_WRBK_ACC)
-          	n_wr_WB++;
-          else
-          	 n_wr++;
-          bwutil += m_config->BL/m_config->data_command_freq_ratio;
-          bwutil_partial += m_config->BL/m_config->data_command_freq_ratio;
+      // transfer done
+      if (!(bk[j]->mrq->txbytes < bk[j]->mrq->nbytes)) {
+        bk[j]->mrq = NULL;
+      }
+    } else
+        // correct row activated for a WRITE
+        if (!issued && !CCDc && !bk[j]->RCDWRc && !(bkgrp[grp]->CCDLc) &&
+            (bk[j]->curr_row == bk[j]->mrq->row) && (bk[j]->mrq->rw == WRITE) &&
+            (RTWc == 0) && (bk[j]->state == BANK_ACTIVE) && !rwq->full()) {
+      if (rw == READ) {
+        rw = WRITE;
+        rwq->set_min_length(m_config->WL);
+      }
+      rwq->push(bk[j]->mrq);
+
+      bk[j]->mrq->txbytes += m_config->dram_atom_size;
+      CCDc = m_config->tCCD;
+      bkgrp[grp]->CCDLc = m_config->tCCDL;
+      WTRc = m_config->tWTR;
+      bk[j]->WTPc = m_config->tWTP;
+      issued = true;
+
+      if (bk[j]->mrq->data->get_access_type() == L2_WRBK_ACC)
+        n_wr_WB++;
+      else
+        n_wr++;
+      bwutil += m_config->BL / m_config->data_command_freq_ratio;
+      bwutil_partial += m_config->BL / m_config->data_command_freq_ratio;
 #ifdef DRAM_VERIFY
-          PRINT_CYCLE=1;
-          printf("\tWR  Bk:%d Row:%03x Col:%03x \n",
-                 j, bk[j]->curr_row,
-                 bk[j]->mrq->col + bk[j]->mrq->txbytes - m_config->dram_atom_size);
+      PRINT_CYCLE = 1;
+      printf("\tWR  Bk:%d Row:%03x Col:%03x \n", j, bk[j]->curr_row,
+             bk[j]->mrq->col + bk[j]->mrq->txbytes - m_config->dram_atom_size);
 #endif
-          // transfer done
-          if ( !(bk[j]->mrq->txbytes < bk[j]->mrq->nbytes) ) {
-             bk[j]->mrq = NULL;
-          }
-       }
-
+      // transfer done
+      if (!(bk[j]->mrq->txbytes < bk[j]->mrq->nbytes)) {
+        bk[j]->mrq = NULL;
+      }
     }
+  }
 
-    return issued;
+  return issued;
 }
 
-bool dram_t::issue_row_command(int j)
-{
-	bool issued = false;
-	unsigned grp = get_bankgrp_number(j);
-    if (bk[j]->mrq) { //if currently servicing a memory request
-        bk[j]->mrq->data->set_status(IN_PARTITION_DRAM,gpu_sim_cycle+gpu_tot_sim_cycle);
-      //     bank is idle
-    //else
-  	  if ( !issued && !RRDc &&
-               (bk[j]->state == BANK_IDLE) &&
-               !bk[j]->RPc && !bk[j]->RCc) { //
+bool dram_t::issue_row_command(int j) {
+  bool issued = false;
+  unsigned grp = get_bankgrp_number(j);
+  if (bk[j]->mrq) {  // if currently servicing a memory request
+    bk[j]->mrq->data->set_status(
+        IN_PARTITION_DRAM, m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+    //     bank is idle
+    // else
+    if (!issued && !RRDc && (bk[j]->state == BANK_IDLE) && !bk[j]->RPc &&
+        !bk[j]->RCc) {  //
 #ifdef DRAM_VERIFY
-          PRINT_CYCLE=1;
-          printf("\tACT BK:%d NewRow:%03x From:%03x \n",
-                 j,bk[j]->mrq->row,bk[j]->curr_row);
+      PRINT_CYCLE = 1;
+      printf("\tACT BK:%d NewRow:%03x From:%03x \n", j, bk[j]->mrq->row,
+             bk[j]->curr_row);
 #endif
-          // activate the row with current memory request
-          bk[j]->curr_row = bk[j]->mrq->row;
-          bk[j]->state = BANK_ACTIVE;
-          RRDc = m_config->tRRD;
-          bk[j]->RCDc = m_config->tRCD;
-          bk[j]->RCDWRc = m_config->tRCDWR;
-          bk[j]->RASc = m_config->tRAS;
-          bk[j]->RCc = m_config->tRC;
-          prio = (j + 1) % m_config->nbk;
-          issued = true;
-          n_act_partial++;
-          n_act++;
-       }
-
-       else
-          // different row activated
-          if ( (!issued) &&
-               (bk[j]->curr_row != bk[j]->mrq->row) &&
-               (bk[j]->state == BANK_ACTIVE) &&
-               (!bk[j]->RASc && !bk[j]->WTPc &&
-				  !bk[j]->RTPc &&
-				  !bkgrp[grp]->RTPLc) ) {
-          // make the bank idle again
-          bk[j]->state = BANK_IDLE;
-          bk[j]->RPc = m_config->tRP;
-          prio = (j + 1) % m_config->nbk;
-          issued = true;
-          n_pre++;
-          n_pre_partial++;
+      // activate the row with current memory request
+      bk[j]->curr_row = bk[j]->mrq->row;
+      bk[j]->state = BANK_ACTIVE;
+      RRDc = m_config->tRRD;
+      bk[j]->RCDc = m_config->tRCD;
+      bk[j]->RCDWRc = m_config->tRCDWR;
+      bk[j]->RASc = m_config->tRAS;
+      bk[j]->RCc = m_config->tRC;
+      prio = (j + 1) % m_config->nbk;
+      issued = true;
+      n_act_partial++;
+      n_act++;
+    }
+
+    else
+        // different row activated
+        if ((!issued) && (bk[j]->curr_row != bk[j]->mrq->row) &&
+            (bk[j]->state == BANK_ACTIVE) &&
+            (!bk[j]->RASc && !bk[j]->WTPc && !bk[j]->RTPc &&
+             !bkgrp[grp]->RTPLc)) {
+      // make the bank idle again
+      bk[j]->state = BANK_IDLE;
+      bk[j]->RPc = m_config->tRP;
+      prio = (j + 1) % m_config->nbk;
+      issued = true;
+      n_pre++;
+      n_pre_partial++;
 #ifdef DRAM_VERIFY
-          PRINT_CYCLE=1;
-          printf("\tPRE BK:%d Row:%03x \n", j,bk[j]->curr_row);
+      PRINT_CYCLE = 1;
+      printf("\tPRE BK:%d Row:%03x \n", j, bk[j]->curr_row);
 #endif
-       }
     }
-    return issued;
+  }
+  return issued;
 }
 
-
-//if mrq is being serviced by dram, gets popped after CL latency fulfilled
-class mem_fetch* dram_t::return_queue_pop()
-{
-    return returnq->pop();
+// if mrq is being serviced by dram, gets popped after CL latency fulfilled
+class mem_fetch *dram_t::return_queue_pop() {
+  return returnq->pop();
 }
 
-class mem_fetch* dram_t::return_queue_top()
-{
-    return returnq->top();
+class mem_fetch *dram_t::return_queue_top() {
+  return returnq->top();
 }
 
-
-void dram_t::print( FILE* simFile) const
-{
-   unsigned i;
-   fprintf(simFile,"DRAM[%d]: %d bks, busW=%d BL=%d CL=%d, ", 
-           id, m_config->nbk, m_config->busW, m_config->BL, m_config->CL );
-   fprintf(simFile,"tRRD=%d tCCD=%d, tRCD=%d tRAS=%d tRP=%d tRC=%d\n",
-           m_config->tRRD, m_config->tCCD, m_config->tRCD, m_config->tRAS, m_config->tRP, m_config->tRC );
-   fprintf(simFile,"n_cmd=%llu n_nop=%llu n_act=%llu n_pre=%llu n_ref_event=%llu n_req=%llu n_rd=%llu n_rd_L2_A=%llu n_write=%llu n_wr_bk=%llu bw_util=%.4g\n",
-           n_cmd, n_nop, n_act, n_pre, n_ref, n_req, n_rd, n_rd_L2_A, n_wr, n_wr_WB,
-           (float)bwutil/n_cmd);
-   fprintf(simFile,"n_activity=%llu dram_eff=%.4g\n",
-           n_activity, (float)bwutil/n_activity);
-   for (i=0;i<m_config->nbk;i++) {
-      fprintf(simFile, "bk%d: %da %di ",i,bk[i]->n_access,bk[i]->n_idle);
-   }
-   fprintf(simFile, "\n");
-   fprintf(simFile, "\n------------------------------------------------------------------------\n");
-
-   printf("\nRow_Buffer_Locality = %.6f", (float)hits_num / access_num);
-   printf("\nRow_Buffer_Locality_read = %.6f", (float)hits_read_num / read_num);
-   printf("\nRow_Buffer_Locality_write = %.6f", (float)hits_write_num / write_num);
-   printf("\nBank_Level_Parallism = %.6f", (float)banks_1time / banks_acess_total);
-   printf("\nBank_Level_Parallism_Col = %.6f", (float)banks_time_rw / banks_access_rw_total);
-   printf("\nBank_Level_Parallism_Ready = %.6f", (float)banks_time_ready /banks_access_ready_total);
-   printf("\nwrite_to_read_ratio_blp_rw_average = %.6f", write_to_read_ratio_blp_rw_average /banks_access_rw_total);
-   printf("\nGrpLevelPara = %.6f \n", (float)bkgrp_parallsim_rw /banks_access_rw_total);
-
-   printf("\nBW Util details:\n");
-   printf("bwutil = %.6f \n", (float)bwutil/n_cmd);
-   printf("total_CMD = %llu \n", n_cmd);
-   printf("util_bw = %llu \n", util_bw);
-   printf("Wasted_Col = %llu \n", wasted_bw_col);
-   printf("Wasted_Row = %llu \n", wasted_bw_row);
-   printf("Idle = %llu \n", idle_bw);
-
-   printf("\nBW Util Bottlenecks: \n");
-   printf("RCDc_limit = %llu \n", RCDc_limit);
-   printf("RCDWRc_limit = %llu \n", RCDWRc_limit);
-   printf("WTRc_limit = %llu \n", WTRc_limit);
-   printf("RTWc_limit = %llu \n", RTWc_limit);
-   printf("CCDLc_limit = %llu \n", CCDLc_limit);
-   printf("rwq = %llu \n", rwq_limit);
-   printf("CCDLc_limit_alone = %llu \n", CCDLc_limit_alone);
-   printf("WTRc_limit_alone = %llu \n", WTRc_limit_alone);
-   printf("RTWc_limit_alone = %llu \n", RTWc_limit_alone);
-
-   printf("\nCommands details: \n");
-   printf("total_CMD = %llu \n", n_cmd);
-   printf("n_nop = %llu \n", n_nop);
-   printf("Read = %llu \n", n_rd);
-   printf("Write = %llu \n",n_wr);
-   printf("L2_Alloc = %llu \n", n_rd_L2_A);
-   printf("L2_WB = %llu \n", n_wr_WB);
-   printf("n_act = %llu \n", n_act);
-   printf("n_pre = %llu \n", n_pre);
-   printf("n_ref = %llu \n", n_ref);
-   printf("n_req = %llu \n", n_req );
-   printf("total_req = %llu \n", n_rd+n_wr+n_rd_L2_A+n_wr_WB);
-
-   printf("\nDual Bus Interface Util: \n");
-   printf("issued_total_row = %llu \n", issued_total_row);
-   printf("issued_total_col = %llu \n", issued_total_col);
-   printf("Row_Bus_Util =  %.6f \n", (float)issued_total_row / n_cmd);
-   printf("CoL_Bus_Util = %.6f \n", (float)issued_total_col / n_cmd);
-   printf("Either_Row_CoL_Bus_Util = %.6f \n",  (float)issued_total / n_cmd);
-   printf("Issued_on_Two_Bus_Simul_Util = %.6f \n",  (float)issued_two /n_cmd);
-   printf("issued_two_Eff = %.6f \n",  (float)issued_two /issued_total);
-   printf("queue_avg = %.6f \n\n", (float)ave_mrqs/n_cmd );
-
-   fprintf(simFile, "\n");
-   fprintf(simFile, "dram_util_bins:");
-   for (i=0;i<10;i++) fprintf(simFile, " %d", dram_util_bins[i]);
-   fprintf(simFile, "\ndram_eff_bins:");
-   for (i=0;i<10;i++) fprintf(simFile, " %d", dram_eff_bins[i]);
-   fprintf(simFile, "\n");
-   if(m_config->scheduler_type== DRAM_FRFCFS)
-       fprintf(simFile, "mrqq: max=%d avg=%g\n", max_mrqs, (float)ave_mrqs/n_cmd);
+void dram_t::print(FILE *simFile) const {
+  unsigned i;
+  fprintf(simFile, "DRAM[%d]: %d bks, busW=%d BL=%d CL=%d, ", id, m_config->nbk,
+          m_config->busW, m_config->BL, m_config->CL);
+  fprintf(simFile, "tRRD=%d tCCD=%d, tRCD=%d tRAS=%d tRP=%d tRC=%d\n",
+          m_config->tRRD, m_config->tCCD, m_config->tRCD, m_config->tRAS,
+          m_config->tRP, m_config->tRC);
+  fprintf(
+      simFile,
+      "n_cmd=%llu n_nop=%llu n_act=%llu n_pre=%llu n_ref_event=%llu n_req=%llu "
+      "n_rd=%llu n_rd_L2_A=%llu n_write=%llu n_wr_bk=%llu bw_util=%.4g\n",
+      n_cmd, n_nop, n_act, n_pre, n_ref, n_req, n_rd, n_rd_L2_A, n_wr, n_wr_WB,
+      (float)bwutil / n_cmd);
+  fprintf(simFile, "n_activity=%llu dram_eff=%.4g\n", n_activity,
+          (float)bwutil / n_activity);
+  for (i = 0; i < m_config->nbk; i++) {
+    fprintf(simFile, "bk%d: %da %di ", i, bk[i]->n_access, bk[i]->n_idle);
+  }
+  fprintf(simFile, "\n");
+  fprintf(simFile,
+          "\n------------------------------------------------------------------"
+          "------\n");
+
+  printf("\nRow_Buffer_Locality = %.6f", (float)hits_num / access_num);
+  printf("\nRow_Buffer_Locality_read = %.6f", (float)hits_read_num / read_num);
+  printf("\nRow_Buffer_Locality_write = %.6f",
+         (float)hits_write_num / write_num);
+  printf("\nBank_Level_Parallism = %.6f",
+         (float)banks_1time / banks_acess_total);
+  printf("\nBank_Level_Parallism_Col = %.6f",
+         (float)banks_time_rw / banks_access_rw_total);
+  printf("\nBank_Level_Parallism_Ready = %.6f",
+         (float)banks_time_ready / banks_access_ready_total);
+  printf("\nwrite_to_read_ratio_blp_rw_average = %.6f",
+         write_to_read_ratio_blp_rw_average / banks_access_rw_total);
+  printf("\nGrpLevelPara = %.6f \n",
+         (float)bkgrp_parallsim_rw / banks_access_rw_total);
+
+  printf("\nBW Util details:\n");
+  printf("bwutil = %.6f \n", (float)bwutil / n_cmd);
+  printf("total_CMD = %llu \n", n_cmd);
+  printf("util_bw = %llu \n", util_bw);
+  printf("Wasted_Col = %llu \n", wasted_bw_col);
+  printf("Wasted_Row = %llu \n", wasted_bw_row);
+  printf("Idle = %llu \n", idle_bw);
+
+  printf("\nBW Util Bottlenecks: \n");
+  printf("RCDc_limit = %llu \n", RCDc_limit);
+  printf("RCDWRc_limit = %llu \n", RCDWRc_limit);
+  printf("WTRc_limit = %llu \n", WTRc_limit);
+  printf("RTWc_limit = %llu \n", RTWc_limit);
+  printf("CCDLc_limit = %llu \n", CCDLc_limit);
+  printf("rwq = %llu \n", rwq_limit);
+  printf("CCDLc_limit_alone = %llu \n", CCDLc_limit_alone);
+  printf("WTRc_limit_alone = %llu \n", WTRc_limit_alone);
+  printf("RTWc_limit_alone = %llu \n", RTWc_limit_alone);
+
+  printf("\nCommands details: \n");
+  printf("total_CMD = %llu \n", n_cmd);
+  printf("n_nop = %llu \n", n_nop);
+  printf("Read = %llu \n", n_rd);
+  printf("Write = %llu \n", n_wr);
+  printf("L2_Alloc = %llu \n", n_rd_L2_A);
+  printf("L2_WB = %llu \n", n_wr_WB);
+  printf("n_act = %llu \n", n_act);
+  printf("n_pre = %llu \n", n_pre);
+  printf("n_ref = %llu \n", n_ref);
+  printf("n_req = %llu \n", n_req);
+  printf("total_req = %llu \n", n_rd + n_wr + n_rd_L2_A + n_wr_WB);
+
+  printf("\nDual Bus Interface Util: \n");
+  printf("issued_total_row = %llu \n", issued_total_row);
+  printf("issued_total_col = %llu \n", issued_total_col);
+  printf("Row_Bus_Util =  %.6f \n", (float)issued_total_row / n_cmd);
+  printf("CoL_Bus_Util = %.6f \n", (float)issued_total_col / n_cmd);
+  printf("Either_Row_CoL_Bus_Util = %.6f \n", (float)issued_total / n_cmd);
+  printf("Issued_on_Two_Bus_Simul_Util = %.6f \n", (float)issued_two / n_cmd);
+  printf("issued_two_Eff = %.6f \n", (float)issued_two / issued_total);
+  printf("queue_avg = %.6f \n\n", (float)ave_mrqs / n_cmd);
+
+  fprintf(simFile, "\n");
+  fprintf(simFile, "dram_util_bins:");
+  for (i = 0; i < 10; i++) fprintf(simFile, " %d", dram_util_bins[i]);
+  fprintf(simFile, "\ndram_eff_bins:");
+  for (i = 0; i < 10; i++) fprintf(simFile, " %d", dram_eff_bins[i]);
+  fprintf(simFile, "\n");
+  if (m_config->scheduler_type == DRAM_FRFCFS)
+    fprintf(simFile, "mrqq: max=%d avg=%g\n", max_mrqs,
+            (float)ave_mrqs / n_cmd);
 }
 
-void dram_t::visualize() const
-{
-   printf("RRDc=%d CCDc=%d mrqq.Length=%d rwq.Length=%d\n", 
-          RRDc, CCDc, mrqq->get_length(),rwq->get_length());
-   for (unsigned i=0;i<m_config->nbk;i++) {
-      printf("BK%d: state=%c curr_row=%03x, %2d %2d %2d %2d %p ", 
-             i, bk[i]->state, bk[i]->curr_row,
-             bk[i]->RCDc, bk[i]->RASc,
-             bk[i]->RPc, bk[i]->RCc,
-             bk[i]->mrq );
-      if (bk[i]->mrq)
-         printf("txf: %d %d", bk[i]->mrq->nbytes, bk[i]->mrq->txbytes);
-      printf("\n");
-   }
-   if ( m_frfcfs_scheduler ) 
-      m_frfcfs_scheduler->print(stdout);
+void dram_t::visualize() const {
+  printf("RRDc=%d CCDc=%d mrqq.Length=%d rwq.Length=%d\n", RRDc, CCDc,
+         mrqq->get_length(), rwq->get_length());
+  for (unsigned i = 0; i < m_config->nbk; i++) {
+    printf("BK%d: state=%c curr_row=%03x, %2d %2d %2d %2d %p ", i, bk[i]->state,
+           bk[i]->curr_row, bk[i]->RCDc, bk[i]->RASc, bk[i]->RPc, bk[i]->RCc,
+           bk[i]->mrq);
+    if (bk[i]->mrq)
+      printf("txf: %d %d", bk[i]->mrq->nbytes, bk[i]->mrq->txbytes);
+    printf("\n");
+  }
+  if (m_frfcfs_scheduler) m_frfcfs_scheduler->print(stdout);
 }
 
-void dram_t::print_stat( FILE* simFile ) 
-{
-   fprintf(simFile,"DRAM (%llu): n_cmd=%llu n_nop=%llu n_act=%llu n_pre=%llu n_ref=%llu n_req=%llu n_rd=%llu n_write=%llu bw_util=%.4g ",
-           id, n_cmd, n_nop, n_act, n_pre, n_ref, n_req, n_rd, n_wr,
-           (float)bwutil/n_cmd);
-   fprintf(simFile, "mrqq: %d %.4g mrqsmax=%d ", max_mrqs, (float)ave_mrqs/n_cmd, max_mrqs_temp);
-   fprintf(simFile, "\n");
-   fprintf(simFile, "dram_util_bins:");
-   for (unsigned i=0;i<10;i++) fprintf(simFile, " %d", dram_util_bins[i]);
-   fprintf(simFile, "\ndram_eff_bins:");
-   for (unsigned i=0;i<10;i++) fprintf(simFile, " %d", dram_eff_bins[i]);
-   fprintf(simFile, "\n");
-   max_mrqs_temp = 0;
+void dram_t::print_stat(FILE *simFile) {
+  fprintf(simFile,
+          "DRAM (%u): n_cmd=%llu n_nop=%llu n_act=%llu n_pre=%llu n_ref=%llu "
+          "n_req=%llu n_rd=%llu n_write=%llu bw_util=%.4g ",
+          id, n_cmd, n_nop, n_act, n_pre, n_ref, n_req, n_rd, n_wr,
+          (float)bwutil / n_cmd);
+  fprintf(simFile, "mrqq: %d %.4g mrqsmax=%llu ", max_mrqs,
+          (float)ave_mrqs / n_cmd, max_mrqs_temp);
+  fprintf(simFile, "\n");
+  fprintf(simFile, "dram_util_bins:");
+  for (unsigned i = 0; i < 10; i++) fprintf(simFile, " %d", dram_util_bins[i]);
+  fprintf(simFile, "\ndram_eff_bins:");
+  for (unsigned i = 0; i < 10; i++) fprintf(simFile, " %d", dram_eff_bins[i]);
+  fprintf(simFile, "\n");
+  max_mrqs_temp = 0;
 }
 
-void dram_t::visualizer_print( gzFile visualizer_file )
-{
-   // dram specific statistics
-   gzprintf(visualizer_file,"dramncmd: %u %u\n",id, n_cmd_partial);  
-   gzprintf(visualizer_file,"dramnop: %u %u\n",id,n_nop_partial);
-   gzprintf(visualizer_file,"dramnact: %u %u\n",id,n_act_partial);
-   gzprintf(visualizer_file,"dramnpre: %u %u\n",id,n_pre_partial);
-   gzprintf(visualizer_file,"dramnreq: %u %u\n",id,n_req_partial);
-   gzprintf(visualizer_file,"dramavemrqs: %u %u\n",id,
-            n_cmd_partial?(ave_mrqs_partial/n_cmd_partial ):0);
-
-   // utilization and efficiency
-   gzprintf(visualizer_file,"dramutil: %u %u\n",  
-            id,n_cmd_partial?100*bwutil_partial/n_cmd_partial:0);
-   gzprintf(visualizer_file,"drameff: %u %u\n", 
-            id,n_activity_partial?100*bwutil_partial/n_activity_partial:0);
-
-   // reset for next interval
-   bwutil_partial = 0;
-   n_activity_partial = 0;
-   ave_mrqs_partial = 0; 
-   n_cmd_partial = 0;
-   n_nop_partial = 0;
-   n_act_partial = 0;
-   n_pre_partial = 0;
-   n_req_partial = 0;
-
-
-   // dram access type classification
-   for (unsigned j = 0; j < m_config->nbk; j++) {
-      gzprintf(visualizer_file,"dramglobal_acc_r: %u %u %u\n", id, j, 
-               m_stats->mem_access_type_stats[GLOBAL_ACC_R][id][j]);
-      gzprintf(visualizer_file,"dramglobal_acc_w: %u %u %u\n", id, j, 
-               m_stats->mem_access_type_stats[GLOBAL_ACC_W][id][j]);
-      gzprintf(visualizer_file,"dramlocal_acc_r: %u %u %u\n", id, j, 
-               m_stats->mem_access_type_stats[LOCAL_ACC_R][id][j]);
-      gzprintf(visualizer_file,"dramlocal_acc_w: %u %u %u\n", id, j, 
-               m_stats->mem_access_type_stats[LOCAL_ACC_W][id][j]);
-      gzprintf(visualizer_file,"dramconst_acc_r: %u %u %u\n", id, j, 
-               m_stats->mem_access_type_stats[CONST_ACC_R][id][j]);
-      gzprintf(visualizer_file,"dramtexture_acc_r: %u %u %u\n", id, j, 
-               m_stats->mem_access_type_stats[TEXTURE_ACC_R][id][j]);
-   }
+void dram_t::visualizer_print(gzFile visualizer_file) {
+  // dram specific statistics
+  gzprintf(visualizer_file, "dramncmd: %u %u\n", id, n_cmd_partial);
+  gzprintf(visualizer_file, "dramnop: %u %u\n", id, n_nop_partial);
+  gzprintf(visualizer_file, "dramnact: %u %u\n", id, n_act_partial);
+  gzprintf(visualizer_file, "dramnpre: %u %u\n", id, n_pre_partial);
+  gzprintf(visualizer_file, "dramnreq: %u %u\n", id, n_req_partial);
+  gzprintf(visualizer_file, "dramavemrqs: %u %u\n", id,
+           n_cmd_partial ? (ave_mrqs_partial / n_cmd_partial) : 0);
+
+  // utilization and efficiency
+  gzprintf(visualizer_file, "dramutil: %u %u\n", id,
+           n_cmd_partial ? 100 * bwutil_partial / n_cmd_partial : 0);
+  gzprintf(visualizer_file, "drameff: %u %u\n", id,
+           n_activity_partial ? 100 * bwutil_partial / n_activity_partial : 0);
+
+  // reset for next interval
+  bwutil_partial = 0;
+  n_activity_partial = 0;
+  ave_mrqs_partial = 0;
+  n_cmd_partial = 0;
+  n_nop_partial = 0;
+  n_act_partial = 0;
+  n_pre_partial = 0;
+  n_req_partial = 0;
+
+  // dram access type classification
+  for (unsigned j = 0; j < m_config->nbk; j++) {
+    gzprintf(visualizer_file, "dramglobal_acc_r: %u %u %u\n", id, j,
+             m_stats->mem_access_type_stats[GLOBAL_ACC_R][id][j]);
+    gzprintf(visualizer_file, "dramglobal_acc_w: %u %u %u\n", id, j,
+             m_stats->mem_access_type_stats[GLOBAL_ACC_W][id][j]);
+    gzprintf(visualizer_file, "dramlocal_acc_r: %u %u %u\n", id, j,
+             m_stats->mem_access_type_stats[LOCAL_ACC_R][id][j]);
+    gzprintf(visualizer_file, "dramlocal_acc_w: %u %u %u\n", id, j,
+             m_stats->mem_access_type_stats[LOCAL_ACC_W][id][j]);
+    gzprintf(visualizer_file, "dramconst_acc_r: %u %u %u\n", id, j,
+             m_stats->mem_access_type_stats[CONST_ACC_R][id][j]);
+    gzprintf(visualizer_file, "dramtexture_acc_r: %u %u %u\n", id, j,
+             m_stats->mem_access_type_stats[TEXTURE_ACC_R][id][j]);
+  }
 }
 
-
-void dram_t::set_dram_power_stats(	unsigned &cmd,
-									unsigned &activity,
-									unsigned &nop,
-									unsigned &act,
-									unsigned &pre,
-									unsigned &rd,
-									unsigned &wr,
-									unsigned &req) const{
-
-	// Point power performance counters to low-level DRAM counters
-	cmd = n_cmd;
-	activity = n_activity;
-	nop = n_nop;
-	act = n_act;
-	pre = n_pre;
-	rd = n_rd;
-	wr = n_wr;
-	req = n_req;
+void dram_t::set_dram_power_stats(unsigned &cmd, unsigned &activity,
+                                  unsigned &nop, unsigned &act, unsigned &pre,
+                                  unsigned &rd, unsigned &wr, unsigned &wr_WB,
+                                  unsigned &req) const {
+  // Point power performance counters to low-level DRAM counters
+  cmd = n_cmd;
+  activity = n_activity;
+  nop = n_nop;
+  act = n_act;
+  pre = n_pre;
+  rd = n_rd;
+  wr = n_wr;
+  wr_WB = n_wr_WB;
+  req = n_req;
 }
 
-unsigned dram_t::get_bankgrp_number(unsigned i)
-{
-	if(m_config->dram_bnkgrp_indexing_policy == HIGHER_BITS) { //higher bits
-		return i>>m_config->bk_tag_length;
-	}
-	else if (m_config->dram_bnkgrp_indexing_policy == LOWER_BITS) { //lower bits
-		return i&((m_config->nbkgrp-1));
-	}
-	else {
-		assert(1);
-	}
+unsigned dram_t::get_bankgrp_number(unsigned i) {
+  if (m_config->dram_bnkgrp_indexing_policy == HIGHER_BITS) {  // higher bits
+    return i >> m_config->bk_tag_length;
+  } else if (m_config->dram_bnkgrp_indexing_policy ==
+             LOWER_BITS) {  // lower bits
+    return i & ((m_config->nbkgrp - 1));
+  } else {
+    assert(1);
+  }
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.h
index d79eed1702..d9a1bbe1e0 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram.h
@@ -29,224 +29,218 @@
 #ifndef DRAM_H
 #define DRAM_H
 
-#include "delayqueue.h"
-#include <set>
-#include <vector>
+#include <stdio.h>
+#include <stdlib.h>
+#include <zlib.h>
 #include <bitset>
+#include <fstream>
+#include <iomanip>
+#include <set>
 #include <sstream>
 #include <string>
-#include <fstream>
-#include <zlib.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include<iomanip>
+#include <vector>
+#include "delayqueue.h"
 
-#define READ 'R'  //define read and write states
+#define READ 'R'  // define read and write states
 #define WRITE 'W'
 #define BANK_IDLE 'I'
 #define BANK_ACTIVE 'A'
 
 class dram_req_t {
-public:
-   dram_req_t( class mem_fetch *data , unsigned banks, unsigned dram_bnk_indexing_policy);
-
-   unsigned int row;
-   unsigned int col;
-   unsigned int bk;
-   unsigned int nbytes;
-   unsigned int txbytes;
-   unsigned int dqbytes;
-   unsigned int age;
-   unsigned int timestamp;
-   unsigned char rw;    //is the request a read or a write?
-   unsigned long long int addr;
-   unsigned int insertion_time;
-   class mem_fetch * data;
+ public:
+  dram_req_t(class mem_fetch *data, unsigned banks,
+             unsigned dram_bnk_indexing_policy, class gpgpu_sim *gpu);
+
+  unsigned int row;
+  unsigned int col;
+  unsigned int bk;
+  unsigned int nbytes;
+  unsigned int txbytes;
+  unsigned int dqbytes;
+  unsigned int age;
+  unsigned int timestamp;
+  unsigned char rw;  // is the request a read or a write?
+  unsigned long long int addr;
+  unsigned int insertion_time;
+  class mem_fetch *data;
+  class gpgpu_sim *m_gpu;
 };
 
-struct bankgrp_t
-{
-	unsigned int CCDLc;
-	unsigned int RTPLc;
+struct bankgrp_t {
+  unsigned int CCDLc;
+  unsigned int RTPLc;
 };
 
-struct bank_t
-{
-   unsigned int RCDc;
-   unsigned int RCDWRc;
-   unsigned int RASc;
-   unsigned int RPc;
-   unsigned int RCc;
-   unsigned int WTPc; // write to precharge
-   unsigned int RTPc; // read to precharge
+struct bank_t {
+  unsigned int RCDc;
+  unsigned int RCDWRc;
+  unsigned int RASc;
+  unsigned int RPc;
+  unsigned int RCc;
+  unsigned int WTPc;  // write to precharge
+  unsigned int RTPc;  // read to precharge
 
-   unsigned char rw;    //is the bank reading or writing?
-   unsigned char state; //is the bank active or idle?
-   unsigned int curr_row;
+  unsigned char rw;     // is the bank reading or writing?
+  unsigned char state;  // is the bank active or idle?
+  unsigned int curr_row;
 
-   dram_req_t *mrq;
+  dram_req_t *mrq;
 
-   unsigned int n_access;
-   unsigned int n_writes;
-   unsigned int n_idle;
+  unsigned int n_access;
+  unsigned int n_writes;
+  unsigned int n_idle;
 
-   unsigned int bkgrpindex;
+  unsigned int bkgrpindex;
 };
 
-enum bank_index_function{
-	LINEAR_BK_INDEX = 0,
-	BITWISE_XORING_BK_INDEX,
-    CUSTOM_BK_INDEX
+enum bank_index_function {
+  LINEAR_BK_INDEX = 0,
+  BITWISE_XORING_BK_INDEX,
+  IPOLY_BK_INDEX,
+  CUSTOM_BK_INDEX
 };
 
-enum bank_grp_bits_position{
-	HIGHER_BITS = 0,
-	LOWER_BITS
-};
+enum bank_grp_bits_position { HIGHER_BITS = 0, LOWER_BITS };
 
 class mem_fetch;
-
-class dram_t 
-{
-public:
-   dram_t( unsigned int parition_id, const struct memory_config *config, class memory_stats_t *stats, 
-           class memory_partition_unit *mp );
-
-   bool full(bool is_write) const;
-   void print( FILE* simFile ) const;
-   void visualize() const;
-   void print_stat( FILE* simFile );
-   unsigned que_length() const; 
-   bool returnq_full() const;
-   unsigned int queue_limit() const;
-   void visualizer_print( gzFile visualizer_file );
-
-   class mem_fetch* return_queue_pop();
-   class mem_fetch* return_queue_top();
-
-   void push( class mem_fetch *data );
-   void cycle();
-   void dram_log (int task);
-
-   class memory_partition_unit *m_memory_partition_unit;
-   unsigned int id;
-
-   // Power Model
-   void set_dram_power_stats(unsigned &cmd,
-								unsigned &activity,
-								unsigned &nop,
-								unsigned &act,
-								unsigned &pre,
-								unsigned &rd,
-								unsigned &wr,
-								unsigned &req) const;
-
-
-
-   const struct memory_config *m_config;
-
-private:
-   bankgrp_t **bkgrp;
-
-   bank_t **bk;
-   unsigned int prio;
-
-   unsigned get_bankgrp_number(unsigned i);
-
-   void scheduler_fifo();
-   void scheduler_frfcfs();
-
-   bool issue_col_command(int j);
-   bool issue_row_command(int j);
-
-   unsigned int RRDc;
-   unsigned int CCDc;
-   unsigned int RTWc;   //read to write penalty applies across banks
-   unsigned int WTRc;   //write to read penalty applies across banks
-
-   unsigned char rw; //was last request a read or write? (important for RTW, WTR)
-
-   unsigned int pending_writes;
-
-   fifo_pipeline<dram_req_t> *rwq;
-   fifo_pipeline<dram_req_t> *mrqq;
-   //buffer to hold packets when DRAM processing is over
-   //should be filled with dram clock and popped with l2or icnt clock
-   fifo_pipeline<mem_fetch> *returnq;
-
-   unsigned int dram_util_bins[10];
-   unsigned int dram_eff_bins[10];
-   unsigned int last_n_cmd, last_n_activity, last_bwutil;
-
-   unsigned long long n_cmd;
-   unsigned long long n_activity;
-   unsigned long long n_nop;
-   unsigned long long n_act;
-   unsigned long long n_pre;
-   unsigned long long n_ref;
-   unsigned long long n_rd;
-   unsigned long long n_rd_L2_A;
-   unsigned long long n_wr;
-   unsigned long long n_wr_WB;
-   unsigned long long n_req;
-   unsigned long long max_mrqs_temp;
-
-   //some statistics to see where BW is wasted?
-   unsigned long long wasted_bw_row;
-   unsigned long long wasted_bw_col;
-   unsigned long long util_bw;
-   unsigned long long idle_bw;
-   unsigned long long RCDc_limit;
-   unsigned long long CCDLc_limit;
-   unsigned long long CCDLc_limit_alone;
-   unsigned long long CCDc_limit;
-   unsigned long long WTRc_limit;
-   unsigned long long WTRc_limit_alone;
-   unsigned long long RCDWRc_limit;
-   unsigned long long RTWc_limit;
-   unsigned long long RTWc_limit_alone;
-   unsigned long long rwq_limit;
-
-   //row locality, BLP and other statistics
-   unsigned long long access_num;
-   unsigned long long read_num;
-   unsigned long long write_num;
-   unsigned long long hits_num;
-   unsigned long long hits_read_num;
-   unsigned long long hits_write_num;
-   unsigned long long banks_1time;
-   unsigned long long banks_acess_total;
-   unsigned long long banks_acess_total_after;
-   unsigned long long banks_time_rw;
-   unsigned long long banks_access_rw_total;
-   unsigned long long banks_time_ready;
-   unsigned long long banks_access_ready_total;
-   unsigned long long issued_two;
-   unsigned long long issued_total;
-   unsigned long long issued_total_row;
-   unsigned long long issued_total_col;
-   double write_to_read_ratio_blp_rw_average;
-   unsigned long long bkgrp_parallsim_rw;
-
-   unsigned int bwutil;
-   unsigned int max_mrqs;
-   unsigned int ave_mrqs;
-
-   class frfcfs_scheduler* m_frfcfs_scheduler;
-
-   unsigned int n_cmd_partial;
-   unsigned int n_activity_partial;
-   unsigned int n_nop_partial; 
-   unsigned int n_act_partial; 
-   unsigned int n_pre_partial; 
-   unsigned int n_req_partial;
-   unsigned int ave_mrqs_partial;
-   unsigned int bwutil_partial;
-
-   class memory_stats_t *m_stats;
-   class Stats_gpgpu* mrqq_Dist; //memory request queue inside DRAM  
-
-   friend class frfcfs_scheduler;
+class memory_config;
+
+class dram_t {
+ public:
+  dram_t(unsigned int parition_id, const memory_config *config,
+         class memory_stats_t *stats, class memory_partition_unit *mp,
+         class gpgpu_sim *gpu);
+
+  bool full(bool is_write) const;
+  void print(FILE *simFile) const;
+  void visualize() const;
+  void print_stat(FILE *simFile);
+  unsigned que_length() const;
+  bool returnq_full() const;
+  unsigned int queue_limit() const;
+  void visualizer_print(gzFile visualizer_file);
+
+  class mem_fetch *return_queue_pop();
+  class mem_fetch *return_queue_top();
+
+  void push(class mem_fetch *data);
+  void cycle();
+  void dram_log(int task);
+
+  class memory_partition_unit *m_memory_partition_unit;
+  class gpgpu_sim *m_gpu;
+  unsigned int id;
+
+  // Power Model
+  void set_dram_power_stats(unsigned &cmd, unsigned &activity, unsigned &nop,
+                            unsigned &act, unsigned &pre, unsigned &rd,
+                            unsigned &wr, unsigned &wr_WB, unsigned &req) const;
+
+  const memory_config *m_config;
+
+ private:
+  bankgrp_t **bkgrp;
+
+  bank_t **bk;
+  unsigned int prio;
+
+  unsigned get_bankgrp_number(unsigned i);
+
+  void scheduler_fifo();
+  void scheduler_frfcfs();
+
+  bool issue_col_command(int j);
+  bool issue_row_command(int j);
+
+  unsigned int RRDc;
+  unsigned int CCDc;
+  unsigned int RTWc;  // read to write penalty applies across banks
+  unsigned int WTRc;  // write to read penalty applies across banks
+
+  unsigned char
+      rw;  // was last request a read or write? (important for RTW, WTR)
+
+  unsigned int pending_writes;
+
+  fifo_pipeline<dram_req_t> *rwq;
+  fifo_pipeline<dram_req_t> *mrqq;
+  // buffer to hold packets when DRAM processing is over
+  // should be filled with dram clock and popped with l2or icnt clock
+  fifo_pipeline<mem_fetch> *returnq;
+
+  unsigned int dram_util_bins[10];
+  unsigned int dram_eff_bins[10];
+  unsigned int last_n_cmd, last_n_activity, last_bwutil;
+
+  unsigned long long n_cmd;
+  unsigned long long n_activity;
+  unsigned long long n_nop;
+  unsigned long long n_act;
+  unsigned long long n_pre;
+  unsigned long long n_ref;
+  unsigned long long n_rd;
+  unsigned long long n_rd_L2_A;
+  unsigned long long n_wr;
+  unsigned long long n_wr_WB;
+  unsigned long long n_req;
+  unsigned long long max_mrqs_temp;
+
+  // some statistics to see where BW is wasted?
+  unsigned long long wasted_bw_row;
+  unsigned long long wasted_bw_col;
+  unsigned long long util_bw;
+  unsigned long long idle_bw;
+  unsigned long long RCDc_limit;
+  unsigned long long CCDLc_limit;
+  unsigned long long CCDLc_limit_alone;
+  unsigned long long CCDc_limit;
+  unsigned long long WTRc_limit;
+  unsigned long long WTRc_limit_alone;
+  unsigned long long RCDWRc_limit;
+  unsigned long long RTWc_limit;
+  unsigned long long RTWc_limit_alone;
+  unsigned long long rwq_limit;
+
+  // row locality, BLP and other statistics
+  unsigned long long access_num;
+  unsigned long long read_num;
+  unsigned long long write_num;
+  unsigned long long hits_num;
+  unsigned long long hits_read_num;
+  unsigned long long hits_write_num;
+  unsigned long long banks_1time;
+  unsigned long long banks_acess_total;
+  unsigned long long banks_acess_total_after;
+  unsigned long long banks_time_rw;
+  unsigned long long banks_access_rw_total;
+  unsigned long long banks_time_ready;
+  unsigned long long banks_access_ready_total;
+  unsigned long long issued_two;
+  unsigned long long issued_total;
+  unsigned long long issued_total_row;
+  unsigned long long issued_total_col;
+  double write_to_read_ratio_blp_rw_average;
+  unsigned long long bkgrp_parallsim_rw;
+
+  unsigned int bwutil;
+  unsigned int max_mrqs;
+  unsigned int ave_mrqs;
+
+  class frfcfs_scheduler *m_frfcfs_scheduler;
+
+  unsigned int n_cmd_partial;
+  unsigned int n_activity_partial;
+  unsigned int n_nop_partial;
+  unsigned int n_act_partial;
+  unsigned int n_pre_partial;
+  unsigned int n_req_partial;
+  unsigned int ave_mrqs_partial;
+  unsigned int bwutil_partial;
+
+  class memory_stats_t *m_stats;
+  class Stats_gpgpu *mrqq_Dist;  // memory request queue inside DRAM
+
+  friend class frfcfs_scheduler;
 };
 
 #endif /*DRAM_H*/
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.cc
index ff500505a7..30065eefdd 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.cc
@@ -26,224 +26,232 @@
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #include "dram_sched.h"
+#include "../abstract_hardware_model.h"
 #include "gpu-misc.h"
 #include "gpu-sim.h"
-#include "../abstract_hardware_model.h"
 #include "mem_latency_stat.h"
 
-frfcfs_scheduler::frfcfs_scheduler( const memory_config *config, dram_t *dm, memory_stats_t *stats )
-{
-   m_config = config;
-   m_stats = stats;
-   m_num_pending = 0;
-   m_num_write_pending = 0;
-   m_dram = dm;
-   m_queue = new std::list<dram_req_t*>[m_config->nbk];
-   m_bins = new std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >[ m_config->nbk ];
-   m_last_row = new std::list<std::list<dram_req_t*>::iterator>*[ m_config->nbk ];
-   curr_row_service_time = new unsigned[m_config->nbk];
-   row_service_timestamp = new unsigned[m_config->nbk];
-   for ( unsigned i=0; i < m_config->nbk; i++ ) {
-      m_queue[i].clear();
-      m_bins[i].clear();
-      m_last_row[i] = NULL;
-      curr_row_service_time[i] = 0;
-      row_service_timestamp[i] = 0;
-   }
-   if(m_config->seperate_write_queue_enabled) {
-	   m_write_queue = new std::list<dram_req_t*>[m_config->nbk];
-	   m_write_bins = new std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >[ m_config->nbk ];
-	   m_last_write_row = new std::list<std::list<dram_req_t*>::iterator>*[ m_config->nbk ];
-
-	   for ( unsigned i=0; i < m_config->nbk; i++ ) {
-	         m_write_queue[i].clear();
-	         m_write_bins[i].clear();
-	         m_last_write_row[i] = NULL;
-	      }
-   }
-   m_mode = READ_MODE;
+frfcfs_scheduler::frfcfs_scheduler(const memory_config *config, dram_t *dm,
+                                   memory_stats_t *stats) {
+  m_config = config;
+  m_stats = stats;
+  m_num_pending = 0;
+  m_num_write_pending = 0;
+  m_dram = dm;
+  m_queue = new std::list<dram_req_t *>[m_config->nbk];
+  m_bins = new std::map<
+      unsigned, std::list<std::list<dram_req_t *>::iterator> >[m_config->nbk];
+  m_last_row =
+      new std::list<std::list<dram_req_t *>::iterator> *[m_config->nbk];
+  curr_row_service_time = new unsigned[m_config->nbk];
+  row_service_timestamp = new unsigned[m_config->nbk];
+  for (unsigned i = 0; i < m_config->nbk; i++) {
+    m_queue[i].clear();
+    m_bins[i].clear();
+    m_last_row[i] = NULL;
+    curr_row_service_time[i] = 0;
+    row_service_timestamp[i] = 0;
+  }
+  if (m_config->seperate_write_queue_enabled) {
+    m_write_queue = new std::list<dram_req_t *>[m_config->nbk];
+    m_write_bins = new std::map<
+        unsigned, std::list<std::list<dram_req_t *>::iterator> >[m_config->nbk];
+    m_last_write_row =
+        new std::list<std::list<dram_req_t *>::iterator> *[m_config->nbk];
 
+    for (unsigned i = 0; i < m_config->nbk; i++) {
+      m_write_queue[i].clear();
+      m_write_bins[i].clear();
+      m_last_write_row[i] = NULL;
+    }
+  }
+  m_mode = READ_MODE;
 }
 
-void frfcfs_scheduler::add_req( dram_req_t *req )
-{
-  if(m_config->seperate_write_queue_enabled && req->data->is_write()) {
-	  assert(m_num_write_pending < m_config->gpgpu_frfcfs_dram_write_queue_size);
-	  m_num_write_pending++;
-	  m_write_queue[req->bk].push_front(req);
-	  std::list<dram_req_t*>::iterator ptr = m_write_queue[req->bk].begin();
-	  m_write_bins[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+void frfcfs_scheduler::add_req(dram_req_t *req) {
+  if (m_config->seperate_write_queue_enabled && req->data->is_write()) {
+    assert(m_num_write_pending < m_config->gpgpu_frfcfs_dram_write_queue_size);
+    m_num_write_pending++;
+    m_write_queue[req->bk].push_front(req);
+    std::list<dram_req_t *>::iterator ptr = m_write_queue[req->bk].begin();
+    m_write_bins[req->bk][req->row].push_front(ptr);  // newest reqs to the
+                                                      // front
   } else {
-	   assert(m_num_pending < m_config->gpgpu_frfcfs_dram_sched_queue_size);
-	   m_num_pending++;
-	   m_queue[req->bk].push_front(req);
-	   std::list<dram_req_t*>::iterator ptr = m_queue[req->bk].begin();
-	   m_bins[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+    assert(m_num_pending < m_config->gpgpu_frfcfs_dram_sched_queue_size);
+    m_num_pending++;
+    m_queue[req->bk].push_front(req);
+    std::list<dram_req_t *>::iterator ptr = m_queue[req->bk].begin();
+    m_bins[req->bk][req->row].push_front(ptr);  // newest reqs to the front
   }
 }
 
-void frfcfs_scheduler::data_collection(unsigned int bank)
-{
-   if (gpu_sim_cycle > row_service_timestamp[bank]) {
-      curr_row_service_time[bank] = gpu_sim_cycle - row_service_timestamp[bank];
-      if (curr_row_service_time[bank] > m_stats->max_servicetime2samerow[m_dram->id][bank])
-         m_stats->max_servicetime2samerow[m_dram->id][bank] = curr_row_service_time[bank];
-   }
-   curr_row_service_time[bank] = 0;
-   row_service_timestamp[bank] = gpu_sim_cycle;
-   if (m_stats->concurrent_row_access[m_dram->id][bank] > m_stats->max_conc_access2samerow[m_dram->id][bank]) {
-      m_stats->max_conc_access2samerow[m_dram->id][bank] = m_stats->concurrent_row_access[m_dram->id][bank];
-   }
-   m_stats->concurrent_row_access[m_dram->id][bank] = 0;
-   m_stats->num_activates[m_dram->id][bank]++;
+void frfcfs_scheduler::data_collection(unsigned int bank) {
+  if (m_dram->m_gpu->gpu_sim_cycle > row_service_timestamp[bank]) {
+    curr_row_service_time[bank] =
+        m_dram->m_gpu->gpu_sim_cycle - row_service_timestamp[bank];
+    if (curr_row_service_time[bank] >
+        m_stats->max_servicetime2samerow[m_dram->id][bank])
+      m_stats->max_servicetime2samerow[m_dram->id][bank] =
+          curr_row_service_time[bank];
+  }
+  curr_row_service_time[bank] = 0;
+  row_service_timestamp[bank] = m_dram->m_gpu->gpu_sim_cycle;
+  if (m_stats->concurrent_row_access[m_dram->id][bank] >
+      m_stats->max_conc_access2samerow[m_dram->id][bank]) {
+    m_stats->max_conc_access2samerow[m_dram->id][bank] =
+        m_stats->concurrent_row_access[m_dram->id][bank];
+  }
+  m_stats->concurrent_row_access[m_dram->id][bank] = 0;
+  m_stats->num_activates[m_dram->id][bank]++;
 }
 
-dram_req_t *frfcfs_scheduler::schedule( unsigned bank, unsigned curr_row )
-{
-   //row
-   bool rowhit = true;
-   std::list<dram_req_t*> *m_current_queue = m_queue;
-   std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> > *m_current_bins = m_bins ;
-   std::list<std::list<dram_req_t*>::iterator> **m_current_last_row = m_last_row;
-
-   if(m_config->seperate_write_queue_enabled) {
-	   if(m_mode == READ_MODE &&
-			  ((m_num_write_pending >= m_config->write_high_watermark )
-			  // || (m_queue[bank].empty() && !m_write_queue[bank].empty())
-			   )) {
-		   m_mode = WRITE_MODE;
-	   }
-	   else if(m_mode == WRITE_MODE &&
-				  (( m_num_write_pending < m_config->write_low_watermark )
-				 //  || (!m_queue[bank].empty() && m_write_queue[bank].empty())
-				   )){
-		   m_mode = READ_MODE;
-	   }
-   }
-
-   if(m_mode == WRITE_MODE) {
-	   m_current_queue = m_write_queue;
-	   m_current_bins = m_write_bins ;
-	   m_current_last_row = m_last_write_row;
-   }
-
-   if ( m_current_last_row[bank] == NULL ) {
-      if ( m_current_queue[bank].empty() )
-         return NULL;
-
-      std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >::iterator bin_ptr = m_current_bins[bank].find( curr_row );
-      if ( bin_ptr == m_current_bins[bank].end()) {
-         dram_req_t *req = m_current_queue[bank].back();
-         bin_ptr = m_current_bins[bank].find( req->row );
-         assert( bin_ptr != m_current_bins[bank].end() ); // where did the request go???
-         m_current_last_row[bank] = &(bin_ptr->second);
-         data_collection(bank);
-         rowhit = false;
-      } else {
-    	  m_current_last_row[bank] = &(bin_ptr->second);
-         rowhit = true;
-      }
-   }
-   std::list<dram_req_t*>::iterator next = m_current_last_row[bank]->back();
-   dram_req_t *req = (*next);
-
-   //rowblp stats
-    m_dram->access_num++;
-    bool is_write = req->data->is_write();
-    if(is_write)
-  	  m_dram->write_num++;
-    else
-  	  m_dram->read_num++;
-
-    if(rowhit) {
-     m_dram->hits_num++;
-     if(is_write)
-    	  m_dram->hits_write_num++;
-      else
-    	  m_dram->hits_read_num++;
+dram_req_t *frfcfs_scheduler::schedule(unsigned bank, unsigned curr_row) {
+  // row
+  bool rowhit = true;
+  std::list<dram_req_t *> *m_current_queue = m_queue;
+  std::map<unsigned, std::list<std::list<dram_req_t *>::iterator> >
+      *m_current_bins = m_bins;
+  std::list<std::list<dram_req_t *>::iterator> **m_current_last_row =
+      m_last_row;
+
+  if (m_config->seperate_write_queue_enabled) {
+    if (m_mode == READ_MODE &&
+        ((m_num_write_pending >= m_config->write_high_watermark)
+         // || (m_queue[bank].empty() && !m_write_queue[bank].empty())
+         )) {
+      m_mode = WRITE_MODE;
+    } else if (m_mode == WRITE_MODE &&
+               ((m_num_write_pending < m_config->write_low_watermark)
+                //  || (!m_queue[bank].empty() && m_write_queue[bank].empty())
+                )) {
+      m_mode = READ_MODE;
     }
+  }
+
+  if (m_mode == WRITE_MODE) {
+    m_current_queue = m_write_queue;
+    m_current_bins = m_write_bins;
+    m_current_last_row = m_last_write_row;
+  }
+
+  if (m_current_last_row[bank] == NULL) {
+    if (m_current_queue[bank].empty()) return NULL;
+
+    std::map<unsigned, std::list<std::list<dram_req_t *>::iterator> >::iterator
+        bin_ptr = m_current_bins[bank].find(curr_row);
+    if (bin_ptr == m_current_bins[bank].end()) {
+      dram_req_t *req = m_current_queue[bank].back();
+      bin_ptr = m_current_bins[bank].find(req->row);
+      assert(bin_ptr !=
+             m_current_bins[bank].end());  // where did the request go???
+      m_current_last_row[bank] = &(bin_ptr->second);
+      data_collection(bank);
+      rowhit = false;
+    } else {
+      m_current_last_row[bank] = &(bin_ptr->second);
+      rowhit = true;
+    }
+  }
+  std::list<dram_req_t *>::iterator next = m_current_last_row[bank]->back();
+  dram_req_t *req = (*next);
+
+  // rowblp stats
+  m_dram->access_num++;
+  bool is_write = req->data->is_write();
+  if (is_write)
+    m_dram->write_num++;
+  else
+    m_dram->read_num++;
 
-   m_stats->concurrent_row_access[m_dram->id][bank]++;
-   m_stats->row_access[m_dram->id][bank]++;
-   m_current_last_row[bank]->pop_back();
+  if (rowhit) {
+    m_dram->hits_num++;
+    if (is_write)
+      m_dram->hits_write_num++;
+    else
+      m_dram->hits_read_num++;
+  }
+
+  m_stats->concurrent_row_access[m_dram->id][bank]++;
+  m_stats->row_access[m_dram->id][bank]++;
+  m_current_last_row[bank]->pop_back();
 
-   m_current_queue[bank].erase(next);
-   if ( m_current_last_row[bank]->empty() ) {
-	   m_current_bins[bank].erase( req->row );
-	   m_current_last_row[bank] = NULL;
-   }
+  m_current_queue[bank].erase(next);
+  if (m_current_last_row[bank]->empty()) {
+    m_current_bins[bank].erase(req->row);
+    m_current_last_row[bank] = NULL;
+  }
 #ifdef DEBUG_FAST_IDEAL_SCHED
-   if ( req )
-      printf("%08u : DRAM(%u) scheduling memory request to bank=%u, row=%u\n", 
-             (unsigned)gpu_sim_cycle, m_dram->id, req->bk, req->row );
+  if (req)
+    printf("%08u : DRAM(%u) scheduling memory request to bank=%u, row=%u\n",
+           (unsigned)gpu_sim_cycle, m_dram->id, req->bk, req->row);
 #endif
 
-   if(m_config->seperate_write_queue_enabled && req->data->is_write()) {
-	   assert( req != NULL && m_num_write_pending != 0 );
-	   m_num_write_pending--;
-   }
-   else {
-	   assert( req != NULL && m_num_pending != 0 );
-	   m_num_pending--;
-   }
+  if (m_config->seperate_write_queue_enabled && req->data->is_write()) {
+    assert(req != NULL && m_num_write_pending != 0);
+    m_num_write_pending--;
+  } else {
+    assert(req != NULL && m_num_pending != 0);
+    m_num_pending--;
+  }
 
-   return req;
+  return req;
 }
 
-
-void frfcfs_scheduler::print( FILE *fp )
-{
-   for ( unsigned b=0; b < m_config->nbk; b++ ) {
-      printf(" %u: queue length = %u\n", b, (unsigned)m_queue[b].size() );
-   }
+void frfcfs_scheduler::print(FILE *fp) {
+  for (unsigned b = 0; b < m_config->nbk; b++) {
+    printf(" %u: queue length = %u\n", b, (unsigned)m_queue[b].size());
+  }
 }
 
-void dram_t::scheduler_frfcfs()
-{
-   unsigned mrq_latency;
-   frfcfs_scheduler *sched = m_frfcfs_scheduler;
-   while ( !mrqq->empty() ) {
-      dram_req_t *req = mrqq->pop();
-
-      // Power stats
-      //if(req->data->get_type() != READ_REPLY && req->data->get_type() != WRITE_ACK)
-      m_stats->total_n_access++;
-
-      if(req->data->get_type() == WRITE_REQUEST){
-    	  m_stats->total_n_writes++;
-      }else if(req->data->get_type() == READ_REQUEST){
-    	  m_stats->total_n_reads++;
-      }
+void dram_t::scheduler_frfcfs() {
+  unsigned mrq_latency;
+  frfcfs_scheduler *sched = m_frfcfs_scheduler;
+  while (!mrqq->empty()) {
+    dram_req_t *req = mrqq->pop();
 
-      req->data->set_status(IN_PARTITION_MC_INPUT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-      sched->add_req(req);
-   }
-
-   dram_req_t *req;
-   unsigned i;
-   for ( i=0; i < m_config->nbk; i++ ) {
-      unsigned b = (i+prio)%m_config->nbk;
-      if ( !bk[b]->mrq ) {
-
-         req = sched->schedule(b, bk[b]->curr_row);
-
-         if ( req ) {
-            req->data->set_status(IN_PARTITION_MC_BANK_ARB_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-            prio = (prio+1)%m_config->nbk;
-            bk[b]->mrq = req;
-            if (m_config->gpgpu_memlatency_stat) {
-               mrq_latency = gpu_sim_cycle + gpu_tot_sim_cycle - bk[b]->mrq->timestamp;
-               m_stats->tot_mrq_latency += mrq_latency;
-               m_stats->tot_mrq_num++;
-               bk[b]->mrq->timestamp = gpu_tot_sim_cycle + gpu_sim_cycle;
-               m_stats->mrq_lat_table[LOGB2(mrq_latency)]++;
-               if (mrq_latency > m_stats->max_mrq_latency) {
-                  m_stats->max_mrq_latency = mrq_latency;
-               }
-            }
-
-            break;
-         }
+    // Power stats
+    // if(req->data->get_type() != READ_REPLY && req->data->get_type() !=
+    // WRITE_ACK)
+    m_stats->total_n_access++;
+
+    if (req->data->get_type() == WRITE_REQUEST) {
+      m_stats->total_n_writes++;
+    } else if (req->data->get_type() == READ_REQUEST) {
+      m_stats->total_n_reads++;
+    }
+
+    req->data->set_status(IN_PARTITION_MC_INPUT_QUEUE,
+                          m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+    sched->add_req(req);
+  }
+
+  dram_req_t *req;
+  unsigned i;
+  for (i = 0; i < m_config->nbk; i++) {
+    unsigned b = (i + prio) % m_config->nbk;
+    if (!bk[b]->mrq) {
+      req = sched->schedule(b, bk[b]->curr_row);
+
+      if (req) {
+        req->data->set_status(IN_PARTITION_MC_BANK_ARB_QUEUE,
+                              m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+        prio = (prio + 1) % m_config->nbk;
+        bk[b]->mrq = req;
+        if (m_config->gpgpu_memlatency_stat) {
+          mrq_latency = m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle -
+                        bk[b]->mrq->timestamp;
+          m_stats->tot_mrq_latency += mrq_latency;
+          m_stats->tot_mrq_num++;
+          bk[b]->mrq->timestamp =
+              m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle;
+          m_stats->mrq_lat_table[LOGB2(mrq_latency)]++;
+          if (mrq_latency > m_stats->max_mrq_latency) {
+            m_stats->max_mrq_latency = mrq_latency;
+          }
+        }
+
+        break;
       }
-   }
+    }
+  }
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.h
index 63f5831f3b..ba28fb2143 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/dram_sched.h
@@ -28,45 +28,45 @@
 #ifndef dram_sched_h_INCLUDED
 #define dram_sched_h_INCLUDED
 
-#include "dram.h"
-#include "shader.h"
-#include "gpu-sim.h"
-#include "gpu-misc.h"
 #include <list>
 #include <map>
+#include "dram.h"
+#include "gpu-misc.h"
+#include "gpu-sim.h"
+#include "shader.h"
 
-enum memory_mode {
-   READ_MODE = 0,
-   WRITE_MODE
-};
+enum memory_mode { READ_MODE = 0, WRITE_MODE };
 
 class frfcfs_scheduler {
-public:
-   frfcfs_scheduler( const memory_config *config, dram_t *dm, memory_stats_t *stats );
-   void add_req( dram_req_t *req );
-   void data_collection(unsigned bank);
-   dram_req_t *schedule( unsigned bank, unsigned curr_row );
-   void print( FILE *fp );
-   unsigned num_pending() const { return m_num_pending;}
-   unsigned num_write_pending() const { return m_num_write_pending;}
+ public:
+  frfcfs_scheduler(const memory_config *config, dram_t *dm,
+                   memory_stats_t *stats);
+  void add_req(dram_req_t *req);
+  void data_collection(unsigned bank);
+  dram_req_t *schedule(unsigned bank, unsigned curr_row);
+  void print(FILE *fp);
+  unsigned num_pending() const { return m_num_pending; }
+  unsigned num_write_pending() const { return m_num_write_pending; }
 
-private:
-   const memory_config *m_config;
-   dram_t *m_dram;
-   unsigned m_num_pending;
-   unsigned m_num_write_pending;
-   std::list<dram_req_t*>                                    *m_queue;
-   std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >    *m_bins;
-   std::list<std::list<dram_req_t*>::iterator>                 **m_last_row;
-   unsigned *curr_row_service_time; //one set of variables for each bank.
-   unsigned *row_service_timestamp; //tracks when scheduler began servicing current row
+ private:
+  const memory_config *m_config;
+  dram_t *m_dram;
+  unsigned m_num_pending;
+  unsigned m_num_write_pending;
+  std::list<dram_req_t *> *m_queue;
+  std::map<unsigned, std::list<std::list<dram_req_t *>::iterator> > *m_bins;
+  std::list<std::list<dram_req_t *>::iterator> **m_last_row;
+  unsigned *curr_row_service_time;  // one set of variables for each bank.
+  unsigned *row_service_timestamp;  // tracks when scheduler began servicing
+                                    // current row
 
-   std::list<dram_req_t*>                                    *m_write_queue;
-   std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >    *m_write_bins;
-   std::list<std::list<dram_req_t*>::iterator>                 **m_last_write_row;
+  std::list<dram_req_t *> *m_write_queue;
+  std::map<unsigned, std::list<std::list<dram_req_t *>::iterator> >
+      *m_write_bins;
+  std::list<std::list<dram_req_t *>::iterator> **m_last_write_row;
 
-   enum memory_mode m_mode;
-   memory_stats_t *m_stats;
+  enum memory_mode m_mode;
+  memory_stats_t *m_stats;
 };
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.cc
index 370f6e6b70..196b62d053 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.cc
@@ -26,1607 +26,1717 @@
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #include "gpu-cache.h"
-#include "stat-tool.h"
 #include <assert.h>
+#include "gpu-sim.h"
+#include "hashing.h"
+#include "stat-tool.h"
 
-// used to allocate memory that is large enough to adapt the changes in cache size across kernels
-
-const char * cache_request_status_str(enum cache_request_status status) 
-{
-   static const char * static_cache_request_status_str[] = {
-      "HIT",
-      "HIT_RESERVED",
-      "MISS",
-      "RESERVATION_FAIL",
-	  "SECTOR_MISS"
-   }; 
-
-   assert(sizeof(static_cache_request_status_str) / sizeof(const char*) == NUM_CACHE_REQUEST_STATUS); 
-   assert(status < NUM_CACHE_REQUEST_STATUS); 
-
-   return static_cache_request_status_str[status]; 
-}
-
-const char * cache_fail_status_str(enum cache_reservation_fail_reason status)
-{
-   static const char * static_cache_reservation_fail_reason_str[] = {
-	  "LINE_ALLOC_FAIL",
-	  "MISS_QUEUE_FULL",
-	  "MSHR_ENRTY_FAIL",
-      "MSHR_MERGE_ENRTY_FAIL",
-      "MSHR_RW_PENDING"
-   };
-
-   assert(sizeof(static_cache_reservation_fail_reason_str) / sizeof(const char*) == NUM_CACHE_RESERVATION_FAIL_STATUS);
-   assert(status < NUM_CACHE_RESERVATION_FAIL_STATUS);
-
-   return static_cache_reservation_fail_reason_str[status];
-}
-
-unsigned l1d_cache_config::set_index(new_addr_type addr) const{
-    unsigned set_index = m_nset; // Default to linear set index function
-    unsigned lower_xor = 0;
-    unsigned upper_xor = 0;
-
-    switch(m_set_index_function){
-    case FERMI_HASH_SET_FUNCTION:
-    case BITWISE_XORING_FUNCTION:
-        /*
-        * Set Indexing function from "A Detailed GPU Cache Model Based on Reuse Distance Theory"
-        * Cedric Nugteren et al.
-        * HPCA 2014
-        */
-        if(m_nset == 32 || m_nset == 64){
-            // Lower xor value is bits 7-11
-            lower_xor = (addr >> m_line_sz_log2) & 0x1F;
-
-            // Upper xor value is bits 13, 14, 15, 17, and 19
-            upper_xor  = (addr & 0xE000)  >> 13; // Bits 13, 14, 15
-            upper_xor |= (addr & 0x20000) >> 14; // Bit 17
-            upper_xor |= (addr & 0x80000) >> 15; // Bit 19
-
-            set_index = (lower_xor ^ upper_xor);
-
-            // 48KB cache prepends the set_index with bit 12
-            if(m_nset == 64)
-                set_index |= (addr & 0x1000) >> 7;
-
-        }else{ /* Else incorrect number of sets for the hashing function */
-            assert("\nGPGPU-Sim cache configuration error: The number of sets should be "
-                    "32 or 64 for the hashing set index function.\n" && 0);
-        }
-        break;
-
-    case HASH_IPOLY_FUNCTION:
-    	/*
-		* Set Indexing function from "Pseudo-randomly interleaved memory."
-		* Rau, B. R et al.
-		* ISCA 1991
-		*
-		* "Sacat: streaming-aware conflict-avoiding thrashing-resistant gpgpu cache management scheme."
-		* Khairy et al.
-		* IEEE TPDS 2017.
-    	*/
-    	if(m_nset == 32 || m_nset == 64){
-		std::bitset<64> a(addr);
-		std::bitset<6> index;
-		index[0] = a[25]^a[24]^a[23]^a[22]^a[21]^a[18]^a[17]^a[15]^a[12]^a[7]; //10
-		index[1] = a[26]^a[25]^a[24]^a[23]^a[22]^a[19]^a[18]^a[16]^a[13]^a[8]; //10
-		index[2] = a[26]^a[22]^a[21]^a[20]^a[19]^a[18]^a[15]^a[14]^a[12]^a[9]; //10
-		index[3] = a[23]^a[22]^a[21]^a[20]^a[19]^a[16]^a[15]^a[13]^a[10]; //9
-		index[4] = a[24]^a[23]^a[22]^a[21]^a[20]^a[17]^a[16]^a[14]^a[11]; //9
-
-		 if(m_nset == 64)
-			 index[5] = a[12];
-
-		set_index = index.to_ulong();
-
-    	}else{ /* Else incorrect number of sets for the hashing function */
-    	            assert("\nGPGPU-Sim cache configuration error: The number of sets should be "
-    	                    "32 or 64 for the hashing set index function.\n" && 0);
-    	 }
-        break;
-
-    case CUSTOM_SET_FUNCTION:
-        /* No custom set function implemented */
-        break;
-
-    case LINEAR_SET_FUNCTION:
-        set_index = (addr >> m_line_sz_log2) & (m_nset-1);
-        break;
+// used to allocate memory that is large enough to adapt the changes in cache
+// size across kernels
 
-    default:
-    	 assert("\nUndefined set index function.\n" && 0);
-    	 break;
-    }
+const char *cache_request_status_str(enum cache_request_status status) {
+  static const char *static_cache_request_status_str[] = {
+      "HIT",         "HIT_RESERVED", "MISS", "RESERVATION_FAIL",
+      "SECTOR_MISS", "MSHR_HIT"};
 
-    // Linear function selected or custom set index function not implemented
-    assert((set_index < m_nset) && "\nError: Set index out of bounds. This is caused by "
-            "an incorrect or unimplemented custom set index function.\n");
-
-    return set_index;
-}
-
-void l2_cache_config::init(linear_to_raw_address_translation *address_mapping){
-	cache_config::init(m_config_string,FuncCachePreferNone);
-	m_address_mapping = address_mapping;
-}
-
-unsigned l2_cache_config::set_index(new_addr_type addr) const{
-	if(!m_address_mapping){
-		return(addr >> m_line_sz_log2) & (m_nset-1);
-	}else{
-		// Calculate set index without memory partition bits to reduce set camping
-		new_addr_type part_addr = m_address_mapping->partition_address(addr);
-		return(part_addr >> m_line_sz_log2) & (m_nset -1);
-	}
-}
-
-tag_array::~tag_array() 
-{
-	unsigned cache_lines_num = m_config.get_max_num_lines();
-	for(unsigned i=0; i<cache_lines_num; ++i)
-		delete m_lines[i];
-    delete[] m_lines;
-}
-
-tag_array::tag_array( cache_config &config,
-                      int core_id,
-                      int type_id,
-                      cache_block_t** new_lines)
-    : m_config( config ),
-      m_lines( new_lines )
-{
-    init( core_id, type_id );
-}
-
-void tag_array::update_cache_parameters(cache_config &config)
-{
-	m_config=config;
-}
-
-tag_array::tag_array( cache_config &config,
-                      int core_id,
-                      int type_id )
-    : m_config( config )
-{
-    //assert( m_config.m_write_policy == READ_ONLY ); Old assert
-	unsigned cache_lines_num = config.get_max_num_lines();
-	m_lines = new cache_block_t*[cache_lines_num];
-	if(config.m_cache_type == NORMAL)
-	{
-		for(unsigned i=0; i<cache_lines_num; ++i)
-			m_lines[i] = new line_cache_block();
-	}
-	else if(config.m_cache_type == SECTOR)
-	{
-		for(unsigned i=0; i<cache_lines_num; ++i)
-			m_lines[i] = new sector_cache_block();
-	}
-	else
-		assert(0);
-
-    init( core_id, type_id );
-}
-
-void tag_array::init( int core_id, int type_id )
-{
-    m_access = 0;
-    m_miss = 0;
-    m_pending_hit = 0;
-    m_res_fail = 0;
-    m_sector_miss = 0;
-    // initialize snapshot counters for visualizer
-    m_prev_snapshot_access = 0;
-    m_prev_snapshot_miss = 0;
-    m_prev_snapshot_pending_hit = 0;
-    m_core_id = core_id; 
-    m_type_id = type_id;
-    is_used = false;
-}
-
-void tag_array::add_pending_line(mem_fetch *mf){
-	assert(mf);
-	new_addr_type addr = m_config.block_addr(mf->get_addr());
-	line_table::const_iterator i = pending_lines.find(addr);
-	if ( i == pending_lines.end() ) {
-		pending_lines[addr] = mf->get_inst().get_uid();
-	}
-}
-
-void tag_array::remove_pending_line(mem_fetch *mf){
-	assert(mf);
-	new_addr_type addr = m_config.block_addr(mf->get_addr());
-	line_table::const_iterator i = pending_lines.find(addr);
-	if ( i != pending_lines.end() ) {
-		pending_lines.erase(addr);
-	}
-}
-
-enum cache_request_status tag_array::probe( new_addr_type addr, unsigned &idx, mem_fetch* mf, bool probe_mode) const {
-    mem_access_sector_mask_t mask = mf->get_access_sector_mask();
-    return probe(addr, idx, mask, probe_mode, mf);
-}
-
-
-enum cache_request_status tag_array::probe( new_addr_type addr, unsigned &idx, mem_access_sector_mask_t mask, bool probe_mode, mem_fetch* mf) const {
-    //assert( m_config.m_write_policy == READ_ONLY );
-    unsigned set_index = m_config.set_index(addr);
-    new_addr_type tag = m_config.tag(addr);
-
-    unsigned invalid_line = (unsigned)-1;
-    unsigned valid_line = (unsigned)-1;
-    unsigned long long valid_timestamp = (unsigned)-1;
-
-    bool all_reserved = true;
-
-    // check for hit or pending hit
-    for (unsigned way=0; way<m_config.m_assoc; way++) {
-        unsigned index = set_index*m_config.m_assoc+way;
-        cache_block_t *line = m_lines[index];
-        if (line->m_tag == tag) {
-            if ( line->get_status(mask) == RESERVED ) {
-                idx = index;
-                return HIT_RESERVED;
-            } else if ( line->get_status(mask) == VALID ) {
-                idx = index;
-                return HIT;
-            } else if ( line->get_status(mask) == MODIFIED) {
-            	if(line->is_readable(mask)) {
-					idx = index;
-					return HIT;
-            	}
-            	else {
-            		idx = index;
-            		return SECTOR_MISS;
-            	}
-
-            } else if ( line->is_valid_line() && line->get_status(mask) == INVALID ) {
-                idx = index;
-                return SECTOR_MISS;
-            }else {
-                assert( line->get_status(mask) == INVALID );
-            }
-        }
-        if (!line->is_reserved_line()) {
-            all_reserved = false;
-            if (line->is_invalid_line()) {
-                invalid_line = index;
-            } else {
-                // valid line : keep track of most appropriate replacement candidate
-                if ( m_config.m_replacement_policy == LRU ) {
-                    if ( line->get_last_access_time() < valid_timestamp ) {
-                        valid_timestamp = line->get_last_access_time();
-                        valid_line = index;
-                    }
-                } else if ( m_config.m_replacement_policy == FIFO ) {
-                    if ( line->get_alloc_time() < valid_timestamp ) {
-                        valid_timestamp = line->get_alloc_time();
-                        valid_line = index;
-                    }
-                }
-            }
-        }
-    }
-    if ( all_reserved ) {
-        assert( m_config.m_alloc_policy == ON_MISS ); 
-        return RESERVATION_FAIL; // miss and not enough space in cache to allocate on miss
-    }
+  assert(sizeof(static_cache_request_status_str) / sizeof(const char *) ==
+         NUM_CACHE_REQUEST_STATUS);
+  assert(status < NUM_CACHE_REQUEST_STATUS);
 
-    if ( invalid_line != (unsigned)-1 ) {
-        idx = invalid_line;
-    } else if ( valid_line != (unsigned)-1) {
-        idx = valid_line;
-    } else abort(); // if an unreserved block exists, it is either invalid or replaceable 
+  return static_cache_request_status_str[status];
+}
 
+const char *cache_fail_status_str(enum cache_reservation_fail_reason status) {
+  static const char *static_cache_reservation_fail_reason_str[] = {
+      "LINE_ALLOC_FAIL", "MISS_QUEUE_FULL", "MSHR_ENRTY_FAIL",
+      "MSHR_MERGE_ENRTY_FAIL", "MSHR_RW_PENDING"};
 
-    if(probe_mode && m_config.is_streaming()){
-		line_table::const_iterator i = pending_lines.find(m_config.block_addr(addr));
-		assert(mf);
-		if ( !mf->is_write() && i != pending_lines.end() ) {
-			 if(i->second != mf->get_inst().get_uid())
-				 return SECTOR_MISS;
-		}
-    }
+  assert(sizeof(static_cache_reservation_fail_reason_str) /
+             sizeof(const char *) ==
+         NUM_CACHE_RESERVATION_FAIL_STATUS);
+  assert(status < NUM_CACHE_RESERVATION_FAIL_STATUS);
 
-    return MISS;
+  return static_cache_reservation_fail_reason_str[status];
 }
 
-enum cache_request_status tag_array::access( new_addr_type addr, unsigned time, unsigned &idx, mem_fetch* mf)
-{
-    bool wb=false;
-    evicted_block_info evicted;
-    enum cache_request_status result = access(addr,time,idx,wb,evicted,mf);
-    assert(!wb);
-    return result;
-}
-
-enum cache_request_status tag_array::access( new_addr_type addr, unsigned time, unsigned &idx, bool &wb, evicted_block_info &evicted, mem_fetch* mf )
-{
-    m_access++;
-    is_used = true;
-    shader_cache_access_log(m_core_id, m_type_id, 0); // log accesses to cache
-    enum cache_request_status status = probe(addr,idx,mf);
-    switch (status) {
-    case HIT_RESERVED: 
-        m_pending_hit++;
-    case HIT: 
-        m_lines[idx]->set_last_access_time(time, mf->get_access_sector_mask());
-        break;
-    case MISS:
-        m_miss++;
-        shader_cache_access_log(m_core_id, m_type_id, 1); // log cache misses
-        if ( m_config.m_alloc_policy == ON_MISS ) {
-            if( m_lines[idx]->is_modified_line()) {
-                wb = true;
-                evicted.set_info(m_lines[idx]->m_block_addr, m_lines[idx]->get_modified_size());
-            }
-            m_lines[idx]->allocate( m_config.tag(addr), m_config.block_addr(addr), time, mf->get_access_sector_mask());
-        }
-        break;
-    case SECTOR_MISS:
-    	assert(m_config.m_cache_type == SECTOR);
-    	m_sector_miss++;
-		shader_cache_access_log(m_core_id, m_type_id, 1); // log cache misses
-		if ( m_config.m_alloc_policy == ON_MISS ) {
-			((sector_cache_block*)m_lines[idx])->allocate_sector( time, mf->get_access_sector_mask() );
-		}
-		break;
-    case RESERVATION_FAIL:
-        m_res_fail++;
-        shader_cache_access_log(m_core_id, m_type_id, 1); // log cache misses
-        break;
-    default:
-        fprintf( stderr, "tag_array::access - Error: Unknown"
-            "cache_request_status %d\n", status );
-        abort();
-    }
-    return status;
+unsigned l1d_cache_config::set_bank(new_addr_type addr) const {
+  // For sector cache, we select one sector per bank (sector interleaving)
+  // This is what was found in Volta (one sector per bank, sector interleaving)
+  // otherwise, line interleaving
+  return cache_config::hash_function(addr, l1_banks,
+                                     l1_banks_byte_interleaving_log2,
+                                     l1_banks_log2, l1_banks_hashing_function);
 }
 
-void tag_array::fill( new_addr_type addr, unsigned time, mem_fetch* mf)
-{
-    fill(addr, time, mf->get_access_sector_mask());
+unsigned cache_config::set_index(new_addr_type addr) const {
+  return cache_config::hash_function(addr, m_nset, m_line_sz_log2, m_nset_log2,
+                                     m_set_index_function);
 }
 
-void tag_array::fill( new_addr_type addr, unsigned time, mem_access_sector_mask_t mask )
-{
-    //assert( m_config.m_alloc_policy == ON_FILL );
-    unsigned idx;
-    enum cache_request_status status = probe(addr,idx,mask);
-    //assert(status==MISS||status==SECTOR_MISS); // MSHR should have prevented redundant memory request
-    if(status==MISS)
-    	m_lines[idx]->allocate( m_config.tag(addr), m_config.block_addr(addr), time, mask );
-    else if (status==SECTOR_MISS) {
-    	assert(m_config.m_cache_type == SECTOR);
-    	((sector_cache_block*)m_lines[idx])->allocate_sector( time, mask );
+unsigned cache_config::hash_function(new_addr_type addr, unsigned m_nset,
+                                     unsigned m_line_sz_log2,
+                                     unsigned m_nset_log2,
+                                     unsigned m_index_function) const {
+  unsigned set_index = 0;
+
+  switch (m_index_function) {
+    case FERMI_HASH_SET_FUNCTION: {
+      /*
+       * Set Indexing function from "A Detailed GPU Cache Model Based on Reuse
+       * Distance Theory" Cedric Nugteren et al. HPCA 2014
+       */
+      unsigned lower_xor = 0;
+      unsigned upper_xor = 0;
+
+      if (m_nset == 32 || m_nset == 64) {
+        // Lower xor value is bits 7-11
+        lower_xor = (addr >> m_line_sz_log2) & 0x1F;
+
+        // Upper xor value is bits 13, 14, 15, 17, and 19
+        upper_xor = (addr & 0xE000) >> 13;    // Bits 13, 14, 15
+        upper_xor |= (addr & 0x20000) >> 14;  // Bit 17
+        upper_xor |= (addr & 0x80000) >> 15;  // Bit 19
+
+        set_index = (lower_xor ^ upper_xor);
+
+        // 48KB cache prepends the set_index with bit 12
+        if (m_nset == 64) set_index |= (addr & 0x1000) >> 7;
+
+      } else { /* Else incorrect number of sets for the hashing function */
+        assert(
+            "\nGPGPU-Sim cache configuration error: The number of sets should "
+            "be "
+            "32 or 64 for the hashing set index function.\n" &&
+            0);
+      }
+      break;
+    }
+
+    case BITWISE_XORING_FUNCTION: {
+      new_addr_type higher_bits = addr >> (m_line_sz_log2 + m_nset_log2);
+      unsigned index = (addr >> m_line_sz_log2) & (m_nset - 1);
+      set_index = bitwise_hash_function(higher_bits, index, m_nset);
+      break;
+    }
+    case HASH_IPOLY_FUNCTION: {
+      new_addr_type higher_bits = addr >> (m_line_sz_log2 + m_nset_log2);
+      unsigned index = (addr >> m_line_sz_log2) & (m_nset - 1);
+      set_index = ipoly_hash_function(higher_bits, index, m_nset);
+      break;
+    }
+    case CUSTOM_SET_FUNCTION: {
+      /* No custom set function implemented */
+      break;
+    }
+
+    case LINEAR_SET_FUNCTION: {
+      set_index = (addr >> m_line_sz_log2) & (m_nset - 1);
+      break;
     }
 
-    m_lines[idx]->fill(time, mask);
+    default: {
+      assert("\nUndefined set index function.\n" && 0);
+      break;
+    }
+  }
+
+  // Linear function selected or custom set index function not implemented
+  assert((set_index < m_nset) &&
+         "\nError: Set index out of bounds. This is caused by "
+         "an incorrect or unimplemented custom set index function.\n");
+
+  return set_index;
 }
 
-void tag_array::fill( unsigned index, unsigned time, mem_fetch* mf)
-{
-    assert( m_config.m_alloc_policy == ON_MISS );
-    m_lines[index]->fill(time, mf->get_access_sector_mask());
+void l2_cache_config::init(linear_to_raw_address_translation *address_mapping) {
+  cache_config::init(m_config_string, FuncCachePreferNone);
+  m_address_mapping = address_mapping;
 }
 
+unsigned l2_cache_config::set_index(new_addr_type addr) const {
+  new_addr_type part_addr = addr;
 
-//TODO: we need write back the flushed data to the upper level
-void tag_array::flush() 
-{
-	if(!is_used)
-		return;
+  if (m_address_mapping) {
+    // Calculate set index without memory partition bits to reduce set camping
+    part_addr = m_address_mapping->partition_address(addr);
+  }
+
+  return cache_config::set_index(part_addr);
+}
+
+tag_array::~tag_array() {
+  unsigned cache_lines_num = m_config.get_max_num_lines();
+  for (unsigned i = 0; i < cache_lines_num; ++i) delete m_lines[i];
+  delete[] m_lines;
+}
+
+tag_array::tag_array(cache_config &config, int core_id, int type_id,
+                     cache_block_t **new_lines)
+    : m_config(config), m_lines(new_lines) {
+  init(core_id, type_id);
+}
+
+void tag_array::update_cache_parameters(cache_config &config) {
+  m_config = config;
+}
+
+tag_array::tag_array(cache_config &config, int core_id, int type_id)
+    : m_config(config) {
+  // assert( m_config.m_write_policy == READ_ONLY ); Old assert
+  unsigned cache_lines_num = config.get_max_num_lines();
+  m_lines = new cache_block_t *[cache_lines_num];
+  if (config.m_cache_type == NORMAL) {
+    for (unsigned i = 0; i < cache_lines_num; ++i)
+      m_lines[i] = new line_cache_block();
+  } else if (config.m_cache_type == SECTOR) {
+    for (unsigned i = 0; i < cache_lines_num; ++i)
+      m_lines[i] = new sector_cache_block();
+  } else
+    assert(0);
+
+  init(core_id, type_id);
+}
+
+void tag_array::init(int core_id, int type_id) {
+  m_access = 0;
+  m_miss = 0;
+  m_pending_hit = 0;
+  m_res_fail = 0;
+  m_sector_miss = 0;
+  // initialize snapshot counters for visualizer
+  m_prev_snapshot_access = 0;
+  m_prev_snapshot_miss = 0;
+  m_prev_snapshot_pending_hit = 0;
+  m_core_id = core_id;
+  m_type_id = type_id;
+  is_used = false;
+  m_dirty = 0;
+}
+
+void tag_array::add_pending_line(mem_fetch *mf) {
+  assert(mf);
+  new_addr_type addr = m_config.block_addr(mf->get_addr());
+  line_table::const_iterator i = pending_lines.find(addr);
+  if (i == pending_lines.end()) {
+    pending_lines[addr] = mf->get_inst().get_uid();
+  }
+}
+
+void tag_array::remove_pending_line(mem_fetch *mf) {
+  assert(mf);
+  new_addr_type addr = m_config.block_addr(mf->get_addr());
+  line_table::const_iterator i = pending_lines.find(addr);
+  if (i != pending_lines.end()) {
+    pending_lines.erase(addr);
+  }
+}
+
+enum cache_request_status tag_array::probe(new_addr_type addr, unsigned &idx,
+                                           mem_fetch *mf, bool is_write,
+                                           bool probe_mode) const {
+  mem_access_sector_mask_t mask = mf->get_access_sector_mask();
+  return probe(addr, idx, mask, is_write, probe_mode, mf);
+}
+
+enum cache_request_status tag_array::probe(new_addr_type addr, unsigned &idx,
+                                           mem_access_sector_mask_t mask,
+                                           bool is_write, bool probe_mode,
+                                           mem_fetch *mf) const {
+  // assert( m_config.m_write_policy == READ_ONLY );
+  unsigned set_index = m_config.set_index(addr);
+  new_addr_type tag = m_config.tag(addr);
+
+  unsigned invalid_line = (unsigned)-1;
+  unsigned valid_line = (unsigned)-1;
+  unsigned long long valid_timestamp = (unsigned)-1;
+
+  bool all_reserved = true;
+  // check for hit or pending hit
+  for (unsigned way = 0; way < m_config.m_assoc; way++) {
+    unsigned index = set_index * m_config.m_assoc + way;
+    cache_block_t *line = m_lines[index];
+    if (line->m_tag == tag) {
+      if (line->get_status(mask) == RESERVED) {
+        idx = index;
+        return HIT_RESERVED;
+      } else if (line->get_status(mask) == VALID) {
+        idx = index;
+        return HIT;
+      } else if (line->get_status(mask) == MODIFIED) {
+        if ((!is_write && line->is_readable(mask)) || is_write) {
+          idx = index;
+          return HIT;
+        } else {
+          idx = index;
+          return SECTOR_MISS;
+        }
 
-    for (unsigned i=0; i < m_config.get_num_lines(); i++)
-    	if(m_lines[i]->is_modified_line()) {
-    	for(unsigned j=0; j < SECTOR_CHUNCK_SIZE; j++)
-    		m_lines[i]->set_status(INVALID, mem_access_sector_mask_t().set(j)) ;
-    	}
+      } else if (line->is_valid_line() && line->get_status(mask) == INVALID) {
+        idx = index;
+        return SECTOR_MISS;
+      } else {
+        assert(line->get_status(mask) == INVALID);
+      }
+    }
+    if (!line->is_reserved_line()) {
+      // percentage of dirty lines in the cache
+      // number of dirty lines / total lines in the cache
+      float dirty_line_percentage =
+          ((float)m_dirty / (m_config.m_nset * m_config.m_assoc)) * 100;
+      // If the cacheline is from a load op (not modified), 
+      // or the total dirty cacheline is above a specific value,
+      // Then this cacheline is eligible to be considered for replacement candidate
+      // i.e. Only evict clean cachelines until total dirty cachelines reach the limit.
+      if (!line->is_modified_line() ||
+          dirty_line_percentage >= m_config.m_wr_percent) {
+        all_reserved = false;
+        if (line->is_invalid_line()) {
+          invalid_line = index;
+        } else {
+          // valid line : keep track of most appropriate replacement candidate
+          if (m_config.m_replacement_policy == LRU) {
+            if (line->get_last_access_time() < valid_timestamp) {
+              valid_timestamp = line->get_last_access_time();
+              valid_line = index;
+            }
+          } else if (m_config.m_replacement_policy == FIFO) {
+            if (line->get_alloc_time() < valid_timestamp) {
+              valid_timestamp = line->get_alloc_time();
+              valid_line = index;
+            }
+          }
+        }
+      }
+    }
+  }
+  if (all_reserved) {
+    assert(m_config.m_alloc_policy == ON_MISS);
+    return RESERVATION_FAIL;  // miss and not enough space in cache to allocate
+                              // on miss
+  }
+
+  if (invalid_line != (unsigned)-1) {
+    idx = invalid_line;
+  } else if (valid_line != (unsigned)-1) {
+    idx = valid_line;
+  } else
+    abort();  // if an unreserved block exists, it is either invalid or
+              // replaceable
+
+  return MISS;
+}
+
+enum cache_request_status tag_array::access(new_addr_type addr, unsigned time,
+                                            unsigned &idx, mem_fetch *mf) {
+  bool wb = false;
+  evicted_block_info evicted;
+  enum cache_request_status result = access(addr, time, idx, wb, evicted, mf);
+  assert(!wb);
+  return result;
+}
+
+enum cache_request_status tag_array::access(new_addr_type addr, unsigned time,
+                                            unsigned &idx, bool &wb,
+                                            evicted_block_info &evicted,
+                                            mem_fetch *mf) {
+  m_access++;
+  is_used = true;
+  shader_cache_access_log(m_core_id, m_type_id, 0);  // log accesses to cache
+  enum cache_request_status status = probe(addr, idx, mf, mf->is_write());
+  switch (status) {
+    case HIT_RESERVED:
+      m_pending_hit++;
+    case HIT:
+      m_lines[idx]->set_last_access_time(time, mf->get_access_sector_mask());
+      break;
+    case MISS:
+      m_miss++;
+      shader_cache_access_log(m_core_id, m_type_id, 1);  // log cache misses
+      if (m_config.m_alloc_policy == ON_MISS) {
+        if (m_lines[idx]->is_modified_line()) {
+          wb = true;
+          // m_lines[idx]->set_byte_mask(mf);
+          evicted.set_info(m_lines[idx]->m_block_addr,
+                           m_lines[idx]->get_modified_size(),
+                           m_lines[idx]->get_dirty_byte_mask(),
+                           m_lines[idx]->get_dirty_sector_mask());
+          m_dirty--;
+        }
+        m_lines[idx]->allocate(m_config.tag(addr), m_config.block_addr(addr),
+                               time, mf->get_access_sector_mask());
+      }
+      break;
+    case SECTOR_MISS:
+      assert(m_config.m_cache_type == SECTOR);
+      m_sector_miss++;
+      shader_cache_access_log(m_core_id, m_type_id, 1);  // log cache misses
+      if (m_config.m_alloc_policy == ON_MISS) {
+        bool before = m_lines[idx]->is_modified_line();
+        ((sector_cache_block *)m_lines[idx])
+            ->allocate_sector(time, mf->get_access_sector_mask());
+        if (before && !m_lines[idx]->is_modified_line()) {
+          m_dirty--;
+        }
+      }
+      break;
+    case RESERVATION_FAIL:
+      m_res_fail++;
+      shader_cache_access_log(m_core_id, m_type_id, 1);  // log cache misses
+      break;
+    default:
+      fprintf(stderr,
+              "tag_array::access - Error: Unknown"
+              "cache_request_status %d\n",
+              status);
+      abort();
+  }
+  return status;
+}
+
+void tag_array::fill(new_addr_type addr, unsigned time, mem_fetch *mf,
+                     bool is_write) {
+  fill(addr, time, mf->get_access_sector_mask(), mf->get_access_byte_mask(),
+       is_write);
+}
+
+void tag_array::fill(new_addr_type addr, unsigned time,
+                     mem_access_sector_mask_t mask,
+                     mem_access_byte_mask_t byte_mask, bool is_write) {
+  // assert( m_config.m_alloc_policy == ON_FILL );
+  unsigned idx;
+  enum cache_request_status status = probe(addr, idx, mask, is_write);
+  bool before = m_lines[idx]->is_modified_line();
+  // assert(status==MISS||status==SECTOR_MISS); // MSHR should have prevented
+  // redundant memory request
+  if (status == MISS) {
+    m_lines[idx]->allocate(m_config.tag(addr), m_config.block_addr(addr), time,
+                           mask);
+  } else if (status == SECTOR_MISS) {
+    assert(m_config.m_cache_type == SECTOR);
+    ((sector_cache_block *)m_lines[idx])->allocate_sector(time, mask);
+  }
+  if (before && !m_lines[idx]->is_modified_line()) {
+    m_dirty--;
+  }
+  before = m_lines[idx]->is_modified_line();
+  m_lines[idx]->fill(time, mask, byte_mask);
+  if (m_lines[idx]->is_modified_line() && !before) {
+    m_dirty++;
+  }
+}
+
+void tag_array::fill(unsigned index, unsigned time, mem_fetch *mf) {
+  assert(m_config.m_alloc_policy == ON_MISS);
+  bool before = m_lines[index]->is_modified_line();
+  m_lines[index]->fill(time, mf->get_access_sector_mask(), mf->get_access_byte_mask());
+  if (m_lines[index]->is_modified_line() && !before) {
+    m_dirty++;
+  }
+}
+
+// TODO: we need write back the flushed data to the upper level
+void tag_array::flush() {
+  if (!is_used) return;
+
+  for (unsigned i = 0; i < m_config.get_num_lines(); i++)
+    if (m_lines[i]->is_modified_line()) {
+      for (unsigned j = 0; j < SECTOR_CHUNCK_SIZE; j++) {
+        m_lines[i]->set_status(INVALID, mem_access_sector_mask_t().set(j));
+      }
+    }
 
-    is_used = false;
+  m_dirty = 0;
+  is_used = false;
 }
 
-void tag_array::invalidate()
-{
-	if(!is_used)
-		return;
+void tag_array::invalidate() {
+  if (!is_used) return;
 
-    for (unsigned i=0; i < m_config.get_num_lines(); i++)
-    	for(unsigned j=0; j < SECTOR_CHUNCK_SIZE; j++)
-    		m_lines[i]->set_status(INVALID, mem_access_sector_mask_t().set(j)) ;
+  for (unsigned i = 0; i < m_config.get_num_lines(); i++)
+    for (unsigned j = 0; j < SECTOR_CHUNCK_SIZE; j++)
+      m_lines[i]->set_status(INVALID, mem_access_sector_mask_t().set(j));
 
-    is_used = false;
+  m_dirty = 0;
+  is_used = false;
 }
 
-float tag_array::windowed_miss_rate( ) const
-{
-    unsigned n_access    = m_access - m_prev_snapshot_access;
-    unsigned n_miss      = (m_miss+m_sector_miss) - m_prev_snapshot_miss;
-    // unsigned n_pending_hit = m_pending_hit - m_prev_snapshot_pending_hit;
+float tag_array::windowed_miss_rate() const {
+  unsigned n_access = m_access - m_prev_snapshot_access;
+  unsigned n_miss = (m_miss + m_sector_miss) - m_prev_snapshot_miss;
+  // unsigned n_pending_hit = m_pending_hit - m_prev_snapshot_pending_hit;
 
-    float missrate = 0.0f;
-    if (n_access != 0)
-        missrate = (float) (n_miss+m_sector_miss) / n_access;
-    return missrate;
+  float missrate = 0.0f;
+  if (n_access != 0) missrate = (float)(n_miss + m_sector_miss) / n_access;
+  return missrate;
 }
 
-void tag_array::new_window()
-{
-    m_prev_snapshot_access = m_access;
-    m_prev_snapshot_miss = m_miss;
-    m_prev_snapshot_miss = m_miss + m_sector_miss;
-    m_prev_snapshot_pending_hit = m_pending_hit;
+void tag_array::new_window() {
+  m_prev_snapshot_access = m_access;
+  m_prev_snapshot_miss = m_miss;
+  m_prev_snapshot_miss = m_miss + m_sector_miss;
+  m_prev_snapshot_pending_hit = m_pending_hit;
 }
 
-void tag_array::print( FILE *stream, unsigned &total_access, unsigned &total_misses ) const
-{
-    m_config.print(stream);
-    fprintf( stream, "\t\tAccess = %d, Miss = %d, Sector_Miss = %d, Total_Miss = %d (%.3g), PendingHit = %d (%.3g)\n",
-             m_access, m_miss, m_sector_miss, (m_miss+m_sector_miss), (float) (m_miss+m_sector_miss) / m_access,
-             m_pending_hit, (float) m_pending_hit / m_access);
-    total_misses+=(m_miss+m_sector_miss);
-    total_access+=m_access;
+void tag_array::print(FILE *stream, unsigned &total_access,
+                      unsigned &total_misses) const {
+  m_config.print(stream);
+  fprintf(stream,
+          "\t\tAccess = %d, Miss = %d, Sector_Miss = %d, Total_Miss = %d "
+          "(%.3g), PendingHit = %d (%.3g)\n",
+          m_access, m_miss, m_sector_miss, (m_miss + m_sector_miss),
+          (float)(m_miss + m_sector_miss) / m_access, m_pending_hit,
+          (float)m_pending_hit / m_access);
+  total_misses += (m_miss + m_sector_miss);
+  total_access += m_access;
 }
 
-void tag_array::get_stats(unsigned &total_access, unsigned &total_misses, unsigned &total_hit_res, unsigned &total_res_fail) const{
-    // Update statistics from the tag array
-    total_access    = m_access;
-    total_misses    = (m_miss+m_sector_miss);
-    total_hit_res   = m_pending_hit;
-    total_res_fail  = m_res_fail;
+void tag_array::get_stats(unsigned &total_access, unsigned &total_misses,
+                          unsigned &total_hit_res,
+                          unsigned &total_res_fail) const {
+  // Update statistics from the tag array
+  total_access = m_access;
+  total_misses = (m_miss + m_sector_miss);
+  total_hit_res = m_pending_hit;
+  total_res_fail = m_res_fail;
 }
 
-
-bool was_write_sent( const std::list<cache_event> &events )
-{
-    for( std::list<cache_event>::const_iterator e=events.begin(); e!=events.end(); e++ ) {
-        if( (*e).m_cache_event_type == WRITE_REQUEST_SENT )
-            return true;
-    }
-    return false;
+bool was_write_sent(const std::list<cache_event> &events) {
+  for (std::list<cache_event>::const_iterator e = events.begin();
+       e != events.end(); e++) {
+    if ((*e).m_cache_event_type == WRITE_REQUEST_SENT) return true;
+  }
+  return false;
 }
 
-bool was_writeback_sent( const std::list<cache_event> &events, cache_event& wb_event)
-{
-    for( std::list<cache_event>::const_iterator e=events.begin(); e!=events.end(); e++ ) {
-        if( (*e).m_cache_event_type == WRITE_BACK_REQUEST_SENT )
-        	wb_event = *e;
-            return true;
+bool was_writeback_sent(const std::list<cache_event> &events,
+                        cache_event &wb_event) {
+  for (std::list<cache_event>::const_iterator e = events.begin();
+       e != events.end(); e++) {
+    if ((*e).m_cache_event_type == WRITE_BACK_REQUEST_SENT) {
+      wb_event = *e;
+      return true;
     }
-    return false;
+  }
+  return false;
 }
 
-bool was_read_sent( const std::list<cache_event> &events )
-{
-    for( std::list<cache_event>::const_iterator e=events.begin(); e!=events.end(); e++ ) {
-        if( (*e).m_cache_event_type == READ_REQUEST_SENT )
-            return true;
-    }
-    return false;
+bool was_read_sent(const std::list<cache_event> &events) {
+  for (std::list<cache_event>::const_iterator e = events.begin();
+       e != events.end(); e++) {
+    if ((*e).m_cache_event_type == READ_REQUEST_SENT) return true;
+  }
+  return false;
 }
 
-bool was_writeallocate_sent( const std::list<cache_event> &events )
-{
-    for( std::list<cache_event>::const_iterator e=events.begin(); e!=events.end(); e++ ) {
-        if( (*e).m_cache_event_type == WRITE_ALLOCATE_SENT )
-            return true;
-    }
-    return false;
+bool was_writeallocate_sent(const std::list<cache_event> &events) {
+  for (std::list<cache_event>::const_iterator e = events.begin();
+       e != events.end(); e++) {
+    if ((*e).m_cache_event_type == WRITE_ALLOCATE_SENT) return true;
+  }
+  return false;
 }
-/****************************************************************** MSHR ******************************************************************/
+/****************************************************************** MSHR
+ * ******************************************************************/
 
 /// Checks if there is a pending request to the lower memory level already
-bool mshr_table::probe( new_addr_type block_addr ) const{
-    table::const_iterator a = m_data.find(block_addr);
-    return a != m_data.end();
+bool mshr_table::probe(new_addr_type block_addr) const {
+  table::const_iterator a = m_data.find(block_addr);
+  return a != m_data.end();
 }
 
 /// Checks if there is space for tracking a new memory access
-bool mshr_table::full( new_addr_type block_addr ) const{
-    table::const_iterator i=m_data.find(block_addr);
-    if ( i != m_data.end() )
-        return i->second.m_list.size() >= m_max_merged;
-    else
-        return m_data.size() >= m_num_entries;
+bool mshr_table::full(new_addr_type block_addr) const {
+  table::const_iterator i = m_data.find(block_addr);
+  if (i != m_data.end())
+    return i->second.m_list.size() >= m_max_merged;
+  else
+    return m_data.size() >= m_num_entries;
 }
 
 /// Add or merge this access
-void mshr_table::add( new_addr_type block_addr, mem_fetch *mf ){
-	m_data[block_addr].m_list.push_back(mf);
-	assert( m_data.size() <= m_num_entries );
-	assert( m_data[block_addr].m_list.size() <= m_max_merged );
-	// indicate that this MSHR entry contains an atomic operation
-	if ( mf->isatomic() ) {
-		m_data[block_addr].m_has_atomic = true;
-	}
+void mshr_table::add(new_addr_type block_addr, mem_fetch *mf) {
+  m_data[block_addr].m_list.push_back(mf);
+  assert(m_data.size() <= m_num_entries);
+  assert(m_data[block_addr].m_list.size() <= m_max_merged);
+  // indicate that this MSHR entry contains an atomic operation
+  if (mf->isatomic()) {
+    m_data[block_addr].m_has_atomic = true;
+  }
 }
 
 /// check is_read_after_write_pending
-bool mshr_table::is_read_after_write_pending( new_addr_type block_addr){
-	std::list<mem_fetch*> my_list = m_data[block_addr].m_list;
-	bool write_found = false;
-	for (std::list<mem_fetch*>::iterator it=my_list.begin(); it != my_list.end(); ++it)
-	{
-		if((*it)->is_write()) //Pending Write Request
-			write_found = true;
-		else if(write_found)   //Pending Read Request and we found previous Write
-				return true;
-	}
-
-	return false;
+bool mshr_table::is_read_after_write_pending(new_addr_type block_addr) {
+  std::list<mem_fetch *> my_list = m_data[block_addr].m_list;
+  bool write_found = false;
+  for (std::list<mem_fetch *>::iterator it = my_list.begin();
+       it != my_list.end(); ++it) {
+    if ((*it)->is_write())  // Pending Write Request
+      write_found = true;
+    else if (write_found)  // Pending Read Request and we found previous Write
+      return true;
+  }
 
+  return false;
 }
 
 /// Accept a new cache fill response: mark entry ready for processing
-void mshr_table::mark_ready( new_addr_type block_addr, bool &has_atomic ){
-    assert( !busy() );
-    table::iterator a = m_data.find(block_addr);
-    assert( a != m_data.end() );
-    m_current_response.push_back( block_addr );
-    has_atomic = a->second.m_has_atomic;
-    assert( m_current_response.size() <= m_data.size() );
+void mshr_table::mark_ready(new_addr_type block_addr, bool &has_atomic) {
+  assert(!busy());
+  table::iterator a = m_data.find(block_addr);
+  assert(a != m_data.end());
+  m_current_response.push_back(block_addr);
+  has_atomic = a->second.m_has_atomic;
+  assert(m_current_response.size() <= m_data.size());
 }
 
 /// Returns next ready access
-mem_fetch *mshr_table::next_access(){
-    assert( access_ready() );
-    new_addr_type block_addr = m_current_response.front();
-    assert( !m_data[block_addr].m_list.empty() );
-    mem_fetch *result = m_data[block_addr].m_list.front();
-    m_data[block_addr].m_list.pop_front();
-    if ( m_data[block_addr].m_list.empty() ) {
-        // release entry
-        m_data.erase(block_addr);
-        m_current_response.pop_front();
+mem_fetch *mshr_table::next_access() {
+  assert(access_ready());
+  new_addr_type block_addr = m_current_response.front();
+  assert(!m_data[block_addr].m_list.empty());
+  mem_fetch *result = m_data[block_addr].m_list.front();
+  m_data[block_addr].m_list.pop_front();
+  if (m_data[block_addr].m_list.empty()) {
+    // release entry
+    m_data.erase(block_addr);
+    m_current_response.pop_front();
+  }
+  return result;
+}
+
+void mshr_table::display(FILE *fp) const {
+  fprintf(fp, "MSHR contents\n");
+  for (table::const_iterator e = m_data.begin(); e != m_data.end(); ++e) {
+    unsigned block_addr = e->first;
+    fprintf(fp, "MSHR: tag=0x%06x, atomic=%d %zu entries : ", block_addr,
+            e->second.m_has_atomic, e->second.m_list.size());
+    if (!e->second.m_list.empty()) {
+      mem_fetch *mf = e->second.m_list.front();
+      fprintf(fp, "%p :", mf);
+      mf->print(fp);
+    } else {
+      fprintf(fp, " no memory requests???\n");
     }
-    return result;
-}
-
-void mshr_table::display( FILE *fp ) const{
-    fprintf(fp,"MSHR contents\n");
-    for ( table::const_iterator e=m_data.begin(); e!=m_data.end(); ++e ) {
-        unsigned block_addr = e->first;
-        fprintf(fp,"MSHR: tag=0x%06x, atomic=%d %zu entries : ", block_addr, e->second.m_has_atomic, e->second.m_list.size());
-        if ( !e->second.m_list.empty() ) {
-            mem_fetch *mf = e->second.m_list.front();
-            fprintf(fp,"%p :",mf);
-            mf->print(fp);
-        } else {
-            fprintf(fp," no memory requests???\n");
-        }
+  }
+}
+/***************************************************************** Caches
+ * *****************************************************************/
+cache_stats::cache_stats() {
+  m_stats.resize(NUM_MEM_ACCESS_TYPE);
+  m_stats_pw.resize(NUM_MEM_ACCESS_TYPE);
+  m_fail_stats.resize(NUM_MEM_ACCESS_TYPE);
+  for (unsigned i = 0; i < NUM_MEM_ACCESS_TYPE; ++i) {
+    m_stats[i].resize(NUM_CACHE_REQUEST_STATUS, 0);
+    m_stats_pw[i].resize(NUM_CACHE_REQUEST_STATUS, 0);
+    m_fail_stats[i].resize(NUM_CACHE_RESERVATION_FAIL_STATUS, 0);
+  }
+  m_cache_port_available_cycles = 0;
+  m_cache_data_port_busy_cycles = 0;
+  m_cache_fill_port_busy_cycles = 0;
+}
+
+void cache_stats::clear() {
+  ///
+  /// Zero out all current cache statistics
+  ///
+  for (unsigned i = 0; i < NUM_MEM_ACCESS_TYPE; ++i) {
+    std::fill(m_stats[i].begin(), m_stats[i].end(), 0);
+    std::fill(m_stats_pw[i].begin(), m_stats_pw[i].end(), 0);
+    std::fill(m_fail_stats[i].begin(), m_fail_stats[i].end(), 0);
+  }
+  m_cache_port_available_cycles = 0;
+  m_cache_data_port_busy_cycles = 0;
+  m_cache_fill_port_busy_cycles = 0;
+}
+
+void cache_stats::clear_pw() {
+  ///
+  /// Zero out per-window cache statistics
+  ///
+  for (unsigned i = 0; i < NUM_MEM_ACCESS_TYPE; ++i) {
+    std::fill(m_stats_pw[i].begin(), m_stats_pw[i].end(), 0);
+  }
+}
+
+void cache_stats::inc_stats(int access_type, int access_outcome) {
+  ///
+  /// Increment the stat corresponding to (access_type, access_outcome) by 1.
+  ///
+  if (!check_valid(access_type, access_outcome))
+    assert(0 && "Unknown cache access type or access outcome");
+
+  m_stats[access_type][access_outcome]++;
+}
+
+void cache_stats::inc_stats_pw(int access_type, int access_outcome) {
+  ///
+  /// Increment the corresponding per-window cache stat
+  ///
+  if (!check_valid(access_type, access_outcome))
+    assert(0 && "Unknown cache access type or access outcome");
+  m_stats_pw[access_type][access_outcome]++;
+}
+
+void cache_stats::inc_fail_stats(int access_type, int fail_outcome) {
+  if (!check_fail_valid(access_type, fail_outcome))
+    assert(0 && "Unknown cache access type or access fail");
+
+  m_fail_stats[access_type][fail_outcome]++;
+}
+
+enum cache_request_status cache_stats::select_stats_status(
+    enum cache_request_status probe, enum cache_request_status access) const {
+  ///
+  /// This function selects how the cache access outcome should be counted.
+  /// HIT_RESERVED is considered as a MISS in the cores, however, it should be
+  /// counted as a HIT_RESERVED in the caches.
+  ///
+  if (probe == HIT_RESERVED && access != RESERVATION_FAIL)
+    return probe;
+  else if (probe == SECTOR_MISS && access == MISS)
+    return probe;
+  else
+    return access;
+}
+
+unsigned long long &cache_stats::operator()(int access_type, int access_outcome,
+                                            bool fail_outcome) {
+  ///
+  /// Simple method to read/modify the stat corresponding to (access_type,
+  /// access_outcome) Used overloaded () to avoid the need for separate
+  /// read/write member functions
+  ///
+  if (fail_outcome) {
+    if (!check_fail_valid(access_type, access_outcome))
+      assert(0 && "Unknown cache access type or fail outcome");
+
+    return m_fail_stats[access_type][access_outcome];
+  } else {
+    if (!check_valid(access_type, access_outcome))
+      assert(0 && "Unknown cache access type or access outcome");
+
+    return m_stats[access_type][access_outcome];
+  }
+}
+
+unsigned long long cache_stats::operator()(int access_type, int access_outcome,
+                                           bool fail_outcome) const {
+  ///
+  /// Const accessor into m_stats.
+  ///
+  if (fail_outcome) {
+    if (!check_fail_valid(access_type, access_outcome))
+      assert(0 && "Unknown cache access type or fail outcome");
+
+    return m_fail_stats[access_type][access_outcome];
+  } else {
+    if (!check_valid(access_type, access_outcome))
+      assert(0 && "Unknown cache access type or access outcome");
+
+    return m_stats[access_type][access_outcome];
+  }
+}
+
+cache_stats cache_stats::operator+(const cache_stats &cs) {
+  ///
+  /// Overloaded + operator to allow for simple stat accumulation
+  ///
+  cache_stats ret;
+  for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
+    for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
+      ret(type, status, false) =
+          m_stats[type][status] + cs(type, status, false);
     }
-}
-/***************************************************************** Caches *****************************************************************/
-cache_stats::cache_stats(){
-    m_stats.resize(NUM_MEM_ACCESS_TYPE);
-    m_stats_pw.resize(NUM_MEM_ACCESS_TYPE);
-    m_fail_stats.resize(NUM_MEM_ACCESS_TYPE);
-    for(unsigned i=0; i<NUM_MEM_ACCESS_TYPE; ++i){
-        m_stats[i].resize(NUM_CACHE_REQUEST_STATUS, 0);
-        m_stats_pw[i].resize(NUM_CACHE_REQUEST_STATUS, 0);
-		m_fail_stats[i].resize(NUM_CACHE_RESERVATION_FAIL_STATUS, 0);
-	}
-    m_cache_port_available_cycles = 0; 
-    m_cache_data_port_busy_cycles = 0; 
-    m_cache_fill_port_busy_cycles = 0; 
-}
-
-void cache_stats::clear(){
-    ///
-    /// Zero out all current cache statistics
-    ///
-    for(unsigned i=0; i<NUM_MEM_ACCESS_TYPE; ++i){
-        std::fill(m_stats[i].begin(), m_stats[i].end(), 0);
-		std::fill(m_fail_stats[i].begin(), m_fail_stats[i].end(), 0);
-	}
-    m_cache_port_available_cycles = 0; 
-    m_cache_data_port_busy_cycles = 0; 
-    m_cache_fill_port_busy_cycles = 0; 
-}
-
-void cache_stats::clear_pw(){
-    ///
-    /// Zero out per-window cache statistics
-    ///
-    for(unsigned i=0; i<NUM_MEM_ACCESS_TYPE; ++i){
-        std::fill(m_stats_pw[i].begin(), m_stats_pw[i].end(), 0);
-	}
-}
-
-void cache_stats::inc_stats(int access_type, int access_outcome){
-    ///
-    /// Increment the stat corresponding to (access_type, access_outcome) by 1.
-    ///
-    if(!check_valid(access_type, access_outcome))
-        assert(0 && "Unknown cache access type or access outcome");
-
-    m_stats[access_type][access_outcome]++;
-}
-
-void cache_stats::inc_stats_pw(int access_type, int access_outcome){
-    ///
-    /// Increment the corresponding per-window cache stat
-    ///
-    if(!check_valid(access_type, access_outcome))
-        assert(0 && "Unknown cache access type or access outcome");
-    m_stats_pw[access_type][access_outcome]++;
-}
-
-void cache_stats::inc_fail_stats(int access_type, int fail_outcome){
-
-	if(!check_fail_valid(access_type, fail_outcome))
-		 assert(0 && "Unknown cache access type or access fail");
-
-	m_fail_stats[access_type][fail_outcome]++;
-}
-
-
-enum cache_request_status cache_stats::select_stats_status(enum cache_request_status probe, enum cache_request_status access) const {
-	///
-	/// This function selects how the cache access outcome should be counted. HIT_RESERVED is considered as a MISS
-	/// in the cores, however, it should be counted as a HIT_RESERVED in the caches.
-	///
-	if(probe == HIT_RESERVED && access != RESERVATION_FAIL)
-		return probe;
-	else if(probe == SECTOR_MISS && access == MISS)
-		return probe;
-	else
-		return access;
-}
-
-unsigned long long &cache_stats::operator()(int access_type, int access_outcome, bool fail_outcome){
-    ///
-    /// Simple method to read/modify the stat corresponding to (access_type, access_outcome)
-    /// Used overloaded () to avoid the need for separate read/write member functions
-    ///
-	if(fail_outcome) {
-		if(!check_fail_valid(access_type, access_outcome))
-			assert(0 && "Unknown cache access type or fail outcome");
-
-		return m_fail_stats[access_type][access_outcome];
-	}
-	else {
-		if(!check_valid(access_type, access_outcome))
-			assert(0 && "Unknown cache access type or access outcome");
-
-		return m_stats[access_type][access_outcome];
-	}
-}
-
-unsigned long long cache_stats::operator()(int access_type, int access_outcome, bool fail_outcome) const{
-    ///
-    /// Const accessor into m_stats.
-    ///
-	if(fail_outcome) {
-		if(!check_fail_valid(access_type, access_outcome))
-			assert(0 && "Unknown cache access type or fail outcome");
-
-		return m_fail_stats[access_type][access_outcome];
-	}
-	else {
-		if(!check_valid(access_type, access_outcome))
-			assert(0 && "Unknown cache access type or access outcome");
-
-		return m_stats[access_type][access_outcome];
-	}
-}
-
-cache_stats cache_stats::operator+(const cache_stats &cs){
-    ///
-    /// Overloaded + operator to allow for simple stat accumulation
-    ///
-    cache_stats ret;
-    for(unsigned type=0; type<NUM_MEM_ACCESS_TYPE; ++type){
-        for(unsigned status=0; status<NUM_CACHE_REQUEST_STATUS; ++status){
-            ret(type, status, false) = m_stats[type][status] + cs(type, status, false);
-        }
-		for(unsigned status=0; status<NUM_CACHE_RESERVATION_FAIL_STATUS; ++status){
-			   ret(type, status, true) = m_fail_stats[type][status] + cs(type, status, true);
-		   }
-        }
-    ret.m_cache_port_available_cycles = m_cache_port_available_cycles + cs.m_cache_port_available_cycles; 
-    ret.m_cache_data_port_busy_cycles = m_cache_data_port_busy_cycles + cs.m_cache_data_port_busy_cycles; 
-    ret.m_cache_fill_port_busy_cycles = m_cache_fill_port_busy_cycles + cs.m_cache_fill_port_busy_cycles; 
-    return ret;
-}
-
-cache_stats &cache_stats::operator+=(const cache_stats &cs){
-    ///
-    /// Overloaded += operator to allow for simple stat accumulation
-    ///
-    for(unsigned type=0; type<NUM_MEM_ACCESS_TYPE; ++type){
-        for(unsigned status=0; status<NUM_CACHE_REQUEST_STATUS; ++status){
-            m_stats[type][status] += cs(type, status, false);
-        }
-        for(unsigned status=0; status<NUM_CACHE_REQUEST_STATUS; ++status){
-            m_stats_pw[type][status] += cs(type, status, false);
-        }
-        for(unsigned status=0; status<NUM_CACHE_RESERVATION_FAIL_STATUS; ++status){
-            m_fail_stats[type][status] += cs(type, status, true);
-	   }
+    for (unsigned status = 0; status < NUM_CACHE_RESERVATION_FAIL_STATUS;
+         ++status) {
+      ret(type, status, true) =
+          m_fail_stats[type][status] + cs(type, status, true);
     }
-    m_cache_port_available_cycles += cs.m_cache_port_available_cycles; 
-    m_cache_data_port_busy_cycles += cs.m_cache_data_port_busy_cycles; 
-    m_cache_fill_port_busy_cycles += cs.m_cache_fill_port_busy_cycles; 
-    return *this;
-}
-
-void cache_stats::print_stats(FILE *fout, const char *cache_name) const{
-    ///
-    /// Print out each non-zero cache statistic for every memory access type and status
-    /// "cache_name" defaults to "Cache_stats" when no argument is provided, otherwise
-    /// the provided name is used.
-    /// The printed format is "<cache_name>[<request_type>][<request_status>] = <stat_value>"
-    ///
-    std::vector< unsigned > total_access;
-    total_access.resize(NUM_MEM_ACCESS_TYPE, 0);
-    std::string m_cache_name = cache_name;
-    for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
-        for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
-            fprintf(fout, "\t%s[%s][%s] = %llu\n",
-                m_cache_name.c_str(),
-                mem_access_type_str((enum mem_access_type)type),
-                cache_request_status_str((enum cache_request_status)status),
-                m_stats[type][status]);
-
-            if(status != RESERVATION_FAIL)
-            	 total_access[type]+= m_stats[type][status];
-        }
+  }
+  ret.m_cache_port_available_cycles =
+      m_cache_port_available_cycles + cs.m_cache_port_available_cycles;
+  ret.m_cache_data_port_busy_cycles =
+      m_cache_data_port_busy_cycles + cs.m_cache_data_port_busy_cycles;
+  ret.m_cache_fill_port_busy_cycles =
+      m_cache_fill_port_busy_cycles + cs.m_cache_fill_port_busy_cycles;
+  return ret;
+}
+
+cache_stats &cache_stats::operator+=(const cache_stats &cs) {
+  ///
+  /// Overloaded += operator to allow for simple stat accumulation
+  ///
+  for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
+    for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
+      m_stats[type][status] += cs(type, status, false);
     }
-    for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
-    	 if(total_access[type] > 0)
-    	  fprintf(fout, "\t%s[%s][%s] = %llu\n",
-				m_cache_name.c_str(),
-				mem_access_type_str((enum mem_access_type)type),
-				"TOTAL_ACCESS",
-				total_access[type]);
+    for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
+      m_stats_pw[type][status] += cs(type, status, false);
     }
-}
-
-void cache_stats::print_fail_stats(FILE *fout, const char *cache_name) const{
-	std::string m_cache_name = cache_name;
-	    for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
-	        for (unsigned fail = 0; fail < NUM_CACHE_RESERVATION_FAIL_STATUS; ++fail) {
-	            if(m_fail_stats[type][fail] > 0){
-	                fprintf(fout, "\t%s[%s][%s] = %u\n",
-	                    m_cache_name.c_str(),
-	                    mem_access_type_str((enum mem_access_type)type),
-						cache_fail_status_str((enum cache_reservation_fail_reason)fail),
-						m_fail_stats[type][fail]);
-	            }
-	        }
-	    }
-}
-
-void cache_sub_stats::print_port_stats(FILE *fout, const char *cache_name) const
-{
-    float data_port_util = 0.0f; 
-    if (port_available_cycles > 0) {
-        data_port_util = (float) data_port_busy_cycles / port_available_cycles; 
+    for (unsigned status = 0; status < NUM_CACHE_RESERVATION_FAIL_STATUS;
+         ++status) {
+      m_fail_stats[type][status] += cs(type, status, true);
     }
-    fprintf(fout, "%s_data_port_util = %.3f\n", cache_name, data_port_util); 
-    float fill_port_util = 0.0f; 
-    if (port_available_cycles > 0) {
-        fill_port_util = (float) fill_port_busy_cycles / port_available_cycles; 
+  }
+  m_cache_port_available_cycles += cs.m_cache_port_available_cycles;
+  m_cache_data_port_busy_cycles += cs.m_cache_data_port_busy_cycles;
+  m_cache_fill_port_busy_cycles += cs.m_cache_fill_port_busy_cycles;
+  return *this;
+}
+
+void cache_stats::print_stats(FILE *fout, const char *cache_name) const {
+  ///
+  /// Print out each non-zero cache statistic for every memory access type and
+  /// status "cache_name" defaults to "Cache_stats" when no argument is
+  /// provided, otherwise the provided name is used. The printed format is
+  /// "<cache_name>[<request_type>][<request_status>] = <stat_value>"
+  ///
+  std::vector<unsigned> total_access;
+  total_access.resize(NUM_MEM_ACCESS_TYPE, 0);
+  std::string m_cache_name = cache_name;
+  for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
+    for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
+      fprintf(fout, "\t%s[%s][%s] = %llu\n", m_cache_name.c_str(),
+              mem_access_type_str((enum mem_access_type)type),
+              cache_request_status_str((enum cache_request_status)status),
+              m_stats[type][status]);
+
+      if (status != RESERVATION_FAIL && status != MSHR_HIT)
+        // MSHR_HIT is a special type of SECTOR_MISS
+        // so its already included in the SECTOR_MISS
+        total_access[type] += m_stats[type][status];
     }
-    fprintf(fout, "%s_fill_port_util = %.3f\n", cache_name, fill_port_util); 
-}
-
-unsigned long long cache_stats::get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status) const{
-    ///
-    /// Returns a sum of the stats corresponding to each "access_type" and "access_status" pair.
-    /// "access_type" is an array of "num_access_type" mem_access_types.
-    /// "access_status" is an array of "num_access_status" cache_request_statuses.
-    ///
-    unsigned long long total=0;
-    for(unsigned type =0; type < num_access_type; ++type){
-        for(unsigned status=0; status < num_access_status; ++status){
-            if(!check_valid((int)access_type[type], (int)access_status[status]))
-                assert(0 && "Unknown cache access type or access outcome");
-            total += m_stats[access_type[type]][access_status[status]];
-        }
+  }
+  for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
+    if (total_access[type] > 0)
+      fprintf(fout, "\t%s[%s][%s] = %u\n", m_cache_name.c_str(),
+              mem_access_type_str((enum mem_access_type)type), "TOTAL_ACCESS",
+              total_access[type]);
+  }
+}
+
+void cache_stats::print_fail_stats(FILE *fout, const char *cache_name) const {
+  std::string m_cache_name = cache_name;
+  for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
+    for (unsigned fail = 0; fail < NUM_CACHE_RESERVATION_FAIL_STATUS; ++fail) {
+      if (m_fail_stats[type][fail] > 0) {
+        fprintf(fout, "\t%s[%s][%s] = %llu\n", m_cache_name.c_str(),
+                mem_access_type_str((enum mem_access_type)type),
+                cache_fail_status_str((enum cache_reservation_fail_reason)fail),
+                m_fail_stats[type][fail]);
+      }
     }
-    return total;
+  }
+}
+
+void cache_sub_stats::print_port_stats(FILE *fout,
+                                       const char *cache_name) const {
+  float data_port_util = 0.0f;
+  if (port_available_cycles > 0) {
+    data_port_util = (float)data_port_busy_cycles / port_available_cycles;
+  }
+  fprintf(fout, "%s_data_port_util = %.3f\n", cache_name, data_port_util);
+  float fill_port_util = 0.0f;
+  if (port_available_cycles > 0) {
+    fill_port_util = (float)fill_port_busy_cycles / port_available_cycles;
+  }
+  fprintf(fout, "%s_fill_port_util = %.3f\n", cache_name, fill_port_util);
+}
+
+unsigned long long cache_stats::get_stats(
+    enum mem_access_type *access_type, unsigned num_access_type,
+    enum cache_request_status *access_status,
+    unsigned num_access_status) const {
+  ///
+  /// Returns a sum of the stats corresponding to each "access_type" and
+  /// "access_status" pair. "access_type" is an array of "num_access_type"
+  /// mem_access_types. "access_status" is an array of "num_access_status"
+  /// cache_request_statuses.
+  ///
+  unsigned long long total = 0;
+  for (unsigned type = 0; type < num_access_type; ++type) {
+    for (unsigned status = 0; status < num_access_status; ++status) {
+      if (!check_valid((int)access_type[type], (int)access_status[status]))
+        assert(0 && "Unknown cache access type or access outcome");
+      total += m_stats[access_type[type]][access_status[status]];
+    }
+  }
+  return total;
 }
 
-void cache_stats::get_sub_stats(struct cache_sub_stats &css) const{
-    ///
-    /// Overwrites "css" with the appropriate statistics from this cache.
-    ///
-    struct cache_sub_stats t_css;
-    t_css.clear();
+void cache_stats::get_sub_stats(struct cache_sub_stats &css) const {
+  ///
+  /// Overwrites "css" with the appropriate statistics from this cache.
+  ///
+  struct cache_sub_stats t_css;
+  t_css.clear();
 
-    for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
-        for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
-            if(status == HIT || status == MISS || status == SECTOR_MISS || status == HIT_RESERVED)
-                t_css.accesses += m_stats[type][status];
+  for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
+    for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
+      if (status == HIT || status == MISS || status == SECTOR_MISS ||
+          status == HIT_RESERVED)
+        t_css.accesses += m_stats[type][status];
 
-            if(status == MISS || status == SECTOR_MISS)
-                t_css.misses += m_stats[type][status];
+      if (status == MISS || status == SECTOR_MISS)
+        t_css.misses += m_stats[type][status];
 
-            if(status == HIT_RESERVED)
-                t_css.pending_hits += m_stats[type][status];
+      if (status == HIT_RESERVED) t_css.pending_hits += m_stats[type][status];
 
-            if(status == RESERVATION_FAIL)
-                t_css.res_fails += m_stats[type][status];
-        }
+      if (status == RESERVATION_FAIL) t_css.res_fails += m_stats[type][status];
     }
+  }
 
-    t_css.port_available_cycles = m_cache_port_available_cycles; 
-    t_css.data_port_busy_cycles = m_cache_data_port_busy_cycles; 
-    t_css.fill_port_busy_cycles = m_cache_fill_port_busy_cycles; 
+  t_css.port_available_cycles = m_cache_port_available_cycles;
+  t_css.data_port_busy_cycles = m_cache_data_port_busy_cycles;
+  t_css.fill_port_busy_cycles = m_cache_fill_port_busy_cycles;
 
-    css = t_css;
+  css = t_css;
 }
 
-void cache_stats::get_sub_stats_pw(struct cache_sub_stats_pw &css) const{
-    ///
-    /// Overwrites "css" with the appropriate statistics from this cache.
-    ///
-    struct cache_sub_stats_pw t_css;
-    t_css.clear();
+void cache_stats::get_sub_stats_pw(struct cache_sub_stats_pw &css) const {
+  ///
+  /// Overwrites "css" with the appropriate statistics from this cache.
+  ///
+  struct cache_sub_stats_pw t_css;
+  t_css.clear();
 
-    for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
-        for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
-            if(status == HIT || status == MISS || status == SECTOR_MISS || status == HIT_RESERVED)
-                t_css.accesses += m_stats_pw[type][status];
+  for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
+    for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
+      if (status == HIT || status == MISS || status == SECTOR_MISS ||
+          status == HIT_RESERVED)
+        t_css.accesses += m_stats_pw[type][status];
 
-            if(status == HIT){
-                if(type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R){
-                    t_css.read_hits += m_stats_pw[type][status];
-                } else if(type == GLOBAL_ACC_W){
-                    t_css.write_hits += m_stats_pw[type][status];
-                }
-            }
+      if (status == HIT) {
+        if (type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R) {
+          t_css.read_hits += m_stats_pw[type][status];
+        } else if (type == GLOBAL_ACC_W) {
+          t_css.write_hits += m_stats_pw[type][status];
+        }
+      }
 
-            if(status == MISS || status == SECTOR_MISS){
-                if(type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R){
-                    t_css.read_misses += m_stats_pw[type][status];
-                } else if(type == GLOBAL_ACC_W){
-                    t_css.write_misses += m_stats_pw[type][status];
-                }
-            }
+      if (status == MISS || status == SECTOR_MISS) {
+        if (type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R) {
+          t_css.read_misses += m_stats_pw[type][status];
+        } else if (type == GLOBAL_ACC_W) {
+          t_css.write_misses += m_stats_pw[type][status];
+        }
+      }
 
-            if(status == HIT_RESERVED){
-                if(type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R){
-                    t_css.read_pending_hits += m_stats_pw[type][status];
-                } else if(type == GLOBAL_ACC_W){
-                    t_css.write_pending_hits += m_stats_pw[type][status];
-                }
-            }
+      if (status == HIT_RESERVED) {
+        if (type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R) {
+          t_css.read_pending_hits += m_stats_pw[type][status];
+        } else if (type == GLOBAL_ACC_W) {
+          t_css.write_pending_hits += m_stats_pw[type][status];
+        }
+      }
 
-            if(status == RESERVATION_FAIL){
-                if(type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R){
-                    t_css.read_res_fails += m_stats_pw[type][status];
-                } else if(type == GLOBAL_ACC_W){
-                    t_css.write_res_fails += m_stats_pw[type][status];
-                }
-            }
+      if (status == RESERVATION_FAIL) {
+        if (type == GLOBAL_ACC_R || type == CONST_ACC_R || type == INST_ACC_R) {
+          t_css.read_res_fails += m_stats_pw[type][status];
+        } else if (type == GLOBAL_ACC_W) {
+          t_css.write_res_fails += m_stats_pw[type][status];
         }
+      }
     }
+  }
 
-    css = t_css;
+  css = t_css;
 }
 
-bool cache_stats::check_valid(int type, int status) const{
-    ///
-    /// Verify a valid access_type/access_status
-    ///
-    if((type >= 0) && (type < NUM_MEM_ACCESS_TYPE) && (status >= 0) && (status < NUM_CACHE_REQUEST_STATUS))
-        return true;
-    else
-        return false;
+bool cache_stats::check_valid(int type, int status) const {
+  ///
+  /// Verify a valid access_type/access_status
+  ///
+  if ((type >= 0) && (type < NUM_MEM_ACCESS_TYPE) && (status >= 0) &&
+      (status < NUM_CACHE_REQUEST_STATUS))
+    return true;
+  else
+    return false;
 }
 
-bool cache_stats::check_fail_valid(int type, int fail) const{
-    ///
-    /// Verify a valid access_type/access_status
-    ///
-    if((type >= 0) && (type < NUM_MEM_ACCESS_TYPE) && (fail >= 0) && (fail < NUM_CACHE_RESERVATION_FAIL_STATUS))
-        return true;
-    else
-        return false;
+bool cache_stats::check_fail_valid(int type, int fail) const {
+  ///
+  /// Verify a valid access_type/access_status
+  ///
+  if ((type >= 0) && (type < NUM_MEM_ACCESS_TYPE) && (fail >= 0) &&
+      (fail < NUM_CACHE_RESERVATION_FAIL_STATUS))
+    return true;
+  else
+    return false;
 }
 
-void cache_stats::sample_cache_port_utility(bool data_port_busy, bool fill_port_busy) 
-{
-    m_cache_port_available_cycles += 1; 
-    if (data_port_busy) {
-        m_cache_data_port_busy_cycles += 1; 
-    } 
-    if (fill_port_busy) {
-        m_cache_fill_port_busy_cycles += 1; 
-    } 
+void cache_stats::sample_cache_port_utility(bool data_port_busy,
+                                            bool fill_port_busy) {
+  m_cache_port_available_cycles += 1;
+  if (data_port_busy) {
+    m_cache_data_port_busy_cycles += 1;
+  }
+  if (fill_port_busy) {
+    m_cache_fill_port_busy_cycles += 1;
+  }
 }
 
-baseline_cache::bandwidth_management::bandwidth_management(cache_config &config) 
-: m_config(config)
-{
-    m_data_port_occupied_cycles = 0; 
-    m_fill_port_occupied_cycles = 0; 
+baseline_cache::bandwidth_management::bandwidth_management(cache_config &config)
+    : m_config(config) {
+  m_data_port_occupied_cycles = 0;
+  m_fill_port_occupied_cycles = 0;
 }
 
-/// use the data port based on the outcome and events generated by the mem_fetch request 
-void baseline_cache::bandwidth_management::use_data_port(mem_fetch *mf, enum cache_request_status outcome, const std::list<cache_event> &events)
-{
-    unsigned data_size = mf->get_data_size(); 
-    unsigned port_width = m_config.m_data_port_width; 
-    switch (outcome) {
+/// use the data port based on the outcome and events generated by the mem_fetch
+/// request
+void baseline_cache::bandwidth_management::use_data_port(
+    mem_fetch *mf, enum cache_request_status outcome,
+    const std::list<cache_event> &events) {
+  unsigned data_size = mf->get_data_size();
+  unsigned port_width = m_config.m_data_port_width;
+  switch (outcome) {
     case HIT: {
-        unsigned data_cycles = data_size / port_width + ((data_size % port_width > 0)? 1 : 0); 
-        m_data_port_occupied_cycles += data_cycles; 
-        } break; 
+      unsigned data_cycles =
+          data_size / port_width + ((data_size % port_width > 0) ? 1 : 0);
+      m_data_port_occupied_cycles += data_cycles;
+    } break;
     case HIT_RESERVED:
     case MISS: {
-        // the data array is accessed to read out the entire line for write-back 
-    	// in case of sector cache we need to write bank only the modified sectors
-    	cache_event ev(WRITE_BACK_REQUEST_SENT);
-        if (was_writeback_sent(events, ev)) {
-            unsigned data_cycles = ev.m_evicted_block.m_modified_size / port_width;
-            m_data_port_occupied_cycles += data_cycles; 
-        }
-        } break; 
+      // the data array is accessed to read out the entire line for write-back
+      // in case of sector cache we need to write bank only the modified sectors
+      cache_event ev(WRITE_BACK_REQUEST_SENT);
+      if (was_writeback_sent(events, ev)) {
+        unsigned data_cycles = ev.m_evicted_block.m_modified_size / port_width;
+        m_data_port_occupied_cycles += data_cycles;
+      }
+    } break;
     case SECTOR_MISS:
     case RESERVATION_FAIL:
-        // Does not consume any port bandwidth 
-        break; 
-    default: 
-        assert(0); 
-        break; 
-    } 
-}
-
-/// use the fill port 
-void baseline_cache::bandwidth_management::use_fill_port(mem_fetch *mf)
-{
-    // assume filling the entire line with the returned request 
-    unsigned fill_cycles = m_config.get_atom_sz() / m_config.m_data_port_width;
-    m_fill_port_occupied_cycles += fill_cycles; 
-}
-
-/// called every cache cycle to free up the ports 
-void baseline_cache::bandwidth_management::replenish_port_bandwidth()
-{
-    if (m_data_port_occupied_cycles > 0) {
-        m_data_port_occupied_cycles -= 1; 
-    }
-    assert(m_data_port_occupied_cycles >= 0); 
-
-    if (m_fill_port_occupied_cycles > 0) {
-        m_fill_port_occupied_cycles -= 1; 
-    }
-    assert(m_fill_port_occupied_cycles >= 0); 
+      // Does not consume any port bandwidth
+      break;
+    default:
+      assert(0);
+      break;
+  }
 }
 
-/// query for data port availability 
-bool baseline_cache::bandwidth_management::data_port_free() const
-{
-    return (m_data_port_occupied_cycles == 0); 
+/// use the fill port
+void baseline_cache::bandwidth_management::use_fill_port(mem_fetch *mf) {
+  // assume filling the entire line with the returned request
+  unsigned fill_cycles = m_config.get_atom_sz() / m_config.m_data_port_width;
+  m_fill_port_occupied_cycles += fill_cycles;
 }
 
-/// query for fill port availability 
-bool baseline_cache::bandwidth_management::fill_port_free() const
-{
-    return (m_fill_port_occupied_cycles == 0); 
+/// called every cache cycle to free up the ports
+void baseline_cache::bandwidth_management::replenish_port_bandwidth() {
+  if (m_data_port_occupied_cycles > 0) {
+    m_data_port_occupied_cycles -= 1;
+  }
+  assert(m_data_port_occupied_cycles >= 0);
+
+  if (m_fill_port_occupied_cycles > 0) {
+    m_fill_port_occupied_cycles -= 1;
+  }
+  assert(m_fill_port_occupied_cycles >= 0);
 }
 
-/// Sends next request to lower level of memory
-void baseline_cache::cycle(){
-    if ( !m_miss_queue.empty() ) {
-        mem_fetch *mf = m_miss_queue.front();
-        if ( !m_memport->full(mf->size(),mf->get_is_write()) ) {
-            m_miss_queue.pop_front();
-            m_memport->push(mf);
-        }
-    }
-    bool data_port_busy = !m_bandwidth_management.data_port_free(); 
-    bool fill_port_busy = !m_bandwidth_management.fill_port_free(); 
-    m_stats.sample_cache_port_utility(data_port_busy, fill_port_busy); 
-    m_bandwidth_management.replenish_port_bandwidth(); 
+/// query for data port availability
+bool baseline_cache::bandwidth_management::data_port_free() const {
+  return (m_data_port_occupied_cycles == 0);
 }
 
-/// Interface for response from lower memory level (model bandwidth restictions in caller)
-void baseline_cache::fill(mem_fetch *mf, unsigned time){
+/// query for fill port availability
+bool baseline_cache::bandwidth_management::fill_port_free() const {
+  return (m_fill_port_occupied_cycles == 0);
+}
 
-	if(m_config.m_mshr_type == SECTOR_ASSOC) {
-	assert(mf->get_original_mf());
-	extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf->get_original_mf());
-    assert( e != m_extra_mf_fields.end() );
+/// Sends next request to lower level of memory
+void baseline_cache::cycle() {
+  if (!m_miss_queue.empty()) {
+    mem_fetch *mf = m_miss_queue.front();
+    if (!m_memport->full(mf->size(), mf->get_is_write())) {
+      m_miss_queue.pop_front();
+      m_memport->push(mf);
+    }
+  }
+  bool data_port_busy = !m_bandwidth_management.data_port_free();
+  bool fill_port_busy = !m_bandwidth_management.fill_port_free();
+  m_stats.sample_cache_port_utility(data_port_busy, fill_port_busy);
+  m_bandwidth_management.replenish_port_bandwidth();
+}
+
+/// Interface for response from lower memory level (model bandwidth restictions
+/// in caller)
+void baseline_cache::fill(mem_fetch *mf, unsigned time) {
+  if (m_config.m_mshr_type == SECTOR_ASSOC) {
+    assert(mf->get_original_mf());
+    extra_mf_fields_lookup::iterator e =
+        m_extra_mf_fields.find(mf->get_original_mf());
+    assert(e != m_extra_mf_fields.end());
     e->second.pending_read--;
 
-    if(e->second.pending_read > 0) {
-    	//wait for the other requests to come back
-    	delete mf;
-    	return;
-      } else {
-    	mem_fetch *temp = mf;
-    	mf = mf->get_original_mf();
-    	delete temp;
-      }
-	}
-
-    extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
-    assert( e != m_extra_mf_fields.end() );
-    assert( e->second.m_valid );
-    mf->set_data_size( e->second.m_data_size );
-    mf->set_addr( e->second.m_addr );
-    if ( m_config.m_alloc_policy == ON_MISS )
-        m_tag_array->fill(e->second.m_cache_index,time,mf);
-    else if ( m_config.m_alloc_policy == ON_FILL ) {
-        m_tag_array->fill(e->second.m_block_addr,time,mf);
-        if(m_config.is_streaming())
-        	m_tag_array->remove_pending_line(mf);
+    if (e->second.pending_read > 0) {
+      // wait for the other requests to come back
+      delete mf;
+      return;
+    } else {
+      mem_fetch *temp = mf;
+      mf = mf->get_original_mf();
+      delete temp;
     }
-    else abort();
-    bool has_atomic = false;
-    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);
-    if (has_atomic) {
-        assert(m_config.m_alloc_policy == ON_MISS);
-        cache_block_t* block = m_tag_array->get_block(e->second.m_cache_index);
-        block->set_status(MODIFIED, mf->get_access_sector_mask()); // mark line as dirty for atomic operation
+  }
+
+  extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
+  assert(e != m_extra_mf_fields.end());
+  assert(e->second.m_valid);
+  mf->set_data_size(e->second.m_data_size);
+  mf->set_addr(e->second.m_addr);
+  if (m_config.m_alloc_policy == ON_MISS)
+    m_tag_array->fill(e->second.m_cache_index, time, mf);
+  else if (m_config.m_alloc_policy == ON_FILL) {
+    m_tag_array->fill(e->second.m_block_addr, time, mf, mf->is_write());
+  } else
+    abort();
+  bool has_atomic = false;
+  m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);
+  if (has_atomic) {
+    assert(m_config.m_alloc_policy == ON_MISS);
+    cache_block_t *block = m_tag_array->get_block(e->second.m_cache_index);
+    if (!block->is_modified_line()) {
+      m_tag_array->inc_dirty();
     }
-    m_extra_mf_fields.erase(mf);
-    m_bandwidth_management.use_fill_port(mf); 
+    block->set_status(MODIFIED,
+                      mf->get_access_sector_mask());  // mark line as dirty for
+                                                      // atomic operation
+    block->set_byte_mask(mf);
+  }
+  m_extra_mf_fields.erase(mf);
+  m_bandwidth_management.use_fill_port(mf);
 }
 
 /// Checks if mf is waiting to be filled by lower memory level
-bool baseline_cache::waiting_for_fill( mem_fetch *mf ){
-    extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
-    return e != m_extra_mf_fields.end();
+bool baseline_cache::waiting_for_fill(mem_fetch *mf) {
+  extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
+  return e != m_extra_mf_fields.end();
 }
 
-void baseline_cache::print(FILE *fp, unsigned &accesses, unsigned &misses) const{
-    fprintf( fp, "Cache %s:\t", m_name.c_str() );
-    m_tag_array->print(fp,accesses,misses);
+void baseline_cache::print(FILE *fp, unsigned &accesses,
+                           unsigned &misses) const {
+  fprintf(fp, "Cache %s:\t", m_name.c_str());
+  m_tag_array->print(fp, accesses, misses);
 }
 
-void baseline_cache::display_state( FILE *fp ) const{
-    fprintf(fp,"Cache %s:\n", m_name.c_str() );
-    m_mshrs.display(fp);
-    fprintf(fp,"\n");
+void baseline_cache::display_state(FILE *fp) const {
+  fprintf(fp, "Cache %s:\n", m_name.c_str());
+  m_mshrs.display(fp);
+  fprintf(fp, "\n");
 }
 
 /// Read miss handler without writeback
-void baseline_cache::send_read_request(new_addr_type addr, new_addr_type block_addr, unsigned cache_index, mem_fetch *mf,
-		unsigned time, bool &do_miss, std::list<cache_event> &events, bool read_only, bool wa){
-
-	bool wb=false;
-	evicted_block_info e;
-	send_read_request(addr, block_addr, cache_index, mf, time, do_miss, wb, e, events, read_only, wa);
+void baseline_cache::send_read_request(new_addr_type addr,
+                                       new_addr_type block_addr,
+                                       unsigned cache_index, mem_fetch *mf,
+                                       unsigned time, bool &do_miss,
+                                       std::list<cache_event> &events,
+                                       bool read_only, bool wa) {
+  bool wb = false;
+  evicted_block_info e;
+  send_read_request(addr, block_addr, cache_index, mf, time, do_miss, wb, e,
+                    events, read_only, wa);
 }
 
 /// Read miss handler. Check MSHR hit or MSHR available
-void baseline_cache::send_read_request(new_addr_type addr, new_addr_type block_addr, unsigned cache_index, mem_fetch *mf,
-		unsigned time, bool &do_miss, bool &wb, evicted_block_info &evicted, std::list<cache_event> &events, bool read_only, bool wa){
-
-	new_addr_type mshr_addr = m_config.mshr_addr(mf->get_addr());
-    bool mshr_hit = m_mshrs.probe(mshr_addr);
-    bool mshr_avail = !m_mshrs.full(mshr_addr);
-    if ( mshr_hit && mshr_avail ) {
-    	if(read_only)
-    		m_tag_array->access(block_addr,time,cache_index,mf);
-    	else
-    		m_tag_array->access(block_addr,time,cache_index,wb,evicted,mf);
-
-        m_mshrs.add(mshr_addr,mf);
-        do_miss = true;
-
-    } else if ( !mshr_hit && mshr_avail && (m_miss_queue.size() < m_config.m_miss_queue_size) ) {
-    	if(read_only)
-    		m_tag_array->access(block_addr,time,cache_index,mf);
-    	else
-    		m_tag_array->access(block_addr,time,cache_index,wb,evicted,mf);
-
-        m_mshrs.add(mshr_addr,mf);
-        if(m_config.is_streaming() && m_config.m_cache_type == SECTOR){
-			m_tag_array->add_pending_line(mf);
-		}
-        m_extra_mf_fields[mf] = extra_mf_fields(mshr_addr,mf->get_addr(),cache_index, mf->get_data_size(), m_config);
-        mf->set_data_size( m_config.get_atom_sz() );
-        mf->set_addr( mshr_addr );
-        m_miss_queue.push_back(mf);
-        mf->set_status(m_miss_queue_status,time);
-        if(!wa)
-        	events.push_back(cache_event(READ_REQUEST_SENT));
-
-        do_miss = true;
-    }
-    else if(mshr_hit && !mshr_avail)
-        m_stats.inc_fail_stats(mf->get_access_type(), MSHR_MERGE_ENRTY_FAIL);
-    else if (!mshr_hit && !mshr_avail)
-    	 m_stats.inc_fail_stats(mf->get_access_type(), MSHR_ENRTY_FAIL);
+void baseline_cache::send_read_request(new_addr_type addr,
+                                       new_addr_type block_addr,
+                                       unsigned cache_index, mem_fetch *mf,
+                                       unsigned time, bool &do_miss, bool &wb,
+                                       evicted_block_info &evicted,
+                                       std::list<cache_event> &events,
+                                       bool read_only, bool wa) {
+  new_addr_type mshr_addr = m_config.mshr_addr(mf->get_addr());
+  bool mshr_hit = m_mshrs.probe(mshr_addr);
+  bool mshr_avail = !m_mshrs.full(mshr_addr);
+  if (mshr_hit && mshr_avail) {
+    if (read_only)
+      m_tag_array->access(block_addr, time, cache_index, mf);
     else
-    	assert(0);
-}
+      m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
 
+    m_mshrs.add(mshr_addr, mf);
+    m_stats.inc_stats(mf->get_access_type(), MSHR_HIT);
+    do_miss = true;
 
-/// Sends write request to lower level memory (write or writeback)
-void data_cache::send_write_request(mem_fetch *mf, cache_event request, unsigned time, std::list<cache_event> &events){
+  } else if (!mshr_hit && mshr_avail &&
+             (m_miss_queue.size() < m_config.m_miss_queue_size)) {
+    if (read_only)
+      m_tag_array->access(block_addr, time, cache_index, mf);
+    else
+      m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
 
-	events.push_back(request);
+    m_mshrs.add(mshr_addr, mf);
+    m_extra_mf_fields[mf] = extra_mf_fields(
+        mshr_addr, mf->get_addr(), cache_index, mf->get_data_size(), m_config);
+    mf->set_data_size(m_config.get_atom_sz());
+    mf->set_addr(mshr_addr);
     m_miss_queue.push_back(mf);
-    mf->set_status(m_miss_queue_status,time);
+    mf->set_status(m_miss_queue_status, time);
+    if (!wa) events.push_back(cache_event(READ_REQUEST_SENT));
+
+    do_miss = true;
+  } else if (mshr_hit && !mshr_avail)
+    m_stats.inc_fail_stats(mf->get_access_type(), MSHR_MERGE_ENRTY_FAIL);
+  else if (!mshr_hit && !mshr_avail)
+    m_stats.inc_fail_stats(mf->get_access_type(), MSHR_ENRTY_FAIL);
+  else
+    assert(0);
 }
 
+/// Sends write request to lower level memory (write or writeback)
+void data_cache::send_write_request(mem_fetch *mf, cache_event request,
+                                    unsigned time,
+                                    std::list<cache_event> &events) {
+  events.push_back(request);
+  m_miss_queue.push_back(mf);
+  mf->set_status(m_miss_queue_status, time);
+}
+
+void data_cache::update_m_readable(mem_fetch *mf, unsigned cache_index) {
+  cache_block_t *block = m_tag_array->get_block(cache_index);
+  for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; i++) {
+    if (mf->get_access_sector_mask().test(i)) {
+      bool all_set = true;
+      for (unsigned k = i * SECTOR_SIZE; k < (i + 1) * SECTOR_SIZE; k++) {
+        // If any bit in the byte mask (within the sector) is not set, 
+        // the sector is unreadble
+        if (!block->get_dirty_byte_mask().test(k)) {
+          all_set = false;
+          break;
+        }
+      }
+      if (all_set)
+        block->set_m_readable(true, mf->get_access_sector_mask());
+    }
+  }
+}
 
 /****** Write-hit functions (Set by config file) ******/
 
 /// Write-back hit: Mark block as modified
-cache_request_status data_cache::wr_hit_wb(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status ){
-	new_addr_type block_addr = m_config.block_addr(addr);
-	m_tag_array->access(block_addr,time,cache_index,mf); // update LRU state
-	cache_block_t* block = m_tag_array->get_block(cache_index);
-	block->set_status(MODIFIED, mf->get_access_sector_mask());
-
-	return HIT;
+cache_request_status data_cache::wr_hit_wb(new_addr_type addr,
+                                           unsigned cache_index, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events,
+                                           enum cache_request_status status) {
+  new_addr_type block_addr = m_config.block_addr(addr);
+  m_tag_array->access(block_addr, time, cache_index, mf);  // update LRU state
+  cache_block_t *block = m_tag_array->get_block(cache_index);
+  if (!block->is_modified_line()) {
+    m_tag_array->inc_dirty();
+  }
+  block->set_status(MODIFIED, mf->get_access_sector_mask());
+  block->set_byte_mask(mf);
+  update_m_readable(mf,cache_index);
+
+  return HIT;
 }
 
 /// Write-through hit: Directly send request to lower level memory
-cache_request_status data_cache::wr_hit_wt(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status ){
-	if(miss_queue_full(0)) {
-		m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-		return RESERVATION_FAIL; // cannot handle request this cycle
-	}
-
-	new_addr_type block_addr = m_config.block_addr(addr);
-	m_tag_array->access(block_addr,time,cache_index,mf); // update LRU state
-	cache_block_t* block = m_tag_array->get_block(cache_index);
-	block->set_status(MODIFIED, mf->get_access_sector_mask());
-
-	// generate a write-through
-	send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
+cache_request_status data_cache::wr_hit_wt(new_addr_type addr,
+                                           unsigned cache_index, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events,
+                                           enum cache_request_status status) {
+  if (miss_queue_full(0)) {
+    m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+    return RESERVATION_FAIL;  // cannot handle request this cycle
+  }
+
+  new_addr_type block_addr = m_config.block_addr(addr);
+  m_tag_array->access(block_addr, time, cache_index, mf);  // update LRU state
+  cache_block_t *block = m_tag_array->get_block(cache_index);
+  if (!block->is_modified_line()) {
+    m_tag_array->inc_dirty();
+  }
+  block->set_status(MODIFIED, mf->get_access_sector_mask());
+  block->set_byte_mask(mf);
+  update_m_readable(mf,cache_index);
+
+  // generate a write-through
+  send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
+
+  return HIT;
+}
+
+/// Write-evict hit: Send request to lower level memory and invalidate
+/// corresponding block
+cache_request_status data_cache::wr_hit_we(new_addr_type addr,
+                                           unsigned cache_index, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events,
+                                           enum cache_request_status status) {
+  if (miss_queue_full(0)) {
+    m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+    return RESERVATION_FAIL;  // cannot handle request this cycle
+  }
+
+  // generate a write-through/evict
+  cache_block_t *block = m_tag_array->get_block(cache_index);
+  send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
+
+  // Invalidate block
+  block->set_status(INVALID, mf->get_access_sector_mask());
+
+  return HIT;
+}
 
-	return HIT;
+/// Global write-evict, local write-back: Useful for private caches
+enum cache_request_status data_cache::wr_hit_global_we_local_wb(
+    new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events, enum cache_request_status status) {
+  bool evict = (mf->get_access_type() ==
+                GLOBAL_ACC_W);  // evict a line that hits on global memory write
+  if (evict)
+    return wr_hit_we(addr, cache_index, mf, time, events,
+                     status);  // Write-evict
+  else
+    return wr_hit_wb(addr, cache_index, mf, time, events,
+                     status);  // Write-back
 }
 
-/// Write-evict hit: Send request to lower level memory and invalidate corresponding block
-cache_request_status data_cache::wr_hit_we(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status ){
-	if(miss_queue_full(0)) {
-		m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-		return RESERVATION_FAIL; // cannot handle request this cycle
-	}
+/****** Write-miss functions (Set by config file) ******/
 
-	// generate a write-through/evict
-	cache_block_t* block = m_tag_array->get_block(cache_index);
-	send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
+/// Write-allocate miss: Send write request to lower level memory
+// and send a read request for the same block
+enum cache_request_status data_cache::wr_miss_wa_naive(
+    new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events, enum cache_request_status status) {
+  new_addr_type block_addr = m_config.block_addr(addr);
+  new_addr_type mshr_addr = m_config.mshr_addr(mf->get_addr());
+
+  // Write allocate, maximum 3 requests (write miss, read request, write back
+  // request) Conservatively ensure the worst-case request can be handled this
+  // cycle
+  bool mshr_hit = m_mshrs.probe(mshr_addr);
+  bool mshr_avail = !m_mshrs.full(mshr_addr);
+  if (miss_queue_full(2) ||
+      (!(mshr_hit && mshr_avail) &&
+       !(!mshr_hit && mshr_avail &&
+         (m_miss_queue.size() < m_config.m_miss_queue_size)))) {
+    // check what is the exactly the failure reason
+    if (miss_queue_full(2))
+      m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+    else if (mshr_hit && !mshr_avail)
+      m_stats.inc_fail_stats(mf->get_access_type(), MSHR_MERGE_ENRTY_FAIL);
+    else if (!mshr_hit && !mshr_avail)
+      m_stats.inc_fail_stats(mf->get_access_type(), MSHR_ENRTY_FAIL);
+    else
+      assert(0);
 
-	// Invalidate block
-	block->set_status(INVALID, mf->get_access_sector_mask());
+    return RESERVATION_FAIL;
+  }
+
+  send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
+  // Tries to send write allocate request, returns true on success and false on
+  // failure
+  // if(!send_write_allocate(mf, addr, block_addr, cache_index, time, events))
+  //    return RESERVATION_FAIL;
+
+  const mem_access_t *ma =
+      new mem_access_t(m_wr_alloc_type, mf->get_addr(), m_config.get_atom_sz(),
+                       false,  // Now performing a read
+                       mf->get_access_warp_mask(), mf->get_access_byte_mask(),
+                       mf->get_access_sector_mask(), m_gpu->gpgpu_ctx);
+
+  mem_fetch *n_mf =
+      new mem_fetch(*ma, NULL, mf->get_ctrl_size(), mf->get_wid(),
+                    mf->get_sid(), mf->get_tpc(), mf->get_mem_config(),
+                    m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle);
+
+  bool do_miss = false;
+  bool wb = false;
+  evicted_block_info evicted;
+
+  // Send read request resulting from write miss
+  send_read_request(addr, block_addr, cache_index, n_mf, time, do_miss, wb,
+                    evicted, events, false, true);
+
+  events.push_back(cache_event(WRITE_ALLOCATE_SENT));
+
+  if (do_miss) {
+    // If evicted block is modified and not a write-through
+    // (already modified lower level)
+    if (wb && (m_config.m_write_policy != WRITE_THROUGH)) {
+      assert(status ==
+             MISS);  // SECTOR_MISS and HIT_RESERVED should not send write back
+      mem_fetch *wb = m_memfetch_creator->alloc(
+          evicted.m_block_addr, m_wrbk_type, mf->get_access_warp_mask(),
+          evicted.m_byte_mask, evicted.m_sector_mask, evicted.m_modified_size,
+          true, m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, -1, -1, -1,
+          NULL);
+      // the evicted block may have wrong chip id when advanced L2 hashing  is
+      // used, so set the right chip address from the original mf
+      wb->set_chip(mf->get_tlx_addr().chip);
+      wb->set_parition(mf->get_tlx_addr().sub_partition);
+      send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted),
+                         time, events);
+    }
+    return MISS;
+  }
 
-	return HIT;
+  return RESERVATION_FAIL;
 }
 
-/// Global write-evict, local write-back: Useful for private caches
-enum cache_request_status data_cache::wr_hit_global_we_local_wb(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status ){
-	bool evict = (mf->get_access_type() == GLOBAL_ACC_W); // evict a line that hits on global memory write
-	if(evict)
-		return wr_hit_we(addr, cache_index, mf, time, events, status); // Write-evict
-	else
-		return wr_hit_wb(addr, cache_index, mf, time, events, status); // Write-back
-}
+enum cache_request_status data_cache::wr_miss_wa_fetch_on_write(
+    new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events, enum cache_request_status status) {
+  new_addr_type block_addr = m_config.block_addr(addr);
+  new_addr_type mshr_addr = m_config.mshr_addr(mf->get_addr());
 
-/****** Write-miss functions (Set by config file) ******/
+  if (mf->get_access_byte_mask().count() == m_config.get_atom_sz()) {
+    // if the request writes to the whole cache line/sector, then, write and set
+    // cache line Modified. and no need to send read request to memory or
+    // reserve mshr
 
-/// Write-allocate miss: Send write request to lower level memory
-// and send a read request for the same block
-enum cache_request_status
-data_cache::wr_miss_wa_naive( new_addr_type addr,
-                        unsigned cache_index, mem_fetch *mf,
-                        unsigned time, std::list<cache_event> &events,
-                        enum cache_request_status status )
-{
-    new_addr_type block_addr = m_config.block_addr(addr);
-    new_addr_type mshr_addr = m_config.mshr_addr(mf->get_addr());
+    if (miss_queue_full(0)) {
+      m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+      return RESERVATION_FAIL;  // cannot handle request this cycle
+    }
+
+    bool wb = false;
+    evicted_block_info evicted;
 
-    // Write allocate, maximum 3 requests (write miss, read request, write back request)
-    // Conservatively ensure the worst-case request can be handled this cycle
+    cache_request_status status =
+        m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
+    assert(status != HIT);
+    cache_block_t *block = m_tag_array->get_block(cache_index);
+    if (!block->is_modified_line()) {
+      m_tag_array->inc_dirty();
+    }
+    block->set_status(MODIFIED, mf->get_access_sector_mask());
+    block->set_byte_mask(mf);
+    if (status == HIT_RESERVED)
+      block->set_ignore_on_fill(true, mf->get_access_sector_mask());
+
+    if (status != RESERVATION_FAIL) {
+      // If evicted block is modified and not a write-through
+      // (already modified lower level)
+      if (wb && (m_config.m_write_policy != WRITE_THROUGH)) {
+        mem_fetch *wb = m_memfetch_creator->alloc(
+            evicted.m_block_addr, m_wrbk_type, mf->get_access_warp_mask(),
+            evicted.m_byte_mask, evicted.m_sector_mask, evicted.m_modified_size,
+            true, m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, -1, -1, -1,
+            NULL);
+        // the evicted block may have wrong chip id when advanced L2 hashing  is
+        // used, so set the right chip address from the original mf
+        wb->set_chip(mf->get_tlx_addr().chip);
+        wb->set_parition(mf->get_tlx_addr().sub_partition);
+        send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted),
+                           time, events);
+      }
+      return MISS;
+    }
+    return RESERVATION_FAIL;
+  } else {
     bool mshr_hit = m_mshrs.probe(mshr_addr);
     bool mshr_avail = !m_mshrs.full(mshr_addr);
-    if(miss_queue_full(2) 
-        || (!(mshr_hit && mshr_avail) 
-        && !(!mshr_hit && mshr_avail && (m_miss_queue.size() < m_config.m_miss_queue_size)))) {
-    	//check what is the exactly the failure reason
-    	 if(miss_queue_full(2) )
-    		 m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-         else if(mshr_hit && !mshr_avail)
-    	      m_stats.inc_fail_stats(mf->get_access_type(), MSHR_MERGE_ENRTY_FAIL);
-    	 else if (!mshr_hit && !mshr_avail)
-    	    m_stats.inc_fail_stats(mf->get_access_type(), MSHR_ENRTY_FAIL);
-    	 else
-    		 assert(0);
-
-        return RESERVATION_FAIL;
+    if (miss_queue_full(1) ||
+        (!(mshr_hit && mshr_avail) &&
+         !(!mshr_hit && mshr_avail &&
+           (m_miss_queue.size() < m_config.m_miss_queue_size)))) {
+      // check what is the exactly the failure reason
+      if (miss_queue_full(1))
+        m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+      else if (mshr_hit && !mshr_avail)
+        m_stats.inc_fail_stats(mf->get_access_type(), MSHR_MERGE_ENRTY_FAIL);
+      else if (!mshr_hit && !mshr_avail)
+        m_stats.inc_fail_stats(mf->get_access_type(), MSHR_ENRTY_FAIL);
+      else
+        assert(0);
+
+      return RESERVATION_FAIL;
     }
 
-    send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
-    // Tries to send write allocate request, returns true on success and false on failure
-    //if(!send_write_allocate(mf, addr, block_addr, cache_index, time, events))
-    //    return RESERVATION_FAIL;
-
-    const mem_access_t *ma = new  mem_access_t( m_wr_alloc_type,
-                        mf->get_addr(),
-						m_config.get_atom_sz(),
-                        false, // Now performing a read
-                        mf->get_access_warp_mask(),
-                        mf->get_access_byte_mask(),
-		                mf->get_access_sector_mask());
-
-    mem_fetch *n_mf = new mem_fetch( *ma,
-                    NULL,
-                    mf->get_ctrl_size(),
-                    mf->get_wid(),
-                    mf->get_sid(),
-                    mf->get_tpc(),
-                    mf->get_mem_config());
+    // prevent Write - Read - Write in pending mshr
+    // allowing another write will override the value of the first write, and
+    // the pending read request will read incorrect result from the second write
+    if (m_mshrs.probe(mshr_addr) &&
+        m_mshrs.is_read_after_write_pending(mshr_addr) && mf->is_write()) {
+      // assert(0);
+      m_stats.inc_fail_stats(mf->get_access_type(), MSHR_RW_PENDING);
+      return RESERVATION_FAIL;
+    }
+
+    const mem_access_t *ma = new mem_access_t(
+        m_wr_alloc_type, mf->get_addr(), m_config.get_atom_sz(),
+        false,  // Now performing a read
+        mf->get_access_warp_mask(), mf->get_access_byte_mask(),
+        mf->get_access_sector_mask(), m_gpu->gpgpu_ctx);
 
+    mem_fetch *n_mf = new mem_fetch(
+        *ma, NULL, mf->get_ctrl_size(), mf->get_wid(), mf->get_sid(),
+        mf->get_tpc(), mf->get_mem_config(),
+        m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, NULL, mf);
+
+    new_addr_type block_addr = m_config.block_addr(addr);
     bool do_miss = false;
     bool wb = false;
     evicted_block_info evicted;
-
-    // Send read request resulting from write miss
     send_read_request(addr, block_addr, cache_index, n_mf, time, do_miss, wb,
-        evicted, events, false, true);
+                      evicted, events, false, true);
+
+    cache_block_t *block = m_tag_array->get_block(cache_index);
+    block->set_modified_on_fill(true, mf->get_access_sector_mask());
+    block->set_byte_mask_on_fill(true);
 
     events.push_back(cache_event(WRITE_ALLOCATE_SENT));
 
-    if( do_miss ){
-        // If evicted block is modified and not a write-through
-        // (already modified lower level)
-        if( wb && (m_config.m_write_policy != WRITE_THROUGH) ) { 
-        	assert(status == MISS);   //SECTOR_MISS and HIT_RESERVED should not send write back
-            mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,
-                m_wrbk_type,evicted.m_modified_size,true);
-            send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted), time, events);
-        }
-        return MISS;
+    if (do_miss) {
+      // If evicted block is modified and not a write-through
+      // (already modified lower level)
+      if (wb && (m_config.m_write_policy != WRITE_THROUGH)) {
+        mem_fetch *wb = m_memfetch_creator->alloc(
+            evicted.m_block_addr, m_wrbk_type, mf->get_access_warp_mask(),
+            evicted.m_byte_mask, evicted.m_sector_mask, evicted.m_modified_size,
+            true, m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, -1, -1, -1,
+            NULL);
+        // the evicted block may have wrong chip id when advanced L2 hashing  is
+        // used, so set the right chip address from the original mf
+        wb->set_chip(mf->get_tlx_addr().chip);
+        wb->set_parition(mf->get_tlx_addr().sub_partition);
+        send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted),
+                           time, events);
+      }
+      return MISS;
     }
-
     return RESERVATION_FAIL;
+  }
 }
 
+enum cache_request_status data_cache::wr_miss_wa_lazy_fetch_on_read(
+    new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events, enum cache_request_status status) {
+  new_addr_type block_addr = m_config.block_addr(addr);
 
-enum cache_request_status
-data_cache::wr_miss_wa_fetch_on_write( new_addr_type addr,
-                        unsigned cache_index, mem_fetch *mf,
-                        unsigned time, std::list<cache_event> &events,
-                        enum cache_request_status status )
-{
-    new_addr_type block_addr = m_config.block_addr(addr);
-    new_addr_type mshr_addr = m_config.mshr_addr(mf->get_addr());
-
-	if(mf->get_access_byte_mask().count() == m_config.get_atom_sz())
-	{
-		//if the request writes to the whole cache line/sector, then, write and set cache line Modified.
-		//and no need to send read request to memory or reserve mshr
-
-		if(miss_queue_full(0)) {
-			m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-			return RESERVATION_FAIL; // cannot handle request this cycle
-		}
-
-		bool wb = false;
-		evicted_block_info evicted;
-
-		cache_request_status status =  m_tag_array->access(block_addr,time,cache_index,wb,evicted,mf);
-		assert(status != HIT);
-		cache_block_t* block = m_tag_array->get_block(cache_index);
-		block->set_status(MODIFIED, mf->get_access_sector_mask());
-		if(status == HIT_RESERVED)
-			block->set_ignore_on_fill(true, mf->get_access_sector_mask());
-
-		if( status != RESERVATION_FAIL ){
-			   // If evicted block is modified and not a write-through
-			   // (already modified lower level)
-			   if( wb && (m_config.m_write_policy != WRITE_THROUGH) ) {
-				   mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,
-					   m_wrbk_type,evicted.m_modified_size,true);
-				   send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted), time, events);
-			   }
-			   return MISS;
-		   }
-		return RESERVATION_FAIL;
-	}
-	else
-	{
-		bool mshr_hit = m_mshrs.probe(mshr_addr);
-		bool mshr_avail = !m_mshrs.full(mshr_addr);
-		if(miss_queue_full(1)
-			|| (!(mshr_hit && mshr_avail)
-			&& !(!mshr_hit && mshr_avail && (m_miss_queue.size() < m_config.m_miss_queue_size)))) {
-			//check what is the exactly the failure reason
-			 if(miss_queue_full(1) )
-				 m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-			 else if(mshr_hit && !mshr_avail)
-				  m_stats.inc_fail_stats(mf->get_access_type(), MSHR_MERGE_ENRTY_FAIL);
-			 else if (!mshr_hit && !mshr_avail)
-				m_stats.inc_fail_stats(mf->get_access_type(), MSHR_ENRTY_FAIL);
-			 else
-				 assert(0);
-
-			return RESERVATION_FAIL;
-		}
-
-
-		  //prevent Write - Read - Write in pending mshr
-		  //allowing another write will override the value of the first write, and the pending read request will read incorrect result from the second write
-		  if(m_mshrs.probe(mshr_addr) && m_mshrs.is_read_after_write_pending(mshr_addr) && mf->is_write())
-		  {
-			  //assert(0);
-			  m_stats.inc_fail_stats(mf->get_access_type(), MSHR_RW_PENDING);
-			  return RESERVATION_FAIL;
-		  }
-
-		  const mem_access_t *ma = new  mem_access_t( m_wr_alloc_type,
-									mf->get_addr(),
-									m_config.get_atom_sz(),
-									false, // Now performing a read
-									mf->get_access_warp_mask(),
-									mf->get_access_byte_mask(),
-									mf->get_access_sector_mask());
-
-		  mem_fetch *n_mf = new mem_fetch( *ma,
-								NULL,
-								mf->get_ctrl_size(),
-								mf->get_wid(),
-								mf->get_sid(),
-								mf->get_tpc(),
-								mf->get_mem_config(),
-								NULL,
-								mf);
-
-
-			new_addr_type block_addr = m_config.block_addr(addr);
-			bool do_miss = false;
-			bool wb = false;
-			evicted_block_info evicted;
-			send_read_request( addr,
-							   block_addr,
-							   cache_index,
-							   n_mf, time, do_miss, wb, evicted, events, false, true);
-
-			cache_block_t* block = m_tag_array->get_block(cache_index);
-			block->set_modified_on_fill(true, mf->get_access_sector_mask());
-
-			events.push_back(cache_event(WRITE_ALLOCATE_SENT));
-
-			if( do_miss ){
-				// If evicted block is modified and not a write-through
-				// (already modified lower level)
-				if(wb && (m_config.m_write_policy != WRITE_THROUGH) ){
-					mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,
-						m_wrbk_type,evicted.m_modified_size,true);
-					send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted), time, events);
-			}
-				return MISS;
-			}
-	   return RESERVATION_FAIL;
-	}
-}
-
-enum cache_request_status
-data_cache::wr_miss_wa_lazy_fetch_on_read( new_addr_type addr,
-                        unsigned cache_index, mem_fetch *mf,
-                        unsigned time, std::list<cache_event> &events,
-                        enum cache_request_status status )
-{
-
-	    new_addr_type block_addr = m_config.block_addr(addr);
-	    new_addr_type mshr_addr = m_config.mshr_addr(mf->get_addr());
-
-
-		//if the request writes to the whole cache line/sector, then, write and set cache line Modified.
-		//and no need to send read request to memory or reserve mshr
-
-		if(miss_queue_full(0)) {
-			m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-			return RESERVATION_FAIL; // cannot handle request this cycle
-		}
-
-		bool wb = false;
-		evicted_block_info evicted;
-
-		cache_request_status m_status =  m_tag_array->access(block_addr,time,cache_index,wb,evicted,mf);
-		assert(m_status != HIT);
-		cache_block_t* block = m_tag_array->get_block(cache_index);
-		block->set_status(MODIFIED, mf->get_access_sector_mask());
-		if(m_status == HIT_RESERVED) {
-			block->set_ignore_on_fill(true, mf->get_access_sector_mask());
-			block->set_modified_on_fill(true, mf->get_access_sector_mask());
-		}
-
-		if(mf->get_access_byte_mask().count() == m_config.get_atom_sz())
-		{
-			block->set_m_readable(true, mf->get_access_sector_mask());
-		} else
-		{
-			block->set_m_readable(false, mf->get_access_sector_mask());
-		}
-
-		if( m_status != RESERVATION_FAIL ){
-			   // If evicted block is modified and not a write-through
-			   // (already modified lower level)
-			   if( wb && (m_config.m_write_policy != WRITE_THROUGH) ) {
-				   mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,
-					   m_wrbk_type,evicted.m_modified_size,true);
-				   send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted), time, events);
-			   }
-			   return MISS;
-		   }
-		return RESERVATION_FAIL;
-}
+  // if the request writes to the whole cache line/sector, then, write and set
+  // cache line Modified. and no need to send read request to memory or reserve
+  // mshr
 
-/// No write-allocate miss: Simply send write request to lower level memory
-enum cache_request_status
-data_cache::wr_miss_no_wa( new_addr_type addr,
-                           unsigned cache_index,
-                           mem_fetch *mf,
-                           unsigned time,
-                           std::list<cache_event> &events,
-                           enum cache_request_status status )
-{
-    if(miss_queue_full(0)) {
-    	m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-    	return RESERVATION_FAIL; // cannot handle request this cycle
+  if (miss_queue_full(0)) {
+    m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+    return RESERVATION_FAIL;  // cannot handle request this cycle
+  }
+
+  if (m_config.m_write_policy == WRITE_THROUGH) {
+    send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
+  }
+
+  bool wb = false;
+  evicted_block_info evicted;
+
+  cache_request_status m_status =
+      m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
+  assert(m_status != HIT);
+  cache_block_t *block = m_tag_array->get_block(cache_index);
+  if (!block->is_modified_line()) {
+    m_tag_array->inc_dirty();
+  }
+  block->set_status(MODIFIED, mf->get_access_sector_mask());
+  block->set_byte_mask(mf);
+  if (m_status == HIT_RESERVED) {
+    block->set_ignore_on_fill(true, mf->get_access_sector_mask());
+    block->set_modified_on_fill(true, mf->get_access_sector_mask());
+    block->set_byte_mask_on_fill(true);
+  }
+
+  if (mf->get_access_byte_mask().count() == m_config.get_atom_sz()) {
+    block->set_m_readable(true, mf->get_access_sector_mask());
+  } else {
+    block->set_m_readable(false, mf->get_access_sector_mask());
+    if (m_status == HIT_RESERVED)
+      block->set_readable_on_fill(true, mf->get_access_sector_mask());
+  }
+  update_m_readable(mf,cache_index);
+
+  if (m_status != RESERVATION_FAIL) {
+    // If evicted block is modified and not a write-through
+    // (already modified lower level)
+    if (wb && (m_config.m_write_policy != WRITE_THROUGH)) {
+      mem_fetch *wb = m_memfetch_creator->alloc(
+          evicted.m_block_addr, m_wrbk_type, mf->get_access_warp_mask(),
+          evicted.m_byte_mask, evicted.m_sector_mask, evicted.m_modified_size,
+          true, m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, -1, -1, -1,
+          NULL);
+      // the evicted block may have wrong chip id when advanced L2 hashing  is
+      // used, so set the right chip address from the original mf
+      wb->set_chip(mf->get_tlx_addr().chip);
+      wb->set_parition(mf->get_tlx_addr().sub_partition);
+      send_write_request(wb, cache_event(WRITE_BACK_REQUEST_SENT, evicted),
+                         time, events);
     }
+    return MISS;
+  }
+  return RESERVATION_FAIL;
+}
 
+/// No write-allocate miss: Simply send write request to lower level memory
+enum cache_request_status data_cache::wr_miss_no_wa(
+    new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events, enum cache_request_status status) {
+  if (miss_queue_full(0)) {
+    m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+    return RESERVATION_FAIL;  // cannot handle request this cycle
+  }
 
-    // on miss, generate write through (no write buffering -- too many threads for that)
-    send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
+  // on miss, generate write through (no write buffering -- too many threads for
+  // that)
+  send_write_request(mf, cache_event(WRITE_REQUEST_SENT), time, events);
 
-    return MISS;
+  return MISS;
 }
 
 /****** Read hit functions (Set by config file) ******/
 
 /// Baseline read hit: Update LRU status of block.
 // Special case for atomic instructions -> Mark block as modified
-enum cache_request_status
-data_cache::rd_hit_base( new_addr_type addr,
-                         unsigned cache_index,
-                         mem_fetch *mf,
-                         unsigned time,
-                         std::list<cache_event> &events,
-                         enum cache_request_status status )
-{
-    new_addr_type block_addr = m_config.block_addr(addr);
-    m_tag_array->access(block_addr,time,cache_index,mf);
-    // Atomics treated as global read/write requests - Perform read, mark line as
-    // MODIFIED
-    if(mf->isatomic()){ 
-        assert(mf->get_access_type() == GLOBAL_ACC_R);
-        cache_block_t* block = m_tag_array->get_block(cache_index);
-        block->set_status(MODIFIED, mf->get_access_sector_mask()) ;  // mark line as dirty
+enum cache_request_status data_cache::rd_hit_base(
+    new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events, enum cache_request_status status) {
+  new_addr_type block_addr = m_config.block_addr(addr);
+  m_tag_array->access(block_addr, time, cache_index, mf);
+  // Atomics treated as global read/write requests - Perform read, mark line as
+  // MODIFIED
+  if (mf->isatomic()) {
+    assert(mf->get_access_type() == GLOBAL_ACC_R);
+    cache_block_t *block = m_tag_array->get_block(cache_index);
+    if (!block->is_modified_line()) {
+      m_tag_array->inc_dirty();
     }
-    return HIT;
+    block->set_status(MODIFIED,
+                      mf->get_access_sector_mask());  // mark line as
+    block->set_byte_mask(mf);
+  }
+  return HIT;
 }
 
 /****** Read miss functions (Set by config file) ******/
 
 /// Baseline read miss: Send read request to lower level memory,
 // perform write-back as necessary
-enum cache_request_status
-data_cache::rd_miss_base( new_addr_type addr,
-                          unsigned cache_index,
-                          mem_fetch *mf,
-                          unsigned time,
-                          std::list<cache_event> &events,
-                          enum cache_request_status status ){
-    if(miss_queue_full(1)) {
-        // cannot handle request this cycle
-        // (might need to generate two requests)
-    	m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-        return RESERVATION_FAIL; 
-    }
-
-    new_addr_type block_addr = m_config.block_addr(addr);
-    bool do_miss = false;
-    bool wb = false;
-    evicted_block_info evicted;
-    send_read_request( addr,
-                       block_addr,
-                       cache_index,
-                       mf, time, do_miss, wb, evicted, events, false, false);
-
-    if( do_miss ){
-        // If evicted block is modified and not a write-through
-        // (already modified lower level)
-        if(wb && (m_config.m_write_policy != WRITE_THROUGH) ){ 
-            mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,
-                m_wrbk_type,evicted.m_modified_size,true);
-        send_write_request(wb, WRITE_BACK_REQUEST_SENT, time, events);
-    }
-        return MISS;
+enum cache_request_status data_cache::rd_miss_base(
+    new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events, enum cache_request_status status) {
+  if (miss_queue_full(1)) {
+    // cannot handle request this cycle
+    // (might need to generate two requests)
+    m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
+    return RESERVATION_FAIL;
+  }
+
+  new_addr_type block_addr = m_config.block_addr(addr);
+  bool do_miss = false;
+  bool wb = false;
+  evicted_block_info evicted;
+  send_read_request(addr, block_addr, cache_index, mf, time, do_miss, wb,
+                    evicted, events, false, false);
+
+  if (do_miss) {
+    // If evicted block is modified and not a write-through
+    // (already modified lower level)
+    if (wb && (m_config.m_write_policy != WRITE_THROUGH)) {
+      mem_fetch *wb = m_memfetch_creator->alloc(
+          evicted.m_block_addr, m_wrbk_type, mf->get_access_warp_mask(),
+          evicted.m_byte_mask, evicted.m_sector_mask, evicted.m_modified_size,
+          true, m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, -1, -1, -1,
+          NULL);
+      // the evicted block may have wrong chip id when advanced L2 hashing  is
+      // used, so set the right chip address from the original mf
+      wb->set_chip(mf->get_tlx_addr().chip);
+      wb->set_parition(mf->get_tlx_addr().sub_partition);
+      send_write_request(wb, WRITE_BACK_REQUEST_SENT, time, events);
     }
-        return RESERVATION_FAIL;
+    return MISS;
+  }
+  return RESERVATION_FAIL;
 }
 
 /// Access cache for read_only_cache: returns RESERVATION_FAIL if
 // request could not be accepted (for any reason)
-enum cache_request_status
-read_only_cache::access( new_addr_type addr,
-                         mem_fetch *mf,
-                         unsigned time,
-                         std::list<cache_event> &events )
-{
-    assert( mf->get_data_size() <= m_config.get_atom_sz());
-    assert(m_config.m_write_policy == READ_ONLY);
-    assert(!mf->get_is_write());
-    new_addr_type block_addr = m_config.block_addr(addr);
-    unsigned cache_index = (unsigned)-1;
-    enum cache_request_status status = m_tag_array->probe(block_addr,cache_index,mf);
-    enum cache_request_status cache_status = RESERVATION_FAIL;
-
-    if ( status == HIT ) {
-        cache_status = m_tag_array->access(block_addr,time,cache_index,mf); // update LRU state
-    }else if ( status != RESERVATION_FAIL ) {
-        if(!miss_queue_full(0)){
-            bool do_miss=false;
-            send_read_request(addr, block_addr, cache_index, mf, time, do_miss, events, true, false);
-            if(do_miss)
-                cache_status = MISS;
-            else
-                cache_status = RESERVATION_FAIL;
-        }else{
-            cache_status = RESERVATION_FAIL;
-            m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
-        }
-    }else {
-    	m_stats.inc_fail_stats(mf->get_access_type(), LINE_ALLOC_FAIL);
+enum cache_request_status read_only_cache::access(
+    new_addr_type addr, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events) {
+  assert(mf->get_data_size() <= m_config.get_atom_sz());
+  assert(m_config.m_write_policy == READ_ONLY);
+  assert(!mf->get_is_write());
+  new_addr_type block_addr = m_config.block_addr(addr);
+  unsigned cache_index = (unsigned)-1;
+  enum cache_request_status status =
+      m_tag_array->probe(block_addr, cache_index, mf, mf->is_write());
+  enum cache_request_status cache_status = RESERVATION_FAIL;
+
+  if (status == HIT) {
+    cache_status = m_tag_array->access(block_addr, time, cache_index,
+                                       mf);  // update LRU state
+  } else if (status != RESERVATION_FAIL) {
+    if (!miss_queue_full(0)) {
+      bool do_miss = false;
+      send_read_request(addr, block_addr, cache_index, mf, time, do_miss,
+                        events, true, false);
+      if (do_miss)
+        cache_status = MISS;
+      else
+        cache_status = RESERVATION_FAIL;
+    } else {
+      cache_status = RESERVATION_FAIL;
+      m_stats.inc_fail_stats(mf->get_access_type(), MISS_QUEUE_FULL);
     }
+  } else {
+    m_stats.inc_fail_stats(mf->get_access_type(), LINE_ALLOC_FAIL);
+  }
 
-    m_stats.inc_stats(mf->get_access_type(), m_stats.select_stats_status(status, cache_status));
-    m_stats.inc_stats_pw(mf->get_access_type(), m_stats.select_stats_status(status, cache_status));
-    return cache_status;
+  m_stats.inc_stats(mf->get_access_type(),
+                    m_stats.select_stats_status(status, cache_status));
+  m_stats.inc_stats_pw(mf->get_access_type(),
+                       m_stats.select_stats_status(status, cache_status));
+  return cache_status;
 }
 
 //! A general function that takes the result of a tag_array probe
 //  and performs the correspding functions based on the cache configuration
 //  The access fucntion calls this function
-enum cache_request_status
-data_cache::process_tag_probe( bool wr,
-                               enum cache_request_status probe_status,
-                               new_addr_type addr,
-                               unsigned cache_index,
-                               mem_fetch* mf,
-                               unsigned time,
-                               std::list<cache_event>& events )
-{
-    // Each function pointer ( m_[rd/wr]_[hit/miss] ) is set in the
-    // data_cache constructor to reflect the corresponding cache configuration
-    // options. Function pointers were used to avoid many long conditional
-    // branches resulting from many cache configuration options.
-    cache_request_status access_status = probe_status;
-    if(wr){ // Write
-        if(probe_status == HIT){
-            access_status = (this->*m_wr_hit)( addr,
-                                      cache_index,
-                                      mf, time, events, probe_status );
-        }else if ( (probe_status != RESERVATION_FAIL) || (probe_status == RESERVATION_FAIL && m_config.m_write_alloc_policy == NO_WRITE_ALLOCATE) ) {
-            access_status = (this->*m_wr_miss)( addr,
-                                       cache_index,
-                                       mf, time, events, probe_status );
-        }else {
-        	//the only reason for reservation fail here is LINE_ALLOC_FAIL (i.e all lines are reserved)
-        	m_stats.inc_fail_stats(mf->get_access_type(), LINE_ALLOC_FAIL);
-        }
-    }else{ // Read
-        if(probe_status == HIT){
-            access_status = (this->*m_rd_hit)( addr,
-                                      cache_index,
-                                      mf, time, events, probe_status );
-        }else if ( probe_status != RESERVATION_FAIL ) {
-            access_status = (this->*m_rd_miss)( addr,
-                                       cache_index,
-                                       mf, time, events, probe_status );
-        }else {
-        	//the only reason for reservation fail here is LINE_ALLOC_FAIL (i.e all lines are reserved)
-        	m_stats.inc_fail_stats(mf->get_access_type(), LINE_ALLOC_FAIL);
-        }
+enum cache_request_status data_cache::process_tag_probe(
+    bool wr, enum cache_request_status probe_status, new_addr_type addr,
+    unsigned cache_index, mem_fetch *mf, unsigned time,
+    std::list<cache_event> &events) {
+  // Each function pointer ( m_[rd/wr]_[hit/miss] ) is set in the
+  // data_cache constructor to reflect the corresponding cache configuration
+  // options. Function pointers were used to avoid many long conditional
+  // branches resulting from many cache configuration options.
+  cache_request_status access_status = probe_status;
+  if (wr) {  // Write
+    if (probe_status == HIT) {
+      access_status =
+          (this->*m_wr_hit)(addr, cache_index, mf, time, events, probe_status);
+    } else if ((probe_status != RESERVATION_FAIL) ||
+               (probe_status == RESERVATION_FAIL &&
+                m_config.m_write_alloc_policy == NO_WRITE_ALLOCATE)) {
+      access_status =
+          (this->*m_wr_miss)(addr, cache_index, mf, time, events, probe_status);
+    } else {
+      // the only reason for reservation fail here is LINE_ALLOC_FAIL (i.e all
+      // lines are reserved)
+      m_stats.inc_fail_stats(mf->get_access_type(), LINE_ALLOC_FAIL);
+    }
+  } else {  // Read
+    if (probe_status == HIT) {
+      access_status =
+          (this->*m_rd_hit)(addr, cache_index, mf, time, events, probe_status);
+    } else if (probe_status != RESERVATION_FAIL) {
+      access_status =
+          (this->*m_rd_miss)(addr, cache_index, mf, time, events, probe_status);
+    } else {
+      // the only reason for reservation fail here is LINE_ALLOC_FAIL (i.e all
+      // lines are reserved)
+      m_stats.inc_fail_stats(mf->get_access_type(), LINE_ALLOC_FAIL);
     }
+  }
 
-    m_bandwidth_management.use_data_port(mf, access_status, events); 
-    return access_status;
+  m_bandwidth_management.use_data_port(mf, access_status, events);
+  return access_status;
 }
 
 // Both the L1 and L2 currently use the same access function.
@@ -1634,51 +1744,41 @@ data_cache::process_tag_probe( bool wr,
 // of caching policies.
 // Both the L1 and L2 override this function to provide a means of
 // performing actions specific to each cache when such actions are implemnted.
-enum cache_request_status
-data_cache::access( new_addr_type addr,
-                    mem_fetch *mf,
-                    unsigned time,
-                    std::list<cache_event> &events )
-{
-
-    assert( mf->get_data_size() <= m_config.get_atom_sz());
-    bool wr = mf->get_is_write();
-    new_addr_type block_addr = m_config.block_addr(addr);
-    unsigned cache_index = (unsigned)-1;
-    enum cache_request_status probe_status
-        = m_tag_array->probe( block_addr, cache_index, mf, true);
-    enum cache_request_status access_status
-        = process_tag_probe( wr, probe_status, addr, cache_index, mf, time, events );
-    m_stats.inc_stats(mf->get_access_type(),
-        m_stats.select_stats_status(probe_status, access_status));
-    m_stats.inc_stats_pw(mf->get_access_type(),
-        m_stats.select_stats_status(probe_status, access_status));
-    return access_status;
+enum cache_request_status data_cache::access(new_addr_type addr, mem_fetch *mf,
+                                             unsigned time,
+                                             std::list<cache_event> &events) {
+  assert(mf->get_data_size() <= m_config.get_atom_sz());
+  bool wr = mf->get_is_write();
+  new_addr_type block_addr = m_config.block_addr(addr);
+  unsigned cache_index = (unsigned)-1;
+  enum cache_request_status probe_status =
+      m_tag_array->probe(block_addr, cache_index, mf, mf->is_write(), true);
+  enum cache_request_status access_status =
+      process_tag_probe(wr, probe_status, addr, cache_index, mf, time, events);
+  m_stats.inc_stats(mf->get_access_type(),
+                    m_stats.select_stats_status(probe_status, access_status));
+  m_stats.inc_stats_pw(mf->get_access_type(), m_stats.select_stats_status(
+                                                  probe_status, access_status));
+  return access_status;
 }
 
 /// This is meant to model the first level data cache in Fermi.
 /// It is write-evict (global) or write-back (local) at the
 /// granularity of individual blocks (Set by GPGPU-Sim configuration file)
 /// (the policy used in fermi according to the CUDA manual)
-enum cache_request_status
-l1_cache::access( new_addr_type addr,
-                  mem_fetch *mf,
-                  unsigned time,
-                  std::list<cache_event> &events )
-{
-    return data_cache::access( addr, mf, time, events );
+enum cache_request_status l1_cache::access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events) {
+  return data_cache::access(addr, mf, time, events);
 }
 
 // The l2 cache access function calls the base data_cache access
 // implementation.  When the L2 needs to diverge from L1, L2 specific
 // changes should be made here.
-enum cache_request_status
-l2_cache::access( new_addr_type addr,
-                  mem_fetch *mf,
-                  unsigned time,
-                  std::list<cache_event> &events )
-{
-    return data_cache::access( addr, mf, time, events );
+enum cache_request_status l2_cache::access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events) {
+  return data_cache::access(addr, mf, time, events);
 }
 
 /// Access function for tex_cache
@@ -1686,141 +1786,144 @@ l2_cache::access( new_addr_type addr,
 /// otherwise returns HIT_RESERVED or MISS; NOTE: *never* returns HIT
 /// since unlike a normal CPU cache, a "HIT" in texture cache does not
 /// mean the data is ready (still need to get through fragment fifo)
-enum cache_request_status tex_cache::access( new_addr_type addr, mem_fetch *mf,
-    unsigned time, std::list<cache_event> &events )
-{
-    if ( m_fragment_fifo.full() || m_request_fifo.full() || m_rob.full() )
-        return RESERVATION_FAIL;
-
-    assert( mf->get_data_size() <= m_config.get_line_sz());
+enum cache_request_status tex_cache::access(new_addr_type addr, mem_fetch *mf,
+                                            unsigned time,
+                                            std::list<cache_event> &events) {
+  if (m_fragment_fifo.full() || m_request_fifo.full() || m_rob.full())
+    return RESERVATION_FAIL;
 
-    // at this point, we will accept the request : access tags and immediately allocate line
-    new_addr_type block_addr = m_config.block_addr(addr);
-    unsigned cache_index = (unsigned)-1;
-    enum cache_request_status status = m_tags.access(block_addr,time,cache_index,mf);
-    enum cache_request_status cache_status = RESERVATION_FAIL;
-    assert( status != RESERVATION_FAIL );
-    assert( status != HIT_RESERVED ); // as far as tags are concerned: HIT or MISS
-    m_fragment_fifo.push( fragment_entry(mf,cache_index,status==MISS,mf->get_data_size()) );
-    if ( status == MISS ) {
-        // we need to send a memory request...
-        unsigned rob_index = m_rob.push( rob_entry(cache_index, mf, block_addr) );
-        m_extra_mf_fields[mf] = extra_mf_fields(rob_index, m_config);
-        mf->set_data_size(m_config.get_line_sz());
-        m_tags.fill(cache_index,time,mf); // mark block as valid
-        m_request_fifo.push(mf);
-        mf->set_status(m_request_queue_status,time);
-        events.push_back(cache_event(READ_REQUEST_SENT));
-        cache_status = MISS;
-    } else {
-        // the value *will* *be* in the cache already
-        cache_status = HIT_RESERVED;
-    }
-    m_stats.inc_stats(mf->get_access_type(), m_stats.select_stats_status(status, cache_status));
-    m_stats.inc_stats_pw(mf->get_access_type(), m_stats.select_stats_status(status, cache_status));
-    return cache_status;
-}
-
-void tex_cache::cycle(){
-    // send next request to lower level of memory
-    if ( !m_request_fifo.empty() ) {
-        mem_fetch *mf = m_request_fifo.peek();
-        if ( !m_memport->full(mf->get_ctrl_size(),false) ) {
-            m_request_fifo.pop();
-            m_memport->push(mf);
-        }
+  assert(mf->get_data_size() <= m_config.get_line_sz());
+
+  // at this point, we will accept the request : access tags and immediately
+  // allocate line
+  new_addr_type block_addr = m_config.block_addr(addr);
+  unsigned cache_index = (unsigned)-1;
+  enum cache_request_status status =
+      m_tags.access(block_addr, time, cache_index, mf);
+  enum cache_request_status cache_status = RESERVATION_FAIL;
+  assert(status != RESERVATION_FAIL);
+  assert(status != HIT_RESERVED);  // as far as tags are concerned: HIT or MISS
+  m_fragment_fifo.push(
+      fragment_entry(mf, cache_index, status == MISS, mf->get_data_size()));
+  if (status == MISS) {
+    // we need to send a memory request...
+    unsigned rob_index = m_rob.push(rob_entry(cache_index, mf, block_addr));
+    m_extra_mf_fields[mf] = extra_mf_fields(rob_index, m_config);
+    mf->set_data_size(m_config.get_line_sz());
+    m_tags.fill(cache_index, time, mf);  // mark block as valid
+    m_request_fifo.push(mf);
+    mf->set_status(m_request_queue_status, time);
+    events.push_back(cache_event(READ_REQUEST_SENT));
+    cache_status = MISS;
+  } else {
+    // the value *will* *be* in the cache already
+    cache_status = HIT_RESERVED;
+  }
+  m_stats.inc_stats(mf->get_access_type(),
+                    m_stats.select_stats_status(status, cache_status));
+  m_stats.inc_stats_pw(mf->get_access_type(),
+                       m_stats.select_stats_status(status, cache_status));
+  return cache_status;
+}
+
+void tex_cache::cycle() {
+  // send next request to lower level of memory
+  if (!m_request_fifo.empty()) {
+    mem_fetch *mf = m_request_fifo.peek();
+    if (!m_memport->full(mf->get_ctrl_size(), false)) {
+      m_request_fifo.pop();
+      m_memport->push(mf);
     }
-    // read ready lines from cache
-    if ( !m_fragment_fifo.empty() && !m_result_fifo.full() ) {
-        const fragment_entry &e = m_fragment_fifo.peek();
-        if ( e.m_miss ) {
-            // check head of reorder buffer to see if data is back from memory
-            unsigned rob_index = m_rob.next_pop_index();
-            const rob_entry &r = m_rob.peek(rob_index);
-            assert( r.m_request == e.m_request );
-            //assert( r.m_block_addr == m_config.block_addr(e.m_request->get_addr()) );
-            if ( r.m_ready ) {
-                assert( r.m_index == e.m_cache_index );
-                m_cache[r.m_index].m_valid = true;
-                m_cache[r.m_index].m_block_addr = r.m_block_addr;
-                m_result_fifo.push(e.m_request);
-                m_rob.pop();
-                m_fragment_fifo.pop();
-            }
-        } else {
-            // hit:
-            assert( m_cache[e.m_cache_index].m_valid );
-            assert( m_cache[e.m_cache_index].m_block_addr
-                == m_config.block_addr(e.m_request->get_addr()) );
-            m_result_fifo.push( e.m_request );
-            m_fragment_fifo.pop();
-        }
+  }
+  // read ready lines from cache
+  if (!m_fragment_fifo.empty() && !m_result_fifo.full()) {
+    const fragment_entry &e = m_fragment_fifo.peek();
+    if (e.m_miss) {
+      // check head of reorder buffer to see if data is back from memory
+      unsigned rob_index = m_rob.next_pop_index();
+      const rob_entry &r = m_rob.peek(rob_index);
+      assert(r.m_request == e.m_request);
+      // assert( r.m_block_addr == m_config.block_addr(e.m_request->get_addr())
+      // );
+      if (r.m_ready) {
+        assert(r.m_index == e.m_cache_index);
+        m_cache[r.m_index].m_valid = true;
+        m_cache[r.m_index].m_block_addr = r.m_block_addr;
+        m_result_fifo.push(e.m_request);
+        m_rob.pop();
+        m_fragment_fifo.pop();
+      }
+    } else {
+      // hit:
+      assert(m_cache[e.m_cache_index].m_valid);
+      assert(m_cache[e.m_cache_index].m_block_addr ==
+             m_config.block_addr(e.m_request->get_addr()));
+      m_result_fifo.push(e.m_request);
+      m_fragment_fifo.pop();
     }
+  }
 }
 
 /// Place returning cache block into reorder buffer
-void tex_cache::fill( mem_fetch *mf, unsigned time )
-{
-	if(m_config.m_mshr_type == SECTOR_TEX_FIFO) {
-	assert(mf->get_original_mf());
-	extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf->get_original_mf());
-    assert( e != m_extra_mf_fields.end() );
+void tex_cache::fill(mem_fetch *mf, unsigned time) {
+  if (m_config.m_mshr_type == SECTOR_TEX_FIFO) {
+    assert(mf->get_original_mf());
+    extra_mf_fields_lookup::iterator e =
+        m_extra_mf_fields.find(mf->get_original_mf());
+    assert(e != m_extra_mf_fields.end());
     e->second.pending_read--;
 
-    if(e->second.pending_read > 0) {
-    	//wait for the other requests to come back
-    	delete mf;
-    	return;
-      } else {
-    	mem_fetch *temp = mf;
-    	mf = mf->get_original_mf();
-    	delete temp;
-      }
-	}
-
-    extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
-    assert( e != m_extra_mf_fields.end() );
-    assert( e->second.m_valid );
-    assert( !m_rob.empty() );
-    mf->set_status(m_rob_status,time);
-
-    unsigned rob_index = e->second.m_rob_index;
-    rob_entry &r = m_rob.peek(rob_index);
-    assert( !r.m_ready );
-    r.m_ready = true;
-    r.m_time = time;
-    assert( r.m_block_addr == m_config.block_addr(mf->get_addr()) );
-}
-
-void tex_cache::display_state( FILE *fp ) const
-{
-    fprintf(fp,"%s (texture cache) state:\n", m_name.c_str() );
-    fprintf(fp,"fragment fifo entries  = %u / %u\n",
-        m_fragment_fifo.size(), m_fragment_fifo.capacity() );
-    fprintf(fp,"reorder buffer entries = %u / %u\n",
-        m_rob.size(), m_rob.capacity() );
-    fprintf(fp,"request fifo entries   = %u / %u\n",
-        m_request_fifo.size(), m_request_fifo.capacity() );
-    if ( !m_rob.empty() )
-        fprintf(fp,"reorder buffer contents:\n");
-    for ( int n=m_rob.size()-1; n>=0; n-- ) {
-        unsigned index = (m_rob.next_pop_index() + n)%m_rob.capacity();
-        const rob_entry &r = m_rob.peek(index);
-        fprintf(fp, "tex rob[%3d] : %s ",
-            index, (r.m_ready?"ready  ":"pending") );
-        if ( r.m_ready )
-            fprintf(fp,"@%6u", r.m_time );
-        else
-            fprintf(fp,"       ");
-        fprintf(fp,"[idx=%4u]",r.m_index);
-        r.m_request->print(fp,false);
-    }
-    if ( !m_fragment_fifo.empty() ) {
-        fprintf(fp,"fragment fifo (oldest) :");
-        fragment_entry &f = m_fragment_fifo.peek();
-        fprintf(fp,"%s:          ", f.m_miss?"miss":"hit ");
-        f.m_request->print(fp,false);
+    if (e->second.pending_read > 0) {
+      // wait for the other requests to come back
+      delete mf;
+      return;
+    } else {
+      mem_fetch *temp = mf;
+      mf = mf->get_original_mf();
+      delete temp;
     }
+  }
+
+  extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
+  assert(e != m_extra_mf_fields.end());
+  assert(e->second.m_valid);
+  assert(!m_rob.empty());
+  mf->set_status(m_rob_status, time);
+
+  unsigned rob_index = e->second.m_rob_index;
+  rob_entry &r = m_rob.peek(rob_index);
+  assert(!r.m_ready);
+  r.m_ready = true;
+  r.m_time = time;
+  assert(r.m_block_addr == m_config.block_addr(mf->get_addr()));
+}
+
+void tex_cache::display_state(FILE *fp) const {
+  fprintf(fp, "%s (texture cache) state:\n", m_name.c_str());
+  fprintf(fp, "fragment fifo entries  = %u / %u\n", m_fragment_fifo.size(),
+          m_fragment_fifo.capacity());
+  fprintf(fp, "reorder buffer entries = %u / %u\n", m_rob.size(),
+          m_rob.capacity());
+  fprintf(fp, "request fifo entries   = %u / %u\n", m_request_fifo.size(),
+          m_request_fifo.capacity());
+  if (!m_rob.empty()) fprintf(fp, "reorder buffer contents:\n");
+  for (int n = m_rob.size() - 1; n >= 0; n--) {
+    unsigned index = (m_rob.next_pop_index() + n) % m_rob.capacity();
+    const rob_entry &r = m_rob.peek(index);
+    fprintf(fp, "tex rob[%3d] : %s ", index,
+            (r.m_ready ? "ready  " : "pending"));
+    if (r.m_ready)
+      fprintf(fp, "@%6u", r.m_time);
+    else
+      fprintf(fp, "       ");
+    fprintf(fp, "[idx=%4u]", r.m_index);
+    r.m_request->print(fp, false);
+  }
+  if (!m_fragment_fifo.empty()) {
+    fprintf(fp, "fragment fifo (oldest) :");
+    fragment_entry &f = m_fragment_fifo.peek();
+    fprintf(fp, "%s:          ", f.m_miss ? "miss" : "hit ");
+    f.m_request->print(fp, false);
+  }
 }
 /******************************************************************************************************************************************/
-
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.h
index 69f2e2f7f5..4989077a1b 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache.h
@@ -30,464 +30,509 @@
 
 #include <stdio.h>
 #include <stdlib.h>
-#include "gpu-misc.h"
-#include "mem_fetch.h"
 #include "../abstract_hardware_model.h"
 #include "../tr1_hash_map.h"
+#include "gpu-misc.h"
+#include "mem_fetch.h"
 
-#include "addrdec.h"
 #include <iostream>
+#include "addrdec.h"
 
 #define MAX_DEFAULT_CACHE_SIZE_MULTIBLIER 4
 
-enum cache_block_state {
-    INVALID=0,
-    RESERVED,
-    VALID,
-    MODIFIED
-};
+enum cache_block_state { INVALID = 0, RESERVED, VALID, MODIFIED };
 
 enum cache_request_status {
-    HIT = 0,
-    HIT_RESERVED,
-    MISS,
-    RESERVATION_FAIL, 
-	SECTOR_MISS,
-    NUM_CACHE_REQUEST_STATUS
+  HIT = 0,
+  HIT_RESERVED,
+  MISS,
+  RESERVATION_FAIL,
+  SECTOR_MISS,
+  MSHR_HIT,
+  NUM_CACHE_REQUEST_STATUS
 };
 
 enum cache_reservation_fail_reason {
-	LINE_ALLOC_FAIL= 0,// all line are reserved
-	MISS_QUEUE_FULL,   // MISS queue (i.e. interconnect or DRAM) is full
-	MSHR_ENRTY_FAIL,
-	MSHR_MERGE_ENRTY_FAIL,
-	MSHR_RW_PENDING,
-    NUM_CACHE_RESERVATION_FAIL_STATUS
+  LINE_ALLOC_FAIL = 0,  // all line are reserved
+  MISS_QUEUE_FULL,      // MISS queue (i.e. interconnect or DRAM) is full
+  MSHR_ENRTY_FAIL,
+  MSHR_MERGE_ENRTY_FAIL,
+  MSHR_RW_PENDING,
+  NUM_CACHE_RESERVATION_FAIL_STATUS
 };
 
 enum cache_event_type {
-    WRITE_BACK_REQUEST_SENT,
-    READ_REQUEST_SENT,
-    WRITE_REQUEST_SENT,
-	WRITE_ALLOCATE_SENT
+  WRITE_BACK_REQUEST_SENT,
+  READ_REQUEST_SENT,
+  WRITE_REQUEST_SENT,
+  WRITE_ALLOCATE_SENT
 };
 
 struct evicted_block_info {
-	new_addr_type m_block_addr;
-	unsigned m_modified_size;
-	evicted_block_info() {
-		m_block_addr = 0;
-		m_modified_size = 0;
-	}
-	void set_info(new_addr_type block_addr, unsigned modified_size){
-		m_block_addr = block_addr;
-		m_modified_size = modified_size;
-	}
+  new_addr_type m_block_addr;
+  unsigned m_modified_size;
+  mem_access_byte_mask_t m_byte_mask;
+  mem_access_sector_mask_t m_sector_mask;
+  evicted_block_info() {
+    m_block_addr = 0;
+    m_modified_size = 0;
+    m_byte_mask.reset();
+    m_sector_mask.reset();
+  }
+  void set_info(new_addr_type block_addr, unsigned modified_size) {
+    m_block_addr = block_addr;
+    m_modified_size = modified_size;
+  }
+  void set_info(new_addr_type block_addr, unsigned modified_size,
+                mem_access_byte_mask_t byte_mask,
+                mem_access_sector_mask_t sector_mask) {
+    m_block_addr = block_addr;
+    m_modified_size = modified_size;
+    m_byte_mask = byte_mask;
+    m_sector_mask = sector_mask;
+  }
 };
 
 struct cache_event {
-	enum cache_event_type m_cache_event_type;
-	evicted_block_info m_evicted_block; //if it was write_back event, fill the the evicted block info
-
-	cache_event(enum cache_event_type m_cache_event){
-		m_cache_event_type = m_cache_event;
-	}
-
-	cache_event(enum cache_event_type cache_event, evicted_block_info evicted_block){
-	m_cache_event_type = cache_event;
-	m_evicted_block = evicted_block;
-	}
+  enum cache_event_type m_cache_event_type;
+  evicted_block_info m_evicted_block;  // if it was write_back event, fill the
+                                       // the evicted block info
+
+  cache_event(enum cache_event_type m_cache_event) {
+    m_cache_event_type = m_cache_event;
+  }
+
+  cache_event(enum cache_event_type cache_event,
+              evicted_block_info evicted_block) {
+    m_cache_event_type = cache_event;
+    m_evicted_block = evicted_block;
+  }
 };
 
-const char * cache_request_status_str(enum cache_request_status status); 
+const char *cache_request_status_str(enum cache_request_status status);
 
 struct cache_block_t {
-    cache_block_t()
-    {
-        m_tag=0;
-        m_block_addr=0;
-    }
-
-    virtual void allocate( new_addr_type tag, new_addr_type block_addr, unsigned time, mem_access_sector_mask_t sector_mask) = 0;
-    virtual void fill( unsigned time, mem_access_sector_mask_t sector_mask) = 0;
-
-    virtual bool is_invalid_line() = 0;
-    virtual bool is_valid_line() = 0;
-    virtual bool is_reserved_line() = 0;
-    virtual bool is_modified_line() = 0;
-
-    virtual enum cache_block_state get_status( mem_access_sector_mask_t sector_mask) = 0;
-    virtual void set_status(enum cache_block_state m_status, mem_access_sector_mask_t sector_mask) = 0;
-
-    virtual unsigned long long get_last_access_time() = 0;
-    virtual void set_last_access_time(unsigned long long time, mem_access_sector_mask_t sector_mask) = 0;
-    virtual unsigned long long get_alloc_time() = 0;
-    virtual void set_ignore_on_fill(bool m_ignore, mem_access_sector_mask_t sector_mask) = 0;
-    virtual void set_modified_on_fill(bool m_modified, mem_access_sector_mask_t sector_mask) = 0;
-    virtual unsigned get_modified_size() = 0;
-    virtual void set_m_readable(bool readable, mem_access_sector_mask_t sector_mask)=0;
-    virtual bool is_readable(mem_access_sector_mask_t sector_mask)=0;
-    virtual void print_status()=0;
-    virtual ~cache_block_t() {}
-
-
-    new_addr_type    m_tag;
-    new_addr_type    m_block_addr;
+  cache_block_t() {
+    m_tag = 0;
+    m_block_addr = 0;
+  }
 
+  virtual void allocate(new_addr_type tag, new_addr_type block_addr,
+                        unsigned time,
+                        mem_access_sector_mask_t sector_mask) = 0;
+  virtual void fill(unsigned time, mem_access_sector_mask_t sector_mask,
+                    mem_access_byte_mask_t byte_mask) = 0;
+
+  virtual bool is_invalid_line() = 0;
+  virtual bool is_valid_line() = 0;
+  virtual bool is_reserved_line() = 0;
+  virtual bool is_modified_line() = 0;
+
+  virtual enum cache_block_state get_status(
+      mem_access_sector_mask_t sector_mask) = 0;
+  virtual void set_status(enum cache_block_state m_status,
+                          mem_access_sector_mask_t sector_mask) = 0;
+  virtual void set_byte_mask(mem_fetch *mf) = 0;
+  virtual void set_byte_mask(mem_access_byte_mask_t byte_mask) = 0;
+  virtual mem_access_byte_mask_t get_dirty_byte_mask() = 0;
+  virtual mem_access_sector_mask_t get_dirty_sector_mask() = 0;
+  virtual unsigned long long get_last_access_time() = 0;
+  virtual void set_last_access_time(unsigned long long time,
+                                    mem_access_sector_mask_t sector_mask) = 0;
+  virtual unsigned long long get_alloc_time() = 0;
+  virtual void set_ignore_on_fill(bool m_ignore,
+                                  mem_access_sector_mask_t sector_mask) = 0;
+  virtual void set_modified_on_fill(bool m_modified,
+                                    mem_access_sector_mask_t sector_mask) = 0;
+  virtual void set_readable_on_fill(bool readable,
+                                    mem_access_sector_mask_t sector_mask) = 0;
+  virtual void set_byte_mask_on_fill(bool m_modified) = 0;
+  virtual unsigned get_modified_size() = 0;
+  virtual void set_m_readable(bool readable,
+                              mem_access_sector_mask_t sector_mask) = 0;
+  virtual bool is_readable(mem_access_sector_mask_t sector_mask) = 0;
+  virtual void print_status() = 0;
+  virtual ~cache_block_t() {}
+
+  new_addr_type m_tag;
+  new_addr_type m_block_addr;
 };
 
-struct line_cache_block: public cache_block_t  {
-	line_cache_block()
-	    {
-	        m_alloc_time=0;
-	        m_fill_time=0;
-	        m_last_access_time=0;
-	        m_status=INVALID;
-	        m_ignore_on_fill_status = false;
-	        m_set_modified_on_fill = false;
-	        m_readable = true;
-	    }
-	    void allocate( new_addr_type tag, new_addr_type block_addr, unsigned time, mem_access_sector_mask_t sector_mask)
-	    {
-	        m_tag=tag;
-	        m_block_addr=block_addr;
-	        m_alloc_time=time;
-	        m_last_access_time=time;
-	        m_fill_time=0;
-	        m_status=RESERVED;
-	        m_ignore_on_fill_status = false;
-	        m_set_modified_on_fill = false;
-	    }
-		void fill( unsigned time, mem_access_sector_mask_t sector_mask )
-	    {
-	    	//if(!m_ignore_on_fill_status)
-	    	//	assert( m_status == RESERVED );
-
-	    	m_status = m_set_modified_on_fill? MODIFIED : VALID;
-
-	        m_fill_time=time;
-	    }
-		virtual bool is_invalid_line()
-	    {
-	    	return m_status == INVALID;
-	    }
-		virtual bool is_valid_line()
-	    {
-	    	 return m_status == VALID;
-	    }
-		virtual bool is_reserved_line()
-	    {
-	    	 return m_status == RESERVED;
-	    }
-		virtual bool is_modified_line()
-	    {
-	    	return m_status == MODIFIED;
-	    }
-
-		virtual enum cache_block_state get_status(mem_access_sector_mask_t sector_mask)
-	    {
-	    	return m_status;
-	    }
-		virtual void set_status(enum cache_block_state status, mem_access_sector_mask_t sector_mask)
-	    {
-	    	m_status = status;
-	    }
-		virtual unsigned long long get_last_access_time()
-		{
-			return m_last_access_time;
-		}
-		virtual void set_last_access_time(unsigned long long time, mem_access_sector_mask_t sector_mask)
-	    {
-	    	m_last_access_time = time;
-	    }
-		virtual unsigned long long get_alloc_time()
-	    {
-	    	return m_alloc_time;
-	    }
-		virtual void set_ignore_on_fill(bool m_ignore, mem_access_sector_mask_t sector_mask)
-		{
-			m_ignore_on_fill_status = m_ignore;
-		}
-		virtual void set_modified_on_fill(bool m_modified, mem_access_sector_mask_t sector_mask)
-		{
-	    	m_set_modified_on_fill = m_modified;
-		}
-		virtual unsigned  get_modified_size()
-		{
-			return SECTOR_CHUNCK_SIZE * SECTOR_SIZE;   //i.e. cache line size
-		}
-		virtual void set_m_readable(bool readable, mem_access_sector_mask_t sector_mask)
-		{
-			m_readable = readable;
-		}
-		virtual bool is_readable(mem_access_sector_mask_t sector_mask) {
-			return m_readable;
-		}
-		virtual void print_status() {
-			 printf("m_block_addr is %llu, status = %u\n", m_block_addr, m_status);
-		}
-
-
-private:
-	    unsigned long long     m_alloc_time;
-	    unsigned long long     m_last_access_time;
-	    unsigned long long     m_fill_time;
-	    cache_block_state    m_status;
-	    bool m_ignore_on_fill_status;
-	    bool m_set_modified_on_fill;
-	    bool m_readable;
+struct line_cache_block : public cache_block_t {
+  line_cache_block() {
+    m_alloc_time = 0;
+    m_fill_time = 0;
+    m_last_access_time = 0;
+    m_status = INVALID;
+    m_ignore_on_fill_status = false;
+    m_set_modified_on_fill = false;
+    m_set_readable_on_fill = false;
+    m_readable = true;
+  }
+  void allocate(new_addr_type tag, new_addr_type block_addr, unsigned time,
+                mem_access_sector_mask_t sector_mask) {
+    m_tag = tag;
+    m_block_addr = block_addr;
+    m_alloc_time = time;
+    m_last_access_time = time;
+    m_fill_time = 0;
+    m_status = RESERVED;
+    m_ignore_on_fill_status = false;
+    m_set_modified_on_fill = false;
+    m_set_readable_on_fill = false;
+    m_set_byte_mask_on_fill = false;
+  }
+  virtual void fill(unsigned time, mem_access_sector_mask_t sector_mask,
+                    mem_access_byte_mask_t byte_mask) {
+    // if(!m_ignore_on_fill_status)
+    //	assert( m_status == RESERVED );
+
+    m_status = m_set_modified_on_fill ? MODIFIED : VALID;
+
+    if (m_set_readable_on_fill) m_readable = true;
+    if (m_set_byte_mask_on_fill) set_byte_mask(byte_mask);
+
+    m_fill_time = time;
+  }
+  virtual bool is_invalid_line() { return m_status == INVALID; }
+  virtual bool is_valid_line() { return m_status == VALID; }
+  virtual bool is_reserved_line() { return m_status == RESERVED; }
+  virtual bool is_modified_line() { return m_status == MODIFIED; }
+
+  virtual enum cache_block_state get_status(
+      mem_access_sector_mask_t sector_mask) {
+    return m_status;
+  }
+  virtual void set_status(enum cache_block_state status,
+                          mem_access_sector_mask_t sector_mask) {
+    m_status = status;
+  }
+  virtual void set_byte_mask(mem_fetch *mf) {
+    m_dirty_byte_mask = m_dirty_byte_mask | mf->get_access_byte_mask();
+  }
+  virtual void set_byte_mask(mem_access_byte_mask_t byte_mask) {
+    m_dirty_byte_mask = m_dirty_byte_mask | byte_mask;
+  }
+  virtual mem_access_byte_mask_t get_dirty_byte_mask() {
+    return m_dirty_byte_mask;
+  }
+  virtual mem_access_sector_mask_t get_dirty_sector_mask() {
+    mem_access_sector_mask_t sector_mask;
+    if (m_status == MODIFIED) sector_mask.set();
+    return sector_mask;
+  }
+  virtual unsigned long long get_last_access_time() {
+    return m_last_access_time;
+  }
+  virtual void set_last_access_time(unsigned long long time,
+                                    mem_access_sector_mask_t sector_mask) {
+    m_last_access_time = time;
+  }
+  virtual unsigned long long get_alloc_time() { return m_alloc_time; }
+  virtual void set_ignore_on_fill(bool m_ignore,
+                                  mem_access_sector_mask_t sector_mask) {
+    m_ignore_on_fill_status = m_ignore;
+  }
+  virtual void set_modified_on_fill(bool m_modified,
+                                    mem_access_sector_mask_t sector_mask) {
+    m_set_modified_on_fill = m_modified;
+  }
+  virtual void set_readable_on_fill(bool readable,
+                                    mem_access_sector_mask_t sector_mask) {
+    m_set_readable_on_fill = readable;
+  }
+  virtual void set_byte_mask_on_fill(bool m_modified) {
+    m_set_byte_mask_on_fill = m_modified;
+  }
+  virtual unsigned get_modified_size() {
+    return SECTOR_CHUNCK_SIZE * SECTOR_SIZE;  // i.e. cache line size
+  }
+  virtual void set_m_readable(bool readable,
+                              mem_access_sector_mask_t sector_mask) {
+    m_readable = readable;
+  }
+  virtual bool is_readable(mem_access_sector_mask_t sector_mask) {
+    return m_readable;
+  }
+  virtual void print_status() {
+    printf("m_block_addr is %llu, status = %u\n", m_block_addr, m_status);
+  }
+
+ private:
+  unsigned long long m_alloc_time;
+  unsigned long long m_last_access_time;
+  unsigned long long m_fill_time;
+  cache_block_state m_status;
+  bool m_ignore_on_fill_status;
+  bool m_set_modified_on_fill;
+  bool m_set_readable_on_fill;
+  bool m_set_byte_mask_on_fill;
+  bool m_readable;
+  mem_access_byte_mask_t m_dirty_byte_mask;
 };
 
 struct sector_cache_block : public cache_block_t {
-	sector_cache_block()
-    {
-		init();
-    }
-
-	void init() {
-		for(unsigned i =0; i< SECTOR_CHUNCK_SIZE; ++i) {
-			m_sector_alloc_time[i]= 0;
-			m_sector_fill_time[i]= 0;
-			m_last_sector_access_time[i]= 0;
-			m_status[i]= INVALID;
-			m_ignore_on_fill_status[i] = false;
-			m_set_modified_on_fill[i] = false;
-			m_readable[i] = true;
-			}
-			m_line_alloc_time=0;
-			m_line_last_access_time=0;
-			m_line_fill_time=0;
-	}
-
-	virtual void allocate( new_addr_type tag, new_addr_type block_addr, unsigned time, mem_access_sector_mask_t sector_mask )
-    {
-    	allocate_line( tag,  block_addr,  time, sector_mask );
+  sector_cache_block() { init(); }
+
+  void init() {
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; ++i) {
+      m_sector_alloc_time[i] = 0;
+      m_sector_fill_time[i] = 0;
+      m_last_sector_access_time[i] = 0;
+      m_status[i] = INVALID;
+      m_ignore_on_fill_status[i] = false;
+      m_set_modified_on_fill[i] = false;
+      m_set_readable_on_fill[i] = false;
+      m_readable[i] = true;
     }
-
-    void allocate_line( new_addr_type tag, new_addr_type block_addr, unsigned time, mem_access_sector_mask_t sector_mask )
-	{
-		//allocate a new line
-		//assert(m_block_addr != 0 && m_block_addr != block_addr);
-		init();
-		m_tag=tag;
-		m_block_addr=block_addr;
-
-		unsigned sidx = get_sector_index(sector_mask);
-
-		//set sector stats
-		m_sector_alloc_time[sidx]=time;
-		m_last_sector_access_time[sidx]=time;
-		m_sector_fill_time[sidx]=0;
-		m_status[sidx]=RESERVED;
-		m_ignore_on_fill_status[sidx] = false;
-		m_set_modified_on_fill[sidx] = false;
-
-		//set line stats
-		m_line_alloc_time=time;   //only set this for the first allocated sector
-		m_line_last_access_time=time;
-		m_line_fill_time=0;
-	}
-
-    void allocate_sector(unsigned time, mem_access_sector_mask_t sector_mask )
-	{
-    	//allocate invalid sector of this allocated valid line
-    	assert(is_valid_line());
-		unsigned sidx = get_sector_index(sector_mask);
-
-		//set sector stats
-		m_sector_alloc_time[sidx]=time;
-		m_last_sector_access_time[sidx]=time;
-		m_sector_fill_time[sidx]=0;
-		if(m_status[sidx]==MODIFIED)    //this should be the case only for fetch-on-write policy //TO DO
-			m_set_modified_on_fill[sidx] = true;
-		else
-			m_set_modified_on_fill[sidx] = false;
-
-		m_status[sidx]=RESERVED;
-		m_ignore_on_fill_status[sidx] = false;
-		//m_set_modified_on_fill[sidx] = false;
-		m_readable[sidx] = true;
-
-		//set line stats
-		m_line_last_access_time=time;
-		m_line_fill_time=0;
-	}
-
-    virtual void fill( unsigned time, mem_access_sector_mask_t sector_mask)
-    {
-    	unsigned sidx = get_sector_index(sector_mask);
+    m_line_alloc_time = 0;
+    m_line_last_access_time = 0;
+    m_line_fill_time = 0;
+    m_dirty_byte_mask.reset();
+  }
+
+  virtual void allocate(new_addr_type tag, new_addr_type block_addr,
+                        unsigned time, mem_access_sector_mask_t sector_mask) {
+    allocate_line(tag, block_addr, time, sector_mask);
+  }
+
+  void allocate_line(new_addr_type tag, new_addr_type block_addr, unsigned time,
+                     mem_access_sector_mask_t sector_mask) {
+    // allocate a new line
+    // assert(m_block_addr != 0 && m_block_addr != block_addr);
+    init();
+    m_tag = tag;
+    m_block_addr = block_addr;
+
+    unsigned sidx = get_sector_index(sector_mask);
+
+    // set sector stats
+    m_sector_alloc_time[sidx] = time;
+    m_last_sector_access_time[sidx] = time;
+    m_sector_fill_time[sidx] = 0;
+    m_status[sidx] = RESERVED;
+    m_ignore_on_fill_status[sidx] = false;
+    m_set_modified_on_fill[sidx] = false;
+    m_set_readable_on_fill[sidx] = false;
+    m_set_byte_mask_on_fill = false;
+
+    // set line stats
+    m_line_alloc_time = time;  // only set this for the first allocated sector
+    m_line_last_access_time = time;
+    m_line_fill_time = 0;
+  }
+
+  void allocate_sector(unsigned time, mem_access_sector_mask_t sector_mask) {
+    // allocate invalid sector of this allocated valid line
+    assert(is_valid_line());
+    unsigned sidx = get_sector_index(sector_mask);
+
+    // set sector stats
+    m_sector_alloc_time[sidx] = time;
+    m_last_sector_access_time[sidx] = time;
+    m_sector_fill_time[sidx] = 0;
+    if (m_status[sidx] == MODIFIED)  // this should be the case only for
+                                     // fetch-on-write policy //TO DO
+      m_set_modified_on_fill[sidx] = true;
+    else
+      m_set_modified_on_fill[sidx] = false;
+
+    m_set_readable_on_fill[sidx] = false;
+
+    m_status[sidx] = RESERVED;
+    m_ignore_on_fill_status[sidx] = false;
+    // m_set_modified_on_fill[sidx] = false;
+    m_readable[sidx] = true;
+
+    // set line stats
+    m_line_last_access_time = time;
+    m_line_fill_time = 0;
+  }
+
+  virtual void fill(unsigned time, mem_access_sector_mask_t sector_mask,
+                    mem_access_byte_mask_t byte_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
 
     //	if(!m_ignore_on_fill_status[sidx])
     //	         assert( m_status[sidx] == RESERVED );
+    m_status[sidx] = m_set_modified_on_fill[sidx] ? MODIFIED : VALID;
 
-    	m_status[sidx] = m_set_modified_on_fill[sidx]? MODIFIED : VALID;
-
-        m_sector_fill_time[sidx]=time;
-        m_line_fill_time=time;
+    if (m_set_readable_on_fill[sidx]) {
+      m_readable[sidx] = true;
+      m_set_readable_on_fill[sidx] = false;
     }
-    virtual bool is_invalid_line() {
-    	//all the sectors should be invalid
-    	for(unsigned i =0; i< SECTOR_CHUNCK_SIZE; ++i) {
-    		if (m_status[i] != INVALID)
-    			return false;
-    	}
-    	return true;
+    if (m_set_byte_mask_on_fill) set_byte_mask(byte_mask);
+
+    m_sector_fill_time[sidx] = time;
+    m_line_fill_time = time;
+  }
+  virtual bool is_invalid_line() {
+    // all the sectors should be invalid
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; ++i) {
+      if (m_status[i] != INVALID) return false;
     }
-    virtual bool is_valid_line() { return  !(is_invalid_line()); }
-    virtual bool is_reserved_line() {
-    	//if any of the sector is reserved, then the line is reserved
-		for(unsigned i =0; i< SECTOR_CHUNCK_SIZE; ++i) {
-			if (m_status[i] == RESERVED)
-				return true;
-		}
-		return false;
+    return true;
+  }
+  virtual bool is_valid_line() { return !(is_invalid_line()); }
+  virtual bool is_reserved_line() {
+    // if any of the sector is reserved, then the line is reserved
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; ++i) {
+      if (m_status[i] == RESERVED) return true;
     }
-    virtual bool is_modified_line() {
-    	//if any of the sector is modified, then the line is modified
-    	for(unsigned i =0; i< SECTOR_CHUNCK_SIZE; ++i) {
-			if (m_status[i] == MODIFIED)
-				return true;
-		}
-		return false;
+    return false;
+  }
+  virtual bool is_modified_line() {
+    // if any of the sector is modified, then the line is modified
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; ++i) {
+      if (m_status[i] == MODIFIED) return true;
     }
-
-    virtual enum cache_block_state get_status(mem_access_sector_mask_t sector_mask)
-	{
-    	unsigned sidx = get_sector_index(sector_mask);
-
-		return m_status[sidx];
-	}
-
-    virtual void set_status(enum cache_block_state status, mem_access_sector_mask_t sector_mask)
-	{
-		unsigned sidx = get_sector_index(sector_mask);
-		m_status[sidx] = status;
-	}
-
-    virtual unsigned long long get_last_access_time()
-	{
-		return m_line_last_access_time;
-	}
-
-    virtual void set_last_access_time(unsigned long long time, mem_access_sector_mask_t sector_mask)
-	{
-		unsigned sidx = get_sector_index(sector_mask);
-
-		m_last_sector_access_time[sidx] = time;
-		m_line_last_access_time = time;
-	}
-
-    virtual unsigned long long get_alloc_time()
-	{
-		return m_line_alloc_time;
-	}
-
-    virtual void set_ignore_on_fill(bool m_ignore, mem_access_sector_mask_t sector_mask)
-	{
-		unsigned sidx = get_sector_index(sector_mask);
-		m_ignore_on_fill_status[sidx] = m_ignore;
-	}
-
-    virtual void set_modified_on_fill(bool m_modified, mem_access_sector_mask_t sector_mask)
-	{
-		unsigned sidx = get_sector_index(sector_mask);
-		m_set_modified_on_fill[sidx] = m_modified;
-	}
-
-    virtual void set_m_readable(bool readable, mem_access_sector_mask_t sector_mask)
-    {
-    	unsigned sidx = get_sector_index(sector_mask);
-    	m_readable[sidx] = readable;
+    return false;
+  }
+
+  virtual enum cache_block_state get_status(
+      mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+
+    return m_status[sidx];
+  }
+
+  virtual void set_status(enum cache_block_state status,
+                          mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+    m_status[sidx] = status;
+  }
+  virtual void set_byte_mask(mem_fetch *mf) {
+    m_dirty_byte_mask = m_dirty_byte_mask | mf->get_access_byte_mask();
+  }
+  virtual void set_byte_mask(mem_access_byte_mask_t byte_mask) {
+    m_dirty_byte_mask = m_dirty_byte_mask | byte_mask;
+  }
+  virtual mem_access_byte_mask_t get_dirty_byte_mask() {
+    return m_dirty_byte_mask;
+  }
+  virtual mem_access_sector_mask_t get_dirty_sector_mask() {
+    mem_access_sector_mask_t sector_mask;
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; i++) {
+      if (m_status[i] == MODIFIED) sector_mask.set(i);
     }
-
-    virtual bool is_readable(mem_access_sector_mask_t sector_mask) {
-    	unsigned sidx = get_sector_index(sector_mask);
-    	return m_readable[sidx];
-	}
-
-    virtual unsigned  get_modified_size()
-	{
-		unsigned modified=0;
-		for(unsigned i =0; i< SECTOR_CHUNCK_SIZE; ++i) {
-			if (m_status[i] == MODIFIED)
-				modified++;
-		}
-		return modified * SECTOR_SIZE;
-	}
-
-    virtual void print_status() {
-    	 printf("m_block_addr is %llu, status = %u %u %u %u\n", m_block_addr, m_status[0], m_status[1], m_status[2], m_status[3]);
+    return sector_mask;
+  }
+  virtual unsigned long long get_last_access_time() {
+    return m_line_last_access_time;
+  }
+
+  virtual void set_last_access_time(unsigned long long time,
+                                    mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+
+    m_last_sector_access_time[sidx] = time;
+    m_line_last_access_time = time;
+  }
+
+  virtual unsigned long long get_alloc_time() { return m_line_alloc_time; }
+
+  virtual void set_ignore_on_fill(bool m_ignore,
+                                  mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+    m_ignore_on_fill_status[sidx] = m_ignore;
+  }
+
+  virtual void set_modified_on_fill(bool m_modified,
+                                    mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+    m_set_modified_on_fill[sidx] = m_modified;
+  }
+  virtual void set_byte_mask_on_fill(bool m_modified) {
+    m_set_byte_mask_on_fill = m_modified;
+  }
+
+  virtual void set_readable_on_fill(bool readable,
+                                    mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+    m_set_readable_on_fill[sidx] = readable;
+  }
+  virtual void set_m_readable(bool readable,
+                              mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+    m_readable[sidx] = readable;
+  }
+
+  virtual bool is_readable(mem_access_sector_mask_t sector_mask) {
+    unsigned sidx = get_sector_index(sector_mask);
+    return m_readable[sidx];
+  }
+
+  virtual unsigned get_modified_size() {
+    unsigned modified = 0;
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; ++i) {
+      if (m_status[i] == MODIFIED) modified++;
     }
-
-
-private:
-    unsigned m_sector_alloc_time[SECTOR_CHUNCK_SIZE];
-    unsigned m_last_sector_access_time[SECTOR_CHUNCK_SIZE];
-    unsigned m_sector_fill_time[SECTOR_CHUNCK_SIZE];
-    unsigned m_line_alloc_time;
-    unsigned m_line_last_access_time;
-    unsigned m_line_fill_time;
-    cache_block_state    m_status[SECTOR_CHUNCK_SIZE];
-    bool m_ignore_on_fill_status[SECTOR_CHUNCK_SIZE];
-    bool m_set_modified_on_fill[SECTOR_CHUNCK_SIZE];
-    bool m_readable[SECTOR_CHUNCK_SIZE];
-
-    unsigned get_sector_index(mem_access_sector_mask_t sector_mask)
-    {
-    	assert(sector_mask.count() == 1);
-    	for(unsigned i =0; i< SECTOR_CHUNCK_SIZE; ++i) {
-    		if(sector_mask.to_ulong() & (1<<i))
-    			return i;
-    	}
+    return modified * SECTOR_SIZE;
+  }
+
+  virtual void print_status() {
+    printf("m_block_addr is %llu, status = %u %u %u %u\n", m_block_addr,
+           m_status[0], m_status[1], m_status[2], m_status[3]);
+  }
+
+ private:
+  unsigned m_sector_alloc_time[SECTOR_CHUNCK_SIZE];
+  unsigned m_last_sector_access_time[SECTOR_CHUNCK_SIZE];
+  unsigned m_sector_fill_time[SECTOR_CHUNCK_SIZE];
+  unsigned m_line_alloc_time;
+  unsigned m_line_last_access_time;
+  unsigned m_line_fill_time;
+  cache_block_state m_status[SECTOR_CHUNCK_SIZE];
+  bool m_ignore_on_fill_status[SECTOR_CHUNCK_SIZE];
+  bool m_set_modified_on_fill[SECTOR_CHUNCK_SIZE];
+  bool m_set_readable_on_fill[SECTOR_CHUNCK_SIZE];
+  bool m_set_byte_mask_on_fill;
+  bool m_readable[SECTOR_CHUNCK_SIZE];
+  mem_access_byte_mask_t m_dirty_byte_mask;
+
+  unsigned get_sector_index(mem_access_sector_mask_t sector_mask) {
+    assert(sector_mask.count() == 1);
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; ++i) {
+      if (sector_mask.to_ulong() & (1 << i)) return i;
     }
+  }
 };
 
-enum replacement_policy_t {
-    LRU,
-    FIFO
-};
+enum replacement_policy_t { LRU, FIFO };
 
 enum write_policy_t {
-    READ_ONLY,
-    WRITE_BACK,
-    WRITE_THROUGH,
-    WRITE_EVICT,
-    LOCAL_WB_GLOBAL_WT
-};
-
-enum allocation_policy_t {
-    ON_MISS,
-    ON_FILL,
-	STREAMING
+  READ_ONLY,
+  WRITE_BACK,
+  WRITE_THROUGH,
+  WRITE_EVICT,
+  LOCAL_WB_GLOBAL_WT
 };
 
+enum allocation_policy_t { ON_MISS, ON_FILL, STREAMING };
 
 enum write_allocate_policy_t {
-	NO_WRITE_ALLOCATE,
-	WRITE_ALLOCATE,
-	FETCH_ON_WRITE,
-	LAZY_FETCH_ON_READ
+  NO_WRITE_ALLOCATE,
+  WRITE_ALLOCATE,
+  FETCH_ON_WRITE,
+  LAZY_FETCH_ON_READ
 };
 
 enum mshr_config_t {
-    TEX_FIFO, // Tex cache
-    ASSOC, // normal cache
-	SECTOR_TEX_FIFO,  //Tex cache sends requests to high-level sector cache
-	SECTOR_ASSOC // normal cache sends requests to high-level sector cache
+  TEX_FIFO,         // Tex cache
+  ASSOC,            // normal cache
+  SECTOR_TEX_FIFO,  // Tex cache sends requests to high-level sector cache
+  SECTOR_ASSOC      // normal cache sends requests to high-level sector cache
 };
 
-enum set_index_function{
-	LINEAR_SET_FUNCTION = 0,
-	BITWISE_XORING_FUNCTION,
-	HASH_IPOLY_FUNCTION,
-	FERMI_HASH_SET_FUNCTION,
-    CUSTOM_SET_FUNCTION
+enum set_index_function {
+  LINEAR_SET_FUNCTION = 0,
+  BITWISE_XORING_FUNCTION,
+  HASH_IPOLY_FUNCTION,
+  FERMI_HASH_SET_FUNCTION,
+  CUSTOM_SET_FUNCTION
 };
 
-enum cache_type{
-    NORMAL = 0,
-    SECTOR
-};
+enum cache_type { NORMAL = 0, SECTOR };
 
 #define MAX_WARP_PER_SHADER 64
 #define INCT_TOTAL_BUFFER 64
@@ -496,539 +541,641 @@ enum cache_type{
 #define MAX_WARP_PER_SHADER 64
 
 class cache_config {
-public:
-    cache_config() 
-    { 
-        m_valid = false; 
-        m_disabled = false;
-        m_config_string = NULL; // set by option parser
-        m_config_stringPrefL1 = NULL;
-        m_config_stringPrefShared = NULL;
-        m_data_port_width = 0;
-        m_set_index_function = LINEAR_SET_FUNCTION;
-        m_is_streaming = false;
+ public:
+  cache_config() {
+    m_valid = false;
+    m_disabled = false;
+    m_config_string = NULL;  // set by option parser
+    m_config_stringPrefL1 = NULL;
+    m_config_stringPrefShared = NULL;
+    m_data_port_width = 0;
+    m_set_index_function = LINEAR_SET_FUNCTION;
+    m_is_streaming = false;
+    m_wr_percent = 0;
+  }
+  void init(char *config, FuncCache status) {
+    cache_status = status;
+    assert(config);
+    char ct, rp, wp, ap, mshr_type, wap, sif;
+
+    int ntok =
+        sscanf(config, "%c:%u:%u:%u,%c:%c:%c:%c:%c,%c:%u:%u,%u:%u,%u", &ct,
+               &m_nset, &m_line_sz, &m_assoc, &rp, &wp, &ap, &wap, &sif,
+               &mshr_type, &m_mshr_entries, &m_mshr_max_merge,
+               &m_miss_queue_size, &m_result_fifo_entries, &m_data_port_width);
+
+    if (ntok < 12) {
+      if (!strcmp(config, "none")) {
+        m_disabled = true;
+        return;
+      }
+      exit_parse_error();
     }
-    void init(char * config, FuncCache status)
-    {
-    	cache_status= status;
-        assert( config );
-        char ct, rp, wp, ap, mshr_type, wap, sif;
-
-
-        int ntok = sscanf(config,"%c:%u:%u:%u,%c:%c:%c:%c:%c,%c:%u:%u,%u:%u,%u",
-                          &ct, &m_nset, &m_line_sz, &m_assoc, &rp, &wp, &ap, &wap,
-                          &sif,&mshr_type,&m_mshr_entries,&m_mshr_max_merge,
-                          &m_miss_queue_size, &m_result_fifo_entries,
-                          &m_data_port_width);
-
-        if ( ntok < 12 ) {
-            if ( !strcmp(config,"none") ) {
-                m_disabled = true;
-                return;
-            }
-            exit_parse_error();
-        }
-
-        switch (ct) {
-			   case 'N': m_cache_type = NORMAL; break;
-			   case 'S': m_cache_type = SECTOR; break;
-			   default: exit_parse_error();
-        }
-        switch (rp) {
-               case 'L': m_replacement_policy = LRU; break;
-               case 'F': m_replacement_policy = FIFO; break;
-               default: exit_parse_error();
-        }
-        switch (rp) {
-        case 'L': m_replacement_policy = LRU; break;
-        case 'F': m_replacement_policy = FIFO; break;
-        default: exit_parse_error();
-        }
-        switch (wp) {
-        case 'R': m_write_policy = READ_ONLY; break;
-        case 'B': m_write_policy = WRITE_BACK; break;
-        case 'T': m_write_policy = WRITE_THROUGH; break;
-        case 'E': m_write_policy = WRITE_EVICT; break;
-        case 'L': m_write_policy = LOCAL_WB_GLOBAL_WT; break;
-        default: exit_parse_error();
-        }
-        switch (ap) {
-        case 'm': m_alloc_policy = ON_MISS; break;
-        case 'f': m_alloc_policy = ON_FILL; break;
-        case 's': m_alloc_policy = STREAMING; break;
-        default: exit_parse_error();
-        }
-        if(m_alloc_policy == STREAMING) {
-        	//For streaming cache, we set the alloc policy to be on-fill to remove all line_alloc_fail stalls
-        	//we set the MSHRs to be equal to max allocated cache lines. This is possible by moving TAG to be shared between cache line and MSHR enrty (i.e. for each cache line, there is an MSHR rntey associated with it)
-        	//This is the easiest think we can think about to model (mimic) L1 streaming cache in Pascal and Volta
-        	//Based on our microbenchmakrs, MSHRs entries have been increasing substantially in Pascal and Volta
-        	//For more information about streaming cache, see:
-        	// http://on-demand.gputechconf.com/gtc/2017/presentation/s7798-luke-durant-inside-volta.pdf
-        	// https://ieeexplore.ieee.org/document/8344474/
-        	m_is_streaming = true;
-			m_alloc_policy = ON_FILL;
-			m_mshr_entries = m_nset*m_assoc*MAX_DEFAULT_CACHE_SIZE_MULTIBLIER;
-			if(m_cache_type == SECTOR)
-				m_mshr_entries *=  SECTOR_CHUNCK_SIZE;
-			m_mshr_max_merge = MAX_WARP_PER_SM;
-        }
-        switch (mshr_type) {
-        case 'F': m_mshr_type = TEX_FIFO; assert(ntok==14); break;
-        case 'T': m_mshr_type = SECTOR_TEX_FIFO; assert(ntok==14); break;
-        case 'A': m_mshr_type = ASSOC; break;
-        case 'S' : m_mshr_type = SECTOR_ASSOC; break;
-        default: exit_parse_error();
-        }
-        m_line_sz_log2 = LOGB2(m_line_sz);
-        m_nset_log2 = LOGB2(m_nset);
-        m_valid = true;
-        m_atom_sz = (m_cache_type == SECTOR)? SECTOR_SIZE : m_line_sz;
-        original_m_assoc = m_assoc;
-
-        //For more details about difference between FETCH_ON_WRITE and WRITE VALIDAE policies
-        //Read: Jouppi, Norman P. "Cache write policies and performance". ISCA 93.
-        //WRITE_ALLOCATE is the old write policy in GPGPU-sim 3.x, that send WRITE and READ for every write request
-        switch(wap){
-        case 'N': m_write_alloc_policy = NO_WRITE_ALLOCATE; break;
-        case 'W': m_write_alloc_policy = WRITE_ALLOCATE; break;
-        case 'F': m_write_alloc_policy = FETCH_ON_WRITE; break;
-        case 'L': m_write_alloc_policy = LAZY_FETCH_ON_READ; break;
-		default: exit_parse_error();
-        }
-
-        // detect invalid configuration 
-        if (m_alloc_policy == ON_FILL and m_write_policy == WRITE_BACK) {
-            // A writeback cache with allocate-on-fill policy will inevitably lead to deadlock:  
-            // The deadlock happens when an incoming cache-fill evicts a dirty
-            // line, generating a writeback request.  If the memory subsystem
-            // is congested, the interconnection network may not have
-            // sufficient buffer for the writeback request.  This stalls the
-            // incoming cache-fill.  The stall may propagate through the memory
-            // subsystem back to the output port of the same core, creating a
-            // deadlock where the wrtieback request and the incoming cache-fill
-            // are stalling each other.  
-            assert(0 && "Invalid cache configuration: Writeback cache cannot allocate new line on fill. "); 
-        }
-
-        if((m_write_alloc_policy == FETCH_ON_WRITE || m_write_alloc_policy == LAZY_FETCH_ON_READ )&& m_alloc_policy == ON_FILL)
-		{
-			assert(0 && "Invalid cache configuration: FETCH_ON_WRITE and LAZY_FETCH_ON_READ cannot work properly with ON_FILL policy. Cache must be ON_MISS. ");
-		}
-        if(m_cache_type == SECTOR)
-		{
-			assert(m_line_sz / SECTOR_SIZE == SECTOR_CHUNCK_SIZE && m_line_sz % SECTOR_SIZE == 0);
-		}
-
-        // default: port to data array width and granularity = line size 
-        if (m_data_port_width == 0) {
-            m_data_port_width = m_line_sz; 
-        }
-        assert(m_line_sz % m_data_port_width == 0); 
-
-        switch(sif){
-        case 'H': m_set_index_function = FERMI_HASH_SET_FUNCTION; break;
-        case 'P': m_set_index_function = HASH_IPOLY_FUNCTION; break;
-        case 'C': m_set_index_function = CUSTOM_SET_FUNCTION; break;
-        case 'L': m_set_index_function = LINEAR_SET_FUNCTION; break;
-        default: exit_parse_error();
-        }
+
+    switch (ct) {
+      case 'N':
+        m_cache_type = NORMAL;
+        break;
+      case 'S':
+        m_cache_type = SECTOR;
+        break;
+      default:
+        exit_parse_error();
     }
-    bool disabled() const { return m_disabled;}
-    unsigned get_line_sz() const
-    {
-        assert( m_valid );
-        return m_line_sz;
+    switch (rp) {
+      case 'L':
+        m_replacement_policy = LRU;
+        break;
+      case 'F':
+        m_replacement_policy = FIFO;
+        break;
+      default:
+        exit_parse_error();
     }
-    unsigned get_atom_sz() const
-	{
-		assert( m_valid );
-		return m_atom_sz;
-	}
-    unsigned get_num_lines() const
-    {
-        assert( m_valid );
-        return m_nset * m_assoc;
+    switch (wp) {
+      case 'R':
+        m_write_policy = READ_ONLY;
+        break;
+      case 'B':
+        m_write_policy = WRITE_BACK;
+        break;
+      case 'T':
+        m_write_policy = WRITE_THROUGH;
+        break;
+      case 'E':
+        m_write_policy = WRITE_EVICT;
+        break;
+      case 'L':
+        m_write_policy = LOCAL_WB_GLOBAL_WT;
+        break;
+      default:
+        exit_parse_error();
     }
-    unsigned get_max_num_lines() const
-    {
-        assert( m_valid );
-        return MAX_DEFAULT_CACHE_SIZE_MULTIBLIER * m_nset * original_m_assoc;
+    switch (ap) {
+      case 'm':
+        m_alloc_policy = ON_MISS;
+        break;
+      case 'f':
+        m_alloc_policy = ON_FILL;
+        break;
+      case 's':
+        m_alloc_policy = STREAMING;
+        break;
+      default:
+        exit_parse_error();
     }
-    void print( FILE *fp ) const
-    {
-        fprintf( fp, "Size = %d B (%d Set x %d-way x %d byte line)\n", 
-                 m_line_sz * m_nset * m_assoc,
-                 m_nset, m_assoc, m_line_sz );
+    if (m_alloc_policy == STREAMING) {
+      /*
+      For streaming cache:
+      (1) we set the alloc policy to be on-fill to remove all line_alloc_fail
+      stalls. if the whole memory is allocated to the L1 cache, then make the
+      allocation to be on_MISS otherwise, make it ON_FILL to eliminate line
+      allocation fails. i.e. MSHR throughput is the same, independent on the L1
+      cache size/associativity So, we set the allocation policy per kernel
+      basis, see shader.cc, max_cta() function
+
+      (2) We also set the MSHRs to be equal to max
+      allocated cache lines. This is possible by moving TAG to be shared
+      between cache line and MSHR enrty (i.e. for each cache line, there is
+      an MSHR rntey associated with it). This is the easiest think we can
+      think of to model (mimic) L1 streaming cache in Pascal and Volta
+
+      For more information about streaming cache, see:
+      http://on-demand.gputechconf.com/gtc/2017/presentation/s7798-luke-durant-inside-volta.pdf
+      https://ieeexplore.ieee.org/document/8344474/
+      */
+      m_is_streaming = true;
+      m_alloc_policy = ON_FILL;
     }
-
-    virtual unsigned set_index( new_addr_type addr ) const
-    {
-        if(m_set_index_function != LINEAR_SET_FUNCTION){
-            printf("\nGPGPU-Sim cache configuration error: Hashing or "
-                    "custom set index function selected in configuration "
-                    "file for a cache that has not overloaded the set_index "
-                    "function\n");
-            abort();
-        }
-        return(addr >> m_line_sz_log2) & (m_nset-1);
+    switch (mshr_type) {
+      case 'F':
+        m_mshr_type = TEX_FIFO;
+        assert(ntok == 14);
+        break;
+      case 'T':
+        m_mshr_type = SECTOR_TEX_FIFO;
+        assert(ntok == 14);
+        break;
+      case 'A':
+        m_mshr_type = ASSOC;
+        break;
+      case 'S':
+        m_mshr_type = SECTOR_ASSOC;
+        break;
+      default:
+        exit_parse_error();
+    }
+    m_line_sz_log2 = LOGB2(m_line_sz);
+    m_nset_log2 = LOGB2(m_nset);
+    m_valid = true;
+    m_atom_sz = (m_cache_type == SECTOR) ? SECTOR_SIZE : m_line_sz;
+    m_sector_sz_log2 = LOGB2(SECTOR_SIZE);
+    original_m_assoc = m_assoc;
+
+    // For more details about difference between FETCH_ON_WRITE and WRITE
+    // VALIDAE policies Read: Jouppi, Norman P. "Cache write policies and
+    // performance". ISCA 93. WRITE_ALLOCATE is the old write policy in
+    // GPGPU-sim 3.x, that send WRITE and READ for every write request
+    switch (wap) {
+      case 'N':
+        m_write_alloc_policy = NO_WRITE_ALLOCATE;
+        break;
+      case 'W':
+        m_write_alloc_policy = WRITE_ALLOCATE;
+        break;
+      case 'F':
+        m_write_alloc_policy = FETCH_ON_WRITE;
+        break;
+      case 'L':
+        m_write_alloc_policy = LAZY_FETCH_ON_READ;
+        break;
+      default:
+        exit_parse_error();
     }
 
-    new_addr_type tag( new_addr_type addr ) const
-    {
-        // For generality, the tag includes both index and tag. This allows for more complex set index
-        // calculations that can result in different indexes mapping to the same set, thus the full
-        // tag + index is required to check for hit/miss. Tag is now identical to the block address.
+    // detect invalid configuration
+    if ((m_alloc_policy == ON_FILL || m_alloc_policy == STREAMING) and
+        m_write_policy == WRITE_BACK) {
+      // A writeback cache with allocate-on-fill policy will inevitably lead to
+      // deadlock: The deadlock happens when an incoming cache-fill evicts a
+      // dirty line, generating a writeback request.  If the memory subsystem is
+      // congested, the interconnection network may not have sufficient buffer
+      // for the writeback request.  This stalls the incoming cache-fill.  The
+      // stall may propagate through the memory subsystem back to the output
+      // port of the same core, creating a deadlock where the wrtieback request
+      // and the incoming cache-fill are stalling each other.
+      assert(0 &&
+             "Invalid cache configuration: Writeback cache cannot allocate new "
+             "line on fill. ");
+    }
 
-        //return addr >> (m_line_sz_log2+m_nset_log2);
-        return addr & ~(new_addr_type)(m_line_sz-1);
+    if ((m_write_alloc_policy == FETCH_ON_WRITE ||
+         m_write_alloc_policy == LAZY_FETCH_ON_READ) &&
+        m_alloc_policy == ON_FILL) {
+      assert(
+          0 &&
+          "Invalid cache configuration: FETCH_ON_WRITE and LAZY_FETCH_ON_READ "
+          "cannot work properly with ON_FILL policy. Cache must be ON_MISS. ");
     }
-    new_addr_type block_addr( new_addr_type addr ) const
-    {
-        return addr & ~(new_addr_type)(m_line_sz-1);
+    if (m_cache_type == SECTOR) {
+      assert(m_line_sz / SECTOR_SIZE == SECTOR_CHUNCK_SIZE &&
+             m_line_sz % SECTOR_SIZE == 0);
     }
-    new_addr_type mshr_addr( new_addr_type addr ) const
-	{
-    	return addr & ~(new_addr_type)(m_atom_sz-1);
-	}
-    enum mshr_config_t get_mshr_type() const
-	{
-    	return m_mshr_type;
-	}
-    void set_assoc(unsigned n)
-	{
-    	//set new assoc. L1 cache dynamically resized in Volta
-    	m_assoc = n;
-	}
-    unsigned get_nset() const
-	{
-		assert( m_valid );
-		return m_nset;
-	}
-    unsigned get_total_size_inKB() const
-	{
-		assert( m_valid );
-		return (m_assoc*m_nset*m_line_sz)/1024;
-	}
-    bool is_streaming() {
-    	return m_is_streaming;
+
+    // default: port to data array width and granularity = line size
+    if (m_data_port_width == 0) {
+      m_data_port_width = m_line_sz;
     }
-    FuncCache get_cache_status() {return cache_status;}
-    char *m_config_string;
-    char *m_config_stringPrefL1;
-    char *m_config_stringPrefShared;
-    FuncCache cache_status;
-
-protected:
-    void exit_parse_error()
-    {
-        printf("GPGPU-Sim uArch: cache configuration parsing error (%s)\n", m_config_string );
-        abort();
+    assert(m_line_sz % m_data_port_width == 0);
+
+    switch (sif) {
+      case 'H':
+        m_set_index_function = FERMI_HASH_SET_FUNCTION;
+        break;
+      case 'P':
+        m_set_index_function = HASH_IPOLY_FUNCTION;
+        break;
+      case 'C':
+        m_set_index_function = CUSTOM_SET_FUNCTION;
+        break;
+      case 'L':
+        m_set_index_function = LINEAR_SET_FUNCTION;
+        break;
+      case 'X':
+        m_set_index_function = BITWISE_XORING_FUNCTION;
+        break;
+      default:
+        exit_parse_error();
     }
-
-    bool m_valid;
-    bool m_disabled;
-    unsigned m_line_sz;
-    unsigned m_line_sz_log2;
-    unsigned m_nset;
-    unsigned m_nset_log2;
-    unsigned m_assoc;
-    unsigned m_atom_sz;
-    unsigned original_m_assoc;
-    bool m_is_streaming;
-
-    enum replacement_policy_t m_replacement_policy; // 'L' = LRU, 'F' = FIFO
-    enum write_policy_t m_write_policy;             // 'T' = write through, 'B' = write back, 'R' = read only
-    enum allocation_policy_t m_alloc_policy;        // 'm' = allocate on miss, 'f' = allocate on fill
-    enum mshr_config_t m_mshr_type;
-    enum cache_type m_cache_type;
-
-    write_allocate_policy_t m_write_alloc_policy;	// 'W' = Write allocate, 'N' = No write allocate
-
-    union {
-        unsigned m_mshr_entries;
-        unsigned m_fragment_fifo_entries;
-    };
-    union {
-        unsigned m_mshr_max_merge;
-        unsigned m_request_fifo_entries;
-    };
-    union {
-        unsigned m_miss_queue_size;
-        unsigned m_rob_entries;
-    };
-    unsigned m_result_fifo_entries;
-    unsigned m_data_port_width; //< number of byte the cache can access per cycle 
-    enum set_index_function m_set_index_function; // Hash, linear, or custom set index function
-
-    friend class tag_array;
-    friend class baseline_cache;
-    friend class read_only_cache;
-    friend class l1icache_gem5;
-    friend class tex_cache;
-    friend class data_cache;
-    friend class l1_cache;
-    friend class l2_cache;
-    friend class memory_sub_partition;
+  }
+  bool disabled() const { return m_disabled; }
+  unsigned get_line_sz() const {
+    assert(m_valid);
+    return m_line_sz;
+  }
+  unsigned get_atom_sz() const {
+    assert(m_valid);
+    return m_atom_sz;
+  }
+  unsigned get_num_lines() const {
+    assert(m_valid);
+    return m_nset * m_assoc;
+  }
+  unsigned get_max_num_lines() const {
+    assert(m_valid);
+    return get_max_cache_multiplier() * m_nset * original_m_assoc;
+  }
+  unsigned get_max_assoc() const {
+    assert(m_valid);
+    return get_max_cache_multiplier() * original_m_assoc;
+  }
+  void print(FILE *fp) const {
+    fprintf(fp, "Size = %d B (%d Set x %d-way x %d byte line)\n",
+            m_line_sz * m_nset * m_assoc, m_nset, m_assoc, m_line_sz);
+  }
+
+  virtual unsigned set_index(new_addr_type addr) const;
+
+  virtual unsigned get_max_cache_multiplier() const {
+    return MAX_DEFAULT_CACHE_SIZE_MULTIBLIER;
+  }
+
+  unsigned hash_function(new_addr_type addr, unsigned m_nset,
+                         unsigned m_line_sz_log2, unsigned m_nset_log2,
+                         unsigned m_index_function) const;
+
+  new_addr_type tag(new_addr_type addr) const {
+    // For generality, the tag includes both index and tag. This allows for more
+    // complex set index calculations that can result in different indexes
+    // mapping to the same set, thus the full tag + index is required to check
+    // for hit/miss. Tag is now identical to the block address.
+
+    // return addr >> (m_line_sz_log2+m_nset_log2);
+    return addr & ~(new_addr_type)(m_line_sz - 1);
+  }
+  new_addr_type block_addr(new_addr_type addr) const {
+    return addr & ~(new_addr_type)(m_line_sz - 1);
+  }
+  new_addr_type mshr_addr(new_addr_type addr) const {
+    return addr & ~(new_addr_type)(m_atom_sz - 1);
+  }
+  enum mshr_config_t get_mshr_type() const { return m_mshr_type; }
+  void set_assoc(unsigned n) {
+    // set new assoc. L1 cache dynamically resized in Volta
+    m_assoc = n;
+  }
+  unsigned get_nset() const {
+    assert(m_valid);
+    return m_nset;
+  }
+  unsigned get_total_size_inKB() const {
+    assert(m_valid);
+    return (m_assoc * m_nset * m_line_sz) / 1024;
+  }
+  bool is_streaming() { return m_is_streaming; }
+  FuncCache get_cache_status() { return cache_status; }
+  void set_allocation_policy(enum allocation_policy_t alloc) {
+    m_alloc_policy = alloc;
+  }
+  char *m_config_string;
+  char *m_config_stringPrefL1;
+  char *m_config_stringPrefShared;
+  FuncCache cache_status;
+  unsigned m_wr_percent;
+  write_allocate_policy_t get_write_allocate_policy() {
+    return m_write_alloc_policy;
+  }
+  write_policy_t get_write_policy() { return m_write_policy; }
+
+ protected:
+  void exit_parse_error() {
+    printf("GPGPU-Sim uArch: cache configuration parsing error (%s)\n",
+           m_config_string);
+    abort();
+  }
+
+  bool m_valid;
+  bool m_disabled;
+  unsigned m_line_sz;
+  unsigned m_line_sz_log2;
+  unsigned m_nset;
+  unsigned m_nset_log2;
+  unsigned m_assoc;
+  unsigned m_atom_sz;
+  unsigned m_sector_sz_log2;
+  unsigned original_m_assoc;
+  bool m_is_streaming;
+
+  enum replacement_policy_t m_replacement_policy;  // 'L' = LRU, 'F' = FIFO
+  enum write_policy_t
+      m_write_policy;  // 'T' = write through, 'B' = write back, 'R' = read only
+  enum allocation_policy_t
+      m_alloc_policy;  // 'm' = allocate on miss, 'f' = allocate on fill
+  enum mshr_config_t m_mshr_type;
+  enum cache_type m_cache_type;
+
+  write_allocate_policy_t
+      m_write_alloc_policy;  // 'W' = Write allocate, 'N' = No write allocate
+
+  union {
+    unsigned m_mshr_entries;
+    unsigned m_fragment_fifo_entries;
+  };
+  union {
+    unsigned m_mshr_max_merge;
+    unsigned m_request_fifo_entries;
+  };
+  union {
+    unsigned m_miss_queue_size;
+    unsigned m_rob_entries;
+  };
+  unsigned m_result_fifo_entries;
+  unsigned m_data_port_width;  //< number of byte the cache can access per cycle
+  enum set_index_function
+      m_set_index_function;  // Hash, linear, or custom set index function
+
+  friend class tag_array;
+  friend class baseline_cache;
+  friend class read_only_cache;
+  friend class l1icache_gem5;
+  friend class tex_cache;
+  friend class data_cache;
+  friend class l1_cache;
+  friend class l2_cache;
+  friend class memory_sub_partition;
 };
 
-class l1d_cache_config : public cache_config{
-public:
-	l1d_cache_config() : cache_config(){}
-	virtual unsigned set_index(new_addr_type addr) const;
-	unsigned l1_latency;
+class l1d_cache_config : public cache_config {
+ public:
+  l1d_cache_config() : cache_config() {}
+  unsigned set_bank(new_addr_type addr) const;
+  void init(char *config, FuncCache status) {
+    l1_banks_byte_interleaving_log2 = LOGB2(l1_banks_byte_interleaving);
+    l1_banks_log2 = LOGB2(l1_banks);
+    cache_config::init(config, status);
+  }
+  unsigned l1_latency;
+  unsigned l1_banks;
+  unsigned l1_banks_log2;
+  unsigned l1_banks_byte_interleaving;
+  unsigned l1_banks_byte_interleaving_log2;
+  unsigned l1_banks_hashing_function;
+  unsigned m_unified_cache_size;
+  virtual unsigned get_max_cache_multiplier() const {
+    // set * assoc * cacheline size. Then convert Byte to KB
+    // gpgpu_unified_cache_size is in KB while original_sz is in B
+    if (m_unified_cache_size > 0) {
+      unsigned original_size = m_nset * original_m_assoc * m_line_sz / 1024;
+      assert(m_unified_cache_size % original_size == 0);
+      return m_unified_cache_size / original_size;
+    } else {
+      return MAX_DEFAULT_CACHE_SIZE_MULTIBLIER;
+    }
+  }
 };
 
 class l2_cache_config : public cache_config {
-public:
-	l2_cache_config() : cache_config(){}
-	void init(linear_to_raw_address_translation *address_mapping);
-	virtual unsigned set_index(new_addr_type addr) const;
+ public:
+  l2_cache_config() : cache_config() {}
+  void init(linear_to_raw_address_translation *address_mapping);
+  virtual unsigned set_index(new_addr_type addr) const;
 
-private:
-	linear_to_raw_address_translation *m_address_mapping;
+ private:
+  linear_to_raw_address_translation *m_address_mapping;
 };
 
 class tag_array {
-public:
-    // Use this constructor
-    tag_array(cache_config &config, int core_id, int type_id );
-    ~tag_array();
-
-    enum cache_request_status probe( new_addr_type addr, unsigned &idx, mem_fetch* mf, bool probe_mode=false ) const;
-    enum cache_request_status probe( new_addr_type addr, unsigned &idx, mem_access_sector_mask_t mask, bool probe_mode=false, mem_fetch* mf = NULL ) const;
-    enum cache_request_status access( new_addr_type addr, unsigned time, unsigned &idx, mem_fetch* mf );
-    enum cache_request_status access( new_addr_type addr, unsigned time, unsigned &idx, bool &wb, evicted_block_info &evicted, mem_fetch* mf );
-
-    void fill( new_addr_type addr, unsigned time, mem_fetch* mf );
-    void fill( unsigned idx, unsigned time, mem_fetch* mf );
-    void fill( new_addr_type addr, unsigned time, mem_access_sector_mask_t mask );
-
-    unsigned size() const { return m_config.get_num_lines();}
-    cache_block_t* get_block(unsigned idx) { return m_lines[idx];}
-
-    void flush(); // flush all written entries
-    void invalidate(); // invalidate all entries
-    void new_window();
-
-    void print( FILE *stream, unsigned &total_access, unsigned &total_misses ) const;
-    float windowed_miss_rate( ) const;
-    void get_stats(unsigned &total_access, unsigned &total_misses, unsigned &total_hit_res, unsigned &total_res_fail) const;
-
-	void update_cache_parameters(cache_config &config);
-	void add_pending_line(mem_fetch *mf);
-	void remove_pending_line(mem_fetch *mf);
-protected:
-    // This constructor is intended for use only from derived classes that wish to
-    // avoid unnecessary memory allocation that takes place in the
-    // other tag_array constructor
-    tag_array( cache_config &config,
-               int core_id,
-               int type_id,
-               cache_block_t** new_lines );
-    void init( int core_id, int type_id );
-
-protected:
-
-    cache_config &m_config;
-
-    cache_block_t **m_lines; /* nbanks x nset x assoc lines in total */
-
-    unsigned m_access;
-    unsigned m_miss;
-    unsigned m_pending_hit; // number of cache miss that hit a line that is allocated but not filled
-    unsigned m_res_fail;
-    unsigned m_sector_miss;
-
-    // performance counters for calculating the amount of misses within a time window
-    unsigned m_prev_snapshot_access;
-    unsigned m_prev_snapshot_miss;
-    unsigned m_prev_snapshot_pending_hit;
-
-    int m_core_id; // which shader core is using this
-    int m_type_id; // what kind of cache is this (normal, texture, constant)
-
-    bool is_used;  //a flag if the whole cache has ever been accessed before
-
-    typedef tr1_hash_map<new_addr_type,unsigned> line_table;
-    line_table pending_lines;
+ public:
+  // Use this constructor
+  tag_array(cache_config &config, int core_id, int type_id);
+  ~tag_array();
+
+  enum cache_request_status probe(new_addr_type addr, unsigned &idx,
+                                  mem_fetch *mf, bool is_write,
+                                  bool probe_mode = false) const;
+  enum cache_request_status probe(new_addr_type addr, unsigned &idx,
+                                  mem_access_sector_mask_t mask, bool is_write,
+                                  bool probe_mode = false,
+                                  mem_fetch *mf = NULL) const;
+  enum cache_request_status access(new_addr_type addr, unsigned time,
+                                   unsigned &idx, mem_fetch *mf);
+  enum cache_request_status access(new_addr_type addr, unsigned time,
+                                   unsigned &idx, bool &wb,
+                                   evicted_block_info &evicted, mem_fetch *mf);
+
+  void fill(new_addr_type addr, unsigned time, mem_fetch *mf, bool is_write);
+  void fill(unsigned idx, unsigned time, mem_fetch *mf);
+  void fill(new_addr_type addr, unsigned time, mem_access_sector_mask_t mask,
+            mem_access_byte_mask_t byte_mask, bool is_write);
+
+  unsigned size() const { return m_config.get_num_lines(); }
+  cache_block_t *get_block(unsigned idx) { return m_lines[idx]; }
+
+  void flush();       // flush all written entries
+  void invalidate();  // invalidate all entries
+  void new_window();
+
+  void print(FILE *stream, unsigned &total_access,
+             unsigned &total_misses) const;
+  float windowed_miss_rate() const;
+  void get_stats(unsigned &total_access, unsigned &total_misses,
+                 unsigned &total_hit_res, unsigned &total_res_fail) const;
+
+  void update_cache_parameters(cache_config &config);
+  void add_pending_line(mem_fetch *mf);
+  void remove_pending_line(mem_fetch *mf);
+  void inc_dirty() { m_dirty++; }
+
+ protected:
+  // This constructor is intended for use only from derived classes that wish to
+  // avoid unnecessary memory allocation that takes place in the
+  // other tag_array constructor
+  tag_array(cache_config &config, int core_id, int type_id,
+            cache_block_t **new_lines);
+  void init(int core_id, int type_id);
+
+ protected:
+  cache_config &m_config;
+
+  cache_block_t **m_lines; /* nbanks x nset x assoc lines in total */
+
+  unsigned m_access;
+  unsigned m_miss;
+  unsigned m_pending_hit;  // number of cache miss that hit a line that is
+                           // allocated but not filled
+  unsigned m_res_fail;
+  unsigned m_sector_miss;
+  unsigned m_dirty;
+
+  // performance counters for calculating the amount of misses within a time
+  // window
+  unsigned m_prev_snapshot_access;
+  unsigned m_prev_snapshot_miss;
+  unsigned m_prev_snapshot_pending_hit;
+
+  int m_core_id;  // which shader core is using this
+  int m_type_id;  // what kind of cache is this (normal, texture, constant)
+
+  bool is_used;  // a flag if the whole cache has ever been accessed before
+
+  typedef tr1_hash_map<new_addr_type, unsigned> line_table;
+  line_table pending_lines;
 };
 
 class mshr_table {
-public:
-    mshr_table( unsigned num_entries, unsigned max_merged)
-    : m_num_entries(num_entries),
-    m_max_merged(max_merged)
+ public:
+  mshr_table(unsigned num_entries, unsigned max_merged)
+      : m_num_entries(num_entries),
+        m_max_merged(max_merged)
 #if (tr1_hash_map_ismap == 0)
-    ,m_data(2*num_entries)
+        ,
+        m_data(2 * num_entries)
 #endif
-    {
-    }
-
-    /// Checks if there is a pending request to the lower memory level already
-    bool probe( new_addr_type block_addr ) const;
-    /// Checks if there is space for tracking a new memory access
-    bool full( new_addr_type block_addr ) const;
-    /// Add or merge this access
-    void add( new_addr_type block_addr, mem_fetch *mf );
-    /// Returns true if cannot accept new fill responses
-    bool busy() const {return false;}
-    /// Accept a new cache fill response: mark entry ready for processing
-    void mark_ready( new_addr_type block_addr, bool &has_atomic );
-    /// Returns true if ready accesses exist
-    bool access_ready() const {return !m_current_response.empty();}
-    /// Returns next ready access
-    mem_fetch *next_access();
-    void display( FILE *fp ) const;
-    // Returns true if there is a pending read after write
-    bool is_read_after_write_pending(new_addr_type block_addr);
-
-    void check_mshr_parameters( unsigned num_entries, unsigned max_merged )
-    {
-    	assert(m_num_entries==num_entries && "Change of MSHR parameters between kernels is not allowed");
-    	assert(m_max_merged==max_merged && "Change of MSHR parameters between kernels is not allowed");
-    }
-
-private:
-
-    // finite sized, fully associative table, with a finite maximum number of merged requests
-    const unsigned m_num_entries;
-    const unsigned m_max_merged;
-
-    struct mshr_entry {
-        std::list<mem_fetch*> m_list;
-        bool m_has_atomic; 
-        mshr_entry() : m_has_atomic(false) { }
-    }; 
-    typedef tr1_hash_map<new_addr_type,mshr_entry> table;
-    typedef tr1_hash_map<new_addr_type,mshr_entry> line_table;
-    table m_data;
-    line_table pending_lines;
-
-    // it may take several cycles to process the merged requests
-    bool m_current_response_ready;
-    std::list<new_addr_type> m_current_response;
+  {
+  }
+
+  /// Checks if there is a pending request to the lower memory level already
+  bool probe(new_addr_type block_addr) const;
+  /// Checks if there is space for tracking a new memory access
+  bool full(new_addr_type block_addr) const;
+  /// Add or merge this access
+  void add(new_addr_type block_addr, mem_fetch *mf);
+  /// Returns true if cannot accept new fill responses
+  bool busy() const { return false; }
+  /// Accept a new cache fill response: mark entry ready for processing
+  void mark_ready(new_addr_type block_addr, bool &has_atomic);
+  /// Returns true if ready accesses exist
+  bool access_ready() const { return !m_current_response.empty(); }
+  /// Returns next ready access
+  mem_fetch *next_access();
+  void display(FILE *fp) const;
+  // Returns true if there is a pending read after write
+  bool is_read_after_write_pending(new_addr_type block_addr);
+
+  void check_mshr_parameters(unsigned num_entries, unsigned max_merged) {
+    assert(m_num_entries == num_entries &&
+           "Change of MSHR parameters between kernels is not allowed");
+    assert(m_max_merged == max_merged &&
+           "Change of MSHR parameters between kernels is not allowed");
+  }
+
+ private:
+  // finite sized, fully associative table, with a finite maximum number of
+  // merged requests
+  const unsigned m_num_entries;
+  const unsigned m_max_merged;
+
+  struct mshr_entry {
+    std::list<mem_fetch *> m_list;
+    bool m_has_atomic;
+    mshr_entry() : m_has_atomic(false) {}
+  };
+  typedef tr1_hash_map<new_addr_type, mshr_entry> table;
+  typedef tr1_hash_map<new_addr_type, mshr_entry> line_table;
+  table m_data;
+  line_table pending_lines;
+
+  // it may take several cycles to process the merged requests
+  bool m_current_response_ready;
+  std::list<new_addr_type> m_current_response;
 };
 
-
-/***************************************************************** Caches *****************************************************************/
+/***************************************************************** Caches
+ * *****************************************************************/
 ///
-/// Simple struct to maintain cache accesses, misses, pending hits, and reservation fails.
+/// Simple struct to maintain cache accesses, misses, pending hits, and
+/// reservation fails.
 ///
-struct cache_sub_stats{
-    unsigned long long accesses;
-    unsigned long long misses;
-    unsigned long long pending_hits;
-    unsigned long long res_fails;
-
-    unsigned long long port_available_cycles; 
-    unsigned long long data_port_busy_cycles; 
-    unsigned long long fill_port_busy_cycles; 
-
-    cache_sub_stats(){
-        clear();
-    }
-    void clear(){
-        accesses = 0;
-        misses = 0;
-        pending_hits = 0;
-        res_fails = 0;
-        port_available_cycles = 0; 
-        data_port_busy_cycles = 0; 
-        fill_port_busy_cycles = 0; 
-    }
-    cache_sub_stats &operator+=(const cache_sub_stats &css){
-        ///
-        /// Overloading += operator to easily accumulate stats
-        ///
-        accesses += css.accesses;
-        misses += css.misses;
-        pending_hits += css.pending_hits;
-        res_fails += css.res_fails;
-        port_available_cycles += css.port_available_cycles; 
-        data_port_busy_cycles += css.data_port_busy_cycles; 
-        fill_port_busy_cycles += css.fill_port_busy_cycles; 
-        return *this;
-    }
-
-    cache_sub_stats operator+(const cache_sub_stats &cs){
-        ///
-        /// Overloading + operator to easily accumulate stats
-        ///
-        cache_sub_stats ret;
-        ret.accesses = accesses + cs.accesses;
-        ret.misses = misses + cs.misses;
-        ret.pending_hits = pending_hits + cs.pending_hits;
-        ret.res_fails = res_fails + cs.res_fails;
-        ret.port_available_cycles = port_available_cycles + cs.port_available_cycles; 
-        ret.data_port_busy_cycles = data_port_busy_cycles + cs.data_port_busy_cycles; 
-        ret.fill_port_busy_cycles = fill_port_busy_cycles + cs.fill_port_busy_cycles; 
-        return ret;
-    }
-
-    void print_port_stats(FILE *fout, const char *cache_name) const; 
+struct cache_sub_stats {
+  unsigned long long accesses;
+  unsigned long long misses;
+  unsigned long long pending_hits;
+  unsigned long long res_fails;
+
+  unsigned long long port_available_cycles;
+  unsigned long long data_port_busy_cycles;
+  unsigned long long fill_port_busy_cycles;
+
+  cache_sub_stats() { clear(); }
+  void clear() {
+    accesses = 0;
+    misses = 0;
+    pending_hits = 0;
+    res_fails = 0;
+    port_available_cycles = 0;
+    data_port_busy_cycles = 0;
+    fill_port_busy_cycles = 0;
+  }
+  cache_sub_stats &operator+=(const cache_sub_stats &css) {
+    ///
+    /// Overloading += operator to easily accumulate stats
+    ///
+    accesses += css.accesses;
+    misses += css.misses;
+    pending_hits += css.pending_hits;
+    res_fails += css.res_fails;
+    port_available_cycles += css.port_available_cycles;
+    data_port_busy_cycles += css.data_port_busy_cycles;
+    fill_port_busy_cycles += css.fill_port_busy_cycles;
+    return *this;
+  }
+
+  cache_sub_stats operator+(const cache_sub_stats &cs) {
+    ///
+    /// Overloading + operator to easily accumulate stats
+    ///
+    cache_sub_stats ret;
+    ret.accesses = accesses + cs.accesses;
+    ret.misses = misses + cs.misses;
+    ret.pending_hits = pending_hits + cs.pending_hits;
+    ret.res_fails = res_fails + cs.res_fails;
+    ret.port_available_cycles =
+        port_available_cycles + cs.port_available_cycles;
+    ret.data_port_busy_cycles =
+        data_port_busy_cycles + cs.data_port_busy_cycles;
+    ret.fill_port_busy_cycles =
+        fill_port_busy_cycles + cs.fill_port_busy_cycles;
+    return ret;
+  }
+
+  void print_port_stats(FILE *fout, const char *cache_name) const;
 };
 
-
 // Used for collecting AerialVision per-window statistics
-struct cache_sub_stats_pw{
-    unsigned accesses;
-    unsigned write_misses;
-    unsigned write_hits;
-    unsigned write_pending_hits;
-    unsigned write_res_fails;
-
-    unsigned read_misses;
-    unsigned read_hits;
-    unsigned read_pending_hits;
-    unsigned read_res_fails;
-
-    cache_sub_stats_pw(){
-        clear();
-    }
-    void clear(){
-        accesses = 0;
-        write_misses = 0;
-        write_hits = 0;
-        write_pending_hits = 0;
-        write_res_fails = 0;
-        read_misses = 0;
-        read_hits = 0;
-        read_pending_hits = 0;
-        read_res_fails = 0;
-    }
-    cache_sub_stats_pw &operator+=(const cache_sub_stats_pw &css){
-        ///
-        /// Overloading += operator to easily accumulate stats
-        ///
-        accesses += css.accesses;
-        write_misses += css.write_misses;
-        read_misses += css.read_misses;
-        write_pending_hits += css.write_pending_hits;
-        read_pending_hits += css.read_pending_hits;
-        write_res_fails += css.write_res_fails;
-        read_res_fails += css.read_res_fails;
-        return *this;
-    }
-
-    cache_sub_stats_pw operator+(const cache_sub_stats_pw &cs){
-        ///
-        /// Overloading + operator to easily accumulate stats
-        ///
-        cache_sub_stats_pw ret;
-        ret.accesses = accesses + cs.accesses;
-        ret.write_misses = write_misses + cs.write_misses;
-        ret.read_misses = read_misses + cs.read_misses;
-        ret.write_pending_hits = write_pending_hits + cs.write_pending_hits;
-        ret.read_pending_hits = read_pending_hits + cs.read_pending_hits;
-        ret.write_res_fails = write_res_fails + cs.write_res_fails;
-        ret.read_res_fails = read_res_fails + cs.read_res_fails;
-        return ret;
-    }
-
+struct cache_sub_stats_pw {
+  unsigned accesses;
+  unsigned write_misses;
+  unsigned write_hits;
+  unsigned write_pending_hits;
+  unsigned write_res_fails;
+
+  unsigned read_misses;
+  unsigned read_hits;
+  unsigned read_pending_hits;
+  unsigned read_res_fails;
+
+  cache_sub_stats_pw() { clear(); }
+  void clear() {
+    accesses = 0;
+    write_misses = 0;
+    write_hits = 0;
+    write_pending_hits = 0;
+    write_res_fails = 0;
+    read_misses = 0;
+    read_hits = 0;
+    read_pending_hits = 0;
+    read_res_fails = 0;
+  }
+  cache_sub_stats_pw &operator+=(const cache_sub_stats_pw &css) {
+    ///
+    /// Overloading += operator to easily accumulate stats
+    ///
+    accesses += css.accesses;
+    write_misses += css.write_misses;
+    read_misses += css.read_misses;
+    write_pending_hits += css.write_pending_hits;
+    read_pending_hits += css.read_pending_hits;
+    write_res_fails += css.write_res_fails;
+    read_res_fails += css.read_res_fails;
+    return *this;
+  }
+
+  cache_sub_stats_pw operator+(const cache_sub_stats_pw &cs) {
+    ///
+    /// Overloading + operator to easily accumulate stats
+    ///
+    cache_sub_stats_pw ret;
+    ret.accesses = accesses + cs.accesses;
+    ret.write_misses = write_misses + cs.write_misses;
+    ret.read_misses = read_misses + cs.read_misses;
+    ret.write_pending_hits = write_pending_hits + cs.write_pending_hits;
+    ret.read_pending_hits = read_pending_hits + cs.read_pending_hits;
+    ret.write_res_fails = write_res_fails + cs.write_res_fails;
+    ret.read_res_fails = read_res_fails + cs.read_res_fails;
+    return ret;
+  }
 };
 
-
 ///
 /// Cache_stats
 /// Used to record statistics for each cache.
@@ -1036,480 +1183,469 @@ struct cache_sub_stats_pw{
 /// 'cache_request_status' : [mem_access_type][cache_request_status]
 ///
 class cache_stats {
-public:
-    cache_stats();
-    void clear();
-    // Clear AerialVision cache stats after each window
-    void clear_pw();
-    void inc_stats(int access_type, int access_outcome);
-    // Increment AerialVision cache stats
-    void inc_stats_pw(int access_type, int access_outcome);
-    void inc_fail_stats(int access_type, int fail_outcome);
-    enum cache_request_status select_stats_status(enum cache_request_status probe, enum cache_request_status access) const;
-    unsigned long long &operator()(int access_type, int access_outcome, bool fail_outcome);
-    unsigned long long operator()(int access_type, int access_outcome, bool fail_outcome) const;
-    cache_stats operator+(const cache_stats &cs);
-    cache_stats &operator+=(const cache_stats &cs);
-    void print_stats(FILE *fout, const char *cache_name = "Cache_stats") const;
-    void print_fail_stats(FILE *fout, const char *cache_name = "Cache_fail_stats") const;
-
-    unsigned long long get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status)  const;
-    void get_sub_stats(struct cache_sub_stats &css) const;
-
-    // Get per-window cache stats for AerialVision
-    void get_sub_stats_pw(struct cache_sub_stats_pw &css) const;
-
-    void sample_cache_port_utility(bool data_port_busy, bool fill_port_busy); 
-private:
-    bool check_valid(int type, int status) const;
-    bool check_fail_valid(int type, int fail) const;
-
-
-    std::vector< std::vector<unsigned long long> > m_stats;
-    // AerialVision cache stats (per-window)
-    std::vector< std::vector<unsigned long long> > m_stats_pw;
-    std::vector< std::vector<unsigned long long> > m_fail_stats;
-
-    unsigned long long m_cache_port_available_cycles; 
-    unsigned long long m_cache_data_port_busy_cycles; 
-    unsigned long long m_cache_fill_port_busy_cycles; 
+ public:
+  cache_stats();
+  void clear();
+  // Clear AerialVision cache stats after each window
+  void clear_pw();
+  void inc_stats(int access_type, int access_outcome);
+  // Increment AerialVision cache stats
+  void inc_stats_pw(int access_type, int access_outcome);
+  void inc_fail_stats(int access_type, int fail_outcome);
+  enum cache_request_status select_stats_status(
+      enum cache_request_status probe, enum cache_request_status access) const;
+  unsigned long long &operator()(int access_type, int access_outcome,
+                                 bool fail_outcome);
+  unsigned long long operator()(int access_type, int access_outcome,
+                                bool fail_outcome) const;
+  cache_stats operator+(const cache_stats &cs);
+  cache_stats &operator+=(const cache_stats &cs);
+  void print_stats(FILE *fout, const char *cache_name = "Cache_stats") const;
+  void print_fail_stats(FILE *fout,
+                        const char *cache_name = "Cache_fail_stats") const;
+
+  unsigned long long get_stats(enum mem_access_type *access_type,
+                               unsigned num_access_type,
+                               enum cache_request_status *access_status,
+                               unsigned num_access_status) const;
+  void get_sub_stats(struct cache_sub_stats &css) const;
+
+  // Get per-window cache stats for AerialVision
+  void get_sub_stats_pw(struct cache_sub_stats_pw &css) const;
+
+  void sample_cache_port_utility(bool data_port_busy, bool fill_port_busy);
+
+ private:
+  bool check_valid(int type, int status) const;
+  bool check_fail_valid(int type, int fail) const;
+
+  std::vector<std::vector<unsigned long long> > m_stats;
+  // AerialVision cache stats (per-window)
+  std::vector<std::vector<unsigned long long> > m_stats_pw;
+  std::vector<std::vector<unsigned long long> > m_fail_stats;
+
+  unsigned long long m_cache_port_available_cycles;
+  unsigned long long m_cache_data_port_busy_cycles;
+  unsigned long long m_cache_fill_port_busy_cycles;
 };
 
 class cache_t {
-public:
-    virtual ~cache_t() {}
-    virtual enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ) =  0;
-
-    // accessors for cache bandwidth availability 
-    virtual bool data_port_free() const = 0; 
-    virtual bool fill_port_free() const = 0; 
+ public:
+  virtual ~cache_t() {}
+  virtual enum cache_request_status access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events) = 0;
+
+  // accessors for cache bandwidth availability
+  virtual bool data_port_free() const = 0;
+  virtual bool fill_port_free() const = 0;
 };
 
-bool was_write_sent( const std::list<cache_event> &events );
-bool was_read_sent( const std::list<cache_event> &events );
-bool was_writeallocate_sent( const std::list<cache_event> &events );
+bool was_write_sent(const std::list<cache_event> &events);
+bool was_read_sent(const std::list<cache_event> &events);
+bool was_writeallocate_sent(const std::list<cache_event> &events);
 
 /// Baseline cache
 /// Implements common functions for read_only_cache and data_cache
 /// Each subclass implements its own 'access' function
 class baseline_cache : public cache_t {
-public:
-    baseline_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport,
-                     enum mem_fetch_status status )
-    : m_config(config), m_tag_array(new tag_array(config,core_id,type_id)), 
-      m_mshrs(config.m_mshr_entries,config.m_mshr_max_merge),
-      m_bandwidth_management(config) 
-    {
-        init( name, config, memport, status );
-    }
-
-    void init( const char *name,
-               const cache_config &config,
-               mem_fetch_interface *memport,
-               enum mem_fetch_status status )
-    {
-        m_name = name;
-        assert(config.m_mshr_type == ASSOC || config.m_mshr_type == SECTOR_ASSOC);
-        m_memport=memport;
-        m_miss_queue_status = status;
-    }
-
-    virtual ~baseline_cache()
-    {
-        delete m_tag_array;
-    }
-
-	void update_cache_parameters(cache_config &config)
-	{
-		m_config=config;
-		m_tag_array->update_cache_parameters(config);
-		m_mshrs.check_mshr_parameters(config.m_mshr_entries,config.m_mshr_max_merge);
-	}
-
-    virtual enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ) =  0;
-    /// Sends next request to lower level of memory
-    virtual void cycle();
-    /// Interface for response from lower memory level (model bandwidth restictions in caller)
-    virtual void fill( mem_fetch *mf, unsigned time );
-    /// Checks if mf is waiting to be filled by lower memory level
-    virtual bool waiting_for_fill( mem_fetch *mf );
-    /// Are any (accepted) accesses that had to wait for memory now ready? (does not include accesses that "HIT")
-    virtual bool access_ready() const {return m_mshrs.access_ready();}
-    /// Pop next ready access (does not include accesses that "HIT")
-    virtual mem_fetch *next_access(){return m_mshrs.next_access();}
-    // flash invalidate all entries in cache
-    virtual void flush(){m_tag_array->flush();}
-    void invalidate(){m_tag_array->invalidate();}
-    void print(FILE *fp, unsigned &accesses, unsigned &misses) const;
-    void display_state( FILE *fp ) const;
-
-    // Stat collection
-    const cache_stats &get_stats() const {
-        return m_stats;
-    }
-    unsigned get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status)  const{
-        return m_stats.get_stats(access_type, num_access_type, access_status, num_access_status);
-    }
-    void get_sub_stats(struct cache_sub_stats &css) const {
-        m_stats.get_sub_stats(css);
-    }
-    // Clear per-window stats for AerialVision support
-    void clear_pw(){
-        m_stats.clear_pw();
-    }
-    // Per-window sub stats for AerialVision support
-    void get_sub_stats_pw(struct cache_sub_stats_pw &css) const {
-        m_stats.get_sub_stats_pw(css);
-    }
-
-    // accessors for cache bandwidth availability 
-    bool data_port_free() const { return m_bandwidth_management.data_port_free(); } 
-    bool fill_port_free() const { return m_bandwidth_management.fill_port_free(); } 
-
-    // This is a gapping hole we are poking in the system to quickly handle
-    // filling the cache on cudamemcopies. We don't care about anything other than
-    // L2 state after the memcopy - so just force the tag array to act as though
-    // something is read or written without doing anything else.
-    void force_tag_access( new_addr_type addr, unsigned time, mem_access_sector_mask_t mask )
-    {
-        m_tag_array->fill( addr, time, mask );
+ public:
+  baseline_cache(const char *name, cache_config &config, int core_id,
+                 int type_id, mem_fetch_interface *memport,
+                 enum mem_fetch_status status)
+      : m_config(config),
+        m_tag_array(new tag_array(config, core_id, type_id)),
+        m_mshrs(config.m_mshr_entries, config.m_mshr_max_merge),
+        m_bandwidth_management(config) {
+    init(name, config, memport, status);
+  }
+
+  void init(const char *name, const cache_config &config,
+            mem_fetch_interface *memport, enum mem_fetch_status status) {
+    m_name = name;
+    assert(config.m_mshr_type == ASSOC || config.m_mshr_type == SECTOR_ASSOC);
+    m_memport = memport;
+    m_miss_queue_status = status;
+  }
+
+  virtual ~baseline_cache() { delete m_tag_array; }
+
+  void update_cache_parameters(cache_config &config) {
+    m_config = config;
+    m_tag_array->update_cache_parameters(config);
+    m_mshrs.check_mshr_parameters(config.m_mshr_entries,
+                                  config.m_mshr_max_merge);
+  }
+
+  virtual enum cache_request_status access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events) = 0;
+  /// Sends next request to lower level of memory
+  void cycle();
+  /// Interface for response from lower memory level (model bandwidth
+  /// restictions in caller)
+  void fill(mem_fetch *mf, unsigned time);
+  /// Checks if mf is waiting to be filled by lower memory level
+  bool waiting_for_fill(mem_fetch *mf);
+  /// Are any (accepted) accesses that had to wait for memory now ready? (does
+  /// not include accesses that "HIT")
+  bool access_ready() const { return m_mshrs.access_ready(); }
+  /// Pop next ready access (does not include accesses that "HIT")
+  mem_fetch *next_access() { return m_mshrs.next_access(); }
+  // flash invalidate all entries in cache
+  void flush() { m_tag_array->flush(); }
+  void invalidate() { m_tag_array->invalidate(); }
+  void print(FILE *fp, unsigned &accesses, unsigned &misses) const;
+  void display_state(FILE *fp) const;
+
+  // Stat collection
+  const cache_stats &get_stats() const { return m_stats; }
+  unsigned get_stats(enum mem_access_type *access_type,
+                     unsigned num_access_type,
+                     enum cache_request_status *access_status,
+                     unsigned num_access_status) const {
+    return m_stats.get_stats(access_type, num_access_type, access_status,
+                             num_access_status);
+  }
+  void get_sub_stats(struct cache_sub_stats &css) const {
+    m_stats.get_sub_stats(css);
+  }
+  // Clear per-window stats for AerialVision support
+  void clear_pw() { m_stats.clear_pw(); }
+  // Per-window sub stats for AerialVision support
+  void get_sub_stats_pw(struct cache_sub_stats_pw &css) const {
+    m_stats.get_sub_stats_pw(css);
+  }
+
+  // accessors for cache bandwidth availability
+  bool data_port_free() const {
+    return m_bandwidth_management.data_port_free();
+  }
+  bool fill_port_free() const {
+    return m_bandwidth_management.fill_port_free();
+  }
+
+  // This is a gapping hole we are poking in the system to quickly handle
+  // filling the cache on cudamemcopies. We don't care about anything other than
+  // L2 state after the memcopy - so just force the tag array to act as though
+  // something is read or written without doing anything else.
+  void force_tag_access(new_addr_type addr, unsigned time,
+                        mem_access_sector_mask_t mask) {
+    mem_access_byte_mask_t byte_mask;
+    m_tag_array->fill(addr, time, mask, byte_mask, true);
+  }
+
+ protected:
+  // Constructor that can be used by derived classes with custom tag arrays
+  baseline_cache(const char *name, cache_config &config, int core_id,
+                 int type_id, mem_fetch_interface *memport,
+                 enum mem_fetch_status status, tag_array *new_tag_array)
+      : m_config(config),
+        m_tag_array(new_tag_array),
+        m_mshrs(config.m_mshr_entries, config.m_mshr_max_merge),
+        m_bandwidth_management(config) {
+    init(name, config, memport, status);
+  }
+
+ protected:
+  std::string m_name;
+  cache_config &m_config;
+  tag_array *m_tag_array;
+  mshr_table m_mshrs;
+  std::list<mem_fetch *> m_miss_queue;
+  enum mem_fetch_status m_miss_queue_status;
+  mem_fetch_interface *m_memport;
+
+  struct extra_mf_fields {
+    extra_mf_fields() { m_valid = false; }
+    extra_mf_fields(new_addr_type a, new_addr_type ad, unsigned i, unsigned d,
+                    const cache_config &m_config) {
+      m_valid = true;
+      m_block_addr = a;
+      m_addr = ad;
+      m_cache_index = i;
+      m_data_size = d;
+      pending_read = m_config.m_mshr_type == SECTOR_ASSOC
+                         ? m_config.m_line_sz / SECTOR_SIZE
+                         : 0;
     }
+    bool m_valid;
+    new_addr_type m_block_addr;
+    new_addr_type m_addr;
+    unsigned m_cache_index;
+    unsigned m_data_size;
+    // this variable is used when a load request generates multiple load
+    // transactions For example, a read request from non-sector L1 request sends
+    // a request to sector L2
+    unsigned pending_read;
+  };
+
+  typedef std::map<mem_fetch *, extra_mf_fields> extra_mf_fields_lookup;
+
+  extra_mf_fields_lookup m_extra_mf_fields;
+
+  cache_stats m_stats;
+
+  /// Checks whether this request can be handled on this cycle. num_miss equals
+  /// max # of misses to be handled on this cycle
+  bool miss_queue_full(unsigned num_miss) {
+    return ((m_miss_queue.size() + num_miss) >= m_config.m_miss_queue_size);
+  }
+  /// Read miss handler without writeback
+  void send_read_request(new_addr_type addr, new_addr_type block_addr,
+                         unsigned cache_index, mem_fetch *mf, unsigned time,
+                         bool &do_miss, std::list<cache_event> &events,
+                         bool read_only, bool wa);
+  /// Read miss handler. Check MSHR hit or MSHR available
+  void send_read_request(new_addr_type addr, new_addr_type block_addr,
+                         unsigned cache_index, mem_fetch *mf, unsigned time,
+                         bool &do_miss, bool &wb, evicted_block_info &evicted,
+                         std::list<cache_event> &events, bool read_only,
+                         bool wa);
+
+  /// Sub-class containing all metadata for port bandwidth management
+  class bandwidth_management {
+   public:
+    bandwidth_management(cache_config &config);
+
+    /// use the data port based on the outcome and events generated by the
+    /// mem_fetch request
+    void use_data_port(mem_fetch *mf, enum cache_request_status outcome,
+                       const std::list<cache_event> &events);
+
+    /// use the fill port
+    void use_fill_port(mem_fetch *mf);
+
+    /// called every cache cycle to free up the ports
+    void replenish_port_bandwidth();
+
+    /// query for data port availability
+    bool data_port_free() const;
+    /// query for fill port availability
+    bool fill_port_free() const;
+
+   protected:
+    const cache_config &m_config;
 
-protected:
-    // Constructor that can be used by derived classes with custom tag arrays
-    baseline_cache( const char *name,
-                    cache_config &config,
-                    int core_id,
-                    int type_id,
-                    mem_fetch_interface *memport,
-                    enum mem_fetch_status status,
-                    tag_array* new_tag_array )
-    : m_config(config),
-      m_tag_array( new_tag_array ),
-      m_mshrs(config.m_mshr_entries,config.m_mshr_max_merge), 
-      m_bandwidth_management(config) 
-    {
-        init( name, config, memport, status );
-    }
+    int m_data_port_occupied_cycles;  //< Number of cycle that the data port
+                                      // remains used
+    int m_fill_port_occupied_cycles;  //< Number of cycle that the fill port
+                                      // remains used
+  };
 
-protected:
-    std::string m_name;
-    cache_config &m_config;
-    tag_array*  m_tag_array;
-	mshr_table m_mshrs;
-    std::list<mem_fetch*> m_miss_queue;
-    enum mem_fetch_status m_miss_queue_status;
-    mem_fetch_interface *m_memport;
-
-    struct extra_mf_fields {
-        extra_mf_fields()  { m_valid = false;}
-        extra_mf_fields( new_addr_type a, new_addr_type ad, unsigned i, unsigned d, const cache_config& m_config)
-        {
-            m_valid = true;
-            m_block_addr = a;
-            m_addr = ad;
-            m_cache_index = i;
-            m_data_size = d;
-        	pending_read = m_config.m_mshr_type == SECTOR_ASSOC? m_config.m_line_sz/SECTOR_SIZE : 0;
-
-        }
-        bool m_valid;
-        new_addr_type m_block_addr;
-        new_addr_type m_addr;
-        unsigned m_cache_index;
-        unsigned m_data_size;
-        //this variable is used when a load request generates multiple load transactions
-        //For example, a read request from non-sector L1 request sends a request to sector L2
-        unsigned pending_read;
-    };
-
-    typedef std::map<mem_fetch*,extra_mf_fields> extra_mf_fields_lookup;
-
-    extra_mf_fields_lookup m_extra_mf_fields;
-
-    cache_stats m_stats;
-
-    /// Checks whether this request can be handled on this cycle. num_miss equals max # of misses to be handled on this cycle
-    bool miss_queue_full(unsigned num_miss){
-    	  return ( (m_miss_queue.size()+num_miss) >= m_config.m_miss_queue_size );
-    }
-    /// Read miss handler without writeback
-    void send_read_request(new_addr_type addr, new_addr_type block_addr, unsigned cache_index, mem_fetch *mf,
-    		unsigned time, bool &do_miss, std::list<cache_event> &events, bool read_only, bool wa);
-    /// Read miss handler. Check MSHR hit or MSHR available
-    void send_read_request(new_addr_type addr, new_addr_type block_addr, unsigned cache_index, mem_fetch *mf,
-    		unsigned time, bool &do_miss, bool &wb, evicted_block_info &evicted, std::list<cache_event> &events, bool read_only, bool wa);
-
-    /// Sub-class containing all metadata for port bandwidth management 
-    class bandwidth_management 
-    {
-    public: 
-        bandwidth_management(cache_config &config); 
-
-        /// use the data port based on the outcome and events generated by the mem_fetch request 
-        void use_data_port(mem_fetch *mf, enum cache_request_status outcome, const std::list<cache_event> &events); 
-
-        /// use the fill port 
-        void use_fill_port(mem_fetch *mf); 
-
-        /// called every cache cycle to free up the ports 
-        void replenish_port_bandwidth(); 
-
-        /// query for data port availability 
-        bool data_port_free() const; 
-        /// query for fill port availability 
-        bool fill_port_free() const; 
-    protected: 
-        const cache_config &m_config; 
-
-        int m_data_port_occupied_cycles; //< Number of cycle that the data port remains used 
-        int m_fill_port_occupied_cycles; //< Number of cycle that the fill port remains used 
-    }; 
-
-    bandwidth_management m_bandwidth_management; 
+  bandwidth_management m_bandwidth_management;
 };
 
 /// Read only cache
 class read_only_cache : public baseline_cache {
-public:
-    read_only_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport, enum mem_fetch_status status )
-    : baseline_cache(name,config,core_id,type_id,memport,status){}
-
-    /// Access cache for read_only_cache: returns RESERVATION_FAIL if request could not be accepted (for any reason)
-    virtual enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events );
-
-    virtual ~read_only_cache(){}
-
-protected:
-    read_only_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport, enum mem_fetch_status status, tag_array* new_tag_array )
-    : baseline_cache(name,config,core_id,type_id,memport,status, new_tag_array){}
+ public:
+  read_only_cache(const char *name, cache_config &config, int core_id,
+                  int type_id, mem_fetch_interface *memport,
+                  enum mem_fetch_status status)
+      : baseline_cache(name, config, core_id, type_id, memport, status) {}
+
+  /// Access cache for read_only_cache: returns RESERVATION_FAIL if request
+  /// could not be accepted (for any reason)
+  virtual enum cache_request_status access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events);
+
+  virtual ~read_only_cache() {}
+
+ protected:
+  read_only_cache(const char *name, cache_config &config, int core_id,
+                  int type_id, mem_fetch_interface *memport,
+                  enum mem_fetch_status status, tag_array *new_tag_array)
+      : baseline_cache(name, config, core_id, type_id, memport, status,
+                       new_tag_array) {}
 };
 
 /// Data cache - Implements common functions for L1 and L2 data cache
 class data_cache : public baseline_cache {
-public:
-    data_cache( const char *name, cache_config &config,
-    			int core_id, int type_id, mem_fetch_interface *memport,
-                mem_fetch_allocator *mfcreator, enum mem_fetch_status status,
-                mem_access_type wr_alloc_type, mem_access_type wrbk_type )
-    			: baseline_cache(name,config,core_id,type_id,memport,status)
-    {
-        init( mfcreator );
-        m_wr_alloc_type = wr_alloc_type;
-        m_wrbk_type = wrbk_type;
+ public:
+  data_cache(const char *name, cache_config &config, int core_id, int type_id,
+             mem_fetch_interface *memport, mem_fetch_allocator *mfcreator,
+             enum mem_fetch_status status, mem_access_type wr_alloc_type,
+             mem_access_type wrbk_type, class gpgpu_sim *gpu)
+      : baseline_cache(name, config, core_id, type_id, memport, status) {
+    init(mfcreator);
+    m_wr_alloc_type = wr_alloc_type;
+    m_wrbk_type = wrbk_type;
+    m_gpu = gpu;
+  }
+
+  virtual ~data_cache() {}
+
+  virtual void init(mem_fetch_allocator *mfcreator) {
+    m_memfetch_creator = mfcreator;
+
+    // Set read hit function
+    m_rd_hit = &data_cache::rd_hit_base;
+
+    // Set read miss function
+    m_rd_miss = &data_cache::rd_miss_base;
+
+    // Set write hit function
+    switch (m_config.m_write_policy) {
+      // READ_ONLY is now a separate cache class, config is deprecated
+      case READ_ONLY:
+        assert(0 && "Error: Writable Data_cache set as READ_ONLY\n");
+        break;
+      case WRITE_BACK:
+        m_wr_hit = &data_cache::wr_hit_wb;
+        break;
+      case WRITE_THROUGH:
+        m_wr_hit = &data_cache::wr_hit_wt;
+        break;
+      case WRITE_EVICT:
+        m_wr_hit = &data_cache::wr_hit_we;
+        break;
+      case LOCAL_WB_GLOBAL_WT:
+        m_wr_hit = &data_cache::wr_hit_global_we_local_wb;
+        break;
+      default:
+        assert(0 && "Error: Must set valid cache write policy\n");
+        break;  // Need to set a write hit function
     }
 
-    virtual ~data_cache() {}
-
-    virtual void init( mem_fetch_allocator *mfcreator )
-    {
-        m_memfetch_creator=mfcreator;
-
-        // Set read hit function
-        m_rd_hit = &data_cache::rd_hit_base;
-
-        // Set read miss function
-        m_rd_miss = &data_cache::rd_miss_base;
-
-        // Set write hit function
-        switch(m_config.m_write_policy){
-        // READ_ONLY is now a separate cache class, config is deprecated
-        case READ_ONLY:
-            assert(0 && "Error: Writable Data_cache set as READ_ONLY\n");
-            break; 
-        case WRITE_BACK: m_wr_hit = &data_cache::wr_hit_wb; break;
-        case WRITE_THROUGH: m_wr_hit = &data_cache::wr_hit_wt; break;
-        case WRITE_EVICT: m_wr_hit = &data_cache::wr_hit_we; break;
-        case LOCAL_WB_GLOBAL_WT:
-            m_wr_hit = &data_cache::wr_hit_global_we_local_wb;
-            break;
-        default:
-            assert(0 && "Error: Must set valid cache write policy\n");
-            break; // Need to set a write hit function
-        }
-
-        // Set write miss function
-        switch(m_config.m_write_alloc_policy){
-        case NO_WRITE_ALLOCATE: m_wr_miss = &data_cache::wr_miss_no_wa; break;
-		case WRITE_ALLOCATE: m_wr_miss = &data_cache::wr_miss_wa_naive; break;
-		case FETCH_ON_WRITE: m_wr_miss = &data_cache::wr_miss_wa_fetch_on_write; break;
-		case LAZY_FETCH_ON_READ: m_wr_miss = &data_cache::wr_miss_wa_lazy_fetch_on_read; break;
-        default:
-            assert(0 && "Error: Must set valid cache write miss policy\n");
-            break; // Need to set a write miss function
-        }
+    // Set write miss function
+    switch (m_config.m_write_alloc_policy) {
+      case NO_WRITE_ALLOCATE:
+        m_wr_miss = &data_cache::wr_miss_no_wa;
+        break;
+      case WRITE_ALLOCATE:
+        m_wr_miss = &data_cache::wr_miss_wa_naive;
+        break;
+      case FETCH_ON_WRITE:
+        m_wr_miss = &data_cache::wr_miss_wa_fetch_on_write;
+        break;
+      case LAZY_FETCH_ON_READ:
+        m_wr_miss = &data_cache::wr_miss_wa_lazy_fetch_on_read;
+        break;
+      default:
+        assert(0 && "Error: Must set valid cache write miss policy\n");
+        break;  // Need to set a write miss function
     }
-
-    virtual enum cache_request_status access( new_addr_type addr,
-                                              mem_fetch *mf,
-                                              unsigned time,
-                                              std::list<cache_event> &events );
-protected:
-    data_cache( const char *name,
-                cache_config &config,
-                int core_id,
-                int type_id,
-                mem_fetch_interface *memport,
-                mem_fetch_allocator *mfcreator,
-                enum mem_fetch_status status,
-                tag_array* new_tag_array,
-                mem_access_type wr_alloc_type,
-                mem_access_type wrbk_type)
-    : baseline_cache(name, config, core_id, type_id, memport,status, new_tag_array)
-    {
-        init( mfcreator );
-        m_wr_alloc_type = wr_alloc_type;
-        m_wrbk_type = wrbk_type;
-    }
-
-    mem_access_type m_wr_alloc_type; // Specifies type of write allocate request (e.g., L1 or L2)
-    mem_access_type m_wrbk_type; // Specifies type of writeback request (e.g., L1 or L2)
-
-    //! A general function that takes the result of a tag_array probe
-    //  and performs the correspding functions based on the cache configuration
-    //  The access fucntion calls this function
-    enum cache_request_status
-        process_tag_probe( bool wr,
-                           enum cache_request_status status,
-                           new_addr_type addr,
-                           unsigned cache_index,
-                           mem_fetch* mf,
-                           unsigned time,
-                           std::list<cache_event>& events );
-
-protected:
-    mem_fetch_allocator *m_memfetch_creator;
-
-    // Functions for data cache access
-    /// Sends write request to lower level memory (write or writeback)
-    void send_write_request( mem_fetch *mf,
-                             cache_event request,
-                             unsigned time,
-                             std::list<cache_event> &events);
-
-    // Member Function pointers - Set by configuration options
-    // to the functions below each grouping
-    /******* Write-hit configs *******/
-    enum cache_request_status
-        (data_cache::*m_wr_hit)( new_addr_type addr,
-                                 unsigned cache_index,
-                                 mem_fetch *mf,
-                                 unsigned time,
-                                 std::list<cache_event> &events,
-                                 enum cache_request_status status );
-    /// Marks block as MODIFIED and updates block LRU
-    enum cache_request_status
-        wr_hit_wb( new_addr_type addr,
-                   unsigned cache_index,
-                   mem_fetch *mf,
-                   unsigned time,
-                   std::list<cache_event> &events,
-                   enum cache_request_status status ); // write-back
-    enum cache_request_status
-        wr_hit_wt( new_addr_type addr,
-                   unsigned cache_index,
-                   mem_fetch *mf,
-                   unsigned time,
-                   std::list<cache_event> &events,
-                   enum cache_request_status status ); // write-through
-
-    /// Marks block as INVALID and sends write request to lower level memory
-    enum cache_request_status
-        wr_hit_we( new_addr_type addr,
-                   unsigned cache_index,
-                   mem_fetch *mf,
-                   unsigned time,
-                   std::list<cache_event> &events,
-                   enum cache_request_status status ); // write-evict
-    enum cache_request_status
-        wr_hit_global_we_local_wb( new_addr_type addr,
-                                   unsigned cache_index,
-                                   mem_fetch *mf,
-                                   unsigned time,
-                                   std::list<cache_event> &events,
-                                   enum cache_request_status status );
-        // global write-evict, local write-back
-
-
-    /******* Write-miss configs *******/
-    enum cache_request_status
-        (data_cache::*m_wr_miss)( new_addr_type addr,
-                                  unsigned cache_index,
-                                  mem_fetch *mf,
-                                  unsigned time,
-                                  std::list<cache_event> &events,
-                                  enum cache_request_status status );
-    /// Sends read request, and possible write-back request,
-    //  to lower level memory for a write miss with write-allocate
-    enum cache_request_status
-    	wr_miss_wa_naive( new_addr_type addr,
-                        unsigned cache_index,
-                        mem_fetch *mf,
-                        unsigned time,
-                        std::list<cache_event> &events,
-                        enum cache_request_status status ); // write-allocate-send-write-and-read-request
-	enum cache_request_status
-			   wr_miss_wa_fetch_on_write( new_addr_type addr,
-							unsigned cache_index,
-							mem_fetch *mf,
-							unsigned time,
-							std::list<cache_event> &events,
-							enum cache_request_status status ); // write-allocate with fetch-on-every-write
-	enum cache_request_status
-				   wr_miss_wa_lazy_fetch_on_read( new_addr_type addr,
-								unsigned cache_index,
-								mem_fetch *mf,
-								unsigned time,
-								std::list<cache_event> &events,
-								enum cache_request_status status ); // write-allocate with read-fetch-only
-	enum cache_request_status
-				wr_miss_wa_write_validate( new_addr_type addr,
-							unsigned cache_index,
-							mem_fetch *mf,
-							unsigned time,
-							std::list<cache_event> &events,
-							enum cache_request_status status ); // write-allocate that writes with no read fetch
-    enum cache_request_status
-        wr_miss_no_wa( new_addr_type addr,
-                       unsigned cache_index,
-                       mem_fetch *mf,
-                       unsigned time,
-                       std::list<cache_event> &events,
-                       enum cache_request_status status ); // no write-allocate
-
-    // Currently no separate functions for reads
-    /******* Read-hit configs *******/
-    enum cache_request_status
-        (data_cache::*m_rd_hit)( new_addr_type addr,
-                                 unsigned cache_index,
-                                 mem_fetch *mf,
-                                 unsigned time,
-                                 std::list<cache_event> &events,
-                                 enum cache_request_status status );
-    enum cache_request_status
-        rd_hit_base( new_addr_type addr,
-                     unsigned cache_index,
-                     mem_fetch *mf,
-                     unsigned time,
-                     std::list<cache_event> &events,
-                     enum cache_request_status status );
-
-    /******* Read-miss configs *******/
-    enum cache_request_status
-        (data_cache::*m_rd_miss)( new_addr_type addr,
-                                  unsigned cache_index,
-                                  mem_fetch *mf,
-                                  unsigned time,
-                                  std::list<cache_event> &events,
-                                  enum cache_request_status status );
-    enum cache_request_status
-        rd_miss_base( new_addr_type addr,
-                      unsigned cache_index,
-                      mem_fetch*mf,
-                      unsigned time,
-                      std::list<cache_event> &events,
-                      enum cache_request_status status );
-
+  }
+
+  virtual enum cache_request_status access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events);
+
+ protected:
+  data_cache(const char *name, cache_config &config, int core_id, int type_id,
+             mem_fetch_interface *memport, mem_fetch_allocator *mfcreator,
+             enum mem_fetch_status status, tag_array *new_tag_array,
+             mem_access_type wr_alloc_type, mem_access_type wrbk_type,
+             class gpgpu_sim *gpu)
+      : baseline_cache(name, config, core_id, type_id, memport, status,
+                       new_tag_array) {
+    init(mfcreator);
+    m_wr_alloc_type = wr_alloc_type;
+    m_wrbk_type = wrbk_type;
+    m_gpu = gpu;
+  }
+
+  mem_access_type m_wr_alloc_type;  // Specifies type of write allocate request
+                                    // (e.g., L1 or L2)
+  mem_access_type
+      m_wrbk_type;  // Specifies type of writeback request (e.g., L1 or L2)
+  class gpgpu_sim *m_gpu;
+
+  //! A general function that takes the result of a tag_array probe
+  //  and performs the correspding functions based on the cache configuration
+  //  The access fucntion calls this function
+  enum cache_request_status process_tag_probe(bool wr,
+                                              enum cache_request_status status,
+                                              new_addr_type addr,
+                                              unsigned cache_index,
+                                              mem_fetch *mf, unsigned time,
+                                              std::list<cache_event> &events);
+
+ protected:
+  mem_fetch_allocator *m_memfetch_creator;
+
+  // Functions for data cache access
+  /// Sends write request to lower level memory (write or writeback)
+  void send_write_request(mem_fetch *mf, cache_event request, unsigned time,
+                          std::list<cache_event> &events);
+  void update_m_readable(mem_fetch *mf, unsigned cache_index);
+  // Member Function pointers - Set by configuration options
+  // to the functions below each grouping
+  /******* Write-hit configs *******/
+  enum cache_request_status (data_cache::*m_wr_hit)(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events, enum cache_request_status status);
+  /// Marks block as MODIFIED and updates block LRU
+  enum cache_request_status wr_hit_wb(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status status);  // write-back
+  enum cache_request_status wr_hit_wt(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status status);  // write-through
+
+  /// Marks block as INVALID and sends write request to lower level memory
+  enum cache_request_status wr_hit_we(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status status);  // write-evict
+  enum cache_request_status wr_hit_global_we_local_wb(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events, enum cache_request_status status);
+  // global write-evict, local write-back
+
+  /******* Write-miss configs *******/
+  enum cache_request_status (data_cache::*m_wr_miss)(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events, enum cache_request_status status);
+  /// Sends read request, and possible write-back request,
+  //  to lower level memory for a write miss with write-allocate
+  enum cache_request_status wr_miss_wa_naive(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status
+          status);  // write-allocate-send-write-and-read-request
+  enum cache_request_status wr_miss_wa_fetch_on_write(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status
+          status);  // write-allocate with fetch-on-every-write
+  enum cache_request_status wr_miss_wa_lazy_fetch_on_read(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status status);  // write-allocate with read-fetch-only
+  enum cache_request_status wr_miss_wa_write_validate(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status
+          status);  // write-allocate that writes with no read fetch
+  enum cache_request_status wr_miss_no_wa(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events,
+      enum cache_request_status status);  // no write-allocate
+
+  // Currently no separate functions for reads
+  /******* Read-hit configs *******/
+  enum cache_request_status (data_cache::*m_rd_hit)(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events, enum cache_request_status status);
+  enum cache_request_status rd_hit_base(new_addr_type addr,
+                                        unsigned cache_index, mem_fetch *mf,
+                                        unsigned time,
+                                        std::list<cache_event> &events,
+                                        enum cache_request_status status);
+
+  /******* Read-miss configs *******/
+  enum cache_request_status (data_cache::*m_rd_miss)(
+      new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time,
+      std::list<cache_event> &events, enum cache_request_status status);
+  enum cache_request_status rd_miss_base(new_addr_type addr,
+                                         unsigned cache_index, mem_fetch *mf,
+                                         unsigned time,
+                                         std::list<cache_event> &events,
+                                         enum cache_request_status status);
 };
 
 /// This is meant to model the first level data cache in Fermi.
@@ -1517,242 +1653,242 @@ protected:
 /// the granularity of individual blocks
 /// (the policy used in fermi according to the CUDA manual)
 class l1_cache : public data_cache {
-public:
-    l1_cache(const char *name, cache_config &config,
-            int core_id, int type_id, mem_fetch_interface *memport,
-            mem_fetch_allocator *mfcreator, enum mem_fetch_status status )
-            : data_cache(name,config,core_id,type_id,memport,mfcreator,status, L1_WR_ALLOC_R, L1_WRBK_ACC){}
-
-    virtual ~l1_cache(){}
-
-    virtual enum cache_request_status
-        access( new_addr_type addr,
-                mem_fetch *mf,
-                unsigned time,
-                std::list<cache_event> &events );
-
-protected:
-    l1_cache( const char *name,
-              cache_config &config,
-              int core_id,
-              int type_id,
-              mem_fetch_interface *memport,
-              mem_fetch_allocator *mfcreator,
-              enum mem_fetch_status status,
-              tag_array* new_tag_array )
-    : data_cache( name,
-                  config,
-                  core_id,type_id,memport,mfcreator,status, new_tag_array, L1_WR_ALLOC_R, L1_WRBK_ACC ){}
-
+ public:
+  l1_cache(const char *name, cache_config &config, int core_id, int type_id,
+           mem_fetch_interface *memport, mem_fetch_allocator *mfcreator,
+           enum mem_fetch_status status, class gpgpu_sim *gpu)
+      : data_cache(name, config, core_id, type_id, memport, mfcreator, status,
+                   L1_WR_ALLOC_R, L1_WRBK_ACC, gpu) {}
+
+  virtual ~l1_cache() {}
+
+  virtual enum cache_request_status access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events);
+
+ protected:
+  l1_cache(const char *name, cache_config &config, int core_id, int type_id,
+           mem_fetch_interface *memport, mem_fetch_allocator *mfcreator,
+           enum mem_fetch_status status, tag_array *new_tag_array,
+           class gpgpu_sim *gpu)
+      : data_cache(name, config, core_id, type_id, memport, mfcreator, status,
+                   new_tag_array, L1_WR_ALLOC_R, L1_WRBK_ACC, gpu) {}
 };
 
 /// Models second level shared cache with global write-back
 /// and write-allocate policies
 class l2_cache : public data_cache {
-public:
-    l2_cache(const char *name,  cache_config &config,
-            int core_id, int type_id, mem_fetch_interface *memport,
-            mem_fetch_allocator *mfcreator, enum mem_fetch_status status )
-            : data_cache(name,config,core_id,type_id,memport,mfcreator,status, L2_WR_ALLOC_R, L2_WRBK_ACC){}
-
-    virtual ~l2_cache() {}
-
-    virtual enum cache_request_status
-        access( new_addr_type addr,
-                mem_fetch *mf,
-                unsigned time,
-                std::list<cache_event> &events );
+ public:
+  l2_cache(const char *name, cache_config &config, int core_id, int type_id,
+           mem_fetch_interface *memport, mem_fetch_allocator *mfcreator,
+           enum mem_fetch_status status, class gpgpu_sim *gpu)
+      : data_cache(name, config, core_id, type_id, memport, mfcreator, status,
+                   L2_WR_ALLOC_R, L2_WRBK_ACC, gpu) {}
+
+  virtual ~l2_cache() {}
+
+  virtual enum cache_request_status access(new_addr_type addr, mem_fetch *mf,
+                                           unsigned time,
+                                           std::list<cache_event> &events);
 };
 
 /*****************************************************************************/
 
 // See the following paper to understand this cache model:
-// 
-// Igehy, et al., Prefetching in a Texture Cache Architecture, 
+//
+// Igehy, et al., Prefetching in a Texture Cache Architecture,
 // Proceedings of the 1998 Eurographics/SIGGRAPH Workshop on Graphics Hardware
 // http://www-graphics.stanford.edu/papers/texture_prefetch/
 class tex_cache : public cache_t {
-public:
-    tex_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport,
-               enum mem_fetch_status request_status, 
-               enum mem_fetch_status rob_status )
-    : m_config(config), 
-    m_tags(config,core_id,type_id), 
-    m_fragment_fifo(config.m_fragment_fifo_entries), 
-    m_request_fifo(config.m_request_fifo_entries),
-    m_rob(config.m_rob_entries),
-    m_result_fifo(config.m_result_fifo_entries)
-    {
-        m_name = name;
-        assert(config.m_mshr_type == TEX_FIFO || config.m_mshr_type == SECTOR_TEX_FIFO );
-        assert(config.m_write_policy == READ_ONLY);
-        assert(config.m_alloc_policy == ON_MISS);
-        m_memport=memport;
-        m_cache = new data_block[ config.get_num_lines() ];
-        m_request_queue_status = request_status;
-        m_rob_status = rob_status;
+ public:
+  tex_cache(const char *name, cache_config &config, int core_id, int type_id,
+            mem_fetch_interface *memport, enum mem_fetch_status request_status,
+            enum mem_fetch_status rob_status)
+      : m_config(config),
+        m_tags(config, core_id, type_id),
+        m_fragment_fifo(config.m_fragment_fifo_entries),
+        m_request_fifo(config.m_request_fifo_entries),
+        m_rob(config.m_rob_entries),
+        m_result_fifo(config.m_result_fifo_entries) {
+    m_name = name;
+    assert(config.m_mshr_type == TEX_FIFO ||
+           config.m_mshr_type == SECTOR_TEX_FIFO);
+    assert(config.m_write_policy == READ_ONLY);
+    assert(config.m_alloc_policy == ON_MISS);
+    m_memport = memport;
+    m_cache = new data_block[config.get_num_lines()];
+    m_request_queue_status = request_status;
+    m_rob_status = rob_status;
+  }
+
+  /// Access function for tex_cache
+  /// return values: RESERVATION_FAIL if request could not be accepted
+  /// otherwise returns HIT_RESERVED or MISS; NOTE: *never* returns HIT
+  /// since unlike a normal CPU cache, a "HIT" in texture cache does not
+  /// mean the data is ready (still need to get through fragment fifo)
+  enum cache_request_status access(new_addr_type addr, mem_fetch *mf,
+                                   unsigned time,
+                                   std::list<cache_event> &events);
+  void cycle();
+  /// Place returning cache block into reorder buffer
+  void fill(mem_fetch *mf, unsigned time);
+  /// Are any (accepted) accesses that had to wait for memory now ready? (does
+  /// not include accesses that "HIT")
+  bool access_ready() const { return !m_result_fifo.empty(); }
+  /// Pop next ready access (includes both accesses that "HIT" and those that
+  /// "MISS")
+  mem_fetch *next_access() { return m_result_fifo.pop(); }
+  void display_state(FILE *fp) const;
+
+  // accessors for cache bandwidth availability - stubs for now
+  bool data_port_free() const { return true; }
+  bool fill_port_free() const { return true; }
+
+  // Stat collection
+  const cache_stats &get_stats() const { return m_stats; }
+  unsigned get_stats(enum mem_access_type *access_type,
+                     unsigned num_access_type,
+                     enum cache_request_status *access_status,
+                     unsigned num_access_status) const {
+    return m_stats.get_stats(access_type, num_access_type, access_status,
+                             num_access_status);
+  }
+
+  void get_sub_stats(struct cache_sub_stats &css) const {
+    m_stats.get_sub_stats(css);
+  }
+
+ private:
+  std::string m_name;
+  const cache_config &m_config;
+
+  struct fragment_entry {
+    fragment_entry() {}
+    fragment_entry(mem_fetch *mf, unsigned idx, bool m, unsigned d) {
+      m_request = mf;
+      m_cache_index = idx;
+      m_miss = m;
+      m_data_size = d;
     }
+    mem_fetch *m_request;    // request information
+    unsigned m_cache_index;  // where to look for data
+    bool m_miss;             // true if sent memory request
+    unsigned m_data_size;
+  };
+
+  struct rob_entry {
+    rob_entry() {
+      m_ready = false;
+      m_time = 0;
+      m_request = NULL;
+    }
+    rob_entry(unsigned i, mem_fetch *mf, new_addr_type a) {
+      m_ready = false;
+      m_index = i;
+      m_time = 0;
+      m_request = mf;
+      m_block_addr = a;
+    }
+    bool m_ready;
+    unsigned m_time;   // which cycle did this entry become ready?
+    unsigned m_index;  // where in cache should block be placed?
+    mem_fetch *m_request;
+    new_addr_type m_block_addr;
+  };
+
+  struct data_block {
+    data_block() { m_valid = false; }
+    bool m_valid;
+    new_addr_type m_block_addr;
+  };
+
+  // TODO: replace fifo_pipeline with this?
+  template <class T>
+  class fifo {
+   public:
+    fifo(unsigned size) {
+      m_size = size;
+      m_num = 0;
+      m_head = 0;
+      m_tail = 0;
+      m_data = new T[size];
+    }
+    bool full() const { return m_num == m_size; }
+    bool empty() const { return m_num == 0; }
+    unsigned size() const { return m_num; }
+    unsigned capacity() const { return m_size; }
+    unsigned push(const T &e) {
+      assert(!full());
+      m_data[m_head] = e;
+      unsigned result = m_head;
+      inc_head();
+      return result;
+    }
+    T pop() {
+      assert(!empty());
+      T result = m_data[m_tail];
+      inc_tail();
+      return result;
+    }
+    const T &peek(unsigned index) const {
+      assert(index < m_size);
+      return m_data[index];
+    }
+    T &peek(unsigned index) {
+      assert(index < m_size);
+      return m_data[index];
+    }
+    T &peek() const { return m_data[m_tail]; }
+    unsigned next_pop_index() const { return m_tail; }
 
-    /// Access function for tex_cache
-    /// return values: RESERVATION_FAIL if request could not be accepted
-    /// otherwise returns HIT_RESERVED or MISS; NOTE: *never* returns HIT
-    /// since unlike a normal CPU cache, a "HIT" in texture cache does not
-    /// mean the data is ready (still need to get through fragment fifo)
-    enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events );
-    void cycle();
-    /// Place returning cache block into reorder buffer
-    void fill( mem_fetch *mf, unsigned time );
-    /// Are any (accepted) accesses that had to wait for memory now ready? (does not include accesses that "HIT")
-    bool access_ready() const{return !m_result_fifo.empty();}
-    /// Pop next ready access (includes both accesses that "HIT" and those that "MISS")
-    mem_fetch *next_access(){return m_result_fifo.pop();}
-    void display_state( FILE *fp ) const;
-
-    // accessors for cache bandwidth availability - stubs for now 
-    bool data_port_free() const { return true; }
-    bool fill_port_free() const { return true; }
-
-    // Stat collection
-    const cache_stats &get_stats() const {
-        return m_stats;
+   private:
+    void inc_head() {
+      m_head = (m_head + 1) % m_size;
+      m_num++;
     }
-    unsigned get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status) const{
-        return m_stats.get_stats(access_type, num_access_type, access_status, num_access_status);
+    void inc_tail() {
+      assert(m_num > 0);
+      m_tail = (m_tail + 1) % m_size;
+      m_num--;
     }
 
-    void get_sub_stats(struct cache_sub_stats &css) const{
-        m_stats.get_sub_stats(css);
+    unsigned m_head;  // next entry goes here
+    unsigned m_tail;  // oldest entry found here
+    unsigned m_num;   // how many in fifo?
+    unsigned m_size;  // maximum number of entries in fifo
+    T *m_data;
+  };
+
+  tag_array m_tags;
+  fifo<fragment_entry> m_fragment_fifo;
+  fifo<mem_fetch *> m_request_fifo;
+  fifo<rob_entry> m_rob;
+  data_block *m_cache;
+  fifo<mem_fetch *> m_result_fifo;  // next completed texture fetch
+
+  mem_fetch_interface *m_memport;
+  enum mem_fetch_status m_request_queue_status;
+  enum mem_fetch_status m_rob_status;
+
+  struct extra_mf_fields {
+    extra_mf_fields() { m_valid = false; }
+    extra_mf_fields(unsigned i, const cache_config &m_config) {
+      m_valid = true;
+      m_rob_index = i;
+      pending_read = m_config.m_mshr_type == SECTOR_TEX_FIFO
+                         ? m_config.m_line_sz / SECTOR_SIZE
+                         : 0;
     }
-private:
-    std::string m_name;
-    const cache_config &m_config;
+    bool m_valid;
+    unsigned m_rob_index;
+    unsigned pending_read;
+  };
+
+  cache_stats m_stats;
+
+  typedef std::map<mem_fetch *, extra_mf_fields> extra_mf_fields_lookup;
 
-    struct fragment_entry {
-        fragment_entry() {}
-        fragment_entry( mem_fetch *mf, unsigned idx, bool m, unsigned d )
-        {
-            m_request=mf;
-            m_cache_index=idx;
-            m_miss=m;
-            m_data_size=d;
-        }
-        mem_fetch *m_request;     // request information
-        unsigned   m_cache_index; // where to look for data
-        bool       m_miss;        // true if sent memory request
-        unsigned   m_data_size;
-    };
-
-    struct rob_entry {
-        rob_entry() { m_ready = false; m_time=0; m_request=NULL;}
-        rob_entry( unsigned i, mem_fetch *mf, new_addr_type a ) 
-        { 
-            m_ready=false; 
-            m_index=i;
-            m_time=0;
-            m_request=mf; 
-            m_block_addr=a;
-        }
-        bool m_ready;
-        unsigned m_time; // which cycle did this entry become ready?
-        unsigned m_index; // where in cache should block be placed?
-        mem_fetch *m_request;
-        new_addr_type m_block_addr;
-    };
-
-    struct data_block {
-        data_block() { m_valid = false;}
-        bool m_valid;
-        new_addr_type m_block_addr;
-    };
-
-    // TODO: replace fifo_pipeline with this?
-    template<class T> class fifo {
-    public:
-        fifo( unsigned size ) 
-        { 
-            m_size=size; 
-            m_num=0; 
-            m_head=0; 
-            m_tail=0; 
-            m_data = new T[size];
-        }
-        bool full() const { return m_num == m_size;}
-        bool empty() const { return m_num == 0;}
-        unsigned size() const { return m_num;}
-        unsigned capacity() const { return m_size;}
-        unsigned push( const T &e ) 
-        { 
-            assert(!full()); 
-            m_data[m_head] = e; 
-            unsigned result = m_head;
-            inc_head(); 
-            return result;
-        }
-        T pop() 
-        { 
-            assert(!empty()); 
-            T result = m_data[m_tail];
-            inc_tail();
-            return result;
-        }
-        const T &peek( unsigned index ) const 
-        { 
-            assert( index < m_size );
-            return m_data[index]; 
-        }
-        T &peek( unsigned index ) 
-        { 
-            assert( index < m_size );
-            return m_data[index]; 
-        }
-        T &peek() const
-        { 
-            return m_data[m_tail]; 
-        }
-        unsigned next_pop_index() const 
-        {
-            return m_tail;
-        }
-    private:
-        void inc_head() { m_head = (m_head+1)%m_size; m_num++;}
-        void inc_tail() { assert(m_num>0); m_tail = (m_tail+1)%m_size; m_num--;}
-
-        unsigned   m_head; // next entry goes here
-        unsigned   m_tail; // oldest entry found here
-        unsigned   m_num;  // how many in fifo?
-        unsigned   m_size; // maximum number of entries in fifo
-        T         *m_data;
-    };
-
-    tag_array               m_tags;
-    fifo<fragment_entry>    m_fragment_fifo;
-    fifo<mem_fetch*>        m_request_fifo;
-    fifo<rob_entry>         m_rob;
-    data_block             *m_cache;
-    fifo<mem_fetch*>        m_result_fifo; // next completed texture fetch
-
-    mem_fetch_interface    *m_memport;
-    enum mem_fetch_status   m_request_queue_status;
-    enum mem_fetch_status   m_rob_status;
-
-    struct extra_mf_fields {
-        extra_mf_fields()  { m_valid = false;}
-        extra_mf_fields( unsigned i, const cache_config &m_config )
-        {
-            m_valid = true;
-            m_rob_index = i;
-            pending_read = m_config.m_mshr_type == SECTOR_TEX_FIFO? m_config.m_line_sz/SECTOR_SIZE : 0;
-        }
-        bool m_valid;
-        unsigned m_rob_index;
-        unsigned pending_read;
-    };
-
-    cache_stats m_stats;
-
-    typedef std::map<mem_fetch*,extra_mf_fields> extra_mf_fields_lookup;
-
-    extra_mf_fields_lookup m_extra_mf_fields;
+  extra_mf_fields_lookup m_extra_mf_fields;
 };
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache_gem5.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache_gem5.cc
index b1ac4e72ca..eb1b76f03f 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache_gem5.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-cache_gem5.cc
@@ -18,7 +18,8 @@ l1icache_gem5::access(new_addr_type addr, mem_fetch *mf, unsigned time,
     assert(!mf->get_is_write());
     new_addr_type block_addr = m_config.block_addr(addr);
     unsigned cache_index = (unsigned)-1;
-    enum cache_request_status status = m_tag_array->probe(block_addr,cache_index, mf);
+    // TODO schi , not write?
+    enum cache_request_status status = m_tag_array->probe(block_addr,cache_index, mf, false);
     if ( status == HIT ) {
         m_tag_array->access(block_addr,time,cache_index, mf); // update LRU state
         return HIT;
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.cc
index 0b4936927d..4d20153dec 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.cc
@@ -31,47 +31,47 @@
 
 #include "gpu-sim.h"
 
-#include <stdio.h>
-#include <stdlib.h>
 #include <math.h>
 #include <signal.h>
+#include <stdio.h>
+#include <stdlib.h>
 #include "zlib.h"
 
-
-#include "shader.h"
-#include "shader_trace.h"
 #include "dram.h"
 #include "mem_fetch.h"
+#include "shader.h"
+#include "shader_trace.h"
 
 #include <time.h>
+#include "addrdec.h"
+#include "delayqueue.h"
+#include "dram.h"
 #include "gpu-cache.h"
 #include "gpu-misc.h"
-#include "delayqueue.h"
-#include "shader.h"
 #include "icnt_wrapper.h"
-#include "dram.h"
-#include "addrdec.h"
-#include "stat-tool.h"
 #include "l2cache.h"
+#include "shader.h"
+#include "stat-tool.h"
 
+#include "../../libcuda_sim/gpgpu_context.h"
+#include "../abstract_hardware_model.h"
+#include "../cuda-sim/cuda-sim.h"
+#include "../cuda-sim/cuda_device_runtime.h"
 #include "../cuda-sim/ptx-stats.h"
 #include "../statwrapper.h"
-#include "../abstract_hardware_model.h"
 #include "../debug.h"
 #include "../gpgpusim_entrypoint.h"
-#include "../cuda-sim/cuda-sim.h"
 #include "../cuda-sim/ptx_ir.h"
 #include "../trace.h"
 #include "mem_latency_stat.h"
 #include "power_stat.h"
-#include "visualizer.h"
 #include "stats.h"
-#include "../cuda-sim/cuda_device_runtime.h"
+#include "visualizer.h"
 
 #ifdef GPGPUSIM_POWER_MODEL
 #include "power_interface.h"
 #else
-class  gpgpu_sim_wrapper {};
+class gpgpu_sim_wrapper {};
 #endif
 
 #include <stdio.h>
@@ -80,1613 +80,2034 @@ class  gpgpu_sim_wrapper {};
 #include <sstream>
 #include <string>
 
-#define MAX(a,b) (((a)>(b))?(a):(b))
-
-
-bool g_interactive_debugger_enabled=false;
-
-unsigned long long  gpu_sim_cycle = 0;
-unsigned long long  gpu_tot_sim_cycle = 0;
+#define MAX(a, b) (((a) > (b)) ? (a) : (b))
 
+bool g_interactive_debugger_enabled = false;
 
-// performance counter for stalls due to congestion.
-unsigned int gpu_stall_dramfull = 0; 
-unsigned int gpu_stall_icnt2sh = 0;
-unsigned long long partiton_reqs_in_parallel = 0;
-unsigned long long partiton_reqs_in_parallel_total = 0;
-unsigned long long partiton_reqs_in_parallel_util = 0;
-unsigned long long partiton_reqs_in_parallel_util_total = 0;
-unsigned long long  gpu_sim_cycle_parition_util = 0;
-unsigned long long  gpu_tot_sim_cycle_parition_util = 0;
-unsigned long long partiton_replys_in_parallel = 0;
-unsigned long long partiton_replys_in_parallel_total = 0;
-
-tr1_hash_map<new_addr_type,unsigned> address_random_interleaving;
+tr1_hash_map<new_addr_type, unsigned> address_random_interleaving;
 
 /* Clock Domains */
 
-#define  CORE  0x01
-#define  L2    0x02
-#define  DRAM  0x04
-#define  ICNT  0x08  
-
+#define CORE 0x01
+#define L2 0x02
+#define DRAM 0x04
+#define ICNT 0x08
 
 #define MEM_LATENCY_STAT_IMPL
 
-
-
-
 #include "mem_latency_stat.h"
 
-void power_config::reg_options(class OptionParser * opp)
-{
-
-
-	  option_parser_register(opp, "-gpuwattch_xml_file", OPT_CSTR,
-			  	  	  	  	 &g_power_config_name,"GPUWattch XML file",
-	                   "gpuwattch.xml");
-
-	   option_parser_register(opp, "-power_simulation_enabled", OPT_BOOL,
-	                          &g_power_simulation_enabled, "Turn on power simulator (1=On, 0=Off)",
-	                          "0");
-
-	   option_parser_register(opp, "-power_per_cycle_dump", OPT_BOOL,
-	                          &g_power_per_cycle_dump, "Dump detailed power output each cycle",
-	                          "0");
-
-	   // Output Data Formats
-	   option_parser_register(opp, "-power_trace_enabled", OPT_BOOL,
-	                          &g_power_trace_enabled, "produce a file for the power trace (1=On, 0=Off)",
-	                          "0");
-
-	   option_parser_register(opp, "-power_trace_zlevel", OPT_INT32,
-	                          &g_power_trace_zlevel, "Compression level of the power trace output log (0=no comp, 9=highest)",
-	                          "6");
-
-	   option_parser_register(opp, "-steady_power_levels_enabled", OPT_BOOL,
-	                          &g_steady_power_levels_enabled, "produce a file for the steady power levels (1=On, 0=Off)",
-	                          "0");
-
-	   option_parser_register(opp, "-steady_state_definition", OPT_CSTR,
-			   	  &gpu_steady_state_definition, "allowed deviation:number of samples",
-	                 	  "8:4");
 
-}
-
-void memory_config::reg_options(class OptionParser * opp)
-{
-    option_parser_register(opp, "-perf_sim_memcpy", OPT_BOOL, &m_perf_sim_memcpy, 
-                                "Fill the L2 cache on memcpy", "1");
-    option_parser_register(opp, "-gpgpu_dram_scheduler", OPT_INT32, &scheduler_type, 
-                                "0 = fifo, 1 = FR-FCFS (defaul)", "1");
-    option_parser_register(opp, "-gpgpu_dram_partition_queues", OPT_CSTR, &gpgpu_L2_queue_config, 
-                           "i2$:$2d:d2$:$2i",
-                           "8:8:8:8");
-
-    option_parser_register(opp, "-l2_ideal", OPT_BOOL, &l2_ideal, 
-                           "Use a ideal L2 cache that always hit",
-                           "0");
-    option_parser_register(opp, "-gpgpu_cache:dl2", OPT_CSTR, &m_L2_config.m_config_string, 
-                   "unified banked L2 data cache config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}",
-                   "64:128:8,L:B:m:N,A:16:4,4");
-    option_parser_register(opp, "-gpgpu_cache:dl2_texture_only", OPT_BOOL, &m_L2_texure_only, 
-                           "L2 cache used for texture only",
-                           "1");
-    option_parser_register(opp, "-gpgpu_n_mem", OPT_UINT32, &m_n_mem, 
-                 "number of memory modules (e.g. memory controllers) in gpu",
-                 "8");
-    option_parser_register(opp, "-gpgpu_n_sub_partition_per_mchannel", OPT_UINT32, &m_n_sub_partition_per_memory_channel, 
-                 "number of memory subpartition in each memory module",
-                 "1");
-    option_parser_register(opp, "-gpgpu_n_mem_per_ctrlr", OPT_UINT32, &gpu_n_mem_per_ctrlr, 
-                 "number of memory chips per memory controller",
-                 "1");
-    option_parser_register(opp, "-gpgpu_memlatency_stat", OPT_INT32, &gpgpu_memlatency_stat, 
-                "track and display latency statistics 0x2 enables MC, 0x4 enables queue logs",
-                "0");
-    option_parser_register(opp, "-gpgpu_frfcfs_dram_sched_queue_size", OPT_INT32, &gpgpu_frfcfs_dram_sched_queue_size, 
-                "0 = unlimited (default); # entries per chip",
-                "0");
-    option_parser_register(opp, "-gpgpu_dram_return_queue_size", OPT_INT32, &gpgpu_dram_return_queue_size, 
-                "0 = unlimited (default); # entries per chip",
-                "0");
-    option_parser_register(opp, "-gpgpu_dram_buswidth", OPT_UINT32, &busW, 
-                 "default = 4 bytes (8 bytes per cycle at DDR)",
-                 "4");
-    option_parser_register(opp, "-gpgpu_dram_burst_length", OPT_UINT32, &BL, 
-                 "Burst length of each DRAM request (default = 4 data bus cycle)",
-                 "4");
-    option_parser_register(opp, "-dram_data_command_freq_ratio", OPT_UINT32, &data_command_freq_ratio, 
-                 "Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)",
-                 "2");
-    option_parser_register(opp, "-gpgpu_dram_timing_opt", OPT_CSTR, &gpgpu_dram_timing_opt, 
-                "DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}",
-                "4:2:8:12:21:13:34:9:4:5:13:1:0:0");
-    option_parser_register(opp, "-rop_latency", OPT_UINT32, &rop_latency,
-                     "ROP queue latency (default 85)",
-                     "85");
-    option_parser_register(opp, "-dram_latency", OPT_UINT32, &dram_latency,
-                     "DRAM latency (default 30)",
-                     "30");
-    option_parser_register(opp, "-dual_bus_interface", OPT_UINT32, &dual_bus_interface,
-                                        "dual_bus_interface (default = 0) ",
-                                        "0");
-    option_parser_register(opp, "-dram_bnk_indexing_policy", OPT_UINT32, &dram_bnk_indexing_policy,
-                                            "dram_bnk_indexing_policy (0 = normal indexing, 1 = Xoring with the higher bits) (Default = 0)",
-                                            "0");
-    option_parser_register(opp, "-dram_bnkgrp_indexing_policy", OPT_UINT32, &dram_bnkgrp_indexing_policy,
-                                            "dram_bnkgrp_indexing_policy (0 = take higher bits, 1 = take lower bits) (Default = 0)",
-                                            "0");
-    option_parser_register(opp, "-Seperate_Write_Queue_Enable", OPT_BOOL, &seperate_write_queue_enabled,
-                           "Seperate_Write_Queue_Enable",
-                           "0");
-    option_parser_register(opp, "-Write_Queue_Size", OPT_CSTR, &write_queue_size_opt,
-                                  "Write_Queue_Size",
-                                  "32:28:16");
-    option_parser_register(opp, "-Elimnate_rw_turnaround", OPT_BOOL, &elimnate_rw_turnaround,
-                               "elimnate_rw_turnaround i.e set tWTR and tRTW = 0",
-                               "0");
-    option_parser_register(opp, "-icnt_flit_size", OPT_UINT32, &icnt_flit_size,
-                               "icnt_flit_size",
-                               "32");
-    m_address_mapping.addrdec_setoption(opp);
-}
-
-void shader_core_config::reg_options(class OptionParser * opp)
-{
-    option_parser_register(opp, "-gpgpu_simd_model", OPT_INT32, &model, 
-                   "1 = post-dominator", "1");
-    option_parser_register(opp, "-gpgpu_shader_core_pipeline", OPT_CSTR, &gpgpu_shader_core_pipeline_opt, 
-                   "shader core pipeline config, i.e., {<nthread>:<warpsize>}",
-                   "1024:32");
-    option_parser_register(opp, "-gpgpu_tex_cache:l1", OPT_CSTR, &m_L1T_config.m_config_string, 
-                   "per-shader L1 texture cache  (READ-ONLY) config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}",
-                   "8:128:5,L:R:m:N,F:128:4,128:2");
-    option_parser_register(opp, "-gpgpu_const_cache:l1", OPT_CSTR, &m_L1C_config.m_config_string, 
-                   "per-shader L1 constant memory cache  (READ-ONLY) config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} ",
-                   "64:64:2,L:R:f:N,A:2:32,4" );
-    option_parser_register(opp, "-gpgpu_cache:il1", OPT_CSTR, &m_L1I_config.m_config_string, 
-                   "shader L1 instruction cache config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} ",
-                   "4:256:4,L:R:f:N,A:2:32,4" );
-    option_parser_register(opp, "-gpgpu_cache:dl1", OPT_CSTR, &m_L1D_config.m_config_string,
-                   "per-shader L1 data cache config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
-                   "none" );
-    option_parser_register(opp, "-l1_latency", OPT_UINT32, &m_L1D_config.l1_latency,
-                 "L1 Hit Latency",
-                 "0");
-    option_parser_register(opp, "-smem_latency", OPT_UINT32, &smem_latency,
-                 "smem Latency",
-                 "3");
-    option_parser_register(opp, "-gpgpu_cache:dl1PrefL1", OPT_CSTR, &m_L1D_config.m_config_stringPrefL1,
-                   "per-shader L1 data cache config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
-                   "none" );
-    option_parser_register(opp, "-gpgpu_cache:dl1PrefShared", OPT_CSTR, &m_L1D_config.m_config_stringPrefShared,
-                   "per-shader L1 data cache config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
-                   "none" );
-    option_parser_register(opp, "-gmem_skip_L1D", OPT_BOOL, &gmem_skip_L1D, 
-                   "global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)",
-                   "0");
-
-    option_parser_register(opp, "-gpgpu_perfect_mem", OPT_BOOL, &gpgpu_perfect_mem, 
-                 "enable perfect memory mode (no cache miss)",
-                 "0");
-    option_parser_register(opp, "-n_regfile_gating_group", OPT_UINT32, &n_regfile_gating_group,
-                 "group of lanes that should be read/written together)",
-                 "4");
-    option_parser_register(opp, "-gpgpu_clock_gated_reg_file", OPT_BOOL, &gpgpu_clock_gated_reg_file,
-                 "enable clock gated reg file for power calculations",
-                 "0");
-    option_parser_register(opp, "-gpgpu_clock_gated_lanes", OPT_BOOL, &gpgpu_clock_gated_lanes,
-                 "enable clock gated lanes for power calculations",
-                 "0");
-    option_parser_register(opp, "-gpgpu_shader_registers", OPT_UINT32, &gpgpu_shader_registers, 
-                 "Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)",
-                 "8192");
-    option_parser_register(opp, "-gpgpu_registers_per_block", OPT_UINT32, &gpgpu_registers_per_block,
-                 "Maximum number of registers per CTA. (default 8192)",
-                 "8192");
-    option_parser_register(opp, "-gpgpu_ignore_resources_limitation", OPT_BOOL, &gpgpu_ignore_resources_limitation,
-                 "gpgpu_ignore_resources_limitation (default 0)",
-                 "0");
-    option_parser_register(opp, "-gpgpu_shader_cta", OPT_UINT32, &max_cta_per_core, 
-                 "Maximum number of concurrent CTAs in shader (default 8)",
-                 "8");
-    option_parser_register(opp, "-gpgpu_num_cta_barriers", OPT_UINT32, &max_barriers_per_cta,
-                 "Maximum number of named barriers per CTA (default 16)",
-                 "16");
-    option_parser_register(opp, "-gpgpu_n_clusters", OPT_UINT32, &n_simt_clusters, 
-                 "number of processing clusters",
-                 "10");
-    option_parser_register(opp, "-gpgpu_n_cores_per_cluster", OPT_UINT32, &n_simt_cores_per_cluster, 
-                 "number of simd cores per cluster",
-                 "3");
-    option_parser_register(opp, "-gpgpu_n_cluster_ejection_buffer_size", OPT_UINT32, &n_simt_ejection_buffer_size, 
-                 "number of packets in ejection buffer",
-                 "8");
-    option_parser_register(opp, "-gpgpu_n_ldst_response_buffer_size", OPT_UINT32, &ldst_unit_response_queue_size, 
-                 "number of response packets in ld/st unit ejection buffer",
-                 "2");
-    option_parser_register(opp, "-gpgpu_shmem_per_block", OPT_UINT32, &gpgpu_shmem_per_block,
-                 "Size of shared memory per thread block or CTA (default 48kB)",
-                 "49152");
-    option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_size,
-                 "Size of shared memory per shader core (default 16kB)",
-                 "16384");
-    option_parser_register(opp, "-adaptive_volta_cache_config", OPT_BOOL, &adaptive_volta_cache_config,
-                 "adaptive_volta_cache_config",
-                 "0");
-    option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_sizeDefault,
-                 "Size of shared memory per shader core (default 16kB)",
-                 "16384");
-    option_parser_register(opp, "-gpgpu_shmem_size_PrefL1", OPT_UINT32, &gpgpu_shmem_sizePrefL1,
-                 "Size of shared memory per shader core (default 16kB)",
-                 "16384");
-    option_parser_register(opp, "-gpgpu_shmem_size_PrefShared", OPT_UINT32, &gpgpu_shmem_sizePrefShared,
-                 "Size of shared memory per shader core (default 16kB)",
-                 "16384");
-    option_parser_register(opp, "-gpgpu_shmem_num_banks", OPT_UINT32, &num_shmem_bank, 
-                 "Number of banks in the shared memory in each shader core (default 16)",
-                 "16");
-    option_parser_register(opp, "-gpgpu_shmem_limited_broadcast", OPT_BOOL, &shmem_limited_broadcast, 
-                 "Limit shared memory to do one broadcast per cycle (default on)",
-                 "1");
-    option_parser_register(opp, "-gpgpu_shmem_warp_parts", OPT_INT32, &mem_warp_parts,  
-                 "Number of portions a warp is divided into for shared memory bank conflict check ",
-                 "2");
-    option_parser_register(opp, "-mem_unit_ports", OPT_INT32, &mem_unit_ports,
-                 "The number of memory transactions allowed per core cycle",
-                 "1");
-    option_parser_register(opp, "-gpgpu_shmem_warp_parts", OPT_INT32, &mem_warp_parts,
-				 "Number of portions a warp is divided into for shared memory bank conflict check ",
-				 "2");
-    option_parser_register(opp, "-gpgpu_warpdistro_shader", OPT_INT32, &gpgpu_warpdistro_shader, 
-                "Specify which shader core to collect the warp size distribution from", 
-                "-1");
-    option_parser_register(opp, "-gpgpu_warp_issue_shader", OPT_INT32, &gpgpu_warp_issue_shader, 
-                "Specify which shader core to collect the warp issue distribution from", 
-                "0");
-    option_parser_register(opp, "-gpgpu_local_mem_map", OPT_BOOL, &gpgpu_local_mem_map, 
-                "Mapping from local memory space address to simulated GPU physical address space (default = enabled)", 
-		"1");
-    option_parser_register(opp, "-gpgpu_num_reg_banks", OPT_INT32, &gpgpu_num_reg_banks, 
-                "Number of register banks (default = 8)", 
-                "8");
-    option_parser_register(opp, "-gpgpu_reg_bank_use_warp_id", OPT_BOOL, &gpgpu_reg_bank_use_warp_id,
-             "Use warp ID in mapping registers to banks (default = off)",
-             "0");
-    option_parser_register(opp, "-sub_core_model", OPT_BOOL, &sub_core_model,
-             "Sub Core Volta/Pascal model (default = off)",
-             "0");
-    option_parser_register(opp, "-enable_specialized_operand_collector", OPT_BOOL, &enable_specialized_operand_collector,
-                "enable_specialized_operand_collector",
-                "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_units_sp", OPT_INT32, &gpgpu_operand_collector_num_units_sp,
-                "number of collector units (default = 4)",
-                "4");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_units_dp", OPT_INT32, &gpgpu_operand_collector_num_units_dp,
-                   "number of collector units (default = 0)",
-                   "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_units_sfu", OPT_INT32, &gpgpu_operand_collector_num_units_sfu,
-                "number of collector units (default = 4)", 
-                "4");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_units_int", OPT_INT32, &gpgpu_operand_collector_num_units_int,
-                "number of collector units (default = 0)",
-                "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_units_tensor_core", OPT_INT32, &gpgpu_operand_collector_num_units_tensor_core,
-                "number of collector units (default = 4)", 
-                "4");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_units_mem", OPT_INT32, &gpgpu_operand_collector_num_units_mem,
-                "number of collector units (default = 2)", 
-                "2");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_units_gen", OPT_INT32, &gpgpu_operand_collector_num_units_gen,
-                "number of collector units (default = 0)", 
-                "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sp", OPT_INT32, &gpgpu_operand_collector_num_in_ports_sp,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_dp", OPT_INT32, &gpgpu_operand_collector_num_in_ports_dp,
-                           "number of collector unit in ports (default = 0)",
-                           "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sfu", OPT_INT32, &gpgpu_operand_collector_num_in_ports_sfu,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_int", OPT_INT32, &gpgpu_operand_collector_num_in_ports_int,
-                           "number of collector unit in ports (default = 0)",
-                           "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_tensor_core", OPT_INT32, &gpgpu_operand_collector_num_in_ports_tensor_core,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_mem", OPT_INT32, &gpgpu_operand_collector_num_in_ports_mem,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_gen", OPT_INT32, &gpgpu_operand_collector_num_in_ports_gen,
-                           "number of collector unit in ports (default = 0)", 
-                           "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sp", OPT_INT32, &gpgpu_operand_collector_num_out_ports_sp,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_dp", OPT_INT32, &gpgpu_operand_collector_num_out_ports_dp,
-                           "number of collector unit in ports (default = 0)",
-                           "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sfu", OPT_INT32, &gpgpu_operand_collector_num_out_ports_sfu,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_int", OPT_INT32, &gpgpu_operand_collector_num_out_ports_int,
-                           "number of collector unit in ports (default = 0)",
-                           "0");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_tensor_core", OPT_INT32, &gpgpu_operand_collector_num_out_ports_tensor_core,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_mem", OPT_INT32, &gpgpu_operand_collector_num_out_ports_mem,
-                           "number of collector unit in ports (default = 1)", 
-                           "1");
-    option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_gen", OPT_INT32, &gpgpu_operand_collector_num_out_ports_gen,
-                           "number of collector unit in ports (default = 0)", 
-                           "0");
-    option_parser_register(opp, "-gpgpu_coalesce_arch", OPT_INT32, &gpgpu_coalesce_arch, 
-    		            "Coalescing arch (GT200 = 13, Fermi = 20)",
-                            "13");
-    option_parser_register(opp, "-gpgpu_num_sched_per_core", OPT_INT32, &gpgpu_num_sched_per_core, 
-                            "Number of warp schedulers per core", 
-                            "1");
-    option_parser_register(opp, "-gpgpu_max_insn_issue_per_warp", OPT_INT32, &gpgpu_max_insn_issue_per_warp,
-    		            "Max number of instructions that can be issued per warp in one cycle by scheduler (either 1 or 2)",
-			    "2");
-    option_parser_register(opp, "-gpgpu_dual_issue_diff_exec_units", OPT_BOOL, &gpgpu_dual_issue_diff_exec_units,
-			    "should dual issue use two different execution unit resources (Default = 1)",
-			    "1");
-    option_parser_register(opp, "-gpgpu_simt_core_sim_order", OPT_INT32, &simt_core_sim_order,
-                            "Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)",
-                            "1");
-    option_parser_register(opp, "-gpgpu_pipeline_widths", OPT_CSTR, &pipeline_widths_string,
-                            "Pipeline widths "
-                            "ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE",
-                            "1,1,1,1,1,1,1,1,1,1,1,1,1" );
-    option_parser_register(opp, "-gpgpu_tensor_core_avail", OPT_INT32, &gpgpu_tensor_core_avail,
-                            "Tensor Core Available (default=0)",
-                            "0");
-    option_parser_register(opp, "-gpgpu_num_sp_units", OPT_INT32, &gpgpu_num_sp_units,
-                            "Number of SP units (default=1)",
-                            "1");
-    option_parser_register(opp, "-gpgpu_num_dp_units", OPT_INT32, &gpgpu_num_dp_units,
-                            "Number of DP units (default=0)",
-                            "0");
-    option_parser_register(opp, "-gpgpu_num_int_units", OPT_INT32, &gpgpu_num_int_units,
-                            "Number of INT units (default=0)",
-                            "0");
-    option_parser_register(opp, "-gpgpu_num_sfu_units", OPT_INT32, &gpgpu_num_sfu_units,
-                            "Number of SF units (default=1)",
-                            "1");
-    option_parser_register(opp, "-gpgpu_num_tensor_core_units", OPT_INT32, &gpgpu_num_tensor_core_units,
-                            "Number of tensor_core units (default=1)",
-                            "1");
-    option_parser_register(opp, "-gpgpu_num_mem_units", OPT_INT32, &gpgpu_num_mem_units,
-                            "Number if ldst units (default=1) WARNING: not hooked up to anything",
-                             "1");
-    option_parser_register(opp, "-gpgpu_scheduler", OPT_CSTR, &gpgpu_scheduler_string,
-                                "Scheduler configuration: < lrr | gto | two_level_active > "
-                                "If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>"
-                                "For complete list of prioritization values see shader.h enum scheduler_prioritization_type"
-                                "Default: gto",
-                                 "gto");
-
-    option_parser_register(opp, "-gpgpu_concurrent_kernel_sm", OPT_BOOL, &gpgpu_concurrent_kernel_sm, 
-                "Support concurrent kernels on a SM (default = disabled)", 
-                "0");
-
-}
-
-void gpgpu_sim_config::reg_options(option_parser_t opp)
-{
-    gpgpu_functional_sim_config::reg_options(opp);
-    m_shader_config.reg_options(opp);
-    m_memory_config.reg_options(opp);
-    power_config::reg_options(opp);
-   option_parser_register(opp, "-gpgpu_max_cycle", OPT_INT32, &gpu_max_cycle_opt, 
-               "terminates gpu simulation early (0 = no limit)",
-               "0");
-   option_parser_register(opp, "-gpgpu_max_insn", OPT_INT32, &gpu_max_insn_opt, 
-               "terminates gpu simulation early (0 = no limit)",
-               "0");
-   option_parser_register(opp, "-gpgpu_max_cta", OPT_INT32, &gpu_max_cta_opt, 
-               "terminates gpu simulation early (0 = no limit)",
-               "0");
-   option_parser_register(opp, "-gpgpu_runtime_stat", OPT_CSTR, &gpgpu_runtime_stat, 
-                  "display runtime statistics such as dram utilization {<freq>:<flag>}",
-                  "10000:0");
-   option_parser_register(opp, "-liveness_message_freq", OPT_INT64, &liveness_message_freq, 
-               "Minimum number of seconds between simulation liveness messages (0 = always print)",
-               "1");
-   option_parser_register(opp, "-gpgpu_compute_capability_major", OPT_UINT32, &gpgpu_compute_capability_major,
-                 "Major compute capability version number",
-                 "7");
-   option_parser_register(opp, "-gpgpu_compute_capability_minor", OPT_UINT32, &gpgpu_compute_capability_minor,
-                 "Minor compute capability version number",
-                 "0");
-   option_parser_register(opp, "-gpgpu_flush_l1_cache", OPT_BOOL, &gpgpu_flush_l1_cache,
-                "Flush L1 cache at the end of each kernel call",
-                "0");
-   option_parser_register(opp, "-gpgpu_flush_l2_cache", OPT_BOOL, &gpgpu_flush_l2_cache,
-                   "Flush L2 cache at the end of each kernel call",
-                   "0");
-   option_parser_register(opp, "-gpgpu_deadlock_detect", OPT_BOOL, &gpu_deadlock_detect, 
-                "Stop the simulation at deadlock (1=on (default), 0=off)", 
-                "1");
-   option_parser_register(opp, "-gpgpu_ptx_instruction_classification", OPT_INT32, 
-               &gpgpu_ptx_instruction_classification, 
-               "if enabled will classify ptx instruction types per kernel (Max 255 kernels now)", 
-               "0");
-   option_parser_register(opp, "-gpgpu_ptx_sim_mode", OPT_INT32, &g_ptx_sim_mode, 
-               "Select between Performance (default) or Functional simulation (1)", 
-               "0");
-   option_parser_register(opp, "-gpgpu_clock_domains", OPT_CSTR, &gpgpu_clock_domains, 
-                  "Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}",
-                  "500.0:2000.0:2000.0:2000.0");
-   option_parser_register(opp, "-gpgpu_max_concurrent_kernel", OPT_INT32, &max_concurrent_kernel,
-                          "maximum kernels that can run concurrently on GPU", "8" );
-   option_parser_register(opp, "-gpgpu_cflog_interval", OPT_INT32, &gpgpu_cflog_interval, 
-               "Interval between each snapshot in control flow logger", 
-               "0");
-   option_parser_register(opp, "-visualizer_enabled", OPT_BOOL,
-                          &g_visualizer_enabled, "Turn on visualizer output (1=On, 0=Off)",
-                          "1");
-   option_parser_register(opp, "-visualizer_outputfile", OPT_CSTR, 
-                          &g_visualizer_filename, "Specifies the output log file for visualizer",
-                          NULL);
-   option_parser_register(opp, "-visualizer_zlevel", OPT_INT32,
-                          &g_visualizer_zlevel, "Compression level of the visualizer output log (0=no comp, 9=highest)",
-                          "6");
-   option_parser_register(opp, "-gpgpu_stack_size_limit", OPT_INT32, &stack_size_limit,
-                          "GPU thread stack size", "1024" );
-   option_parser_register(opp, "-gpgpu_heap_size_limit", OPT_INT32, &heap_size_limit,
-                          "GPU malloc heap size ", "8388608" );
-   option_parser_register(opp, "-gpgpu_runtime_sync_depth_limit", OPT_INT32, &runtime_sync_depth_limit,
-                          "GPU device runtime synchronize depth", "2" );
-   option_parser_register(opp, "-gpgpu_runtime_pending_launch_count_limit", OPT_INT32, &runtime_pending_launch_count_limit,
-                          "GPU device runtime pending launch count", "2048" );
-    option_parser_register(opp, "-trace_enabled", OPT_BOOL, 
-                          &Trace_gpgpu::enabled, "Turn on traces",
-                          "0");
-    option_parser_register(opp, "-trace_components", OPT_CSTR, 
-                          &Trace_gpgpu::config_str, "comma seperated list of traces to enable. "
-                          "Complete list found in trace_streams.tup. "
-                          "Default none",
-                          "none");
-    option_parser_register(opp, "-trace_sampling_core", OPT_INT32, 
-                          &Trace_gpgpu::sampling_core, "The core which is printed using CORE_DPRINTF. Default 0",
-                          "0");
-    option_parser_register(opp, "-trace_sampling_memory_partition", OPT_INT32, 
-                          &Trace_gpgpu::sampling_memory_partition, "The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)",
-                          "-1");
-   ptx_file_line_stats_options(opp);
-
-    //Jin: kernel launch latency
-    extern unsigned g_kernel_launch_latency;
-    option_parser_register(opp, "-gpgpu_kernel_launch_latency", OPT_INT32, 
-                          &g_kernel_launch_latency, "Kernel launch latency in cycles. Default: 0",
-                          "0");
-    extern bool g_cdp_enabled;
-    option_parser_register(opp, "-gpgpu_cdp_enabled", OPT_BOOL, 
-                          &g_cdp_enabled, "Turn on CDP",
-                          "0");
+void power_config::reg_options(class OptionParser *opp) {
+  option_parser_register(opp, "-accelwattch_xml_file", OPT_CSTR,
+                         &g_power_config_name, "AccelWattch XML file",
+                         "accelwattch_sass_sim.xml");
+
+  option_parser_register(opp, "-power_simulation_enabled", OPT_BOOL,
+                         &g_power_simulation_enabled,
+                         "Turn on power simulator (1=On, 0=Off)", "0");
+
+  option_parser_register(opp, "-power_per_cycle_dump", OPT_BOOL,
+                         &g_power_per_cycle_dump,
+                         "Dump detailed power output each cycle", "0");
+
+
+
+
+  option_parser_register(opp, "-hw_perf_file_name", OPT_CSTR,
+                         &g_hw_perf_file_name, "Hardware Performance Statistics file",
+                         "hw_perf.csv");
+
+  option_parser_register(opp, "-hw_perf_bench_name", OPT_CSTR,
+                         &g_hw_perf_bench_name, "Kernel Name in Hardware Performance Statistics file",
+                         "");
+
+  option_parser_register(opp, "-power_simulation_mode", OPT_INT32,
+                         &g_power_simulation_mode,
+                         "Switch performance counter input for power simulation (0=Sim, 1=HW, 2=HW-Sim Hybrid)", "0");
+
+  option_parser_register(opp, "-dvfs_enabled", OPT_BOOL,
+                         &g_dvfs_enabled,
+                         "Turn on DVFS for power model", "0");
+  option_parser_register(opp, "-aggregate_power_stats", OPT_BOOL,
+                         &g_aggregate_power_stats,
+                         "Accumulate power across all kernels", "0");
+
+  //Accelwattch Hyrbid Configuration
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L1_RH", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L1_RH],
+                         "Get L1 Read Hits for Accelwattch-Hybrid from Accel-Sim", "0");
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L1_RM", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L1_RM],
+                         "Get L1 Read Misses for Accelwattch-Hybrid from Accel-Sim", "0");
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L1_WH", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L1_WH],
+                         "Get L1 Write Hits for Accelwattch-Hybrid from Accel-Sim", "0");
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L1_WM", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L1_WM],
+                         "Get L1 Write Misses for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L2_RH", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L2_RH],
+                         "Get L2 Read Hits for Accelwattch-Hybrid from Accel-Sim", "0");
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L2_RM", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L2_RM],
+                         "Get L2 Read Misses for Accelwattch-Hybrid from Accel-Sim", "0");
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L2_WH", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L2_WH],
+                         "Get L2 Write Hits for Accelwattch-Hybrid from Accel-Sim", "0");
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_L2_WM", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_L2_WM],
+                         "Get L2 Write Misses for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_CC_ACC", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_CC_ACC],
+                         "Get Constant Cache Acesses for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_SHARED_ACC", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_SHRD_ACC],
+                         "Get Shared Memory Acesses for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_DRAM_RD", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_DRAM_RD],
+                         "Get DRAM Reads for Accelwattch-Hybrid from Accel-Sim", "0");
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_DRAM_WR", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_DRAM_WR],
+                         "Get DRAM Writes for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_NOC", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_NOC],
+                         "Get Interconnect Acesses for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_PIPE_DUTY", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_PIPE_DUTY],
+                         "Get Pipeline Duty Cycle Acesses for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_NUM_SM_IDLE", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_NUM_SM_IDLE],
+                         "Get Number of Idle SMs for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_CYCLES", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_CYCLES],
+                         "Get Executed Cycles for Accelwattch-Hybrid from Accel-Sim", "0");
+
+  option_parser_register(opp, "-accelwattch_hybrid_perfsim_VOLTAGE", OPT_BOOL,
+                         &accelwattch_hybrid_configuration[HW_VOLTAGE],
+                         "Get Chip Voltage for Accelwattch-Hybrid from Accel-Sim", "0");
+
+
+  // Output Data Formats
+  option_parser_register(
+      opp, "-power_trace_enabled", OPT_BOOL, &g_power_trace_enabled,
+      "produce a file for the power trace (1=On, 0=Off)", "0");
+
+  option_parser_register(
+      opp, "-power_trace_zlevel", OPT_INT32, &g_power_trace_zlevel,
+      "Compression level of the power trace output log (0=no comp, 9=highest)",
+      "6");
+
+  option_parser_register(
+      opp, "-steady_power_levels_enabled", OPT_BOOL,
+      &g_steady_power_levels_enabled,
+      "produce a file for the steady power levels (1=On, 0=Off)", "0");
+
+  option_parser_register(opp, "-steady_state_definition", OPT_CSTR,
+                         &gpu_steady_state_definition,
+                         "allowed deviation:number of samples", "8:4");
+}
+
+void memory_config::reg_options(class OptionParser *opp) {
+  option_parser_register(opp, "-gpgpu_perf_sim_memcpy", OPT_BOOL,
+                         &m_perf_sim_memcpy, "Fill the L2 cache on memcpy",
+                         "1");
+  option_parser_register(opp, "-gpgpu_simple_dram_model", OPT_BOOL,
+                         &simple_dram_model,
+                         "simple_dram_model with fixed latency and BW", "0");
+  option_parser_register(opp, "-gpgpu_dram_scheduler", OPT_INT32,
+                         &scheduler_type, "0 = fifo, 1 = FR-FCFS (defaul)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_dram_partition_queues", OPT_CSTR,
+                         &gpgpu_L2_queue_config, "i2$:$2d:d2$:$2i", "8:8:8:8");
+
+  option_parser_register(opp, "-l2_ideal", OPT_BOOL, &l2_ideal,
+                         "Use a ideal L2 cache that always hit", "0");
+  option_parser_register(opp, "-gpgpu_cache:dl2", OPT_CSTR,
+                         &m_L2_config.m_config_string,
+                         "unified banked L2 data cache config "
+                         " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_"
+                         "alloc>,<mshr>:<N>:<merge>,<mq>}",
+                         "64:128:8,L:B:m:N,A:16:4,4");
+  option_parser_register(opp, "-gpgpu_cache:dl2_texture_only", OPT_BOOL,
+                         &m_L2_texure_only, "L2 cache used for texture only",
+                         "1");
+  option_parser_register(
+      opp, "-gpgpu_n_mem", OPT_UINT32, &m_n_mem,
+      "number of memory modules (e.g. memory controllers) in gpu", "8");
+  option_parser_register(opp, "-gpgpu_n_sub_partition_per_mchannel", OPT_UINT32,
+                         &m_n_sub_partition_per_memory_channel,
+                         "number of memory subpartition in each memory module",
+                         "1");
+  option_parser_register(opp, "-gpgpu_n_mem_per_ctrlr", OPT_UINT32,
+                         &gpu_n_mem_per_ctrlr,
+                         "number of memory chips per memory controller", "1");
+  option_parser_register(opp, "-gpgpu_memlatency_stat", OPT_INT32,
+                         &gpgpu_memlatency_stat,
+                         "track and display latency statistics 0x2 enables MC, "
+                         "0x4 enables queue logs",
+                         "0");
+  option_parser_register(opp, "-gpgpu_frfcfs_dram_sched_queue_size", OPT_INT32,
+                         &gpgpu_frfcfs_dram_sched_queue_size,
+                         "0 = unlimited (default); # entries per chip", "0");
+  option_parser_register(opp, "-gpgpu_dram_return_queue_size", OPT_INT32,
+                         &gpgpu_dram_return_queue_size,
+                         "0 = unlimited (default); # entries per chip", "0");
+  option_parser_register(opp, "-gpgpu_dram_buswidth", OPT_UINT32, &busW,
+                         "default = 4 bytes (8 bytes per cycle at DDR)", "4");
+  option_parser_register(
+      opp, "-gpgpu_dram_burst_length", OPT_UINT32, &BL,
+      "Burst length of each DRAM request (default = 4 data bus cycle)", "4");
+  option_parser_register(opp, "-dram_data_command_freq_ratio", OPT_UINT32,
+                         &data_command_freq_ratio,
+                         "Frequency ratio between DRAM data bus and command "
+                         "bus (default = 2 times, i.e. DDR)",
+                         "2");
+  option_parser_register(
+      opp, "-gpgpu_dram_timing_opt", OPT_CSTR, &gpgpu_dram_timing_opt,
+      "DRAM timing parameters = "
+      "{nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}",
+      "4:2:8:12:21:13:34:9:4:5:13:1:0:0");
+  option_parser_register(opp, "-gpgpu_l2_rop_latency", OPT_UINT32, &rop_latency,
+                         "ROP queue latency (default 85)", "85");
+  option_parser_register(opp, "-dram_latency", OPT_UINT32, &dram_latency,
+                         "DRAM latency (default 30)", "30");
+  option_parser_register(opp, "-dram_dual_bus_interface", OPT_UINT32,
+                         &dual_bus_interface,
+                         "dual_bus_interface (default = 0) ", "0");
+  option_parser_register(opp, "-dram_bnk_indexing_policy", OPT_UINT32,
+                         &dram_bnk_indexing_policy,
+                         "dram_bnk_indexing_policy (0 = normal indexing, 1 = "
+                         "Xoring with the higher bits) (Default = 0)",
+                         "0");
+  option_parser_register(opp, "-dram_bnkgrp_indexing_policy", OPT_UINT32,
+                         &dram_bnkgrp_indexing_policy,
+                         "dram_bnkgrp_indexing_policy (0 = take higher bits, 1 "
+                         "= take lower bits) (Default = 0)",
+                         "0");
+  option_parser_register(opp, "-dram_seperate_write_queue_enable", OPT_BOOL,
+                         &seperate_write_queue_enabled,
+                         "Seperate_Write_Queue_Enable", "0");
+  option_parser_register(opp, "-dram_write_queue_size", OPT_CSTR,
+                         &write_queue_size_opt, "Write_Queue_Size", "32:28:16");
+  option_parser_register(
+      opp, "-dram_elimnate_rw_turnaround", OPT_BOOL, &elimnate_rw_turnaround,
+      "elimnate_rw_turnaround i.e set tWTR and tRTW = 0", "0");
+  option_parser_register(opp, "-icnt_flit_size", OPT_UINT32, &icnt_flit_size,
+                         "icnt_flit_size", "32");
+  m_address_mapping.addrdec_setoption(opp);
+}
+
+void shader_core_config::reg_options(class OptionParser *opp) {
+  option_parser_register(opp, "-gpgpu_simd_model", OPT_INT32, &model,
+                         "1 = post-dominator", "1");
+  option_parser_register(
+      opp, "-gpgpu_shader_core_pipeline", OPT_CSTR,
+      &gpgpu_shader_core_pipeline_opt,
+      "shader core pipeline config, i.e., {<nthread>:<warpsize>}", "1024:32");
+  option_parser_register(opp, "-gpgpu_tex_cache:l1", OPT_CSTR,
+                         &m_L1T_config.m_config_string,
+                         "per-shader L1 texture cache  (READ-ONLY) config "
+                         " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_"
+                         "alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}",
+                         "8:128:5,L:R:m:N,F:128:4,128:2");
+  option_parser_register(
+      opp, "-gpgpu_const_cache:l1", OPT_CSTR, &m_L1C_config.m_config_string,
+      "per-shader L1 constant memory cache  (READ-ONLY) config "
+      " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<"
+      "merge>,<mq>} ",
+      "64:64:2,L:R:f:N,A:2:32,4");
+  option_parser_register(opp, "-gpgpu_cache:il1", OPT_CSTR,
+                         &m_L1I_config.m_config_string,
+                         "shader L1 instruction cache config "
+                         " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_"
+                         "alloc>,<mshr>:<N>:<merge>,<mq>} ",
+                         "4:256:4,L:R:f:N,A:2:32,4");
+  option_parser_register(opp, "-gpgpu_cache:dl1", OPT_CSTR,
+                         &m_L1D_config.m_config_string,
+                         "per-shader L1 data cache config "
+                         " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_"
+                         "alloc>,<mshr>:<N>:<merge>,<mq> | none}",
+                         "none");
+  option_parser_register(opp, "-gpgpu_l1_cache_write_ratio", OPT_UINT32,
+                         &m_L1D_config.m_wr_percent, "L1D write ratio", "0");
+  option_parser_register(opp, "-gpgpu_l1_banks", OPT_UINT32,
+                         &m_L1D_config.l1_banks, "The number of L1 cache banks",
+                         "1");
+  option_parser_register(opp, "-gpgpu_l1_banks_byte_interleaving", OPT_UINT32,
+                         &m_L1D_config.l1_banks_byte_interleaving,
+                         "l1 banks byte interleaving granularity", "32");
+  option_parser_register(opp, "-gpgpu_l1_banks_hashing_function", OPT_UINT32,
+                         &m_L1D_config.l1_banks_hashing_function,
+                         "l1 banks hashing function", "0");
+  option_parser_register(opp, "-gpgpu_l1_latency", OPT_UINT32,
+                         &m_L1D_config.l1_latency, "L1 Hit Latency", "1");
+  option_parser_register(opp, "-gpgpu_smem_latency", OPT_UINT32, &smem_latency,
+                         "smem Latency", "3");
+  option_parser_register(opp, "-gpgpu_cache:dl1PrefL1", OPT_CSTR,
+                         &m_L1D_config.m_config_stringPrefL1,
+                         "per-shader L1 data cache config "
+                         " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_"
+                         "alloc>,<mshr>:<N>:<merge>,<mq> | none}",
+                         "none");
+  option_parser_register(opp, "-gpgpu_cache:dl1PrefShared", OPT_CSTR,
+                         &m_L1D_config.m_config_stringPrefShared,
+                         "per-shader L1 data cache config "
+                         " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_"
+                         "alloc>,<mshr>:<N>:<merge>,<mq> | none}",
+                         "none");
+  option_parser_register(opp, "-gpgpu_gmem_skip_L1D", OPT_BOOL, &gmem_skip_L1D,
+                         "global memory access skip L1D cache (implements "
+                         "-Xptxas -dlcm=cg, default=no skip)",
+                         "0");
+
+  option_parser_register(opp, "-gpgpu_perfect_mem", OPT_BOOL,
+                         &gpgpu_perfect_mem,
+                         "enable perfect memory mode (no cache miss)", "0");
+  option_parser_register(
+      opp, "-n_regfile_gating_group", OPT_UINT32, &n_regfile_gating_group,
+      "group of lanes that should be read/written together)", "4");
+  option_parser_register(
+      opp, "-gpgpu_clock_gated_reg_file", OPT_BOOL, &gpgpu_clock_gated_reg_file,
+      "enable clock gated reg file for power calculations", "0");
+  option_parser_register(
+      opp, "-gpgpu_clock_gated_lanes", OPT_BOOL, &gpgpu_clock_gated_lanes,
+      "enable clock gated lanes for power calculations", "0");
+  option_parser_register(opp, "-gpgpu_shader_registers", OPT_UINT32,
+                         &gpgpu_shader_registers,
+                         "Number of registers per shader core. Limits number "
+                         "of concurrent CTAs. (default 8192)",
+                         "8192");
+  option_parser_register(
+      opp, "-gpgpu_registers_per_block", OPT_UINT32, &gpgpu_registers_per_block,
+      "Maximum number of registers per CTA. (default 8192)", "8192");
+  option_parser_register(opp, "-gpgpu_ignore_resources_limitation", OPT_BOOL,
+                         &gpgpu_ignore_resources_limitation,
+                         "gpgpu_ignore_resources_limitation (default 0)", "0");
+  option_parser_register(
+      opp, "-gpgpu_shader_cta", OPT_UINT32, &max_cta_per_core,
+      "Maximum number of concurrent CTAs in shader (default 8)", "8");
+  option_parser_register(
+      opp, "-gpgpu_num_cta_barriers", OPT_UINT32, &max_barriers_per_cta,
+      "Maximum number of named barriers per CTA (default 16)", "16");
+  option_parser_register(opp, "-gpgpu_n_clusters", OPT_UINT32, &n_simt_clusters,
+                         "number of processing clusters", "10");
+  option_parser_register(opp, "-gpgpu_n_cores_per_cluster", OPT_UINT32,
+                         &n_simt_cores_per_cluster,
+                         "number of simd cores per cluster", "3");
+  option_parser_register(opp, "-gpgpu_n_cluster_ejection_buffer_size",
+                         OPT_UINT32, &n_simt_ejection_buffer_size,
+                         "number of packets in ejection buffer", "8");
+  option_parser_register(
+      opp, "-gpgpu_n_ldst_response_buffer_size", OPT_UINT32,
+      &ldst_unit_response_queue_size,
+      "number of response packets in ld/st unit ejection buffer", "2");
+  option_parser_register(
+      opp, "-gpgpu_shmem_per_block", OPT_UINT32, &gpgpu_shmem_per_block,
+      "Size of shared memory per thread block or CTA (default 48kB)", "49152");
+  option_parser_register(
+      opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_size,
+      "Size of shared memory per shader core (default 16kB)", "16384");
+  option_parser_register(opp, "-gpgpu_shmem_option", OPT_CSTR,
+                         &gpgpu_shmem_option,
+                         "Option list of shared memory sizes", "0");
+  option_parser_register(
+      opp, "-gpgpu_unified_l1d_size", OPT_UINT32,
+      &m_L1D_config.m_unified_cache_size,
+      "Size of unified data cache(L1D + shared memory) in KB", "0");
+  option_parser_register(opp, "-gpgpu_adaptive_cache_config", OPT_BOOL,
+                         &adaptive_cache_config, "adaptive_cache_config", "0");
+  option_parser_register(
+      opp, "-gpgpu_shmem_sizeDefault", OPT_UINT32, &gpgpu_shmem_sizeDefault,
+      "Size of shared memory per shader core (default 16kB)", "16384");
+  option_parser_register(
+      opp, "-gpgpu_shmem_size_PrefL1", OPT_UINT32, &gpgpu_shmem_sizePrefL1,
+      "Size of shared memory per shader core (default 16kB)", "16384");
+  option_parser_register(opp, "-gpgpu_shmem_size_PrefShared", OPT_UINT32,
+                         &gpgpu_shmem_sizePrefShared,
+                         "Size of shared memory per shader core (default 16kB)",
+                         "16384");
+  option_parser_register(
+      opp, "-gpgpu_shmem_num_banks", OPT_UINT32, &num_shmem_bank,
+      "Number of banks in the shared memory in each shader core (default 16)",
+      "16");
+  option_parser_register(
+      opp, "-gpgpu_shmem_limited_broadcast", OPT_BOOL, &shmem_limited_broadcast,
+      "Limit shared memory to do one broadcast per cycle (default on)", "1");
+  option_parser_register(opp, "-gpgpu_shmem_warp_parts", OPT_INT32,
+                         &mem_warp_parts,
+                         "Number of portions a warp is divided into for shared "
+                         "memory bank conflict check ",
+                         "2");
+  option_parser_register(
+      opp, "-gpgpu_mem_unit_ports", OPT_INT32, &mem_unit_ports,
+      "The number of memory transactions allowed per core cycle", "1");
+  option_parser_register(opp, "-gpgpu_shmem_warp_parts", OPT_INT32,
+                         &mem_warp_parts,
+                         "Number of portions a warp is divided into for shared "
+                         "memory bank conflict check ",
+                         "2");
+  option_parser_register(
+      opp, "-gpgpu_warpdistro_shader", OPT_INT32, &gpgpu_warpdistro_shader,
+      "Specify which shader core to collect the warp size distribution from",
+      "-1");
+  option_parser_register(
+      opp, "-gpgpu_warp_issue_shader", OPT_INT32, &gpgpu_warp_issue_shader,
+      "Specify which shader core to collect the warp issue distribution from",
+      "0");
+  option_parser_register(opp, "-gpgpu_local_mem_map", OPT_BOOL,
+                         &gpgpu_local_mem_map,
+                         "Mapping from local memory space address to simulated "
+                         "GPU physical address space (default = enabled)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_num_reg_banks", OPT_INT32,
+                         &gpgpu_num_reg_banks,
+                         "Number of register banks (default = 8)", "8");
+  option_parser_register(
+      opp, "-gpgpu_reg_bank_use_warp_id", OPT_BOOL, &gpgpu_reg_bank_use_warp_id,
+      "Use warp ID in mapping registers to banks (default = off)", "0");
+  option_parser_register(opp, "-gpgpu_sub_core_model", OPT_BOOL,
+                         &sub_core_model,
+                         "Sub Core Volta/Pascal model (default = off)", "0");
+  option_parser_register(opp, "-gpgpu_enable_specialized_operand_collector",
+                         OPT_BOOL, &enable_specialized_operand_collector,
+                         "enable_specialized_operand_collector", "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_units_sp",
+                         OPT_INT32, &gpgpu_operand_collector_num_units_sp,
+                         "number of collector units (default = 4)", "4");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_units_dp",
+                         OPT_INT32, &gpgpu_operand_collector_num_units_dp,
+                         "number of collector units (default = 0)", "0");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_units_sfu",
+                         OPT_INT32, &gpgpu_operand_collector_num_units_sfu,
+                         "number of collector units (default = 4)", "4");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_units_int",
+                         OPT_INT32, &gpgpu_operand_collector_num_units_int,
+                         "number of collector units (default = 0)", "0");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_units_tensor_core",
+                         OPT_INT32,
+                         &gpgpu_operand_collector_num_units_tensor_core,
+                         "number of collector units (default = 4)", "4");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_units_mem",
+                         OPT_INT32, &gpgpu_operand_collector_num_units_mem,
+                         "number of collector units (default = 2)", "2");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_units_gen",
+                         OPT_INT32, &gpgpu_operand_collector_num_units_gen,
+                         "number of collector units (default = 0)", "0");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sp",
+                         OPT_INT32, &gpgpu_operand_collector_num_in_ports_sp,
+                         "number of collector unit in ports (default = 1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_dp",
+                         OPT_INT32, &gpgpu_operand_collector_num_in_ports_dp,
+                         "number of collector unit in ports (default = 0)",
+                         "0");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sfu",
+                         OPT_INT32, &gpgpu_operand_collector_num_in_ports_sfu,
+                         "number of collector unit in ports (default = 1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_int",
+                         OPT_INT32, &gpgpu_operand_collector_num_in_ports_int,
+                         "number of collector unit in ports (default = 0)",
+                         "0");
+  option_parser_register(
+      opp, "-gpgpu_operand_collector_num_in_ports_tensor_core", OPT_INT32,
+      &gpgpu_operand_collector_num_in_ports_tensor_core,
+      "number of collector unit in ports (default = 1)", "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_mem",
+                         OPT_INT32, &gpgpu_operand_collector_num_in_ports_mem,
+                         "number of collector unit in ports (default = 1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_gen",
+                         OPT_INT32, &gpgpu_operand_collector_num_in_ports_gen,
+                         "number of collector unit in ports (default = 0)",
+                         "0");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sp",
+                         OPT_INT32, &gpgpu_operand_collector_num_out_ports_sp,
+                         "number of collector unit in ports (default = 1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_dp",
+                         OPT_INT32, &gpgpu_operand_collector_num_out_ports_dp,
+                         "number of collector unit in ports (default = 0)",
+                         "0");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sfu",
+                         OPT_INT32, &gpgpu_operand_collector_num_out_ports_sfu,
+                         "number of collector unit in ports (default = 1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_int",
+                         OPT_INT32, &gpgpu_operand_collector_num_out_ports_int,
+                         "number of collector unit in ports (default = 0)",
+                         "0");
+  option_parser_register(
+      opp, "-gpgpu_operand_collector_num_out_ports_tensor_core", OPT_INT32,
+      &gpgpu_operand_collector_num_out_ports_tensor_core,
+      "number of collector unit in ports (default = 1)", "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_mem",
+                         OPT_INT32, &gpgpu_operand_collector_num_out_ports_mem,
+                         "number of collector unit in ports (default = 1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_gen",
+                         OPT_INT32, &gpgpu_operand_collector_num_out_ports_gen,
+                         "number of collector unit in ports (default = 0)",
+                         "0");
+  option_parser_register(opp, "-gpgpu_coalesce_arch", OPT_INT32,
+                         &gpgpu_coalesce_arch,
+                         "Coalescing arch (GT200 = 13, Fermi = 20)", "13");
+  option_parser_register(opp, "-gpgpu_num_sched_per_core", OPT_INT32,
+                         &gpgpu_num_sched_per_core,
+                         "Number of warp schedulers per core", "1");
+  option_parser_register(opp, "-gpgpu_max_insn_issue_per_warp", OPT_INT32,
+                         &gpgpu_max_insn_issue_per_warp,
+                         "Max number of instructions that can be issued per "
+                         "warp in one cycle by scheduler (either 1 or 2)",
+                         "2");
+  option_parser_register(opp, "-gpgpu_dual_issue_diff_exec_units", OPT_BOOL,
+                         &gpgpu_dual_issue_diff_exec_units,
+                         "should dual issue use two different execution unit "
+                         "resources (Default = 1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_simt_core_sim_order", OPT_INT32,
+                         &simt_core_sim_order,
+                         "Select the simulation order of cores in a cluster "
+                         "(0=Fix, 1=Round-Robin)",
+                         "1");
+  option_parser_register(
+      opp, "-gpgpu_pipeline_widths", OPT_CSTR, &pipeline_widths_string,
+      "Pipeline widths "
+      "ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_"
+      "INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE",
+      "1,1,1,1,1,1,1,1,1,1,1,1,1");
+  option_parser_register(opp, "-gpgpu_tensor_core_avail", OPT_INT32,
+                         &gpgpu_tensor_core_avail,
+                         "Tensor Core Available (default=0)", "0");
+  option_parser_register(opp, "-gpgpu_num_sp_units", OPT_INT32,
+                         &gpgpu_num_sp_units, "Number of SP units (default=1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_num_dp_units", OPT_INT32,
+                         &gpgpu_num_dp_units, "Number of DP units (default=0)",
+                         "0");
+  option_parser_register(opp, "-gpgpu_num_int_units", OPT_INT32,
+                         &gpgpu_num_int_units,
+                         "Number of INT units (default=0)", "0");
+  option_parser_register(opp, "-gpgpu_num_sfu_units", OPT_INT32,
+                         &gpgpu_num_sfu_units, "Number of SF units (default=1)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_num_tensor_core_units", OPT_INT32,
+                         &gpgpu_num_tensor_core_units,
+                         "Number of tensor_core units (default=1)", "0");
+  option_parser_register(
+      opp, "-gpgpu_num_mem_units", OPT_INT32, &gpgpu_num_mem_units,
+      "Number if ldst units (default=1) WARNING: not hooked up to anything",
+      "1");
+  option_parser_register(
+      opp, "-gpgpu_scheduler", OPT_CSTR, &gpgpu_scheduler_string,
+      "Scheduler configuration: < lrr | gto | two_level_active > "
+      "If "
+      "two_level_active:<num_active_warps>:<inner_prioritization>:<outer_"
+      "prioritization>"
+      "For complete list of prioritization values see shader.h enum "
+      "scheduler_prioritization_type"
+      "Default: gto",
+      "gto");
+
+  option_parser_register(
+      opp, "-gpgpu_concurrent_kernel_sm", OPT_BOOL, &gpgpu_concurrent_kernel_sm,
+      "Support concurrent kernels on a SM (default = disabled)", "0");
+  option_parser_register(opp, "-gpgpu_perfect_inst_const_cache", OPT_BOOL,
+                         &perfect_inst_const_cache,
+                         "perfect inst and const cache mode, so all inst and "
+                         "const hits in the cache(default = disabled)",
+                         "0");
+  option_parser_register(
+      opp, "-gpgpu_inst_fetch_throughput", OPT_INT32, &inst_fetch_throughput,
+      "the number of fetched intruction per warp each cycle", "1");
+  option_parser_register(opp, "-gpgpu_reg_file_port_throughput", OPT_INT32,
+                         &reg_file_port_throughput,
+                         "the number ports of the register file", "1");
+
+  for (unsigned j = 0; j < SPECIALIZED_UNIT_NUM; ++j) {
+    std::stringstream ss;
+    ss << "-specialized_unit_" << j + 1;
+    option_parser_register(opp, ss.str().c_str(), OPT_CSTR,
+                           &specialized_unit_string[j],
+                           "specialized unit config"
+                           " {<enabled>,<num_units>:<latency>:<initiation>,<ID_"
+                           "OC_SPEC>:<OC_EX_SPEC>,<NAME>}",
+                           "0,4,4,4,4,BRA");
+  }
+}
+
+void gpgpu_sim_config::reg_options(option_parser_t opp) {
+  gpgpu_functional_sim_config::reg_options(opp);
+  m_shader_config.reg_options(opp);
+  m_memory_config.reg_options(opp);
+  power_config::reg_options(opp);
+  option_parser_register(opp, "-gpgpu_max_cycle", OPT_INT64, &gpu_max_cycle_opt,
+                         "terminates gpu simulation early (0 = no limit)", "0");
+  option_parser_register(opp, "-gpgpu_max_insn", OPT_INT64, &gpu_max_insn_opt,
+                         "terminates gpu simulation early (0 = no limit)", "0");
+  option_parser_register(opp, "-gpgpu_max_cta", OPT_INT32, &gpu_max_cta_opt,
+                         "terminates gpu simulation early (0 = no limit)", "0");
+  option_parser_register(opp, "-gpgpu_max_completed_cta", OPT_INT32,
+                         &gpu_max_completed_cta_opt,
+                         "terminates gpu simulation early (0 = no limit)", "0");
+  option_parser_register(
+      opp, "-gpgpu_runtime_stat", OPT_CSTR, &gpgpu_runtime_stat,
+      "display runtime statistics such as dram utilization {<freq>:<flag>}",
+      "10000:0");
+  option_parser_register(opp, "-liveness_message_freq", OPT_INT64,
+                         &liveness_message_freq,
+                         "Minimum number of seconds between simulation "
+                         "liveness messages (0 = always print)",
+                         "1");
+  option_parser_register(opp, "-gpgpu_compute_capability_major", OPT_UINT32,
+                         &gpgpu_compute_capability_major,
+                         "Major compute capability version number", "7");
+  option_parser_register(opp, "-gpgpu_compute_capability_minor", OPT_UINT32,
+                         &gpgpu_compute_capability_minor,
+                         "Minor compute capability version number", "0");
+  option_parser_register(opp, "-gpgpu_flush_l1_cache", OPT_BOOL,
+                         &gpgpu_flush_l1_cache,
+                         "Flush L1 cache at the end of each kernel call", "0");
+  option_parser_register(opp, "-gpgpu_flush_l2_cache", OPT_BOOL,
+                         &gpgpu_flush_l2_cache,
+                         "Flush L2 cache at the end of each kernel call", "0");
+  option_parser_register(
+      opp, "-gpgpu_deadlock_detect", OPT_BOOL, &gpu_deadlock_detect,
+      "Stop the simulation at deadlock (1=on (default), 0=off)", "1");
+  option_parser_register(
+      opp, "-gpgpu_ptx_instruction_classification", OPT_INT32,
+      &(gpgpu_ctx->func_sim->gpgpu_ptx_instruction_classification),
+      "if enabled will classify ptx instruction types per kernel (Max 255 "
+      "kernels now)",
+      "0");
+  option_parser_register(
+      opp, "-gpgpu_ptx_sim_mode", OPT_INT32,
+      &(gpgpu_ctx->func_sim->g_ptx_sim_mode),
+      "Select between Performance (default) or Functional simulation (1)", "0");
+  option_parser_register(opp, "-gpgpu_clock_domains", OPT_CSTR,
+                         &gpgpu_clock_domains,
+                         "Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT "
+                         "Clock>:<L2 Clock>:<DRAM Clock>}",
+                         "500.0:2000.0:2000.0:2000.0");
+  option_parser_register(
+      opp, "-gpgpu_max_concurrent_kernel", OPT_INT32, &max_concurrent_kernel,
+      "maximum kernels that can run concurrently on GPU", "8");
+  option_parser_register(
+      opp, "-gpgpu_cflog_interval", OPT_INT32, &gpgpu_cflog_interval,
+      "Interval between each snapshot in control flow logger", "0");
+  option_parser_register(opp, "-visualizer_enabled", OPT_BOOL,
+                         &g_visualizer_enabled,
+                         "Turn on visualizer output (1=On, 0=Off)", "1");
+  option_parser_register(opp, "-visualizer_outputfile", OPT_CSTR,
+                         &g_visualizer_filename,
+                         "Specifies the output log file for visualizer", NULL);
+  option_parser_register(
+      opp, "-visualizer_zlevel", OPT_INT32, &g_visualizer_zlevel,
+      "Compression level of the visualizer output log (0=no comp, 9=highest)",
+      "6");
+  option_parser_register(opp, "-gpgpu_stack_size_limit", OPT_INT32,
+                         &stack_size_limit, "GPU thread stack size", "1024");
+  option_parser_register(opp, "-gpgpu_heap_size_limit", OPT_INT32,
+                         &heap_size_limit, "GPU malloc heap size ", "8388608");
+  option_parser_register(opp, "-gpgpu_runtime_sync_depth_limit", OPT_INT32,
+                         &runtime_sync_depth_limit,
+                         "GPU device runtime synchronize depth", "2");
+  option_parser_register(opp, "-gpgpu_runtime_pending_launch_count_limit",
+                         OPT_INT32, &runtime_pending_launch_count_limit,
+                         "GPU device runtime pending launch count", "2048");
+  option_parser_register(opp, "-trace_enabled", OPT_BOOL, &Trace_gpgpu::enabled,
+                         "Turn on traces", "0");
+  option_parser_register(opp, "-trace_components", OPT_CSTR, &Trace_gpgpu::config_str,
+                         "comma seperated list of traces to enable. "
+                         "Complete list found in trace_streams.tup. "
+                         "Default none",
+                         "none");
+  option_parser_register(
+      opp, "-trace_sampling_core", OPT_INT32, &Trace_gpgpu::sampling_core,
+      "The core which is printed using CORE_DPRINTF. Default 0", "0");
+  option_parser_register(opp, "-trace_sampling_memory_partition", OPT_INT32,
+                         &Trace_gpgpu::sampling_memory_partition,
+                         "The memory partition which is printed using "
+                         "MEMPART_DPRINTF. Default -1 (i.e. all)",
+                         "-1");
+  gpgpu_ctx->stats->ptx_file_line_stats_options(opp);
+
+  // Jin: kernel launch latency
+  option_parser_register(opp, "-gpgpu_kernel_launch_latency", OPT_INT32,
+                         &(gpgpu_ctx->device_runtime->g_kernel_launch_latency),
+                         "Kernel launch latency in cycles. Default: 0", "0");
+  option_parser_register(opp, "-gpgpu_cdp_enabled", OPT_BOOL,
+                         &(gpgpu_ctx->device_runtime->g_cdp_enabled),
+                         "Turn on CDP", "0");
+
+  option_parser_register(opp, "-gpgpu_TB_launch_latency", OPT_INT32,
+                         &(gpgpu_ctx->device_runtime->g_TB_launch_latency),
+                         "thread block launch latency in cycles. Default: 0",
+                         "0");
 }
 
 /////////////////////////////////////////////////////////////////////////////
 
-void increment_x_then_y_then_z( dim3 &i, const dim3 &bound)
-{
-   i.x++;
-   if ( i.x >= bound.x ) {
-      i.x = 0;
-      i.y++;
-      if ( i.y >= bound.y ) {
-         i.y = 0;
-         if( i.z < bound.z ) 
-            i.z++;
-      }
-   }
+void increment_x_then_y_then_z(dim3 &i, const dim3 &bound) {
+  i.x++;
+  if (i.x >= bound.x) {
+    i.x = 0;
+    i.y++;
+    if (i.y >= bound.y) {
+      i.y = 0;
+      if (i.z < bound.z) i.z++;
+    }
+  }
+}
+
+void gpgpu_sim::launch(kernel_info_t *kinfo) {
+  unsigned cta_size = kinfo->threads_per_cta();
+  if (cta_size > m_shader_config->n_thread_per_shader) {
+    printf(
+        "Execution error: Shader kernel CTA (block) size is too large for "
+        "microarch config.\n");
+    printf("                 CTA size (x*y*z) = %u, max supported = %u\n",
+           cta_size, m_shader_config->n_thread_per_shader);
+    printf(
+        "                 => either change -gpgpu_shader argument in "
+        "gpgpusim.config file or\n");
+    printf(
+        "                 modify the CUDA source to decrease the kernel block "
+        "size.\n");
+    abort();
+  }
+  unsigned n = 0;
+  for (n = 0; n < m_running_kernels.size(); n++) {
+    if ((NULL == m_running_kernels[n]) || m_running_kernels[n]->done()) {
+      m_running_kernels[n] = kinfo;
+      break;
+    }
+  }
+  assert(n < m_running_kernels.size());
 }
 
-void gpgpu_sim::launch( kernel_info_t *kinfo )
-{
-   unsigned cta_size = kinfo->threads_per_cta();
-   if ( cta_size > m_shader_config->n_thread_per_shader ) {
-      printf("Execution error: Shader kernel CTA (block) size is too large for microarch config.\n");
-      printf("                 CTA size (x*y*z) = %u, max supported = %u\n", cta_size, 
-             m_shader_config->n_thread_per_shader );
-      printf("                 => either change -gpgpu_shader argument in gpgpusim.config file or\n");
-      printf("                 modify the CUDA source to decrease the kernel block size.\n");
-      abort();
-   }
-   unsigned n=0;
-   for(n=0; n < m_running_kernels.size(); n++ ) {
-       if( (NULL==m_running_kernels[n]) || m_running_kernels[n]->done() ) {
-           m_running_kernels[n] = kinfo;
-           break;
-       }
-   }
-   assert(n < m_running_kernels.size());
-}
-
-bool gpgpu_sim::can_start_kernel()
-{
-   for(unsigned n=0; n < m_running_kernels.size(); n++ ) {
-       if( (NULL==m_running_kernels[n]) || m_running_kernels[n]->done() ) 
-           return true;
-   }
-   return false;
+bool gpgpu_sim::can_start_kernel() {
+  for (unsigned n = 0; n < m_running_kernels.size(); n++) {
+    if ((NULL == m_running_kernels[n]) || m_running_kernels[n]->done())
+      return true;
+  }
+  return false;
 }
 
 bool gpgpu_sim::hit_max_cta_count() const {
-   if (m_config.gpu_max_cta_opt != 0) {
-      if( (gpu_tot_issued_cta + m_total_cta_launched) >= m_config.gpu_max_cta_opt )
-          return true;
-   }
-   return false;
+  if (m_config.gpu_max_cta_opt != 0) {
+    if ((gpu_tot_issued_cta + m_total_cta_launched) >= m_config.gpu_max_cta_opt)
+      return true;
+  }
+  return false;
 }
 
 bool gpgpu_sim::kernel_more_cta_left(kernel_info_t *kernel) const {
-    if(hit_max_cta_count())
-       return false;
+  if (hit_max_cta_count()) return false;
 
-    if(kernel && !kernel->no_more_ctas_to_run())
-        return true;
+  if (kernel && !kernel->no_more_ctas_to_run()) return true;
 
-    return false;
+  return false;
 }
 
-bool gpgpu_sim::get_more_cta_left() const
-{ 
-   if(hit_max_cta_count())
-      return false;
+bool gpgpu_sim::get_more_cta_left() const {
+  if (hit_max_cta_count()) return false;
 
-   for(unsigned n=0; n < m_running_kernels.size(); n++ ) {
-       if( m_running_kernels[n] && !m_running_kernels[n]->no_more_ctas_to_run() ) 
-           return true;
-   }
-   return false;
+  for (unsigned n = 0; n < m_running_kernels.size(); n++) {
+    if (m_running_kernels[n] && !m_running_kernels[n]->no_more_ctas_to_run())
+      return true;
+  }
+  return false;
 }
 
-kernel_info_t *gpgpu_sim::select_kernel()
-{
-    if(m_running_kernels[m_last_issued_kernel] &&
-        !m_running_kernels[m_last_issued_kernel]->no_more_ctas_to_run()) {
-        unsigned launch_uid = m_running_kernels[m_last_issued_kernel]->get_uid(); 
-        if(std::find(m_executed_kernel_uids.begin(), m_executed_kernel_uids.end(), launch_uid) == m_executed_kernel_uids.end()) {
-            m_running_kernels[m_last_issued_kernel]->start_cycle = gpu_sim_cycle + gpu_tot_sim_cycle;
-            m_executed_kernel_uids.push_back(launch_uid); 
-            m_executed_kernel_names.push_back(m_running_kernels[m_last_issued_kernel]->name()); 
-        }
-        return m_running_kernels[m_last_issued_kernel];
+void gpgpu_sim::decrement_kernel_latency() {
+  for (unsigned n = 0; n < m_running_kernels.size(); n++) {
+    if (m_running_kernels[n] && m_running_kernels[n]->m_kernel_TB_latency) {
+      m_running_kernels[n]->m_kernel_TB_latency--;
+      printf("decrese kernel TB latency %d\n", m_running_kernels[n]->m_kernel_TB_latency);
     }
-
-    for(unsigned n=0; n < m_running_kernels.size(); n++ ) {
-        unsigned idx = (n+m_last_issued_kernel+1)%m_config.max_concurrent_kernel;
-        if( kernel_more_cta_left(m_running_kernels[idx]) ){
-            m_last_issued_kernel=idx;
-            m_running_kernels[idx]->start_cycle = gpu_sim_cycle + gpu_tot_sim_cycle;
-            // record this kernel for stat print if it is the first time this kernel is selected for execution  
-            unsigned launch_uid = m_running_kernels[idx]->get_uid(); 
-            assert(std::find(m_executed_kernel_uids.begin(), m_executed_kernel_uids.end(), launch_uid) == m_executed_kernel_uids.end());
-            m_executed_kernel_uids.push_back(launch_uid); 
-            m_executed_kernel_names.push_back(m_running_kernels[idx]->name()); 
-
-            return m_running_kernels[idx];
-        }
+  }
+}
+
+kernel_info_t *gpgpu_sim::select_kernel() {
+  if (m_running_kernels[m_last_issued_kernel] &&
+      !m_running_kernels[m_last_issued_kernel]->no_more_ctas_to_run() &&
+      !m_running_kernels[m_last_issued_kernel]->m_kernel_TB_latency) {
+    unsigned launch_uid = m_running_kernels[m_last_issued_kernel]->get_uid();
+    if (std::find(m_executed_kernel_uids.begin(), m_executed_kernel_uids.end(),
+                  launch_uid) == m_executed_kernel_uids.end()) {
+      m_running_kernels[m_last_issued_kernel]->start_cycle =
+          gpu_sim_cycle + gpu_tot_sim_cycle;
+      m_executed_kernel_uids.push_back(launch_uid);
+      m_executed_kernel_names.push_back(
+          m_running_kernels[m_last_issued_kernel]->name());
     }
-    return NULL;
-}
-
-unsigned gpgpu_sim::finished_kernel()
-{
-    // This should never be called now
-    assert(0);
-    if( m_finished_kernel.empty() ) 
-        return 0;
-    unsigned result = m_finished_kernel.front();
-    m_finished_kernel.pop_front();
-    return result;
-}
-
-void gpgpu_sim::set_kernel_done( kernel_info_t *kernel ) 
-{ 
-    unsigned uid = kernel->get_uid();
-    // TODO schi
-    // m_finished_kernel.push_back(uid);
-    gem5CudaGPU->finishKernel(uid);
-    std::vector<kernel_info_t*>::iterator k;
-    for( k=m_running_kernels.begin(); k!=m_running_kernels.end(); k++ ) {
-        if( *k == kernel ) {
-            kernel->end_cycle = gpu_sim_cycle + gpu_tot_sim_cycle;
-            *k = NULL;
-            break;
-        }
+    return m_running_kernels[m_last_issued_kernel];
+  }
+
+  for (unsigned n = 0; n < m_running_kernels.size(); n++) {
+    unsigned idx =
+        (n + m_last_issued_kernel + 1) % m_config.max_concurrent_kernel;
+    if (kernel_more_cta_left(m_running_kernels[idx]) &&
+        !m_running_kernels[idx]->m_kernel_TB_latency) {
+      m_last_issued_kernel = idx;
+      m_running_kernels[idx]->start_cycle = gpu_sim_cycle + gpu_tot_sim_cycle;
+      // record this kernel for stat print if it is the first time this kernel
+      // is selected for execution
+      unsigned launch_uid = m_running_kernels[idx]->get_uid();
+      assert(std::find(m_executed_kernel_uids.begin(),
+                       m_executed_kernel_uids.end(),
+                       launch_uid) == m_executed_kernel_uids.end());
+      m_executed_kernel_uids.push_back(launch_uid);
+      m_executed_kernel_names.push_back(m_running_kernels[idx]->name());
+
+      return m_running_kernels[idx];
+    }
+  }
+  return NULL;
+}
+
+unsigned gpgpu_sim::finished_kernel() {
+  if (m_finished_kernel.empty()) return 0;
+  unsigned result = m_finished_kernel.front();
+  m_finished_kernel.pop_front();
+  return result;
+}
+
+void gpgpu_sim::set_kernel_done(kernel_info_t *kernel) {
+  unsigned uid = kernel->get_uid();
+  // TODO schi
+  // m_finished_kernel.push_back(uid);
+  gem5CudaGPU->finishKernel(uid);
+  std::vector<kernel_info_t *>::iterator k;
+  for (k = m_running_kernels.begin(); k != m_running_kernels.end(); k++) {
+    if (*k == kernel) {
+      kernel->end_cycle = gpu_sim_cycle + gpu_tot_sim_cycle;
+      *k = NULL;
+      break;
     }
-    assert( k != m_running_kernels.end() ); 
+  }
+  assert(k != m_running_kernels.end());
 }
 
-void gpgpu_sim::stop_all_running_kernels(){
-    std::vector<kernel_info_t *>::iterator k;
-    for(k = m_running_kernels.begin(); k != m_running_kernels.end(); ++k){
-        if(*k != NULL){ // If a kernel is active
-            set_kernel_done(*k); // Stop the kernel
-            assert(*k==NULL);
-        }
+void gpgpu_sim::stop_all_running_kernels() {
+  std::vector<kernel_info_t *>::iterator k;
+  for (k = m_running_kernels.begin(); k != m_running_kernels.end(); ++k) {
+    if (*k != NULL) {       // If a kernel is active
+      set_kernel_done(*k);  // Stop the kernel
+      assert(*k == NULL);
     }
+  }
 }
 
-void set_ptx_warp_size(const struct core_config * warp_size);
+void exec_gpgpu_sim::createSIMTCluster() {
+  m_cluster = new simt_core_cluster *[m_shader_config->n_simt_clusters];
+  for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
+    m_cluster[i] =
+        new exec_simt_core_cluster(this, i, m_shader_config, m_memory_config,
+                                   m_shader_stats, m_memory_stats);
+}
 
 // TODO schi gpgpu_sim::gpgpu_sim( const gpgpu_sim_config &config ) 
-gpgpu_sim::gpgpu_sim( const gpgpu_sim_config &config, gem5::CudaGPU *cuda_gpu )
-    : gpgpu_t(config, cuda_gpu), m_config(config)
-{ 
-    m_shader_config = &m_config.m_shader_config;
-    m_memory_config = &m_config.m_memory_config;
-    set_ptx_warp_size(m_shader_config);
-    ptx_file_line_stats_create_exposed_latency_tracker(m_config.num_shader());
+gpgpu_sim::gpgpu_sim(const gpgpu_sim_config &config, gpgpu_context *ctx, gem5::CudaGPU *cuda_gpu)
+    : gpgpu_t(config, ctx, cuda_gpu), m_config(config)
+{
+  gpgpu_ctx = ctx;
+  m_shader_config = &m_config.m_shader_config;
+  m_memory_config = &m_config.m_memory_config;
+  ctx->ptx_parser->set_ptx_warp_size(m_shader_config);
+  ptx_file_line_stats_create_exposed_latency_tracker(m_config.num_shader());
 
 #ifdef GPGPUSIM_POWER_MODEL
-        m_gpgpusim_wrapper = new gpgpu_sim_wrapper(config.g_power_simulation_enabled,config.g_power_config_name);
+  m_gpgpusim_wrapper = new gpgpu_sim_wrapper(config.g_power_simulation_enabled,
+                                             config.g_power_config_name, config.g_power_simulation_mode, config.g_dvfs_enabled);
 #endif
 
-    m_shader_stats = new shader_core_stats(m_shader_config);
-    m_memory_stats = new memory_stats_t(m_config.num_shader(),m_shader_config,m_memory_config);
-    average_pipeline_duty_cycle = (float *)malloc(sizeof(float));
-    active_sms=(float *)malloc(sizeof(float));
-    // m_power_stats = new power_stat_t(m_shader_config,average_pipeline_duty_cycle,active_sms,m_shader_stats,m_memory_config,m_memory_stats);
-
-    gpu_sim_insn = 0;
-    gpu_tot_sim_insn = 0;
-    gpu_tot_issued_cta = 0;
-    m_total_cta_launched = 0;
-    gpu_deadlock = false;
-
-
-    m_cluster = new simt_core_cluster*[m_shader_config->n_simt_clusters];
-    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) 
-        m_cluster[i] = new simt_core_cluster(this,i,m_shader_config,m_memory_config,m_shader_stats,m_memory_stats);
-
-    m_memory_partition_unit = new memory_partition_unit*[m_memory_config->m_n_mem];
-    m_memory_sub_partition = new memory_sub_partition*[m_memory_config->m_n_mem_sub_partition];
-    for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
-        m_memory_partition_unit[i] = new memory_partition_unit(i, m_memory_config, m_memory_stats);
-        for (unsigned p = 0; p < m_memory_config->m_n_sub_partition_per_memory_channel; p++) {
-            unsigned submpid = i * m_memory_config->m_n_sub_partition_per_memory_channel + p; 
-            m_memory_sub_partition[submpid] = m_memory_partition_unit[i]->get_sub_partition(p); 
-        }
+  m_shader_stats = new shader_core_stats(m_shader_config);
+  m_memory_stats = new memory_stats_t(m_config.num_shader(), m_shader_config,
+                                      m_memory_config, this);
+  average_pipeline_duty_cycle = (float *)malloc(sizeof(float));
+  active_sms = (float *)malloc(sizeof(float));
+  m_power_stats =
+      new power_stat_t(m_shader_config, average_pipeline_duty_cycle, active_sms,
+                       m_shader_stats, m_memory_config, m_memory_stats);
+
+  gpu_sim_insn = 0;
+  gpu_tot_sim_insn = 0;
+  gpu_tot_issued_cta = 0;
+  gpu_completed_cta = 0;
+  m_total_cta_launched = 0;
+  gpu_deadlock = false;
+
+  gpu_stall_dramfull = 0;
+  gpu_stall_icnt2sh = 0;
+  partiton_reqs_in_parallel = 0;
+  partiton_reqs_in_parallel_total = 0;
+  partiton_reqs_in_parallel_util = 0;
+  partiton_reqs_in_parallel_util_total = 0;
+  gpu_sim_cycle_parition_util = 0;
+  gpu_tot_sim_cycle_parition_util = 0;
+  partiton_replys_in_parallel = 0;
+  partiton_replys_in_parallel_total = 0;
+
+  m_memory_partition_unit =
+      new memory_partition_unit *[m_memory_config->m_n_mem];
+  m_memory_sub_partition =
+      new memory_sub_partition *[m_memory_config->m_n_mem_sub_partition];
+  for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
+    m_memory_partition_unit[i] =
+        new memory_partition_unit(i, m_memory_config, m_memory_stats, this);
+    for (unsigned p = 0;
+         p < m_memory_config->m_n_sub_partition_per_memory_channel; p++) {
+      unsigned submpid =
+          i * m_memory_config->m_n_sub_partition_per_memory_channel + p;
+      m_memory_sub_partition[submpid] =
+          m_memory_partition_unit[i]->get_sub_partition(p);
     }
+  }
 
-    icnt_wrapper_init();
-    icnt_create(m_shader_config->n_simt_clusters,m_memory_config->m_n_mem_sub_partition);
+  icnt_wrapper_init();
+  icnt_create(m_shader_config->n_simt_clusters,
+              m_memory_config->m_n_mem_sub_partition);
 
-    time_vector_create(NUM_MEM_REQ_STAT);
-    fprintf(stdout, "GPGPU-Sim uArch: performance model initialization complete.\n");
+  time_vector_create(NUM_MEM_REQ_STAT);
+  fprintf(stdout,
+          "GPGPU-Sim uArch: performance model initialization complete.\n");
 
-    m_running_kernels.resize( config.max_concurrent_kernel, NULL );
-    m_last_issued_kernel = 0;
-    m_last_cluster_issue = m_shader_config->n_simt_clusters-1; // this causes first launch to use simt cluster 0
-    *average_pipeline_duty_cycle=0;
-    *active_sms=0;
+  m_running_kernels.resize(config.max_concurrent_kernel, NULL);
+  m_last_issued_kernel = 0;
+  m_last_cluster_issue = m_shader_config->n_simt_clusters -
+                         1;  // this causes first launch to use simt cluster 0
+  *average_pipeline_duty_cycle = 0;
+  *active_sms = 0;
 
-    last_liveness_message_time = 0;
-   
-   //Jin: functional simulation for CDP
-   m_functional_sim = false;
-   m_functional_sim_kernel = NULL;
-}
+  last_liveness_message_time = 0;
 
-int gpgpu_sim::shared_mem_size() const
-{
-   return m_shader_config->gpgpu_shmem_size;
+  // Jin: functional simulation for CDP
+  m_functional_sim = false;
+  m_functional_sim_kernel = NULL;
 }
 
-int gpgpu_sim::shared_mem_per_block() const
-{
-   return m_shader_config->gpgpu_shmem_per_block;
+int gpgpu_sim::shared_mem_size() const {
+  return m_shader_config->gpgpu_shmem_size;
 }
 
-int gpgpu_sim::num_registers_per_core() const
-{
-   return m_shader_config->gpgpu_shader_registers;
+int gpgpu_sim::shared_mem_per_block() const {
+  return m_shader_config->gpgpu_shmem_per_block;
 }
 
-int gpgpu_sim::num_registers_per_block() const
-{
-   return m_shader_config->gpgpu_registers_per_block;
+int gpgpu_sim::num_registers_per_core() const {
+  return m_shader_config->gpgpu_shader_registers;
 }
 
-int gpgpu_sim::wrp_size() const
-{
-   return m_shader_config->warp_size;
+int gpgpu_sim::num_registers_per_block() const {
+  return m_shader_config->gpgpu_registers_per_block;
 }
 
-int gpgpu_sim::shader_clock() const
-{
-   return m_config.core_freq/1000;
-}
+int gpgpu_sim::wrp_size() const { return m_shader_config->warp_size; }
 
-void gpgpu_sim::set_prop( cudaDeviceProp *prop )
-{
-   m_cuda_properties = prop;
-}
+int gpgpu_sim::shader_clock() const { return m_config.core_freq / 1000; }
 
-int gpgpu_sim::compute_capability_major() const
-{
-   return m_config.gpgpu_compute_capability_major;
+int gpgpu_sim::max_cta_per_core() const {
+  return m_shader_config->max_cta_per_core;
 }
 
-int gpgpu_sim::compute_capability_minor() const
-{
-   return m_config.gpgpu_compute_capability_minor;
+int gpgpu_sim::get_max_cta(const kernel_info_t &k) const {
+  return m_shader_config->max_cta(k);
 }
 
-const struct cudaDeviceProp *gpgpu_sim::get_prop() const
-{
-   return m_cuda_properties;
+void gpgpu_sim::set_prop(cudaDeviceProp *prop) { m_cuda_properties = prop; }
+
+int gpgpu_sim::compute_capability_major() const {
+  return m_config.gpgpu_compute_capability_major;
 }
 
-enum divergence_support_t gpgpu_sim::simd_model() const
-{
-   return m_shader_config->model;
+int gpgpu_sim::compute_capability_minor() const {
+  return m_config.gpgpu_compute_capability_minor;
 }
 
-void gpgpu_sim_config::init_clock_domains(void ) 
-{
-   sscanf(gpgpu_clock_domains,"%lf:%lf:%lf:%lf", 
-          &core_freq, &icnt_freq, &l2_freq, &dram_freq);
-   core_freq = core_freq MhZ;
-   icnt_freq = icnt_freq MhZ;
-   l2_freq = l2_freq MhZ;
-   dram_freq = dram_freq MhZ;        
-   core_period = 1/core_freq;
-   icnt_period = 1/icnt_freq;
-   dram_period = 1/dram_freq;
-   l2_period = 1/l2_freq;
-   printf("GPGPU-Sim uArch: clock freqs: %lf:%lf:%lf:%lf\n",core_freq,icnt_freq,l2_freq,dram_freq);
-   printf("GPGPU-Sim uArch: clock periods: %.20lf:%.20lf:%.20lf:%.20lf\n",core_period,icnt_period,l2_period,dram_period);
-}
-
-void gpgpu_sim::reinit_clock_domains(void)
-{
-   core_time = 0;
-   dram_time = 0;
-   icnt_time = 0;
-   l2_time = 0;
+const struct cudaDeviceProp *gpgpu_sim::get_prop() const {
+  return m_cuda_properties;
 }
 
-bool gpgpu_sim::active()
-{
-    if (m_config.gpu_max_cycle_opt && (gpu_tot_sim_cycle + gpu_sim_cycle) >= m_config.gpu_max_cycle_opt) 
-       return false;
-    if (m_config.gpu_max_insn_opt && (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt) 
-       return false;
-    if (m_config.gpu_max_cta_opt && (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt) )
-       return false;
-    if (m_config.gpu_deadlock_detect && gpu_deadlock) 
-       return false;
-    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) 
-       if( m_cluster[i]->get_not_completed()>0 ) 
-           return true;;
-    for (unsigned i=0;i<m_memory_config->m_n_mem;i++) 
-       if( m_memory_partition_unit[i]->busy()>0 )
-           return true;;
-    if( icnt_busy() )
-        return true;
-    if( get_more_cta_left() )
-        return true;
-    return false;
+enum divergence_support_t gpgpu_sim::simd_model() const {
+  return m_shader_config->model;
 }
 
-void gpgpu_sim::init()
-{
-    // run a CUDA grid on the GPU microarchitecture simulator
-    gpu_sim_cycle = 0;
-    gpu_sim_insn = 0;
-    last_gpu_sim_insn = 0;
-    m_total_cta_launched=0;
-    partiton_reqs_in_parallel = 0;
-    partiton_replys_in_parallel = 0;
-    partiton_reqs_in_parallel_util = 0;
-    gpu_sim_cycle_parition_util = 0;
-
-    reinit_clock_domains();
-    set_param_gpgpu_num_shaders(m_config.num_shader());
-    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) 
-       m_cluster[i]->reinit();
-    m_shader_stats->new_grid();
-    // initialize the control-flow, memory access, memory latency logger
-    if (m_config.g_visualizer_enabled) {
-        create_thread_CFlogger( m_config.num_shader(), m_shader_config->n_thread_per_shader, 0, m_config.gpgpu_cflog_interval );
-    }
-    shader_CTA_count_create( m_config.num_shader(), m_config.gpgpu_cflog_interval);
-    if (m_config.gpgpu_cflog_interval != 0) {
-       insn_warp_occ_create( m_config.num_shader(), m_shader_config->warp_size );
-       shader_warp_occ_create( m_config.num_shader(), m_shader_config->warp_size, m_config.gpgpu_cflog_interval);
-       shader_mem_acc_create( m_config.num_shader(), m_memory_config->m_n_mem, 4, m_config.gpgpu_cflog_interval);
-       shader_mem_lat_create( m_config.num_shader(), m_config.gpgpu_cflog_interval);
-       shader_cache_access_create( m_config.num_shader(), 3, m_config.gpgpu_cflog_interval);
-       set_spill_interval (m_config.gpgpu_cflog_interval * 40);
-    }
+void gpgpu_sim_config::init_clock_domains(void) {
+  sscanf(gpgpu_clock_domains, "%lf:%lf:%lf:%lf", &core_freq, &icnt_freq,
+         &l2_freq, &dram_freq);
+  core_freq = core_freq MhZ;
+  icnt_freq = icnt_freq MhZ;
+  l2_freq = l2_freq MhZ;
+  dram_freq = dram_freq MhZ;
+  core_period = 1 / core_freq;
+  icnt_period = 1 / icnt_freq;
+  dram_period = 1 / dram_freq;
+  l2_period = 1 / l2_freq;
+  printf("GPGPU-Sim uArch: clock freqs: %lf:%lf:%lf:%lf\n", core_freq,
+         icnt_freq, l2_freq, dram_freq);
+  printf("GPGPU-Sim uArch: clock periods: %.20lf:%.20lf:%.20lf:%.20lf\n",
+         core_period, icnt_period, l2_period, dram_period);
+}
 
-    if (g_network_mode)
-       icnt_init();
+void gpgpu_sim::reinit_clock_domains(void) {
+  core_time = 0;
+  dram_time = 0;
+  icnt_time = 0;
+  l2_time = 0;
+}
 
-    // McPAT initialization function. Called on first launch of GPU
+bool gpgpu_sim::active() {
+  if (m_config.gpu_max_cycle_opt &&
+      (gpu_tot_sim_cycle + gpu_sim_cycle) >= m_config.gpu_max_cycle_opt)
+    return false;
+  if (m_config.gpu_max_insn_opt &&
+      (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt)
+    return false;
+  if (m_config.gpu_max_cta_opt &&
+      (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt))
+    return false;
+  if (m_config.gpu_max_completed_cta_opt &&
+      (gpu_completed_cta >= m_config.gpu_max_completed_cta_opt))
+    return false;
+  if (m_config.gpu_deadlock_detect && gpu_deadlock) return false;
+  for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
+    if (m_cluster[i]->get_not_completed() > 0) return true;
+  ;
+  for (unsigned i = 0; i < m_memory_config->m_n_mem; i++)
+    if (m_memory_partition_unit[i]->busy() > 0) return true;
+  ;
+  if (icnt_busy()) return true;
+  if (get_more_cta_left()) return true;
+  return false;
+}
+
+void gpgpu_sim::init() {
+  // run a CUDA grid on the GPU microarchitecture simulator
+  gpu_sim_cycle = 0;
+  gpu_sim_insn = 0;
+  last_gpu_sim_insn = 0;
+  m_total_cta_launched = 0;
+  gpu_completed_cta = 0;
+  partiton_reqs_in_parallel = 0;
+  partiton_replys_in_parallel = 0;
+  partiton_reqs_in_parallel_util = 0;
+  gpu_sim_cycle_parition_util = 0;
+
+// McPAT initialization function. Called on first launch of GPU
 #ifdef GPGPUSIM_POWER_MODEL
-    if(m_config.g_power_simulation_enabled){
-        init_mcpat(m_config, m_gpgpusim_wrapper, m_config.gpu_stat_sample_freq,  gpu_tot_sim_insn, gpu_sim_insn);
-    }
+  if (m_config.g_power_simulation_enabled) {
+    init_mcpat(m_config, m_gpgpusim_wrapper, m_config.gpu_stat_sample_freq,
+               gpu_tot_sim_insn, gpu_sim_insn);
+  }
 #endif
+
+  reinit_clock_domains();
+  gpgpu_ctx->func_sim->set_param_gpgpu_num_shaders(m_config.num_shader());
+  for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
+    m_cluster[i]->reinit();
+  m_shader_stats->new_grid();
+  // initialize the control-flow, memory access, memory latency logger
+  if (m_config.g_visualizer_enabled) {
+    create_thread_CFlogger(gpgpu_ctx, m_config.num_shader(),
+                           m_shader_config->n_thread_per_shader, 0,
+                           m_config.gpgpu_cflog_interval);
+  }
+  shader_CTA_count_create(m_config.num_shader(), m_config.gpgpu_cflog_interval);
+  if (m_config.gpgpu_cflog_interval != 0) {
+    insn_warp_occ_create(m_config.num_shader(), m_shader_config->warp_size);
+    shader_warp_occ_create(m_config.num_shader(), m_shader_config->warp_size,
+                           m_config.gpgpu_cflog_interval);
+    shader_mem_acc_create(m_config.num_shader(), m_memory_config->m_n_mem, 4,
+                          m_config.gpgpu_cflog_interval);
+    shader_mem_lat_create(m_config.num_shader(), m_config.gpgpu_cflog_interval);
+    shader_cache_access_create(m_config.num_shader(), 3,
+                               m_config.gpgpu_cflog_interval);
+    set_spill_interval(m_config.gpgpu_cflog_interval * 40);
+  }
+
+  if (g_network_mode) icnt_init();
 }
 
 void gpgpu_sim::update_stats() {
-    m_memory_stats->memlatstat_lat_pw();
-    gpu_tot_sim_cycle += gpu_sim_cycle;
-    gpu_tot_sim_insn += gpu_sim_insn;
-    gpu_tot_issued_cta += m_total_cta_launched;
-    partiton_reqs_in_parallel_total += partiton_reqs_in_parallel;
-    partiton_replys_in_parallel_total += partiton_replys_in_parallel;
-    partiton_reqs_in_parallel_util_total += partiton_reqs_in_parallel_util;
-    gpu_tot_sim_cycle_parition_util += gpu_sim_cycle_parition_util ;
-    gpu_tot_occupancy += gpu_occupancy;
-
-    gpu_sim_cycle = 0;
-    partiton_reqs_in_parallel = 0;
-    partiton_replys_in_parallel = 0;
-    partiton_reqs_in_parallel_util = 0;
-    gpu_sim_cycle_parition_util = 0;
-    gpu_sim_insn = 0;
-    m_total_cta_launched = 0;
-    gpu_occupancy = occupancy_stats();
-}
-
-void gpgpu_sim::print_stats()
+  m_memory_stats->memlatstat_lat_pw();
+  gpu_tot_sim_cycle += gpu_sim_cycle;
+  gpu_tot_sim_insn += gpu_sim_insn;
+  gpu_tot_issued_cta += m_total_cta_launched;
+  partiton_reqs_in_parallel_total += partiton_reqs_in_parallel;
+  partiton_replys_in_parallel_total += partiton_replys_in_parallel;
+  partiton_reqs_in_parallel_util_total += partiton_reqs_in_parallel_util;
+  gpu_tot_sim_cycle_parition_util += gpu_sim_cycle_parition_util;
+  gpu_tot_occupancy += gpu_occupancy;
+
+  gpu_sim_cycle = 0;
+  partiton_reqs_in_parallel = 0;
+  partiton_replys_in_parallel = 0;
+  partiton_reqs_in_parallel_util = 0;
+  gpu_sim_cycle_parition_util = 0;
+  gpu_sim_insn = 0;
+  m_total_cta_launched = 0;
+  gpu_completed_cta = 0;
+  gpu_occupancy = occupancy_stats();
+}
+
+PowerscalingCoefficients *gpgpu_sim::get_scaling_coeffs()
 {
-    ptx_file_line_stats_write_file();
-    gpu_print_stat();
-
-    if (g_network_mode) {
-        printf("----------------------------Interconnect-DETAILS--------------------------------\n" );
-        icnt_display_stats();
-        icnt_display_overall_stats();
-        printf("----------------------------END-of-Interconnect-DETAILS-------------------------\n" );
-    }
+#ifdef GPGPUSIM_POWER_MODEL
+  return m_gpgpusim_wrapper->get_scaling_coeffs();
+#endif
 }
 
-void gpgpu_sim::deadlock_check()
-{
-   if (m_config.gpu_deadlock_detect && gpu_deadlock) {
-      fflush(stdout);
-      printf("\n\nGPGPU-Sim uArch: ERROR ** deadlock detected: last writeback core %u @ gpu_sim_cycle %u (+ gpu_tot_sim_cycle %u) (%u cycles ago)\n", 
-             gpu_sim_insn_last_update_sid,
-             (unsigned) gpu_sim_insn_last_update, (unsigned) (gpu_tot_sim_cycle-gpu_sim_cycle),
-             (unsigned) (gpu_sim_cycle - gpu_sim_insn_last_update )); 
-      unsigned num_cores=0;
-      for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-         unsigned not_completed = m_cluster[i]->get_not_completed();
-         if( not_completed ) {
-             if ( !num_cores )  {
-                 printf("GPGPU-Sim uArch: DEADLOCK  shader cores no longer committing instructions [core(# threads)]:\n" );
-                 printf("GPGPU-Sim uArch: DEADLOCK  ");
-                 m_cluster[i]->print_not_completed(stdout);
-             } else if (num_cores < 8 ) {
-                 m_cluster[i]->print_not_completed(stdout);
-             } else if (num_cores >= 8 ) {
-                 printf(" + others ... ");
-             }
-             num_cores+=m_shader_config->n_simt_cores_per_cluster;
-         }
-      }
-      printf("\n");
-      for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
-         bool busy = m_memory_partition_unit[i]->busy();
-         if( busy ) 
-             printf("GPGPU-Sim uArch DEADLOCK:  memory partition %u busy\n", i );
-      }
-      if( icnt_busy() ) {
-         printf("GPGPU-Sim uArch DEADLOCK:  iterconnect contains traffic\n");
-         icnt_display_state( stdout );
+void gpgpu_sim::print_stats() {
+  gpgpu_ctx->stats->ptx_file_line_stats_write_file();
+  gpu_print_stat();
+
+  if (g_network_mode) {
+    printf(
+        "----------------------------Interconnect-DETAILS----------------------"
+        "----------\n");
+    icnt_display_stats();
+    icnt_display_overall_stats();
+    printf(
+        "----------------------------END-of-Interconnect-DETAILS---------------"
+        "----------\n");
+  }
+}
+
+void gpgpu_sim::deadlock_check() {
+  if (m_config.gpu_deadlock_detect && gpu_deadlock) {
+    fflush(stdout);
+    printf(
+        "\n\nGPGPU-Sim uArch: ERROR ** deadlock detected: last writeback core "
+        "%u @ gpu_sim_cycle %u (+ gpu_tot_sim_cycle %u) (%u cycles ago)\n",
+        gpu_sim_insn_last_update_sid, (unsigned)gpu_sim_insn_last_update,
+        (unsigned)(gpu_tot_sim_cycle - gpu_sim_cycle),
+        (unsigned)(gpu_sim_cycle - gpu_sim_insn_last_update));
+    unsigned num_cores = 0;
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+      unsigned not_completed = m_cluster[i]->get_not_completed();
+      if (not_completed) {
+        if (!num_cores) {
+          printf(
+              "GPGPU-Sim uArch: DEADLOCK  shader cores no longer committing "
+              "instructions [core(# threads)]:\n");
+          printf("GPGPU-Sim uArch: DEADLOCK  ");
+          m_cluster[i]->print_not_completed(stdout);
+        } else if (num_cores < 8) {
+          m_cluster[i]->print_not_completed(stdout);
+        } else if (num_cores >= 8) {
+          printf(" + others ... ");
+        }
+        num_cores += m_shader_config->n_simt_cores_per_cluster;
       }
-      printf("\nRe-run the simulator in gdb and use debug routines in .gdbinit to debug this\n");
-      fflush(stdout);
-      abort();
-   }
+    }
+    printf("\n");
+    for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
+      bool busy = m_memory_partition_unit[i]->busy();
+      if (busy)
+        printf("GPGPU-Sim uArch DEADLOCK:  memory partition %u busy\n", i);
+    }
+    if (icnt_busy()) {
+      printf("GPGPU-Sim uArch DEADLOCK:  iterconnect contains traffic\n");
+      icnt_display_state(stdout);
+    }
+    printf(
+        "\nRe-run the simulator in gdb and use debug routines in .gdbinit to "
+        "debug this\n");
+    fflush(stdout);
+    abort();
+  }
+}
+
+/// printing the names and uids of a set of executed kernels (usually there is
+/// only one)
+std::string gpgpu_sim::executed_kernel_info_string() {
+  std::stringstream statout;
+
+  statout << "kernel_name = ";
+  for (unsigned int k = 0; k < m_executed_kernel_names.size(); k++) {
+    statout << m_executed_kernel_names[k] << " ";
+  }
+  statout << std::endl;
+  statout << "kernel_launch_uid = ";
+  for (unsigned int k = 0; k < m_executed_kernel_uids.size(); k++) {
+    statout << m_executed_kernel_uids[k] << " ";
+  }
+  statout << std::endl;
+
+  return statout.str();
+}
+
+std::string gpgpu_sim::executed_kernel_name() {
+  std::stringstream statout;  
+  if( m_executed_kernel_names.size() == 1)
+     statout << m_executed_kernel_names[0];
+  else{
+    for (unsigned int k = 0; k < m_executed_kernel_names.size(); k++) {
+      statout << m_executed_kernel_names[k] << " ";
+    }
+  }
+  return statout.str();
 }
-
-/// printing the names and uids of a set of executed kernels (usually there is only one)
-std::string gpgpu_sim::executed_kernel_info_string() 
-{
-   std::stringstream statout; 
-
-   statout << "kernel_name = "; 
-   for (unsigned int k = 0; k < m_executed_kernel_names.size(); k++) {
-      statout << m_executed_kernel_names[k] << " "; 
-   }
-   statout << std::endl; 
-   statout << "kernel_launch_uid = ";
-   for (unsigned int k = 0; k < m_executed_kernel_uids.size(); k++) {
-      statout << m_executed_kernel_uids[k] << " "; 
-   }
-   statout << std::endl; 
-
-   return statout.str(); 
-}
-void gpgpu_sim::set_cache_config(std::string kernel_name,  FuncCache cacheConfig )
-{
-	m_special_cache_config[kernel_name]=cacheConfig ;
+void gpgpu_sim::set_cache_config(std::string kernel_name,
+                                 FuncCache cacheConfig) {
+  m_special_cache_config[kernel_name] = cacheConfig;
 }
 
-FuncCache gpgpu_sim::get_cache_config(std::string kernel_name)
-{
-	for (	std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++){
-		    std::string kernel= iter->first;
-			if (kernel_name.compare(kernel) == 0){
-				return iter->second;
-			}
-	}
-	return (FuncCache)0;
+FuncCache gpgpu_sim::get_cache_config(std::string kernel_name) {
+  for (std::map<std::string, FuncCache>::iterator iter =
+           m_special_cache_config.begin();
+       iter != m_special_cache_config.end(); iter++) {
+    std::string kernel = iter->first;
+    if (kernel_name.compare(kernel) == 0) {
+      return iter->second;
+    }
+  }
+  return (FuncCache)0;
 }
 
-bool gpgpu_sim::has_special_cache_config(std::string kernel_name)
-{
-	for (	std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++){
-	    	std::string kernel= iter->first;
-			if (kernel_name.compare(kernel) == 0){
-				return true;
-			}
-	}
-	return false;
+bool gpgpu_sim::has_special_cache_config(std::string kernel_name) {
+  for (std::map<std::string, FuncCache>::iterator iter =
+           m_special_cache_config.begin();
+       iter != m_special_cache_config.end(); iter++) {
+    std::string kernel = iter->first;
+    if (kernel_name.compare(kernel) == 0) {
+      return true;
+    }
+  }
+  return false;
 }
 
-
-void gpgpu_sim::set_cache_config(std::string kernel_name)
-{
-	if(has_special_cache_config(kernel_name)){
-		change_cache_config(get_cache_config(kernel_name));
-	}else{
-		change_cache_config(FuncCachePreferNone);
-	}
+void gpgpu_sim::set_cache_config(std::string kernel_name) {
+  if (has_special_cache_config(kernel_name)) {
+    change_cache_config(get_cache_config(kernel_name));
+  } else {
+    change_cache_config(FuncCachePreferNone);
+  }
 }
 
-
-void gpgpu_sim::change_cache_config(FuncCache cache_config)
-{
-	if(cache_config != m_shader_config->m_L1D_config.get_cache_status()){
-		printf("FLUSH L1 Cache at configuration change between kernels\n");
-		for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-			m_cluster[i]->cache_flush();
-	    }
-	}
-
-	switch(cache_config){
-	case FuncCachePreferNone:
-		m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
-		m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
-		break;
-	case FuncCachePreferL1:
-		if((m_shader_config->m_L1D_config.m_config_stringPrefL1 == NULL) || (m_shader_config->gpgpu_shmem_sizePrefL1 == (unsigned)-1))
-		{
-			printf("WARNING: missing Preferred L1 configuration\n");
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
-
-		}else{
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefL1, FuncCachePreferL1);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizePrefL1;
-		}
-		break;
-	case FuncCachePreferShared:
-		if((m_shader_config->m_L1D_config.m_config_stringPrefShared == NULL) || (m_shader_config->gpgpu_shmem_sizePrefShared == (unsigned)-1))
-		{
-			printf("WARNING: missing Preferred L1 configuration\n");
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
-		}else{
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefShared, FuncCachePreferShared);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizePrefShared;
-		}
-		break;
-	default:
-		break;
-	}
-}
-
-
-void gpgpu_sim::clear_executed_kernel_info()
-{
-   m_executed_kernel_names.clear();
-   m_executed_kernel_uids.clear();
-}
-void gpgpu_sim::gpu_print_stat() 
-{  
-   FILE *statfout = stdout; 
-
-   std::string kernel_info_str = executed_kernel_info_string(); 
-   fprintf(statfout, "%s", kernel_info_str.c_str()); 
-
-   printf("gpu_sim_cycle = %lld\n", gpu_sim_cycle);
-   printf("gpu_sim_insn = %lld\n", gpu_sim_insn);
-   printf("gpu_ipc = %12.4f\n", (float)gpu_sim_insn / gpu_sim_cycle);
-   printf("gpu_tot_sim_cycle = %lld\n", gpu_tot_sim_cycle+gpu_sim_cycle);
-   printf("gpu_tot_sim_insn = %lld\n", gpu_tot_sim_insn+gpu_sim_insn);
-   printf("gpu_tot_ipc = %12.4f\n", (float)(gpu_tot_sim_insn+gpu_sim_insn) / (gpu_tot_sim_cycle+gpu_sim_cycle));
-   printf("gpu_tot_issued_cta = %lld\n", gpu_tot_issued_cta + m_total_cta_launched);
-   printf("gpu_occupancy = %.4f\% \n", gpu_occupancy.get_occ_fraction() * 100);
-   printf("gpu_tot_occupancy = %.4f\% \n", (gpu_occupancy + gpu_tot_occupancy).get_occ_fraction() * 100);
-
-
-   extern unsigned long long g_max_total_param_size;
-   fprintf(statfout, "max_total_param_size = %llu\n", g_max_total_param_size);
-
-   // performance counter for stalls due to congestion.
-   printf("gpu_stall_dramfull = %d\n", gpu_stall_dramfull);
-   printf("gpu_stall_icnt2sh    = %d\n", gpu_stall_icnt2sh );
-
-   //printf("partiton_reqs_in_parallel = %lld\n", partiton_reqs_in_parallel);
-   //printf("partiton_reqs_in_parallel_total    = %lld\n", partiton_reqs_in_parallel_total );
-   printf("partiton_level_parallism = %12.4f\n", (float)partiton_reqs_in_parallel / gpu_sim_cycle);
-   printf("partiton_level_parallism_total  = %12.4f\n", (float)(partiton_reqs_in_parallel+partiton_reqs_in_parallel_total) / (gpu_tot_sim_cycle+gpu_sim_cycle) );
-   //printf("partiton_reqs_in_parallel_util = %lld\n", partiton_reqs_in_parallel_util);
-   //printf("partiton_reqs_in_parallel_util_total    = %lld\n", partiton_reqs_in_parallel_util_total );
-   //printf("gpu_sim_cycle_parition_util = %lld\n", gpu_sim_cycle_parition_util);
-   // printf("gpu_tot_sim_cycle_parition_util    = %lld\n", gpu_tot_sim_cycle_parition_util );
-   printf("partiton_level_parallism_util = %12.4f\n", (float)partiton_reqs_in_parallel_util / gpu_sim_cycle_parition_util);
-   printf("partiton_level_parallism_util_total  = %12.4f\n", (float)(partiton_reqs_in_parallel_util+partiton_reqs_in_parallel_util_total) / (gpu_sim_cycle_parition_util+gpu_tot_sim_cycle_parition_util) );
-   //printf("partiton_replys_in_parallel = %lld\n", partiton_replys_in_parallel);
-   //printf("partiton_replys_in_parallel_total    = %lld\n", partiton_replys_in_parallel_total );
-   printf("L2_BW  = %12.4f GB/Sec\n", ((float)(partiton_replys_in_parallel * 32) / (gpu_sim_cycle * m_config.icnt_period)) / 1000000000);
-   printf("L2_BW_total  = %12.4f GB/Sec\n", ((float)((partiton_replys_in_parallel+partiton_replys_in_parallel_total) * 32) / ((gpu_tot_sim_cycle+gpu_sim_cycle) * m_config.icnt_period)) / 1000000000 );
-
-   time_t curr_time;
-   time(&curr_time);
-   unsigned long long elapsed_time = MAX( curr_time - g_simulation_starttime, 1 );
-   printf( "gpu_total_sim_rate=%u\n", (unsigned)( ( gpu_tot_sim_insn + gpu_sim_insn ) / elapsed_time ) );
-
-   //shader_print_l1_miss_stat( stdout );
-   shader_print_cache_stats(stdout);
-
-   cache_stats core_cache_stats;
-   core_cache_stats.clear();
-   for(unsigned i=0; i<m_config.num_cluster(); i++){
-       m_cluster[i]->get_cache_stats(core_cache_stats);
-   }
-   printf("\nTotal_core_cache_stats:\n");
-   core_cache_stats.print_stats(stdout, "Total_core_cache_stats_breakdown");
-   printf("\nTotal_core_cache_fail_stats:\n");
-   core_cache_stats.print_fail_stats(stdout, "Total_core_cache_fail_stats_breakdown");
-   shader_print_scheduler_stat( stdout, false );
-
-   m_shader_stats->print(stdout);
+void gpgpu_sim::change_cache_config(FuncCache cache_config) {
+  if (cache_config != m_shader_config->m_L1D_config.get_cache_status()) {
+    printf("FLUSH L1 Cache at configuration change between kernels\n");
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+      m_cluster[i]->cache_invalidate();
+    }
+  }
+
+  switch (cache_config) {
+    case FuncCachePreferNone:
+      m_shader_config->m_L1D_config.init(
+          m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
+      m_shader_config->gpgpu_shmem_size =
+          m_shader_config->gpgpu_shmem_sizeDefault;
+      break;
+    case FuncCachePreferL1:
+      if ((m_shader_config->m_L1D_config.m_config_stringPrefL1 == NULL) ||
+          (m_shader_config->gpgpu_shmem_sizePrefL1 == (unsigned)-1)) {
+        printf("WARNING: missing Preferred L1 configuration\n");
+        m_shader_config->m_L1D_config.init(
+            m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
+        m_shader_config->gpgpu_shmem_size =
+            m_shader_config->gpgpu_shmem_sizeDefault;
+
+      } else {
+        m_shader_config->m_L1D_config.init(
+            m_shader_config->m_L1D_config.m_config_stringPrefL1,
+            FuncCachePreferL1);
+        m_shader_config->gpgpu_shmem_size =
+            m_shader_config->gpgpu_shmem_sizePrefL1;
+      }
+      break;
+    case FuncCachePreferShared:
+      if ((m_shader_config->m_L1D_config.m_config_stringPrefShared == NULL) ||
+          (m_shader_config->gpgpu_shmem_sizePrefShared == (unsigned)-1)) {
+        printf("WARNING: missing Preferred L1 configuration\n");
+        m_shader_config->m_L1D_config.init(
+            m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
+        m_shader_config->gpgpu_shmem_size =
+            m_shader_config->gpgpu_shmem_sizeDefault;
+      } else {
+        m_shader_config->m_L1D_config.init(
+            m_shader_config->m_L1D_config.m_config_stringPrefShared,
+            FuncCachePreferShared);
+        m_shader_config->gpgpu_shmem_size =
+            m_shader_config->gpgpu_shmem_sizePrefShared;
+      }
+      break;
+    default:
+      break;
+  }
+}
+
+void gpgpu_sim::clear_executed_kernel_info() {
+  m_executed_kernel_names.clear();
+  m_executed_kernel_uids.clear();
+}
+void gpgpu_sim::gpu_print_stat() {
+  FILE *statfout = stdout;
+
+  std::string kernel_info_str = executed_kernel_info_string();
+  fprintf(statfout, "%s", kernel_info_str.c_str());
+
+  printf("gpu_sim_cycle = %lld\n", gpu_sim_cycle);
+  printf("gpu_sim_insn = %lld\n", gpu_sim_insn);
+  printf("gpu_ipc = %12.4f\n", (float)gpu_sim_insn / gpu_sim_cycle);
+  printf("gpu_tot_sim_cycle = %lld\n", gpu_tot_sim_cycle + gpu_sim_cycle);
+  printf("gpu_tot_sim_insn = %lld\n", gpu_tot_sim_insn + gpu_sim_insn);
+  printf("gpu_tot_ipc = %12.4f\n", (float)(gpu_tot_sim_insn + gpu_sim_insn) /
+                                       (gpu_tot_sim_cycle + gpu_sim_cycle));
+  printf("gpu_tot_issued_cta = %lld\n",
+         gpu_tot_issued_cta + m_total_cta_launched);
+  printf("gpu_occupancy = %.4f%% \n", gpu_occupancy.get_occ_fraction() * 100);
+  printf("gpu_tot_occupancy = %.4f%% \n",
+         (gpu_occupancy + gpu_tot_occupancy).get_occ_fraction() * 100);
+
+  fprintf(statfout, "max_total_param_size = %llu\n",
+          gpgpu_ctx->device_runtime->g_max_total_param_size);
+
+  // performance counter for stalls due to congestion.
+  printf("gpu_stall_dramfull = %d\n", gpu_stall_dramfull);
+  printf("gpu_stall_icnt2sh    = %d\n", gpu_stall_icnt2sh);
+
+  // printf("partiton_reqs_in_parallel = %lld\n", partiton_reqs_in_parallel);
+  // printf("partiton_reqs_in_parallel_total    = %lld\n",
+  // partiton_reqs_in_parallel_total );
+  printf("partiton_level_parallism = %12.4f\n",
+         (float)partiton_reqs_in_parallel / gpu_sim_cycle);
+  printf("partiton_level_parallism_total  = %12.4f\n",
+         (float)(partiton_reqs_in_parallel + partiton_reqs_in_parallel_total) /
+             (gpu_tot_sim_cycle + gpu_sim_cycle));
+  // printf("partiton_reqs_in_parallel_util = %lld\n",
+  // partiton_reqs_in_parallel_util);
+  // printf("partiton_reqs_in_parallel_util_total    = %lld\n",
+  // partiton_reqs_in_parallel_util_total ); printf("gpu_sim_cycle_parition_util
+  // = %lld\n", gpu_sim_cycle_parition_util);
+  // printf("gpu_tot_sim_cycle_parition_util    = %lld\n",
+  // gpu_tot_sim_cycle_parition_util );
+  printf("partiton_level_parallism_util = %12.4f\n",
+         (float)partiton_reqs_in_parallel_util / gpu_sim_cycle_parition_util);
+  printf("partiton_level_parallism_util_total  = %12.4f\n",
+         (float)(partiton_reqs_in_parallel_util +
+                 partiton_reqs_in_parallel_util_total) /
+             (gpu_sim_cycle_parition_util + gpu_tot_sim_cycle_parition_util));
+  // printf("partiton_replys_in_parallel = %lld\n",
+  // partiton_replys_in_parallel); printf("partiton_replys_in_parallel_total =
+  // %lld\n", partiton_replys_in_parallel_total );
+  printf("L2_BW  = %12.4f GB/Sec\n",
+         ((float)(partiton_replys_in_parallel * 32) /
+          (gpu_sim_cycle * m_config.icnt_period)) /
+             1000000000);
+  printf("L2_BW_total  = %12.4f GB/Sec\n",
+         ((float)((partiton_replys_in_parallel +
+                   partiton_replys_in_parallel_total) *
+                  32) /
+          ((gpu_tot_sim_cycle + gpu_sim_cycle) * m_config.icnt_period)) /
+             1000000000);
+
+  time_t curr_time;
+  time(&curr_time);
+  unsigned long long elapsed_time =
+      MAX(curr_time - gpgpu_ctx->the_gpgpusim->g_simulation_starttime, 1);
+  printf("gpu_total_sim_rate=%u\n",
+         (unsigned)((gpu_tot_sim_insn + gpu_sim_insn) / elapsed_time));
+
+  // shader_print_l1_miss_stat( stdout );
+  shader_print_cache_stats(stdout);
+
+  cache_stats core_cache_stats;
+  core_cache_stats.clear();
+  for (unsigned i = 0; i < m_config.num_cluster(); i++) {
+    m_cluster[i]->get_cache_stats(core_cache_stats);
+  }
+  printf("\nTotal_core_cache_stats:\n");
+  core_cache_stats.print_stats(stdout, "Total_core_cache_stats_breakdown");
+  printf("\nTotal_core_cache_fail_stats:\n");
+  core_cache_stats.print_fail_stats(stdout,
+                                    "Total_core_cache_fail_stats_breakdown");
+  shader_print_scheduler_stat(stdout, false);
+
+  m_shader_stats->print(stdout);
 #ifdef GPGPUSIM_POWER_MODEL
-   if(m_config.g_power_simulation_enabled){
-	   m_gpgpusim_wrapper->print_power_kernel_stats(gpu_sim_cycle, gpu_tot_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, kernel_info_str, true );
-	   mcpat_reset_perf_count(m_gpgpusim_wrapper);
-   }
+  if (m_config.g_power_simulation_enabled) {
+    if(m_config.g_power_simulation_mode > 0){
+        //if(!m_config.g_aggregate_power_stats)
+          mcpat_reset_perf_count(m_gpgpusim_wrapper);
+        calculate_hw_mcpat(m_config, getShaderCoreConfig(), m_gpgpusim_wrapper,
+                  m_power_stats, m_config.gpu_stat_sample_freq,
+                  gpu_tot_sim_cycle, gpu_sim_cycle, gpu_tot_sim_insn,
+                  gpu_sim_insn, m_config.g_power_simulation_mode, m_config.g_dvfs_enabled, 
+                  m_config.g_hw_perf_file_name, m_config.g_hw_perf_bench_name, executed_kernel_name(), m_config.accelwattch_hybrid_configuration, m_config.g_aggregate_power_stats);
+    }
+    m_gpgpusim_wrapper->print_power_kernel_stats(
+        gpu_sim_cycle, gpu_tot_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn,
+        kernel_info_str, true);
+    //if(!m_config.g_aggregate_power_stats)
+      mcpat_reset_perf_count(m_gpgpusim_wrapper);
+  }
 #endif
 
-   // performance counter that are not local to one shader
-   m_memory_stats->memlatstat_print(m_memory_config->m_n_mem,m_memory_config->nbk);
-   for (unsigned i=0;i<m_memory_config->m_n_mem;i++)
-      m_memory_partition_unit[i]->print(stdout);
-
-   // L2 cache stats
-   if(!m_memory_config->m_L2_config.disabled()){
-       cache_stats l2_stats;
-       struct cache_sub_stats l2_css;
-       struct cache_sub_stats total_l2_css;
-       l2_stats.clear();
-       l2_css.clear();
-       total_l2_css.clear();
-
-       printf("\n========= L2 cache stats =========\n");
-       for (unsigned i=0;i<m_memory_config->m_n_mem_sub_partition;i++){
-           m_memory_sub_partition[i]->accumulate_L2cache_stats(l2_stats);
-           m_memory_sub_partition[i]->get_L2cache_sub_stats(l2_css);
-
-           fprintf( stdout, "L2_cache_bank[%d]: Access = %llu, Miss = %llu, Miss_rate = %.3lf, Pending_hits = %llu, Reservation_fails = %llu\n",
-                    i, l2_css.accesses, l2_css.misses, (double)l2_css.misses / (double)l2_css.accesses, l2_css.pending_hits, l2_css.res_fails);
-
-           total_l2_css += l2_css;
-       }
-       if (!m_memory_config->m_L2_config.disabled() && m_memory_config->m_L2_config.get_num_lines()) {
-          //L2c_print_cache_stat();
-          printf("L2_total_cache_accesses = %llu\n", total_l2_css.accesses);
-          printf("L2_total_cache_misses = %llu\n", total_l2_css.misses);
-          if(total_l2_css.accesses > 0)
-              printf("L2_total_cache_miss_rate = %.4lf\n", (double)total_l2_css.misses/(double)total_l2_css.accesses);
-          printf("L2_total_cache_pending_hits = %llu\n", total_l2_css.pending_hits);
-          printf("L2_total_cache_reservation_fails = %llu\n", total_l2_css.res_fails);
-          printf("L2_total_cache_breakdown:\n");
-          l2_stats.print_stats(stdout, "L2_cache_stats_breakdown");
-          printf("L2_total_cache_reservation_fail_breakdown:\n");
-          l2_stats.print_fail_stats(stdout, "L2_cache_stats_fail_breakdown");
-          total_l2_css.print_port_stats(stdout, "L2_cache");
-       }
-   }
-
-   if (m_config.gpgpu_cflog_interval != 0) {
-      spill_log_to_file (stdout, 1, gpu_sim_cycle);
-      insn_warp_occ_print(stdout);
-   }
-   if ( gpgpu_ptx_instruction_classification ) {
-      StatDisp( g_inst_classification_stat[g_ptx_kernel_count]);
-      StatDisp( g_inst_op_classification_stat[g_ptx_kernel_count]);
-   }
+  // performance counter that are not local to one shader
+  m_memory_stats->memlatstat_print(m_memory_config->m_n_mem,
+                                   m_memory_config->nbk);
+  for (unsigned i = 0; i < m_memory_config->m_n_mem; i++)
+    m_memory_partition_unit[i]->print(stdout);
+
+  // L2 cache stats
+  if (!m_memory_config->m_L2_config.disabled()) {
+    cache_stats l2_stats;
+    struct cache_sub_stats l2_css;
+    struct cache_sub_stats total_l2_css;
+    l2_stats.clear();
+    l2_css.clear();
+    total_l2_css.clear();
+
+    printf("\n========= L2 cache stats =========\n");
+    for (unsigned i = 0; i < m_memory_config->m_n_mem_sub_partition; i++) {
+      m_memory_sub_partition[i]->accumulate_L2cache_stats(l2_stats);
+      m_memory_sub_partition[i]->get_L2cache_sub_stats(l2_css);
+
+      fprintf(stdout,
+              "L2_cache_bank[%d]: Access = %llu, Miss = %llu, Miss_rate = "
+              "%.3lf, Pending_hits = %llu, Reservation_fails = %llu\n",
+              i, l2_css.accesses, l2_css.misses,
+              (double)l2_css.misses / (double)l2_css.accesses,
+              l2_css.pending_hits, l2_css.res_fails);
+
+      total_l2_css += l2_css;
+    }
+    if (!m_memory_config->m_L2_config.disabled() &&
+        m_memory_config->m_L2_config.get_num_lines()) {
+      // L2c_print_cache_stat();
+      printf("L2_total_cache_accesses = %llu\n", total_l2_css.accesses);
+      printf("L2_total_cache_misses = %llu\n", total_l2_css.misses);
+      if (total_l2_css.accesses > 0)
+        printf("L2_total_cache_miss_rate = %.4lf\n",
+               (double)total_l2_css.misses / (double)total_l2_css.accesses);
+      printf("L2_total_cache_pending_hits = %llu\n", total_l2_css.pending_hits);
+      printf("L2_total_cache_reservation_fails = %llu\n",
+             total_l2_css.res_fails);
+      printf("L2_total_cache_breakdown:\n");
+      l2_stats.print_stats(stdout, "L2_cache_stats_breakdown");
+      printf("L2_total_cache_reservation_fail_breakdown:\n");
+      l2_stats.print_fail_stats(stdout, "L2_cache_stats_fail_breakdown");
+      total_l2_css.print_port_stats(stdout, "L2_cache");
+    }
+  }
+
+  if (m_config.gpgpu_cflog_interval != 0) {
+    spill_log_to_file(stdout, 1, gpu_sim_cycle);
+    insn_warp_occ_print(stdout);
+  }
+  if (gpgpu_ctx->func_sim->gpgpu_ptx_instruction_classification) {
+    StatDisp(gpgpu_ctx->func_sim->g_inst_classification_stat
+                 [gpgpu_ctx->func_sim->g_ptx_kernel_count]);
+    StatDisp(gpgpu_ctx->func_sim->g_inst_op_classification_stat
+                 [gpgpu_ctx->func_sim->g_ptx_kernel_count]);
+  }
 
 #ifdef GPGPUSIM_POWER_MODEL
-   if(m_config.g_power_simulation_enabled){
-       m_gpgpusim_wrapper->detect_print_steady_state(1,gpu_tot_sim_insn+gpu_sim_insn);
-   }
+  if (m_config.g_power_simulation_enabled) {
+    m_gpgpusim_wrapper->detect_print_steady_state(
+        1, gpu_tot_sim_insn + gpu_sim_insn);
+  }
 #endif
 
+  // Interconnect power stat print
+  long total_simt_to_mem = 0;
+  long total_mem_to_simt = 0;
+  long temp_stm = 0;
+  long temp_mts = 0;
+  for (unsigned i = 0; i < m_config.num_cluster(); i++) {
+    m_cluster[i]->get_icnt_stats(temp_stm, temp_mts);
+    total_simt_to_mem += temp_stm;
+    total_mem_to_simt += temp_mts;
+  }
+  printf("\nicnt_total_pkts_mem_to_simt=%ld\n", total_mem_to_simt);
+  printf("icnt_total_pkts_simt_to_mem=%ld\n", total_simt_to_mem);
 
-   // Interconnect power stat print
-   long total_simt_to_mem=0;
-   long total_mem_to_simt=0;
-   long temp_stm=0;
-   long temp_mts = 0;
-   for(unsigned i=0; i<m_config.num_cluster(); i++){
-	   m_cluster[i]->get_icnt_stats(temp_stm, temp_mts);
-	   total_simt_to_mem += temp_stm;
-	   total_mem_to_simt += temp_mts;
-   }
-   printf("\nicnt_total_pkts_mem_to_simt=%ld\n", total_mem_to_simt);
-   printf("icnt_total_pkts_simt_to_mem=%ld\n", total_simt_to_mem);
-
-   time_vector_print();
-   fflush(stdout);
+  time_vector_print();
+  fflush(stdout);
 
-   clear_executed_kernel_info(); 
+  clear_executed_kernel_info();
 }
 
-
 // performance counter that are not local to one shader
-unsigned gpgpu_sim::threads_per_core() const 
-{ 
-   return m_shader_config->n_thread_per_shader; 
+unsigned gpgpu_sim::threads_per_core() const {
+  return m_shader_config->n_thread_per_shader;
 }
 
-void shader_core_ctx::mem_instruction_stats(const warp_inst_t &inst)
-{
-    unsigned active_count = inst.active_count(); 
-    //this breaks some encapsulation: the is_[space] functions, if you change those, change this.
-    switch (inst.space.get_type()) {
+void shader_core_ctx::mem_instruction_stats(const warp_inst_t &inst) {
+  unsigned active_count = inst.active_count();
+  // this breaks some encapsulation: the is_[space] functions, if you change
+  // those, change this.
+  switch (inst.space.get_type()) {
     case undefined_space:
     case reg_space:
-        break;
+      break;
     case shared_space:
-        m_stats->gpgpu_n_shmem_insn += active_count; 
-        break;
+      m_stats->gpgpu_n_shmem_insn += active_count;
+      break;
     case sstarr_space:
-    	m_stats->gpgpu_n_sstarr_insn += active_count;
-    	break;
+      m_stats->gpgpu_n_sstarr_insn += active_count;
+      break;
     case const_space:
-        m_stats->gpgpu_n_const_insn += active_count;
-        break;
+      m_stats->gpgpu_n_const_insn += active_count;
+      break;
     case param_space_kernel:
     case param_space_local:
-        m_stats->gpgpu_n_param_insn += active_count;
-        break;
+      m_stats->gpgpu_n_param_insn += active_count;
+      break;
     case tex_space:
-        m_stats->gpgpu_n_tex_insn += active_count;
-        break;
+      m_stats->gpgpu_n_tex_insn += active_count;
+      break;
     case global_space:
     case local_space:
-        if( inst.is_store() )
-            m_stats->gpgpu_n_store_insn += active_count;
-        else 
-            m_stats->gpgpu_n_load_insn += active_count;
-        break;
+      if (inst.is_store())
+        m_stats->gpgpu_n_store_insn += active_count;
+      else
+        m_stats->gpgpu_n_load_insn += active_count;
+      break;
     default:
-        abort();
-    }
+      abort();
+  }
 }
-bool shader_core_ctx::can_issue_1block(kernel_info_t & kernel) {
+bool shader_core_ctx::can_issue_1block(kernel_info_t &kernel) {
+  // Jin: concurrent kernels on one SM
+  if (m_config->gpgpu_concurrent_kernel_sm) {
+    if (m_config->max_cta(kernel) < 1) return false;
 
-   //Jin: concurrent kernels on one SM
-   if(m_config->gpgpu_concurrent_kernel_sm) {    
-      if(m_config->max_cta(kernel) < 1)
-           return false;
-
-      return occupy_shader_resource_1block(kernel, false);
-   }
-   else {
-      return (get_n_active_cta() < m_config->max_cta(kernel));
-   } 
+    return occupy_shader_resource_1block(kernel, false);
+  } else {
+    return (get_n_active_cta() < m_config->max_cta(kernel));
+  }
 }
 
 int shader_core_ctx::find_available_hwtid(unsigned int cta_size, bool occupy) {
-   
-   unsigned int step;
-   for(step = 0; step < m_config->n_thread_per_shader; 
-        step += cta_size) {
-
-        unsigned int hw_tid;
-        for(hw_tid = step; hw_tid < step + cta_size;
-            hw_tid++) {
-            if(m_occupied_hwtid.test(hw_tid))
-                break;
-        }
-        if(hw_tid == step + cta_size) //consecutive non-active
-            break;
-   }
-   if(step >= m_config->n_thread_per_shader) //didn't find
-     return -1;
-   else {
-     if(occupy) {
-        for(unsigned hw_tid = step; hw_tid < step + cta_size;
-            hw_tid++)
-            m_occupied_hwtid.set(hw_tid);
-     }
-     return step;
-   }
-}
-
-bool shader_core_ctx::occupy_shader_resource_1block(kernel_info_t & k, bool occupy) {
-   unsigned threads_per_cta  = k.threads_per_cta();
-   const class function_info *kernel = k.entry();
-   unsigned int padded_cta_size = threads_per_cta;
-   unsigned int warp_size = m_config->warp_size; 
-   if (padded_cta_size%warp_size) 
-      padded_cta_size = ((padded_cta_size/warp_size)+1)*(warp_size);
-
-   if(m_occupied_n_threads + padded_cta_size > m_config->n_thread_per_shader)
-     return false;
-
-   if(find_available_hwtid(padded_cta_size, false) == -1)
-     return false;
-
-   const struct gpgpu_ptx_sim_info *kernel_info = ptx_sim_kernel_info(kernel);
-
-   if(m_occupied_shmem + kernel_info->smem > m_config->gpgpu_shmem_size)
-     return false;
-
-   unsigned int used_regs = padded_cta_size * ((kernel_info->regs+3)&~3);
-   if(m_occupied_regs + used_regs > m_config->gpgpu_shader_registers)
-     return false;
-
-   if(m_occupied_ctas +1 > m_config->max_cta_per_core)
-     return false;
-   
-   if(occupy) {
-       m_occupied_n_threads += padded_cta_size;
-       m_occupied_shmem += kernel_info->smem;
-       m_occupied_regs += (padded_cta_size * ((kernel_info->regs+3)&~3));
-       m_occupied_ctas++;
-
-      SHADER_DPRINTF(LIVENESS, "GPGPU-Sim uArch: Occupied %d threads, %d shared mem, %d registers, %d ctas\n",
-            m_occupied_n_threads, m_occupied_shmem, m_occupied_regs, m_occupied_ctas);  
-   }
-
-   return true;
-}
-
-void shader_core_ctx::release_shader_resource_1block(unsigned hw_ctaid, kernel_info_t & k) {
-
-   if(m_config->gpgpu_concurrent_kernel_sm) {
-      unsigned threads_per_cta  = k.threads_per_cta();
-      const class function_info *kernel = k.entry();
-      unsigned int padded_cta_size = threads_per_cta;
-      unsigned int warp_size = m_config->warp_size; 
-      if (padded_cta_size%warp_size) 
-         padded_cta_size = ((padded_cta_size/warp_size)+1)*(warp_size);
-   
-      assert(m_occupied_n_threads >= padded_cta_size);
-      m_occupied_n_threads -= padded_cta_size;
-   
-      int start_thread = m_occupied_cta_to_hwtid[hw_ctaid];
-   
-      for(unsigned hwtid = start_thread; hwtid < start_thread + padded_cta_size;
-       hwtid++)
-          m_occupied_hwtid.reset(hwtid);
-      m_occupied_cta_to_hwtid.erase(hw_ctaid);
-   
-      const struct gpgpu_ptx_sim_info *kernel_info = ptx_sim_kernel_info(kernel);
-   
-      assert(m_occupied_shmem >= (unsigned int)kernel_info->smem);
-      m_occupied_shmem -= kernel_info->smem;
-   
-      unsigned int used_regs = padded_cta_size * ((kernel_info->regs+3)&~3);
-      assert(m_occupied_regs >= used_regs);
-      m_occupied_regs -= used_regs;
-   
-      assert(m_occupied_ctas >= 1);
-      m_occupied_ctas--;
-   }
+  unsigned int step;
+  for (step = 0; step < m_config->n_thread_per_shader; step += cta_size) {
+    unsigned int hw_tid;
+    for (hw_tid = step; hw_tid < step + cta_size; hw_tid++) {
+      if (m_occupied_hwtid.test(hw_tid)) break;
+    }
+    if (hw_tid == step + cta_size)  // consecutive non-active
+      break;
+  }
+  if (step >= m_config->n_thread_per_shader)  // didn't find
+    return -1;
+  else {
+    if (occupy) {
+      for (unsigned hw_tid = step; hw_tid < step + cta_size; hw_tid++)
+        m_occupied_hwtid.set(hw_tid);
+    }
+    return step;
+  }
 }
 
-////////////////////////////////////////////////////////////////////////////////////////////////
+bool shader_core_ctx::occupy_shader_resource_1block(kernel_info_t &k,
+                                                    bool occupy) {
+  unsigned threads_per_cta = k.threads_per_cta();
+  const class function_info *kernel = k.entry();
+  unsigned int padded_cta_size = threads_per_cta;
+  unsigned int warp_size = m_config->warp_size;
+  if (padded_cta_size % warp_size)
+    padded_cta_size = ((padded_cta_size / warp_size) + 1) * (warp_size);
 
-/**
- * Launches a cooperative thread array (CTA). 
- *  
- * @param kernel 
- *    object that tells us which kernel to ask for a CTA from 
- */
+  if (m_occupied_n_threads + padded_cta_size > m_config->n_thread_per_shader)
+    return false;
 
-void shader_core_ctx::issue_block2core( kernel_info_t &kernel ) 
-{
+  if (find_available_hwtid(padded_cta_size, false) == -1) return false;
 
-    if(!m_config->gpgpu_concurrent_kernel_sm)
-        set_max_cta(kernel);
-    else
-        assert(occupy_shader_resource_1block(kernel, true));
-
-    kernel.inc_running();
-
-    // find a free CTA context 
-    unsigned free_cta_hw_id=(unsigned)-1;
-
-    unsigned max_cta_per_core;
-    if(!m_config->gpgpu_concurrent_kernel_sm)
-        max_cta_per_core = kernel_max_cta_per_shader;
-    else
-        max_cta_per_core = m_config->max_cta_per_core;
-    for (unsigned i=0;i<max_cta_per_core;i++ ) {
-      if( m_cta_status[i]==0 ) {
-         free_cta_hw_id=i;
-         break;
-      }
-    }
-    assert( free_cta_hw_id!=(unsigned)-1 );
+  const struct gpgpu_ptx_sim_info *kernel_info = ptx_sim_kernel_info(kernel);
 
-    // determine hardware threads and warps that will be used for this CTA
-    int cta_size = kernel.threads_per_cta();
+  if (m_occupied_shmem + kernel_info->smem > m_config->gpgpu_shmem_size)
+    return false;
 
-    // hw warp id = hw thread id mod warp size, so we need to find a range 
-    // of hardware thread ids corresponding to an integral number of hardware
-    // thread ids
-    int padded_cta_size = cta_size; 
-    if (cta_size%m_config->warp_size)
-      padded_cta_size = ((cta_size/m_config->warp_size)+1)*(m_config->warp_size);
+  unsigned int used_regs = padded_cta_size * ((kernel_info->regs + 3) & ~3);
+  if (m_occupied_regs + used_regs > m_config->gpgpu_shader_registers)
+    return false;
 
-    unsigned int start_thread, end_thread;
+  if (m_occupied_ctas + 1 > m_config->max_cta_per_core) return false;
 
-    if(!m_config->gpgpu_concurrent_kernel_sm) {
-        start_thread = free_cta_hw_id * padded_cta_size;
-        end_thread  = start_thread +  cta_size;
-    }
-    else {
-        start_thread = find_available_hwtid(padded_cta_size, true);
-        assert((int)start_thread != -1);
-        end_thread = start_thread + cta_size;
-        assert(m_occupied_cta_to_hwtid.find(free_cta_hw_id) == m_occupied_cta_to_hwtid.end());
-        m_occupied_cta_to_hwtid[free_cta_hw_id]= start_thread;
-    }
+  if (occupy) {
+    m_occupied_n_threads += padded_cta_size;
+    m_occupied_shmem += kernel_info->smem;
+    m_occupied_regs += (padded_cta_size * ((kernel_info->regs + 3) & ~3));
+    m_occupied_ctas++;
 
-    // reset the microarchitecture state of the selected hardware thread and warp contexts
-    reinit(start_thread, end_thread,false);
-     
-    // initalize scalar threads and determine which hardware warps they are allocated to
-    // bind functional simulation state of threads to hardware resources (simulation) 
-    warp_set_t warps;
-    unsigned nthreads_in_block= 0;
-    function_info *kernel_func_info = kernel.entry();
-    symbol_table * symtab= kernel_func_info->get_symtab();
-    unsigned ctaid= kernel.get_next_cta_id_single();
-    checkpoint *g_checkpoint=  new checkpoint();
-    for (unsigned i = start_thread; i<end_thread; i++) {
-        m_threadState[i].m_cta_id = free_cta_hw_id;
-        unsigned warp_id = i/m_config->warp_size;
-        nthreads_in_block += ptx_sim_init_thread(kernel,&m_thread[i],m_sid,i,cta_size-(i-start_thread),m_config->n_thread_per_shader,this,free_cta_hw_id,warp_id,m_cluster->get_gpu());
-        m_threadState[i].m_active = true; 
-        // load thread local memory and register file
-        if(m_gpu->resume_option==1 && kernel.get_uid()==m_gpu->resume_kernel && ctaid>=m_gpu->resume_CTA && ctaid<m_gpu->checkpoint_CTA_t )
-        {
-            char fname[2048];
-            snprintf(fname,2048,"checkpoint_files/thread_%d_%d_reg.txt",i%cta_size,ctaid );
-            m_thread[i]->resume_reg_thread(fname,symtab);
-            char f1name[2048];
-            snprintf(f1name,2048,"checkpoint_files/local_mem_thread_%d_%d_reg.txt",i%cta_size,ctaid);
-            g_checkpoint->load_global_mem(m_thread[i]->m_local_mem, f1name); 
-        }
-        //
-        warps.set( warp_id );
-    }
-    assert( nthreads_in_block > 0 && nthreads_in_block <= m_config->n_thread_per_shader); // should be at least one, but less than max
-    m_cta_status[free_cta_hw_id]=nthreads_in_block;
-
-    if(m_gpu->resume_option==1 && kernel.get_uid()==m_gpu->resume_kernel && ctaid>=m_gpu->resume_CTA && ctaid<m_gpu->checkpoint_CTA_t )
-    {
-        char f1name[2048];
-        snprintf(f1name,2048,"checkpoint_files/shared_mem_%d.txt", ctaid);
-        
-        g_checkpoint->load_global_mem(m_thread[start_thread]->m_shared_mem, f1name);  
-    }
-    // now that we know which warps are used in this CTA, we can allocate
-    // resources for use in CTA-wide barrier operations
-    m_barriers.allocate_barrier(free_cta_hw_id,warps);
+    SHADER_DPRINTF(LIVENESS,
+                   "GPGPU-Sim uArch: Occupied %u threads, %u shared mem, %u "
+                   "registers, %u ctas\n",
+                   m_occupied_n_threads, m_occupied_shmem, m_occupied_regs,
+                   m_occupied_ctas);
+  }
 
-    // initialize the SIMT stacks and fetch hardware
-    init_warps( free_cta_hw_id, start_thread, end_thread, ctaid, cta_size, kernel.get_uid());
-    m_n_active_cta++;
+  return true;
+}
+
+void shader_core_ctx::release_shader_resource_1block(unsigned hw_ctaid,
+                                                     kernel_info_t &k) {
+  if (m_config->gpgpu_concurrent_kernel_sm) {
+    unsigned threads_per_cta = k.threads_per_cta();
+    const class function_info *kernel = k.entry();
+    unsigned int padded_cta_size = threads_per_cta;
+    unsigned int warp_size = m_config->warp_size;
+    if (padded_cta_size % warp_size)
+      padded_cta_size = ((padded_cta_size / warp_size) + 1) * (warp_size);
+
+    assert(m_occupied_n_threads >= padded_cta_size);
+    m_occupied_n_threads -= padded_cta_size;
+
+    int start_thread = m_occupied_cta_to_hwtid[hw_ctaid];
+
+    for (unsigned hwtid = start_thread; hwtid < start_thread + padded_cta_size;
+         hwtid++)
+      m_occupied_hwtid.reset(hwtid);
+    m_occupied_cta_to_hwtid.erase(hw_ctaid);
+
+    const struct gpgpu_ptx_sim_info *kernel_info = ptx_sim_kernel_info(kernel);
+
+    assert(m_occupied_shmem >= (unsigned int)kernel_info->smem);
+    m_occupied_shmem -= kernel_info->smem;
+
+    unsigned int used_regs = padded_cta_size * ((kernel_info->regs + 3) & ~3);
+    assert(m_occupied_regs >= used_regs);
+    m_occupied_regs -= used_regs;
+
+    assert(m_occupied_ctas >= 1);
+    m_occupied_ctas--;
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////
+
+/**
+ * Launches a cooperative thread array (CTA).
+ *
+ * @param kernel
+ *    object that tells us which kernel to ask for a CTA from
+ */
 
-    shader_CTA_count_log(m_sid, 1);
-    SHADER_DPRINTF(LIVENESS, "GPGPU-Sim uArch: cta:%2u, start_tid:%4u, end_tid:%4u, initialized @(%lld,%lld)\n", 
-        free_cta_hw_id, start_thread, end_thread, gpu_sim_cycle, gpu_tot_sim_cycle );
+unsigned exec_shader_core_ctx::sim_init_thread(
+    kernel_info_t &kernel, ptx_thread_info **thread_info, int sid, unsigned tid,
+    unsigned threads_left, unsigned num_threads, core_t *core,
+    unsigned hw_cta_id, unsigned hw_warp_id, gpgpu_t *gpu) {
+  return ptx_sim_init_thread(kernel, thread_info, sid, tid, threads_left,
+                             num_threads, core, hw_cta_id, hw_warp_id, gpu);
+}
+
+void shader_core_ctx::issue_block2core(kernel_info_t &kernel) {
+  if (!m_config->gpgpu_concurrent_kernel_sm)
+    set_max_cta(kernel);
+  else
+    assert(occupy_shader_resource_1block(kernel, true));
+
+  kernel.inc_running();
+
+  // find a free CTA context
+  unsigned free_cta_hw_id = (unsigned)-1;
+
+  unsigned max_cta_per_core;
+  if (!m_config->gpgpu_concurrent_kernel_sm)
+    max_cta_per_core = kernel_max_cta_per_shader;
+  else
+    max_cta_per_core = m_config->max_cta_per_core;
+  for (unsigned i = 0; i < max_cta_per_core; i++) {
+    if (m_cta_status[i] == 0) {
+      free_cta_hw_id = i;
+      break;
+    }
+  }
+  assert(free_cta_hw_id != (unsigned)-1);
+
+  // determine hardware threads and warps that will be used for this CTA
+  int cta_size = kernel.threads_per_cta();
+
+  // hw warp id = hw thread id mod warp size, so we need to find a range
+  // of hardware thread ids corresponding to an integral number of hardware
+  // thread ids
+  int padded_cta_size = cta_size;
+  if (cta_size % m_config->warp_size)
+    padded_cta_size =
+        ((cta_size / m_config->warp_size) + 1) * (m_config->warp_size);
+
+  unsigned int start_thread, end_thread;
+
+  if (!m_config->gpgpu_concurrent_kernel_sm) {
+    start_thread = free_cta_hw_id * padded_cta_size;
+    end_thread = start_thread + cta_size;
+  } else {
+    start_thread = find_available_hwtid(padded_cta_size, true);
+    assert((int)start_thread != -1);
+    end_thread = start_thread + cta_size;
+    assert(m_occupied_cta_to_hwtid.find(free_cta_hw_id) ==
+           m_occupied_cta_to_hwtid.end());
+    m_occupied_cta_to_hwtid[free_cta_hw_id] = start_thread;
+  }
+
+  // reset the microarchitecture state of the selected hardware thread and warp
+  // contexts
+  reinit(start_thread, end_thread, false);
+
+  // initalize scalar threads and determine which hardware warps they are
+  // allocated to bind functional simulation state of threads to hardware
+  // resources (simulation)
+  warp_set_t warps;
+  unsigned nthreads_in_block = 0;
+  function_info *kernel_func_info = kernel.entry();
+  symbol_table *symtab = kernel_func_info->get_symtab();
+  unsigned ctaid = kernel.get_next_cta_id_single();
+  checkpoint *g_checkpoint = new checkpoint();
+  for (unsigned i = start_thread; i < end_thread; i++) {
+    m_threadState[i].m_cta_id = free_cta_hw_id;
+    unsigned warp_id = i / m_config->warp_size;
+    nthreads_in_block += sim_init_thread(
+        kernel, &m_thread[i], m_sid, i, cta_size - (i - start_thread),
+        m_config->n_thread_per_shader, this, free_cta_hw_id, warp_id,
+        m_cluster->get_gpu());
+    m_threadState[i].m_active = true;
+    // load thread local memory and register file
+    if (m_gpu->resume_option == 1 && kernel.get_uid() == m_gpu->resume_kernel &&
+        ctaid >= m_gpu->resume_CTA && ctaid < m_gpu->checkpoint_CTA_t) {
+      char fname[2048];
+      snprintf(fname, 2048, "checkpoint_files/thread_%d_%d_reg.txt",
+               i % cta_size, ctaid);
+      m_thread[i]->resume_reg_thread(fname, symtab);
+      char f1name[2048];
+      snprintf(f1name, 2048, "checkpoint_files/local_mem_thread_%d_%d_reg.txt",
+               i % cta_size, ctaid);
+      g_checkpoint->load_global_mem(m_thread[i]->m_local_mem, f1name);
+    }
+    //
+    warps.set(warp_id);
+  }
+  assert(nthreads_in_block > 0 &&
+         nthreads_in_block <=
+             m_config->n_thread_per_shader);  // should be at least one, but
+                                              // less than max
+  m_cta_status[free_cta_hw_id] = nthreads_in_block;
+
+  if (m_gpu->resume_option == 1 && kernel.get_uid() == m_gpu->resume_kernel &&
+      ctaid >= m_gpu->resume_CTA && ctaid < m_gpu->checkpoint_CTA_t) {
+    char f1name[2048];
+    snprintf(f1name, 2048, "checkpoint_files/shared_mem_%d.txt", ctaid);
+
+    g_checkpoint->load_global_mem(m_thread[start_thread]->m_shared_mem, f1name);
+  }
+  // now that we know which warps are used in this CTA, we can allocate
+  // resources for use in CTA-wide barrier operations
+  m_barriers.allocate_barrier(free_cta_hw_id, warps);
+
+  // initialize the SIMT stacks and fetch hardware
+  init_warps(free_cta_hw_id, start_thread, end_thread, ctaid, cta_size, kernel);
+  m_n_active_cta++;
+
+  shader_CTA_count_log(m_sid, 1);
+  SHADER_DPRINTF(LIVENESS,
+                 "GPGPU-Sim uArch: cta:%2u, start_tid:%4u, end_tid:%4u, "
+                 "initialized @(%lld,%lld)\n",
+                 free_cta_hw_id, start_thread, end_thread, m_gpu->gpu_sim_cycle,
+                 m_gpu->gpu_tot_sim_cycle);
     m_gpu->gem5CudaGPU->getCudaCore(m_sid)->record_block_issue(free_cta_hw_id);
 
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////
 
-void dram_t::dram_log( int task ) 
-{
-   if (task == SAMPLELOG) {
-      StatAddSample(mrqq_Dist, que_length());   
-   } else if (task == DUMPLOG) {
-      printf ("Queue Length DRAM[%d] ",id);StatDisp(mrqq_Dist);
-   }
+void dram_t::dram_log(int task) {
+  if (task == SAMPLELOG) {
+    StatAddSample(mrqq_Dist, que_length());
+  } else if (task == DUMPLOG) {
+    printf("Queue Length DRAM[%d] ", id);
+    StatDisp(mrqq_Dist);
+  }
+}
+
+// Find next clock domain and increment its time
+int gpgpu_sim::next_clock_domain(void) {
+  double smallest = min3(core_time, icnt_time, dram_time);
+  int mask = 0x00;
+  if (l2_time <= smallest) {
+    smallest = l2_time;
+    mask |= L2;
+    l2_time += m_config.l2_period;
+  }
+  if (icnt_time <= smallest) {
+    mask |= ICNT;
+    icnt_time += m_config.icnt_period;
+  }
+  if (dram_time <= smallest) {
+    mask |= DRAM;
+    dram_time += m_config.dram_period;
+  }
+  if (core_time <= smallest) {
+    mask |= CORE;
+    core_time += m_config.core_period;
+  }
+  return mask;
+}
+
+void gpgpu_sim::issue_block2core() {
+  unsigned last_issued = m_last_cluster_issue;
+  for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+    unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;
+    unsigned num = m_cluster[idx]->issue_block2core();
+    if (num) {
+      m_last_cluster_issue = idx;
+      m_total_cta_launched += num;
+    }
+  }
 }
 
-//Find next clock domain and increment its time
-int gpgpu_sim::next_clock_domain(void) 
-{
-   double smallest = min3(core_time,icnt_time,dram_time);
-   int mask = 0x00;
-   if ( l2_time <= smallest ) {
-      smallest = l2_time;
-      mask |= L2 ;
-      l2_time += m_config.l2_period;
-   }
-   if ( icnt_time <= smallest ) {
-      mask |= ICNT;
-      icnt_time += m_config.icnt_period;
-   }
-   if ( dram_time <= smallest ) {
-      mask |= DRAM;
-      dram_time += m_config.dram_period;
-   }
-   if ( core_time <= smallest ) {
-      mask |= CORE;
-      core_time += m_config.core_period;
-   }
-   return mask;
-}
-
-void gpgpu_sim::issue_block2core()
-{
-    unsigned last_issued = m_last_cluster_issue; 
-    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-        unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;
-        unsigned num = m_cluster[idx]->issue_block2core();
-        if( num ) {
-            m_last_cluster_issue=idx;
-            m_total_cta_launched += num;
+unsigned long long g_single_step =
+    0;  // set this in gdb to single step the pipeline
+
+void gpgpu_sim::core_cycle_start() {
+ int clock_mask = next_clock_domain();
+
+  if (clock_mask & CORE) {
+    // shader core loading (pop from ICNT into core) follows CORE clock
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
+      m_cluster[i]->icnt_cycle();
+  }
+  unsigned partiton_replys_in_parallel_per_cycle = 0;
+  if (clock_mask & ICNT) {
+    // pop from memory controller to interconnect
+    for (unsigned i = 0; i < m_memory_config->m_n_mem_sub_partition; i++) {
+      mem_fetch *mf = m_memory_sub_partition[i]->top();
+      if (mf) {
+        unsigned response_size =
+            mf->get_is_write() ? mf->get_ctrl_size() : mf->size();
+        if (::icnt_has_buffer(m_shader_config->mem2device(i), response_size)) {
+          // if (!mf->get_is_write())
+          mf->set_return_timestamp(gpu_sim_cycle + gpu_tot_sim_cycle);
+          mf->set_status(IN_ICNT_TO_SHADER, gpu_sim_cycle + gpu_tot_sim_cycle);
+          ::icnt_push(m_shader_config->mem2device(i), mf->get_tpc(), mf,
+                      response_size);
+          m_memory_sub_partition[i]->pop();
+          partiton_replys_in_parallel_per_cycle++;
+        } else {
+          gpu_stall_icnt2sh++;
         }
+      } else {
+        m_memory_sub_partition[i]->pop();
+      }
     }
-}
-
-unsigned long long g_single_step=0; // set this in gdb to single step the pipeline
+  }
+  partiton_replys_in_parallel += partiton_replys_in_parallel_per_cycle;
+
+  if (clock_mask & DRAM) {
+    for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
+      if (m_memory_config->simple_dram_model)
+        m_memory_partition_unit[i]->simple_dram_model_cycle();
+      else
+        m_memory_partition_unit[i]
+            ->dram_cycle();  // Issue the dram command (scheduler + delay model)
+      // Update performance counters for DRAM
+      m_memory_partition_unit[i]->set_dram_power_stats(
+          m_power_stats->pwr_mem_stat->n_cmd[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_activity[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_nop[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_act[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_wr_WB[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_req[CURRENT_STAT_IDX][i]);
+    }
+  }
+
+  // L2 operations follow L2 clock domain
+  unsigned partiton_reqs_in_parallel_per_cycle = 0;
+  if (clock_mask & L2) {
+    m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].clear();
+    for (unsigned i = 0; i < m_memory_config->m_n_mem_sub_partition; i++) {
+      // move memory request from interconnect into memory partition (if not
+      // backed up) Note:This needs to be called in DRAM clock domain if there
+      // is no L2 cache in the system In the worst case, we may need to push
+      // SECTOR_CHUNCK_SIZE requests, so ensure you have enough buffer for them
+      if (m_memory_sub_partition[i]->full(SECTOR_CHUNCK_SIZE)) {
+        gpu_stall_dramfull++;
+      } else {
+        mem_fetch *mf = (mem_fetch *)icnt_pop(m_shader_config->mem2device(i));
+        m_memory_sub_partition[i]->push(mf, gpu_sim_cycle + gpu_tot_sim_cycle);
+        if (mf) partiton_reqs_in_parallel_per_cycle++;
+      }
+      m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle + gpu_tot_sim_cycle);
+      m_memory_sub_partition[i]->accumulate_L2cache_stats(
+          m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX]);
+    }
+  }
+  partiton_reqs_in_parallel += partiton_reqs_in_parallel_per_cycle;
+  if (partiton_reqs_in_parallel_per_cycle > 0) {
+    partiton_reqs_in_parallel_util += partiton_reqs_in_parallel_per_cycle;
+    gpu_sim_cycle_parition_util++;
+  }
+
+  if (clock_mask & ICNT) {
+    icnt_transfer();
+  }
 
-void
-gpgpu_sim::core_cycle_start()
-{
+  if (clock_mask & CORE) {
     // L1 cache + shader core pipeline stages
-    // m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].clear();
-    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-       if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {
-             m_cluster[i]->core_cycle();
-             *active_sms+=m_cluster[i]->get_n_active_sms();
-       }
-       // Update core icnt/cache stats for GPUWattch
-       // m_cluster[i]->get_icnt_stats(m_power_stats->pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i]);
-       // m_cluster[i]->get_cache_stats(m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX]);
+    m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].clear();
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+      if (m_cluster[i]->get_not_completed() || get_more_cta_left()) {
+        m_cluster[i]->core_cycle();
+        *active_sms += m_cluster[i]->get_n_active_sms();
+      }
+      // Update core icnt/cache stats for AccelWattch
+      m_cluster[i]->get_icnt_stats(
+          m_power_stats->pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i],
+          m_power_stats->pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i]);
+      m_cluster[i]->get_cache_stats(
+          m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX]);
+      m_cluster[i]->get_current_occupancy(
+          gpu_occupancy.aggregate_warp_slot_filled,
+          gpu_occupancy.aggregate_theoretical_warp_slots);
     }
-    float temp=0;
-    for (unsigned i=0;i<m_shader_config->num_shader();i++){
-      temp+=m_shader_stats->m_pipeline_duty_cycle[i];
+    float temp = 0;
+    for (unsigned i = 0; i < m_shader_config->num_shader(); i++) {
+      temp += m_shader_stats->m_pipeline_duty_cycle[i];
     }
-    temp=temp/m_shader_config->num_shader();
-    *average_pipeline_duty_cycle=((*average_pipeline_duty_cycle)+temp);
-      //cout<<"Average pipeline duty cycle: "<<*average_pipeline_duty_cycle<<endl;
-
-
-    if ( g_single_step && ((gpu_sim_cycle+gpu_tot_sim_cycle) >= g_single_step) ) {
-        asm("int $03");
+    temp = temp / m_shader_config->num_shader();
+    *average_pipeline_duty_cycle = ((*average_pipeline_duty_cycle) + temp);
+    // cout<<"Average pipeline duty cycle:
+    // "<<*average_pipeline_duty_cycle<<endl;
+
+    if (g_single_step &&
+        ((gpu_sim_cycle + gpu_tot_sim_cycle) >= g_single_step)) {
+      raise(SIGTRAP);  // Debug breakpoint
     }
     gpu_sim_cycle++;
-    if ( g_interactive_debugger_enabled )
-       gpgpu_debug();
 
-    // McPAT main cycle (interface with McPAT)
+    if (g_interactive_debugger_enabled) gpgpu_debug();
+
+      // McPAT main cycle (interface with McPAT)
 #ifdef GPGPUSIM_POWER_MODEL
-    if (m_config.g_power_simulation_enabled){
-        mcpat_cycle(m_config, getShaderCoreConfig(), m_gpgpusim_wrapper, m_power_stats, m_config.gpu_stat_sample_freq, gpu_tot_sim_cycle, gpu_sim_cycle, gpu_tot_sim_insn, gpu_sim_insn);
+    if (m_config.g_power_simulation_enabled) {
+      if(m_config.g_power_simulation_mode == 0){
+      mcpat_cycle(m_config, getShaderCoreConfig(), m_gpgpusim_wrapper,
+                  m_power_stats, m_config.gpu_stat_sample_freq,
+                  gpu_tot_sim_cycle, gpu_sim_cycle, gpu_tot_sim_insn,
+                  gpu_sim_insn, m_config.g_dvfs_enabled);
+      }
     }
 #endif
 
     issue_block2core();
+    decrement_kernel_latency();
 
-    // Depending on configuration, flush the caches once all of threads are completed.
+    // Depending on configuration, invalidate the caches once all of threads are
+    // completed.
     int all_threads_complete = 1;
     if (m_config.gpgpu_flush_l1_cache) {
-       for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-          if (m_cluster[i]->get_not_completed() == 0)
-              m_cluster[i]->cache_flush();
-          else
-             all_threads_complete = 0 ;
-       }
+      for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+        if (m_cluster[i]->get_not_completed() == 0)
+          m_cluster[i]->cache_invalidate();
+        else
+          all_threads_complete = 0;
+      }
     }
 
-    if (m_config.gpgpu_flush_l2_cache){
-        if (!m_config.gpgpu_flush_l1_cache){
-            for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-                if (m_cluster[i]->get_not_completed() != 0){
-                    all_threads_complete = 0 ;
-                    break;
-                }
-            }
+    if (m_config.gpgpu_flush_l2_cache) {
+      if (!m_config.gpgpu_flush_l1_cache) {
+        for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+          if (m_cluster[i]->get_not_completed() != 0) {
+            all_threads_complete = 0;
+            break;
+          }
         }
+      }
 
-       if (all_threads_complete && !m_memory_config->m_L2_config.disabled() ) {
-          printf("Flushed L2 caches...\n");
-          if (m_memory_config->m_L2_config.get_num_lines()) {
-             int dlc = 0;
-             for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
-                dlc = m_memory_sub_partition[i]->flushL2();
-                assert (dlc == 0); // need to model actual writes to DRAM here
-                printf("Dirty lines flushed from L2 %d is %d\n", i, dlc  );
-             }
+      if (all_threads_complete && !m_memory_config->m_L2_config.disabled()) {
+        printf("Flushed L2 caches...\n");
+        if (m_memory_config->m_L2_config.get_num_lines()) {
+          int dlc = 0;
+          for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
+            dlc = m_memory_sub_partition[i]->flushL2();
+            assert(dlc == 0);  // TODO: need to model actual writes to DRAM here
+            printf("Dirty lines flushed from L2 %d is %d\n", i, dlc);
           }
-       }
+        }
+      }
     }
 
     if (!(gpu_sim_cycle % m_config.gpu_stat_sample_freq)) {
-       time_t days, hrs, minutes, sec;
-       time_t curr_time;
-       time(&curr_time);
-       unsigned long long  elapsed_time = MAX(curr_time - g_simulation_starttime, 1);
-       if ( (elapsed_time - last_liveness_message_time) >= m_config.liveness_message_freq ) {
-          days    = elapsed_time/(3600*24);
-          hrs     = elapsed_time/3600 - 24*days;
-          minutes = elapsed_time/60 - 60*(hrs + 24*days);
-          sec = elapsed_time - 60*(minutes + 60*(hrs + 24*days));
-          printf("GPGPU-Sim uArch: cycles simulated: %lld  inst.: %lld (ipc=%4.1f) sim_rate=%u (inst/sec) elapsed = %u:%u:%02u:%02u / %s",
-                 gpu_tot_sim_cycle + gpu_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn,
-                 (double)gpu_sim_insn/(double)gpu_sim_cycle,
-                 (unsigned)((gpu_tot_sim_insn+gpu_sim_insn) / elapsed_time),
-                 (unsigned)days,(unsigned)hrs,(unsigned)minutes,(unsigned)sec,
-                 ctime(&curr_time));
-          fflush(stdout);
-          last_liveness_message_time = elapsed_time;
-       }
-       visualizer_printstat();
-       m_memory_stats->memlatstat_lat_pw();
-       if (m_config.gpgpu_runtime_stat && (m_config.gpu_runtime_stat_flag != 0) ) {
-          if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_BW_STAT) {
-             for (unsigned i=0;i<m_memory_config->m_n_mem;i++)
-                m_memory_partition_unit[i]->print_stat(stdout);
-             printf("maxmrqlatency = %d \n", m_memory_stats->max_mrq_latency);
-             printf("maxmflatency = %d \n", m_memory_stats->max_mf_latency);
-          }
-          if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SHD_INFO)
-             shader_print_runtime_stat( stdout );
-          if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_L1MISS)
-             shader_print_l1_miss_stat( stdout );
-          if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SCHED)
-             shader_print_scheduler_stat( stdout, false );
-       }
+      time_t days, hrs, minutes, sec;
+      time_t curr_time;
+      time(&curr_time);
+      unsigned long long elapsed_time =
+          MAX(curr_time - gpgpu_ctx->the_gpgpusim->g_simulation_starttime, 1);
+      if ((elapsed_time - last_liveness_message_time) >=
+              m_config.liveness_message_freq &&
+          GPGPUSIM_DTRACE(LIVENESS)) {
+        days = elapsed_time / (3600 * 24);
+        hrs = elapsed_time / 3600 - 24 * days;
+        minutes = elapsed_time / 60 - 60 * (hrs + 24 * days);
+        sec = elapsed_time - 60 * (minutes + 60 * (hrs + 24 * days));
+
+        unsigned long long active = 0, total = 0;
+        for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+          m_cluster[i]->get_current_occupancy(active, total);
+        }
+        GPGPUSIM_DPRINTFG(LIVENESS,
+                 "uArch: inst.: %lld (ipc=%4.1f, occ=%0.4f\% [%llu / %llu]) "
+                 "sim_rate=%u (inst/sec) elapsed = %u:%u:%02u:%02u / %s",
+                 gpu_tot_sim_insn + gpu_sim_insn,
+                 (double)gpu_sim_insn / (double)gpu_sim_cycle,
+                 float(active) / float(total) * 100, active, total,
+                 (unsigned)((gpu_tot_sim_insn + gpu_sim_insn) / elapsed_time),
+                 (unsigned)days, (unsigned)hrs, (unsigned)minutes,
+                 (unsigned)sec, ctime(&curr_time));
+        fflush(stdout);
+        last_liveness_message_time = elapsed_time;
+      }
+      visualizer_printstat();
+      m_memory_stats->memlatstat_lat_pw();
+      if (m_config.gpgpu_runtime_stat &&
+          (m_config.gpu_runtime_stat_flag != 0)) {
+        if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_BW_STAT) {
+          for (unsigned i = 0; i < m_memory_config->m_n_mem; i++)
+            m_memory_partition_unit[i]->print_stat(stdout);
+          printf("maxmrqlatency = %d \n", m_memory_stats->max_mrq_latency);
+          printf("maxmflatency = %d \n", m_memory_stats->max_mf_latency);
+        }
+        if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SHD_INFO)
+          shader_print_runtime_stat(stdout);
+        if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_L1MISS)
+          shader_print_l1_miss_stat(stdout);
+        if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SCHED)
+          shader_print_scheduler_stat(stdout, false);
+      }
     }
 
-    if (!(gpu_sim_cycle % 2000000)) {
-       // deadlock detection
-       if (m_config.gpu_deadlock_detect && gpu_sim_insn == last_gpu_sim_insn) {
-          gpu_deadlock = true;
-       } else {
-          last_gpu_sim_insn = gpu_sim_insn;
-       }
+    if (!(gpu_sim_cycle % 50000)) {
+      // deadlock detection
+      if (m_config.gpu_deadlock_detect && gpu_sim_insn == last_gpu_sim_insn) {
+        gpu_deadlock = true;
+      } else {
+        last_gpu_sim_insn = gpu_sim_insn;
+      }
     }
     try_snap_shot(gpu_sim_cycle);
-    spill_log_to_file (stdout, 0, gpu_sim_cycle);
+    spill_log_to_file(stdout, 0, gpu_sim_cycle);
+
+#if (CUDART_VERSION >= 5000)
+    // launch device kernel
+    gpgpu_ctx->device_runtime->launch_one_device_kernel();
+#endif
+  }
 }
 
 void
@@ -1909,7 +2330,7 @@ gpgpu_sim::l2_cycle()
 //         time_t curr_time;
 //         time(&curr_time);
 //         unsigned long long  elapsed_time = MAX(curr_time - g_simulation_starttime, 1);
-//         if ( (elapsed_time - last_liveness_message_time) >= m_config.liveness_message_freq && DTRACE(LIVENESS) ) {
+//         if ( (elapsed_time - last_liveness_message_time) >= m_config.liveness_message_freq && GPGPUSIM_DTRACE(LIVENESS) ) {
 //            days    = elapsed_time/(3600*24);
 //            hrs     = elapsed_time/3600 - 24*days;
 //            minutes = elapsed_time/60 - 60*(hrs + 24*days);
@@ -1966,87 +2387,85 @@ gpgpu_sim::l2_cycle()
 //}
 
 
-void shader_core_ctx::dump_warp_state( FILE *fout ) const
-{
-   fprintf(fout, "\n");
-   fprintf(fout, "per warp functional simulation status:\n");
-   for (unsigned w=0; w < m_config->max_warps_per_shader; w++ ) 
-       m_warp[w].print(fout);
+void shader_core_ctx::dump_warp_state(FILE *fout) const {
+  fprintf(fout, "\n");
+  fprintf(fout, "per warp functional simulation status:\n");
+  for (unsigned w = 0; w < m_config->max_warps_per_shader; w++)
+    m_warp[w]->print(fout);
+}
+
+void gpgpu_sim::perf_memcpy_to_gpu(size_t dst_start_addr, size_t count) {
+  if (m_memory_config->m_perf_sim_memcpy) {
+    // if(!m_config.trace_driven_mode)    //in trace-driven mode, CUDA runtime
+    // can start nre data structure at any position 	assert (dst_start_addr %
+    // 32
+    //== 0);
+
+    for (unsigned counter = 0; counter < count; counter += 32) {
+      const unsigned wr_addr = dst_start_addr + counter;
+      addrdec_t raw_addr;
+      mem_access_sector_mask_t mask;
+      mask.set(wr_addr % 128 / 32);
+      m_memory_config->m_address_mapping.addrdec_tlx(wr_addr, &raw_addr);
+      const unsigned partition_id =
+          raw_addr.sub_partition /
+          m_memory_config->m_n_sub_partition_per_memory_channel;
+      m_memory_partition_unit[partition_id]->handle_memcpy_to_gpu(
+          wr_addr, raw_addr.sub_partition, mask);
+    }
+  }
 }
 
+void gpgpu_sim::dump_pipeline(int mask, int s, int m) const {
+  /*
+     You may want to use this function while running GPGPU-Sim in gdb.
+     One way to do that is add the following to your .gdbinit file:
 
-void gpgpu_sim::perf_memcpy_to_gpu( size_t dst_start_addr, size_t count )
-{
-    if (m_memory_config->m_perf_sim_memcpy) {
-       assert (dst_start_addr % 32 == 0);
-
-       for ( unsigned counter = 0; counter < count; counter += 32 ) {
-           const unsigned wr_addr = dst_start_addr + counter;
-           addrdec_t raw_addr;
-           mem_access_sector_mask_t mask;
-           mask.set(wr_addr % 128 / 32);
-           m_memory_config->m_address_mapping.addrdec_tlx( wr_addr, &raw_addr );
-           const unsigned partition_id = raw_addr.sub_partition / m_memory_config->m_n_sub_partition_per_memory_channel;
-           m_memory_partition_unit[ partition_id ]->handle_memcpy_to_gpu( wr_addr, raw_addr.sub_partition, mask );
-       }
-    }
-}
+        define dp
+           call g_the_gpu.dump_pipeline_impl((0x40|0x4|0x1),$arg0,0)
+        end
 
-void gpgpu_sim::dump_pipeline( int mask, int s, int m ) const
-{
-/*
-   You may want to use this function while running GPGPU-Sim in gdb.
-   One way to do that is add the following to your .gdbinit file:
- 
-      define dp
-         call g_the_gpu.dump_pipeline_impl((0x40|0x4|0x1),$arg0,0)
-      end
- 
-   Then, typing "dp 3" will show the contents of the pipeline for shader core 3.
-*/
-
-   printf("Dumping pipeline state...\n");
-   if(!mask) mask = 0xFFFFFFFF;
-   for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-      if(s != -1) {
-         i = s;
-      }
-      if(mask&1) m_cluster[m_shader_config->sid_to_cluster(i)]->display_pipeline(i,stdout,1,mask & 0x2E);
-      if(s != -1) {
-         break;
+     Then, typing "dp 3" will show the contents of the pipeline for shader
+     core 3.
+  */
+
+  printf("Dumping pipeline state...\n");
+  if (!mask) mask = 0xFFFFFFFF;
+  for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+    if (s != -1) {
+      i = s;
+    }
+    if (mask & 1)
+      m_cluster[m_shader_config->sid_to_cluster(i)]->display_pipeline(
+          i, stdout, 1, mask & 0x2E);
+    if (s != -1) {
+      break;
+    }
+  }
+  if (mask & 0x10000) {
+    for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
+      if (m != -1) {
+        i = m;
       }
-   }
-   if(mask&0x10000) {
-      for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
-         if(m != -1) {
-            i=m;
-         }
-         printf("DRAM / memory controller %u:\n", i);
-         if(mask&0x100000) m_memory_partition_unit[i]->print_stat(stdout);
-         if(mask&0x1000000)   m_memory_partition_unit[i]->visualize();
-         if(mask&0x10000000)   m_memory_partition_unit[i]->print(stdout);
-         if(m != -1) {
-            break;
-         }
+      printf("DRAM / memory controller %u:\n", i);
+      if (mask & 0x100000) m_memory_partition_unit[i]->print_stat(stdout);
+      if (mask & 0x1000000) m_memory_partition_unit[i]->visualize();
+      if (mask & 0x10000000) m_memory_partition_unit[i]->print(stdout);
+      if (m != -1) {
+        break;
       }
-   }
-   fflush(stdout);
+    }
+  }
+  fflush(stdout);
 }
 
-const struct shader_core_config * gpgpu_sim::getShaderCoreConfig()
-{
-   return m_shader_config;
+const shader_core_config *gpgpu_sim::getShaderCoreConfig() {
+  return m_shader_config;
 }
 
-const struct memory_config * gpgpu_sim::getMemoryConfig()
-{
-   return m_memory_config;
-}
+const memory_config *gpgpu_sim::getMemoryConfig() { return m_memory_config; }
 
-simt_core_cluster * gpgpu_sim::getSIMTCluster()
-{
-   return *m_cluster;
-}
+simt_core_cluster *gpgpu_sim::getSIMTCluster() { return *m_cluster; }
 
 // TODO schi add
 shader_core_ctx* gpgpu_sim::get_shader(int id)
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.h
index 73665639c3..f79ad4b156 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/gpu-sim.h
@@ -1,50 +1,52 @@
-// Copyright (c) 2009-2011, Tor M. Aamodt, Wilson W.L. Fung
-// The University of British Columbia
+// Copyright (c) 2009-2021, Tor M. Aamodt, Wilson W.L. Fung, Vijay Kandiah, Nikos Hardavellas
+// The University of British Columbia, Northwestern University
 // All rights reserved.
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
 //
-// Redistributions of source code must retain the above copyright notice, this
-// list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// 1. Redistributions of source code must retain the above copyright notice, this
+//    list of conditions and the following disclaimer;
+// 2. Redistributions in binary form must reproduce the above copyright notice,
+//    this list of conditions and the following disclaimer in the documentation
+//    and/or other materials provided with the distribution;
+// 3. Neither the names of The University of British Columbia, Northwestern 
+//    University nor the names of their contributors may be used to
+//    endorse or promote products derived from this software without specific
+//    prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
+
 
 #ifndef GPU_SIM_H
 #define GPU_SIM_H
 
-#include "../option_parser.h"
+#include <stdio.h>
+#include <fstream>
+#include <iostream>
+#include <list>
 #include "../abstract_hardware_model.h"
+#include "../option_parser.h"
 #include "../trace.h"
 #include "addrdec.h"
+#include "gpu-cache.h"
 #include "shader.h"
-#include <iostream>
-#include <fstream>
-#include <list>
-#include <stdio.h>
-
-
 
 // constants for statistics printouts
 #define GPU_RSTAT_SHD_INFO 0x1
-#define GPU_RSTAT_BW_STAT  0x2
+#define GPU_RSTAT_BW_STAT 0x2
 #define GPU_RSTAT_WARP_DIS 0x4
-#define GPU_RSTAT_DWF_MAP  0x8
+#define GPU_RSTAT_DWF_MAP 0x8
 #define GPU_RSTAT_L1MISS 0x10
 #define GPU_RSTAT_PDOM 0x20
 #define GPU_RSTAT_SCHED 0x40
@@ -62,548 +64,684 @@
 #define SAMPLELOG 222
 #define DUMPLOG 333
 
-extern tr1_hash_map<new_addr_type,unsigned> address_random_interleaving;
-
-
-enum dram_ctrl_t {
-   DRAM_FIFO=0,
-   DRAM_FRFCFS=1
+class gpgpu_context;
+
+extern tr1_hash_map<new_addr_type, unsigned> address_random_interleaving;
+
+enum dram_ctrl_t { DRAM_FIFO = 0, DRAM_FRFCFS = 1 };
+
+enum hw_perf_t {
+  HW_BENCH_NAME=0,
+  HW_KERNEL_NAME,
+  HW_L1_RH,
+  HW_L1_RM,
+  HW_L1_WH,
+  HW_L1_WM,
+  HW_CC_ACC,
+  HW_SHRD_ACC,
+  HW_DRAM_RD,
+  HW_DRAM_WR,
+  HW_L2_RH,
+  HW_L2_RM,
+  HW_L2_WH,
+  HW_L2_WM,
+  HW_NOC,
+  HW_PIPE_DUTY,
+  HW_NUM_SM_IDLE,
+  HW_CYCLES,
+  HW_VOLTAGE,
+  HW_TOTAL_STATS
 };
 
-
-
 struct power_config {
-	power_config()
-	{
-		m_valid = true;
-	}
-	void init()
-	{
-
-        // initialize file name if it is not set
-        time_t curr_time;
-        time(&curr_time);
-        char *date = ctime(&curr_time);
-        char *s = date;
-        while (*s) {
-            if (*s == ' ' || *s == '\t' || *s == ':') *s = '-';
-            if (*s == '\n' || *s == '\r' ) *s = 0;
-            s++;
-        }
-        char buf1[1024];
-        snprintf(buf1,1024,"gpgpusim_power_report__%s.log",date);
-        g_power_filename = strdup(buf1);
-        char buf2[1024];
-        snprintf(buf2,1024,"gpgpusim_power_trace_report__%s.log.gz",date);
-        g_power_trace_filename = strdup(buf2);
-        char buf3[1024];
-        snprintf(buf3,1024,"gpgpusim_metric_trace_report__%s.log.gz",date);
-        g_metric_trace_filename = strdup(buf3);
-        char buf4[1024];
-        snprintf(buf4,1024,"gpgpusim_steady_state_tracking_report__%s.log.gz",date);
-        g_steady_state_tracking_filename = strdup(buf4);
-
-        if(g_steady_power_levels_enabled){
-            sscanf(gpu_steady_state_definition,"%lf:%lf", &gpu_steady_power_deviation,&gpu_steady_min_period);
-        }
-
-        //NOTE: After changing the nonlinear model to only scaling idle core,
-        //NOTE: The min_inc_per_active_sm is not used any more
-		if (g_use_nonlinear_model)
-		    sscanf(gpu_nonlinear_model_config,"%lf:%lf", &gpu_idle_core_power,&gpu_min_inc_per_active_sm);
-
-	}
-	void reg_options(class OptionParser * opp);
-
-	char *g_power_config_name;
-
-	bool m_valid;
-    bool g_power_simulation_enabled;
-    bool g_power_trace_enabled;
-    bool g_steady_power_levels_enabled;
-    bool g_power_per_cycle_dump;
-    bool g_power_simulator_debug;
-    char *g_power_filename;
-    char *g_power_trace_filename;
-    char *g_metric_trace_filename;
-    char * g_steady_state_tracking_filename;
-    int g_power_trace_zlevel;
-    char * gpu_steady_state_definition;
-    double gpu_steady_power_deviation;
-    double gpu_steady_min_period;
-
-    //Nonlinear power model
-    bool g_use_nonlinear_model;
-    char * gpu_nonlinear_model_config;
-    double gpu_idle_core_power;
-    double gpu_min_inc_per_active_sm;
-
+  power_config() { m_valid = true; }
+  void init() {
+    // initialize file name if it is not set
+    time_t curr_time;
+    time(&curr_time);
+    char *date = ctime(&curr_time);
+    char *s = date;
+    while (*s) {
+      if (*s == ' ' || *s == '\t' || *s == ':') *s = '-';
+      if (*s == '\n' || *s == '\r') *s = 0;
+      s++;
+    }
+    char buf1[1024];
+    //snprintf(buf1, 1024, "accelwattch_power_report__%s.log", date);
+    snprintf(buf1, 1024, "accelwattch_power_report.log");
+    g_power_filename = strdup(buf1);
+    char buf2[1024];
+    snprintf(buf2, 1024, "gpgpusim_power_trace_report__%s.log.gz", date);
+    g_power_trace_filename = strdup(buf2);
+    char buf3[1024];
+    snprintf(buf3, 1024, "gpgpusim_metric_trace_report__%s.log.gz", date);
+    g_metric_trace_filename = strdup(buf3);
+    char buf4[1024];
+    snprintf(buf4, 1024, "gpgpusim_steady_state_tracking_report__%s.log.gz",
+             date);
+    g_steady_state_tracking_filename = strdup(buf4);
+    // for(int i =0; i< hw_perf_t::HW_TOTAL_STATS; i++){
+    //   accelwattch_hybrid_configuration[i] = 0;
+    // }
+
+    if (g_steady_power_levels_enabled) {
+      sscanf(gpu_steady_state_definition, "%lf:%lf",
+             &gpu_steady_power_deviation, &gpu_steady_min_period);
+    }
 
+    // NOTE: After changing the nonlinear model to only scaling idle core,
+    // NOTE: The min_inc_per_active_sm is not used any more
+    if (g_use_nonlinear_model)
+      sscanf(gpu_nonlinear_model_config, "%lf:%lf", &gpu_idle_core_power,
+             &gpu_min_inc_per_active_sm);
+  }
+  void reg_options(class OptionParser *opp);
+
+  char *g_power_config_name;
+
+  bool m_valid;
+  bool g_power_simulation_enabled;
+  bool g_power_trace_enabled;
+  bool g_steady_power_levels_enabled;
+  bool g_power_per_cycle_dump;
+  bool g_power_simulator_debug;
+  char *g_power_filename;
+  char *g_power_trace_filename;
+  char *g_metric_trace_filename;
+  char *g_steady_state_tracking_filename;
+  int g_power_trace_zlevel;
+  char *gpu_steady_state_definition;
+  double gpu_steady_power_deviation;
+  double gpu_steady_min_period;
+
+
+  char *g_hw_perf_file_name;
+  char *g_hw_perf_bench_name;
+  int g_power_simulation_mode;
+  bool g_dvfs_enabled;
+  bool g_aggregate_power_stats;
+  bool accelwattch_hybrid_configuration[hw_perf_t::HW_TOTAL_STATS];
+
+  // Nonlinear power model
+  bool g_use_nonlinear_model;
+  char *gpu_nonlinear_model_config;
+  double gpu_idle_core_power;
+  double gpu_min_inc_per_active_sm;
 };
 
+class memory_config {
+ public:
+  memory_config(gpgpu_context *ctx) {
+    m_valid = false;
+    gpgpu_dram_timing_opt = NULL;
+    gpgpu_L2_queue_config = NULL;
+    gpgpu_ctx = ctx;
+  }
+  void init() {
+    assert(gpgpu_dram_timing_opt);
+    if (strchr(gpgpu_dram_timing_opt, '=') == NULL) {
+      // dram timing option in ordered variables (legacy)
+      // Disabling bank groups if their values are not specified
+      nbkgrp = 1;
+      tCCDL = 0;
+      tRTPL = 0;
+      sscanf(gpgpu_dram_timing_opt, "%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d",
+             &nbk, &tCCD, &tRRD, &tRCD, &tRAS, &tRP, &tRC, &CL, &WL, &tCDLR,
+             &tWR, &nbkgrp, &tCCDL, &tRTPL);
+    } else {
+      // named dram timing options (unordered)
+      option_parser_t dram_opp = option_parser_create();
+
+      option_parser_register(dram_opp, "nbk", OPT_UINT32, &nbk,
+                             "number of banks", "");
+      option_parser_register(dram_opp, "CCD", OPT_UINT32, &tCCD,
+                             "column to column delay", "");
+      option_parser_register(
+          dram_opp, "RRD", OPT_UINT32, &tRRD,
+          "minimal delay between activation of rows in different banks", "");
+      option_parser_register(dram_opp, "RCD", OPT_UINT32, &tRCD,
+                             "row to column delay", "");
+      option_parser_register(dram_opp, "RAS", OPT_UINT32, &tRAS,
+                             "time needed to activate row", "");
+      option_parser_register(dram_opp, "RP", OPT_UINT32, &tRP,
+                             "time needed to precharge (deactivate) row", "");
+      option_parser_register(dram_opp, "RC", OPT_UINT32, &tRC, "row cycle time",
+                             "");
+      option_parser_register(dram_opp, "CDLR", OPT_UINT32, &tCDLR,
+                             "switching from write to read (changes tWTR)", "");
+      option_parser_register(dram_opp, "WR", OPT_UINT32, &tWR,
+                             "last data-in to row precharge", "");
+
+      option_parser_register(dram_opp, "CL", OPT_UINT32, &CL, "CAS latency",
+                             "");
+      option_parser_register(dram_opp, "WL", OPT_UINT32, &WL, "Write latency",
+                             "");
+
+      // Disabling bank groups if their values are not specified
+      option_parser_register(dram_opp, "nbkgrp", OPT_UINT32, &nbkgrp,
+                             "number of bank groups", "1");
+      option_parser_register(
+          dram_opp, "CCDL", OPT_UINT32, &tCCDL,
+          "column to column delay between accesses to different bank groups",
+          "0");
+      option_parser_register(
+          dram_opp, "RTPL", OPT_UINT32, &tRTPL,
+          "read to precharge delay between accesses to different bank groups",
+          "0");
+
+      option_parser_delimited_string(dram_opp, gpgpu_dram_timing_opt, "=:;");
+      fprintf(stdout, "DRAM Timing Options:\n");
+      option_parser_print(dram_opp, stdout);
+      option_parser_destroy(dram_opp);
+    }
 
-
-struct memory_config {
-   memory_config()
-   {
-       m_valid = false;
-       gpgpu_dram_timing_opt=NULL;
-       gpgpu_L2_queue_config=NULL;
-   }
-   void init()
-   {
-      assert(gpgpu_dram_timing_opt);
-      if (strchr(gpgpu_dram_timing_opt, '=') == NULL) {
-         // dram timing option in ordered variables (legacy)
-         // Disabling bank groups if their values are not specified
-         nbkgrp = 1;
-         tCCDL = 0;
-         tRTPL = 0;
-         sscanf(gpgpu_dram_timing_opt,"%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d:%d",
-                &nbk,&tCCD,&tRRD,&tRCD,&tRAS,&tRP,&tRC,&CL,&WL,&tCDLR,&tWR,&nbkgrp,&tCCDL,&tRTPL);
-      } else {
-         // named dram timing options (unordered)
-         option_parser_t dram_opp = option_parser_create(); 
-
-         option_parser_register(dram_opp, "nbk",  OPT_UINT32, &nbk,   "number of banks", ""); 
-         option_parser_register(dram_opp, "CCD",  OPT_UINT32, &tCCD,  "column to column delay", ""); 
-         option_parser_register(dram_opp, "RRD",  OPT_UINT32, &tRRD,  "minimal delay between activation of rows in different banks", ""); 
-         option_parser_register(dram_opp, "RCD",  OPT_UINT32, &tRCD,  "row to column delay", ""); 
-         option_parser_register(dram_opp, "RAS",  OPT_UINT32, &tRAS,  "time needed to activate row", ""); 
-         option_parser_register(dram_opp, "RP",   OPT_UINT32, &tRP,   "time needed to precharge (deactivate) row", ""); 
-         option_parser_register(dram_opp, "RC",   OPT_UINT32, &tRC,   "row cycle time", ""); 
-         option_parser_register(dram_opp, "CDLR", OPT_UINT32, &tCDLR, "switching from write to read (changes tWTR)", ""); 
-         option_parser_register(dram_opp, "WR",   OPT_UINT32, &tWR,   "last data-in to row precharge", ""); 
-
-         option_parser_register(dram_opp, "CL", OPT_UINT32, &CL, "CAS latency", ""); 
-         option_parser_register(dram_opp, "WL", OPT_UINT32, &WL, "Write latency", ""); 
-
-         //Disabling bank groups if their values are not specified
-         option_parser_register(dram_opp, "nbkgrp", OPT_UINT32, &nbkgrp, "number of bank groups", "1"); 
-         option_parser_register(dram_opp, "CCDL",   OPT_UINT32, &tCCDL,  "column to column delay between accesses to different bank groups", "0"); 
-         option_parser_register(dram_opp, "RTPL",   OPT_UINT32, &tRTPL,  "read to precharge delay between accesses to different bank groups", "0"); 
-
-         option_parser_delimited_string(dram_opp, gpgpu_dram_timing_opt, "=:;"); 
-         fprintf(stdout, "DRAM Timing Options:\n"); 
-         option_parser_print(dram_opp, stdout); 
-         option_parser_destroy(dram_opp); 
-      }
-
-      int nbkt = nbk/nbkgrp;
-      unsigned i;
-      for (i=0; nbkt>0; i++) {
-          nbkt = nbkt>>1;
-      }
-      bk_tag_length = i-1;
-      assert(nbkgrp>0 && "Number of bank groups cannot be zero");
-      tRCDWR = tRCD-(WL+1);
-      if(elimnate_rw_turnaround)
-      {
-    	  tRTW = 0;
-    	  tWTR = 0;
-      } else {
-      tRTW = (CL+(BL/data_command_freq_ratio)+2-WL);
-      tWTR = (WL+(BL/data_command_freq_ratio)+tCDLR);
-      }
-      tWTP = (WL+(BL/data_command_freq_ratio)+tWR);
-      dram_atom_size = BL * busW * gpu_n_mem_per_ctrlr; // burst length x bus width x # chips per partition 
-
-      assert( m_n_sub_partition_per_memory_channel > 0 ); 
-      assert( (nbk % m_n_sub_partition_per_memory_channel == 0) 
-              && "Number of DRAM banks must be a perfect multiple of memory sub partition"); 
-      m_n_mem_sub_partition = m_n_mem * m_n_sub_partition_per_memory_channel; 
-      fprintf(stdout, "Total number of memory sub partition = %u\n", m_n_mem_sub_partition); 
-
-      m_address_mapping.init(m_n_mem, m_n_sub_partition_per_memory_channel);
-      m_L2_config.init(&m_address_mapping);
-
-      m_valid = true;
-
-      sscanf(write_queue_size_opt,"%d:%d:%d",
-                     &gpgpu_frfcfs_dram_write_queue_size,&write_high_watermark,&write_low_watermark);
-   }
-   void reg_options(class OptionParser * opp);
-
-   bool m_valid;
-   mutable l2_cache_config m_L2_config;
-   bool m_L2_texure_only;
-
-   char *gpgpu_dram_timing_opt;
-   char *gpgpu_L2_queue_config;
-   bool l2_ideal;
-   unsigned gpgpu_frfcfs_dram_sched_queue_size;
-   unsigned gpgpu_dram_return_queue_size;
-   enum dram_ctrl_t scheduler_type;
-   bool gpgpu_memlatency_stat;
-   unsigned m_n_mem;
-   unsigned m_n_sub_partition_per_memory_channel;
-   unsigned m_n_mem_sub_partition;
-   unsigned gpu_n_mem_per_ctrlr;
-
-   unsigned rop_latency;
-   unsigned dram_latency;
-
-   // DRAM parameters
-
-   unsigned tCCDL;  //column to column delay when bank groups are enabled
-   unsigned tRTPL;  //read to precharge delay when bank groups are enabled for GDDR5 this is identical to RTPS, if for other DRAM this is different, you will need to split them in two
-
-   unsigned tCCD;   //column to column delay
-   unsigned tRRD;   //minimal time required between activation of rows in different banks
-   unsigned tRCD;   //row to column delay - time required to activate a row before a read
-   unsigned tRCDWR; //row to column delay for a write command
-   unsigned tRAS;   //time needed to activate row
-   unsigned tRP;    //row precharge ie. deactivate row
-   unsigned tRC;    //row cycle time ie. precharge current, then activate different row
-   unsigned tCDLR;  //Last data-in to Read command (switching from write to read)
-   unsigned tWR;    //Last data-in to Row precharge 
-
-   unsigned CL;     //CAS latency
-   unsigned WL;     //WRITE latency
-   unsigned BL;     //Burst Length in bytes (4 in GDDR3, 8 in GDDR5)
-   unsigned tRTW;   //time to switch from read to write
-   unsigned tWTR;   //time to switch from write to read 
-   unsigned tWTP;   //time to switch from write to precharge in the same bank
-   unsigned busW;
-
-   unsigned nbkgrp; // number of bank groups (has to be power of 2)
-   unsigned bk_tag_length; //number of bits that define a bank inside a bank group
-
-   unsigned nbk;
-
-   bool elimnate_rw_turnaround;
-
-   unsigned data_command_freq_ratio; // frequency ratio between DRAM data bus and command bus (2 for GDDR3, 4 for GDDR5)
-   unsigned dram_atom_size; // number of bytes transferred per read or write command 
-
-   linear_to_raw_address_translation m_address_mapping;
-
-   unsigned icnt_flit_size;
-
-   unsigned dram_bnk_indexing_policy;
-   unsigned dram_bnkgrp_indexing_policy;
-   bool dual_bus_interface;
-
-   bool seperate_write_queue_enabled;
-   char *write_queue_size_opt;
-   unsigned gpgpu_frfcfs_dram_write_queue_size;
-   unsigned write_high_watermark;
-   unsigned write_low_watermark;
-   bool m_perf_sim_memcpy;
+    int nbkt = nbk / nbkgrp;
+    unsigned i;
+    for (i = 0; nbkt > 0; i++) {
+      nbkt = nbkt >> 1;
+    }
+    bk_tag_length = i - 1;
+    assert(nbkgrp > 0 && "Number of bank groups cannot be zero");
+    tRCDWR = tRCD - (WL + 1);
+    if (elimnate_rw_turnaround) {
+      tRTW = 0;
+      tWTR = 0;
+    } else {
+      tRTW = (CL + (BL / data_command_freq_ratio) + 2 - WL);
+      tWTR = (WL + (BL / data_command_freq_ratio) + tCDLR);
+    }
+    tWTP = (WL + (BL / data_command_freq_ratio) + tWR);
+    dram_atom_size =
+        BL * busW * gpu_n_mem_per_ctrlr;  // burst length x bus width x # chips
+                                          // per partition
+
+    assert(m_n_sub_partition_per_memory_channel > 0);
+    assert((nbk % m_n_sub_partition_per_memory_channel == 0) &&
+           "Number of DRAM banks must be a perfect multiple of memory sub "
+           "partition");
+    m_n_mem_sub_partition = m_n_mem * m_n_sub_partition_per_memory_channel;
+    fprintf(stdout, "Total number of memory sub partition = %u\n",
+            m_n_mem_sub_partition);
+
+    m_address_mapping.init(m_n_mem, m_n_sub_partition_per_memory_channel);
+    m_L2_config.init(&m_address_mapping);
+
+    m_valid = true;
+
+    sscanf(write_queue_size_opt, "%d:%d:%d",
+           &gpgpu_frfcfs_dram_write_queue_size, &write_high_watermark,
+           &write_low_watermark);
+  }
+  void reg_options(class OptionParser *opp);
+
+  bool m_valid;
+  mutable l2_cache_config m_L2_config;
+  bool m_L2_texure_only;
+
+  char *gpgpu_dram_timing_opt;
+  char *gpgpu_L2_queue_config;
+  bool l2_ideal;
+  unsigned gpgpu_frfcfs_dram_sched_queue_size;
+  unsigned gpgpu_dram_return_queue_size;
+  enum dram_ctrl_t scheduler_type;
+  bool gpgpu_memlatency_stat;
+  unsigned m_n_mem;
+  unsigned m_n_sub_partition_per_memory_channel;
+  unsigned m_n_mem_sub_partition;
+  unsigned gpu_n_mem_per_ctrlr;
+
+  unsigned rop_latency;
+  unsigned dram_latency;
+
+  // DRAM parameters
+
+  unsigned tCCDL;  // column to column delay when bank groups are enabled
+  unsigned tRTPL;  // read to precharge delay when bank groups are enabled for
+                   // GDDR5 this is identical to RTPS, if for other DRAM this is
+                   // different, you will need to split them in two
+
+  unsigned tCCD;    // column to column delay
+  unsigned tRRD;    // minimal time required between activation of rows in
+                    // different banks
+  unsigned tRCD;    // row to column delay - time required to activate a row
+                    // before a read
+  unsigned tRCDWR;  // row to column delay for a write command
+  unsigned tRAS;    // time needed to activate row
+  unsigned tRP;     // row precharge ie. deactivate row
+  unsigned
+      tRC;  // row cycle time ie. precharge current, then activate different row
+  unsigned tCDLR;  // Last data-in to Read command (switching from write to
+                   // read)
+  unsigned tWR;    // Last data-in to Row precharge
+
+  unsigned CL;    // CAS latency
+  unsigned WL;    // WRITE latency
+  unsigned BL;    // Burst Length in bytes (4 in GDDR3, 8 in GDDR5)
+  unsigned tRTW;  // time to switch from read to write
+  unsigned tWTR;  // time to switch from write to read
+  unsigned tWTP;  // time to switch from write to precharge in the same bank
+  unsigned busW;
+
+  unsigned nbkgrp;  // number of bank groups (has to be power of 2)
+  unsigned
+      bk_tag_length;  // number of bits that define a bank inside a bank group
+
+  unsigned nbk;
+
+  bool elimnate_rw_turnaround;
+
+  unsigned
+      data_command_freq_ratio;  // frequency ratio between DRAM data bus and
+                                // command bus (2 for GDDR3, 4 for GDDR5)
+  unsigned
+      dram_atom_size;  // number of bytes transferred per read or write command
+
+  linear_to_raw_address_translation m_address_mapping;
+
+  unsigned icnt_flit_size;
+
+  unsigned dram_bnk_indexing_policy;
+  unsigned dram_bnkgrp_indexing_policy;
+  bool dual_bus_interface;
+
+  bool seperate_write_queue_enabled;
+  char *write_queue_size_opt;
+  unsigned gpgpu_frfcfs_dram_write_queue_size;
+  unsigned write_high_watermark;
+  unsigned write_low_watermark;
+  bool m_perf_sim_memcpy;
+  bool simple_dram_model;
+
+  gpgpu_context *gpgpu_ctx;
 };
 
-// global counters and flags (please try not to add to this list!!!)
-extern unsigned long long  gpu_sim_cycle;
-extern unsigned long long  gpu_tot_sim_cycle;
 extern bool g_interactive_debugger_enabled;
 
-class gpgpu_sim_config : public power_config, public gpgpu_functional_sim_config {
-public:
-    gpgpu_sim_config() { m_valid = false; }
-    void reg_options(class OptionParser * opp);
-    void init() 
-    {
-        gpu_stat_sample_freq = 10000;
-        gpu_runtime_stat_flag = 0;
-        sscanf(gpgpu_runtime_stat, "%d:%x", &gpu_stat_sample_freq, &gpu_runtime_stat_flag);
-        m_shader_config.init();
-        ptx_set_tex_cache_linesize(m_shader_config.m_L1T_config.get_line_sz());
-        m_memory_config.init();
-        init_clock_domains(); 
-        power_config::init();
-        Trace_gpgpu::init();
-
-
-        // initialize file name if it is not set 
-        time_t curr_time;
-        time(&curr_time);
-        char *date = ctime(&curr_time);
-        char *s = date;
-        while (*s) {
-            if (*s == ' ' || *s == '\t' || *s == ':') *s = '-';
-            if (*s == '\n' || *s == '\r' ) *s = 0;
-            s++;
-        }
-        char buf[1024];
-        snprintf(buf,1024,"gpgpusim_visualizer__%s.log.gz",date);
-        g_visualizer_filename = strdup(buf);
-
-        m_valid=true;
+class gpgpu_sim_config : public power_config,
+                         public gpgpu_functional_sim_config {
+ public:
+  gpgpu_sim_config(gpgpu_context *ctx)
+      : m_shader_config(ctx), m_memory_config(ctx) {
+    m_valid = false;
+    gpgpu_ctx = ctx;
+  }
+  void reg_options(class OptionParser *opp);
+  void init() {
+    gpu_stat_sample_freq = 10000;
+    gpu_runtime_stat_flag = 0;
+    sscanf(gpgpu_runtime_stat, "%d:%x", &gpu_stat_sample_freq,
+           &gpu_runtime_stat_flag);
+    m_shader_config.init();
+    ptx_set_tex_cache_linesize(m_shader_config.m_L1T_config.get_line_sz());
+    m_memory_config.init();
+    init_clock_domains();
+    power_config::init();
+    Trace_gpgpu::init();
+
+    // initialize file name if it is not set
+    time_t curr_time;
+    time(&curr_time);
+    char *date = ctime(&curr_time);
+    char *s = date;
+    while (*s) {
+      if (*s == ' ' || *s == '\t' || *s == ':') *s = '-';
+      if (*s == '\n' || *s == '\r') *s = 0;
+      s++;
     }
-
-    unsigned num_shader() const { return m_shader_config.num_shader(); }
-    unsigned num_cluster() const { return m_shader_config.n_simt_clusters; }
-    unsigned get_max_concurrent_kernel() const { return max_concurrent_kernel; }
-    unsigned checkpoint_option;
-
-    size_t stack_limit() const {return stack_size_limit; }
-    size_t heap_limit() const {return heap_size_limit; }
-    size_t sync_depth_limit() const {return runtime_sync_depth_limit; }
-    size_t pending_launch_count_limit() const {return runtime_pending_launch_count_limit;}
-
-private:
-    void init_clock_domains(void ); 
-
-
-    bool m_valid;
-    shader_core_config m_shader_config;
-    memory_config m_memory_config;
-    // clock domains - frequency
-    double core_freq;
-    double icnt_freq;
-    double dram_freq;
-    double l2_freq;
-    double core_period;
-    double icnt_period;
-    double dram_period;
-    double l2_period;
-
-    // GPGPU-Sim timing model options
-    unsigned gpu_max_cycle_opt;
-    unsigned gpu_max_insn_opt;
-    unsigned gpu_max_cta_opt;
-    char *gpgpu_runtime_stat;
-    bool  gpgpu_flush_l1_cache;
-    bool  gpgpu_flush_l2_cache;
-    bool  gpu_deadlock_detect;
-    int   gpgpu_frfcfs_dram_sched_queue_size; 
-    int   gpgpu_cflog_interval;
-    char * gpgpu_clock_domains;
-    unsigned max_concurrent_kernel;
-
-    // visualizer
-    bool  g_visualizer_enabled;
-    char *g_visualizer_filename;
-    int   g_visualizer_zlevel;
-
-
-    // statistics collection
-    int gpu_stat_sample_freq;
-    int gpu_runtime_stat_flag;
-
-    // Device Limits
-    size_t stack_size_limit;
-    size_t heap_size_limit;
-    size_t runtime_sync_depth_limit;
-    size_t runtime_pending_launch_count_limit;	
-
- //gpu compute capability options
-    unsigned int gpgpu_compute_capability_major;
-    unsigned int gpgpu_compute_capability_minor;
-    unsigned long long liveness_message_freq; 
-
-    friend class gpgpu_sim;
+    char buf[1024];
+    snprintf(buf, 1024, "gpgpusim_visualizer__%s.log.gz", date);
+    g_visualizer_filename = strdup(buf);
+
+    m_valid = true;
+  }
+  unsigned get_core_freq() const { return core_freq; }
+  unsigned num_shader() const { return m_shader_config.num_shader(); }
+  unsigned num_cluster() const { return m_shader_config.n_simt_clusters; }
+  unsigned get_max_concurrent_kernel() const { return max_concurrent_kernel; }
+  unsigned checkpoint_option;
+
+  size_t stack_limit() const { return stack_size_limit; }
+  size_t heap_limit() const { return heap_size_limit; }
+  size_t sync_depth_limit() const { return runtime_sync_depth_limit; }
+  size_t pending_launch_count_limit() const {
+    return runtime_pending_launch_count_limit;
+  }
+
+  bool flush_l1() const { return gpgpu_flush_l1_cache; }
+
+ private:
+  void init_clock_domains(void);
+
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+  bool m_valid;
+  shader_core_config m_shader_config;
+  memory_config m_memory_config;
+  // clock domains - frequency
+  double core_freq;
+  double icnt_freq;
+  double dram_freq;
+  double l2_freq;
+  double core_period;
+  double icnt_period;
+  double dram_period;
+  double l2_period;
+
+  // GPGPU-Sim timing model options
+  unsigned long long gpu_max_cycle_opt;
+  unsigned long long gpu_max_insn_opt;
+  unsigned gpu_max_cta_opt;
+  unsigned gpu_max_completed_cta_opt;
+  char *gpgpu_runtime_stat;
+  bool gpgpu_flush_l1_cache;
+  bool gpgpu_flush_l2_cache;
+  bool gpu_deadlock_detect;
+  int gpgpu_frfcfs_dram_sched_queue_size;
+  int gpgpu_cflog_interval;
+  char *gpgpu_clock_domains;
+  unsigned max_concurrent_kernel;
+
+  // visualizer
+  bool g_visualizer_enabled;
+  char *g_visualizer_filename;
+  int g_visualizer_zlevel;
+
+  // statistics collection
+  int gpu_stat_sample_freq;
+  int gpu_runtime_stat_flag;
+
+  // Device Limits
+  size_t stack_size_limit;
+  size_t heap_size_limit;
+  size_t runtime_sync_depth_limit;
+  size_t runtime_pending_launch_count_limit;
+
+  // gpu compute capability options
+  unsigned int gpgpu_compute_capability_major;
+  unsigned int gpgpu_compute_capability_minor;
+  unsigned long long liveness_message_freq;
+
+  friend class gpgpu_sim;
 };
 
 struct occupancy_stats {
-    occupancy_stats() : aggregate_warp_slot_filled(0), aggregate_theoretical_warp_slots(0){}
-    occupancy_stats( unsigned long long wsf, unsigned long long tws )
-        : aggregate_warp_slot_filled(wsf), aggregate_theoretical_warp_slots(tws){}
-
-    unsigned long long aggregate_warp_slot_filled;
-    unsigned long long aggregate_theoretical_warp_slots;
-
-    float get_occ_fraction() const {
-        return float(aggregate_warp_slot_filled) / float(aggregate_theoretical_warp_slots);
-    }
-
-    occupancy_stats& operator+=(const occupancy_stats& rhs) {
-        aggregate_warp_slot_filled += rhs.aggregate_warp_slot_filled;
-        aggregate_theoretical_warp_slots += rhs.aggregate_theoretical_warp_slots;
-        return *this;
-    }
-
-    occupancy_stats operator+(const occupancy_stats& rhs) const{
-        return occupancy_stats( aggregate_warp_slot_filled + rhs.aggregate_warp_slot_filled,
-                                aggregate_theoretical_warp_slots + rhs.aggregate_theoretical_warp_slots
-                               );
-    }
+  occupancy_stats()
+      : aggregate_warp_slot_filled(0), aggregate_theoretical_warp_slots(0) {}
+  occupancy_stats(unsigned long long wsf, unsigned long long tws)
+      : aggregate_warp_slot_filled(wsf),
+        aggregate_theoretical_warp_slots(tws) {}
+
+  unsigned long long aggregate_warp_slot_filled;
+  unsigned long long aggregate_theoretical_warp_slots;
+
+  float get_occ_fraction() const {
+    return float(aggregate_warp_slot_filled) /
+           float(aggregate_theoretical_warp_slots);
+  }
+
+  occupancy_stats &operator+=(const occupancy_stats &rhs) {
+    aggregate_warp_slot_filled += rhs.aggregate_warp_slot_filled;
+    aggregate_theoretical_warp_slots += rhs.aggregate_theoretical_warp_slots;
+    return *this;
+  }
+
+  occupancy_stats operator+(const occupancy_stats &rhs) const {
+    return occupancy_stats(
+        aggregate_warp_slot_filled + rhs.aggregate_warp_slot_filled,
+        aggregate_theoretical_warp_slots +
+            rhs.aggregate_theoretical_warp_slots);
+  }
 };
 
+class gpgpu_context;
+class ptx_instruction;
+
+class watchpoint_event {
+ public:
+  watchpoint_event() {
+    m_thread = NULL;
+    m_inst = NULL;
+  }
+  watchpoint_event(const ptx_thread_info *thd, const ptx_instruction *pI) {
+    m_thread = thd;
+    m_inst = pI;
+  }
+  const ptx_thread_info *thread() const { return m_thread; }
+  const ptx_instruction *inst() const { return m_inst; }
+
+ private:
+  const ptx_thread_info *m_thread;
+  const ptx_instruction *m_inst;
+};
 
 class gpgpu_sim : public gpgpu_t {
-public:
-   gpgpu_sim( const gpgpu_sim_config &config, gem5::CudaGPU *cuda_gpu = NULL );
-
-   void set_prop( struct cudaDeviceProp *prop );
-
-   void launch( kernel_info_t *kinfo );
-   bool can_start_kernel();
-   unsigned finished_kernel();
-   void set_kernel_done( kernel_info_t *kernel );
-   void stop_all_running_kernels();
-
-   void init();
-   // void cycle();
-   void core_cycle_start();
-   void core_cycle_end();
-   void icnt_cycle_start();
-   void icnt_cycle_end();
-   void l2_cycle();
-   void dram_cycle();
-
-   bool active(); 
-   bool cycle_insn_cta_max_hit() {
-       return (m_config.gpu_max_cycle_opt && (gpu_tot_sim_cycle + gpu_sim_cycle) >= m_config.gpu_max_cycle_opt) ||
-           (m_config.gpu_max_insn_opt && (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt) ||
-           (m_config.gpu_max_cta_opt && (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt) );
-   }
-   void print_stats();
-   void update_stats();
-   void deadlock_check();
-
-   void get_pdom_stack_top_info( unsigned sid, unsigned tid, unsigned *pc, unsigned *rpc );
-
-   int shared_mem_size() const;
-   int shared_mem_per_block() const;
-   int compute_capability_major() const;
-   int compute_capability_minor() const;
-   int num_registers_per_core() const;
-   int num_registers_per_block() const;
-   int wrp_size() const;
-   int shader_clock() const;
-   const struct cudaDeviceProp *get_prop() const;
-   enum divergence_support_t simd_model() const; 
-
-   unsigned threads_per_core() const;
-   bool get_more_cta_left() const;
-   bool kernel_more_cta_left(kernel_info_t *kernel) const;
-   bool hit_max_cta_count() const;
-   kernel_info_t *select_kernel();
-
-   const gpgpu_sim_config &get_config() const { return m_config; }
-   void gpu_print_stat();
-   void dump_pipeline( int mask, int s, int m ) const;
-
-   // TODO schi add
-   shader_core_ctx* get_shader(int id);
-
-
-    void perf_memcpy_to_gpu( size_t dst_start_addr, size_t count );
-
-   //The next three functions added to be used by the functional simulation function
-   
-   //! Get shader core configuration
-   /*!
-    * Returning the configuration of the shader core, used by the functional simulation only so far
-    */
-   const struct shader_core_config * getShaderCoreConfig();
-   
-   
-   //! Get shader core Memory Configuration
-    /*!
-    * Returning the memory configuration of the shader core, used by the functional simulation only so far
-    */
-   const struct memory_config * getMemoryConfig();
-   
-   
-   //! Get shader core SIMT cluster
-   /*!
-    * Returning the cluster of of the shader core, used by the functional simulation so far
-    */
-    simt_core_cluster * getSIMTCluster();
-
-
-private:
-   // clocks
-   void reinit_clock_domains(void);
-   int  next_clock_domain(void);
-   void issue_block2core();
-   void print_dram_stats(FILE *fout) const;
-   void shader_print_runtime_stat( FILE *fout );
-   void shader_print_l1_miss_stat( FILE *fout ) const;
-   void shader_print_cache_stats( FILE *fout ) const;
-   void shader_print_scheduler_stat( FILE* fout, bool print_dynamic_info ) const;
-   void visualizer_printstat();
-   void print_shader_cycle_distro( FILE *fout ) const;
-
-   void gpgpu_debug();
-
-///// data /////
-
-   class simt_core_cluster **m_cluster;
-   class memory_partition_unit **m_memory_partition_unit;
-   class memory_sub_partition **m_memory_sub_partition;
-
-   std::vector<kernel_info_t*> m_running_kernels;
-   unsigned m_last_issued_kernel;
-
-   std::list<unsigned> m_finished_kernel;
-   // m_total_cta_launched == per-kernel count. gpu_tot_issued_cta == global count.
-   unsigned long long m_total_cta_launched;
-   unsigned long long gpu_tot_issued_cta;
-
-   unsigned m_last_cluster_issue;
-   float * average_pipeline_duty_cycle;
-   float * active_sms;
-   // time of next rising edge 
-   double core_time;
-   double icnt_time;
-   double dram_time;
-   double l2_time;
-
-   // debug
-   bool gpu_deadlock;
-
-   //// configuration parameters ////
-   const gpgpu_sim_config &m_config;
-  
-   const struct cudaDeviceProp     *m_cuda_properties;
-   const struct shader_core_config *m_shader_config;
-   const struct memory_config      *m_memory_config;
-
-   // stats
-   class shader_core_stats  *m_shader_stats;
-   class memory_stats_t     *m_memory_stats;
-   class power_stat_t *m_power_stats;
-   class gpgpu_sim_wrapper *m_gpgpusim_wrapper;
-   unsigned long long  last_gpu_sim_insn;
-
-   unsigned long long  last_liveness_message_time; 
-
-   std::map<std::string, FuncCache> m_special_cache_config;
-
-   std::vector<std::string> m_executed_kernel_names; //< names of kernel for stat printout 
-   std::vector<unsigned> m_executed_kernel_uids; //< uids of kernel launches for stat printout
-   std::string executed_kernel_info_string(); //< format the kernel information into a string for stat printout
-   void clear_executed_kernel_info(); //< clear the kernel information after stat printout
-
-
-public:
-   unsigned long long  gpu_sim_insn;
-   unsigned long long  gpu_tot_sim_insn;
-   unsigned long long  gpu_sim_insn_last_update;
-   unsigned gpu_sim_insn_last_update_sid;
-   occupancy_stats gpu_occupancy;
-   occupancy_stats gpu_tot_occupancy;
-
-
-   FuncCache get_cache_config(std::string kernel_name);
-   void set_cache_config(std::string kernel_name, FuncCache cacheConfig );
-   bool has_special_cache_config(std::string kernel_name);
-   void change_cache_config(FuncCache cache_config);
-   void set_cache_config(std::string kernel_name);
-
-   //Jin: functional simulation for CDP
-private:
-   //set by stream operation every time a functoinal simulation is done
-   bool m_functional_sim;
-   kernel_info_t * m_functional_sim_kernel;
-
-public:
-   bool is_functional_sim() { return m_functional_sim; }
-   kernel_info_t * get_functional_kernel() { return m_functional_sim_kernel; }
-   void functional_launch(kernel_info_t * k) {
-     m_functional_sim = true;
-     m_functional_sim_kernel = k;
-   }
-   void finish_functional_sim(kernel_info_t * k) {
-     assert(m_functional_sim);
-     assert(m_functional_sim_kernel == k);
-     m_functional_sim = false;
-     m_functional_sim_kernel = NULL;
-   }
+ public:
+  gpgpu_sim(const gpgpu_sim_config &config, gpgpu_context *ctx, gem5::CudaGPU *cuda_gpu = NULL );
+
+  void set_prop(struct cudaDeviceProp *prop);
+
+  void launch(kernel_info_t *kinfo);
+  bool can_start_kernel();
+  unsigned finished_kernel();
+  void set_kernel_done(kernel_info_t *kernel);
+  void stop_all_running_kernels();
+
+  void init();
+  // void cycle();
+  void core_cycle_start();
+  void core_cycle_end();
+  void icnt_cycle_start();
+  void icnt_cycle_end();
+  void l2_cycle();
+  void dram_cycle();
+
+  bool active();
+  bool cycle_insn_cta_max_hit() {
+    return (m_config.gpu_max_cycle_opt && (gpu_tot_sim_cycle + gpu_sim_cycle) >=
+                                              m_config.gpu_max_cycle_opt) ||
+           (m_config.gpu_max_insn_opt &&
+            (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt) ||
+           (m_config.gpu_max_cta_opt &&
+            (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt)) ||
+           (m_config.gpu_max_completed_cta_opt &&
+            (gpu_completed_cta >= m_config.gpu_max_completed_cta_opt));
+  }
+  void print_stats();
+  void update_stats();
+  void deadlock_check();
+  void inc_completed_cta() { gpu_completed_cta++; }
+  void get_pdom_stack_top_info(unsigned sid, unsigned tid, unsigned *pc,
+                               unsigned *rpc);
+
+  int shared_mem_size() const;
+  int shared_mem_per_block() const;
+  int compute_capability_major() const;
+  int compute_capability_minor() const;
+  int num_registers_per_core() const;
+  int num_registers_per_block() const;
+  int wrp_size() const;
+  int shader_clock() const;
+  int max_cta_per_core() const;
+  int get_max_cta(const kernel_info_t &k) const;
+  const struct cudaDeviceProp *get_prop() const;
+  enum divergence_support_t simd_model() const;
+
+  unsigned threads_per_core() const;
+  bool get_more_cta_left() const;
+  bool kernel_more_cta_left(kernel_info_t *kernel) const;
+  bool hit_max_cta_count() const;
+  kernel_info_t *select_kernel();
+  PowerscalingCoefficients *get_scaling_coeffs();
+  void decrement_kernel_latency();
+
+  const gpgpu_sim_config &get_config() const { return m_config; }
+  void gpu_print_stat();
+  void dump_pipeline(int mask, int s, int m) const;
+
+  // TODO schi add
+  shader_core_ctx* get_shader(int id);
+
+  void perf_memcpy_to_gpu(size_t dst_start_addr, size_t count);
+
+  // The next three functions added to be used by the functional simulation
+  // function
+
+  //! Get shader core configuration
+  /*!
+   * Returning the configuration of the shader core, used by the functional
+   * simulation only so far
+   */
+  const shader_core_config *getShaderCoreConfig();
+
+  //! Get shader core Memory Configuration
+  /*!
+   * Returning the memory configuration of the shader core, used by the
+   * functional simulation only so far
+   */
+  const memory_config *getMemoryConfig();
+
+  //! Get shader core SIMT cluster
+  /*!
+   * Returning the cluster of of the shader core, used by the functional
+   * simulation so far
+   */
+  simt_core_cluster *getSIMTCluster();
+
+  void hit_watchpoint(unsigned watchpoint_num, ptx_thread_info *thd,
+                      const ptx_instruction *pI);
+
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+
+ private:
+  // clocks
+  void reinit_clock_domains(void);
+  int next_clock_domain(void);
+  void issue_block2core();
+  void print_dram_stats(FILE *fout) const;
+  void shader_print_runtime_stat(FILE *fout);
+  void shader_print_l1_miss_stat(FILE *fout) const;
+  void shader_print_cache_stats(FILE *fout) const;
+  void shader_print_scheduler_stat(FILE *fout, bool print_dynamic_info) const;
+  void visualizer_printstat();
+  void print_shader_cycle_distro(FILE *fout) const;
+
+  void gpgpu_debug();
+
+ protected:
+  ///// data /////
+  class simt_core_cluster **m_cluster;
+  class memory_partition_unit **m_memory_partition_unit;
+  class memory_sub_partition **m_memory_sub_partition;
+
+  std::vector<kernel_info_t *> m_running_kernels;
+  unsigned m_last_issued_kernel;
+
+  std::list<unsigned> m_finished_kernel;
+  // m_total_cta_launched == per-kernel count. gpu_tot_issued_cta == global
+  // count.
+  unsigned long long m_total_cta_launched;
+  unsigned long long gpu_tot_issued_cta;
+  unsigned gpu_completed_cta;
+
+  unsigned m_last_cluster_issue;
+  float *average_pipeline_duty_cycle;
+  float *active_sms;
+  // time of next rising edge
+  double core_time;
+  double icnt_time;
+  double dram_time;
+  double l2_time;
+
+  // debug
+  bool gpu_deadlock;
+
+  //// configuration parameters ////
+  const gpgpu_sim_config &m_config;
+
+  const struct cudaDeviceProp *m_cuda_properties;
+  const shader_core_config *m_shader_config;
+  const memory_config *m_memory_config;
+
+  // stats
+  class shader_core_stats *m_shader_stats;
+  class memory_stats_t *m_memory_stats;
+  class power_stat_t *m_power_stats;
+  class gpgpu_sim_wrapper *m_gpgpusim_wrapper;
+  unsigned long long last_gpu_sim_insn;
+
+  unsigned long long last_liveness_message_time;
+
+  std::map<std::string, FuncCache> m_special_cache_config;
+
+  std::vector<std::string>
+      m_executed_kernel_names;  //< names of kernel for stat printout
+  std::vector<unsigned>
+      m_executed_kernel_uids;  //< uids of kernel launches for stat printout
+  std::map<unsigned, watchpoint_event> g_watchpoint_hits;
+
+  std::string executed_kernel_info_string();  //< format the kernel information
+                                              // into a string for stat printout
+  std::string executed_kernel_name();
+  void clear_executed_kernel_info();  //< clear the kernel information after
+                                      // stat printout
+  virtual void createSIMTCluster() = 0;
+
+ public:
+  unsigned long long gpu_sim_insn;
+  unsigned long long gpu_tot_sim_insn;
+  unsigned long long gpu_sim_insn_last_update;
+  unsigned gpu_sim_insn_last_update_sid;
+  occupancy_stats gpu_occupancy;
+  occupancy_stats gpu_tot_occupancy;
+
+  // performance counter for stalls due to congestion.
+  unsigned int gpu_stall_dramfull;
+  unsigned int gpu_stall_icnt2sh;
+  unsigned long long partiton_reqs_in_parallel;
+  unsigned long long partiton_reqs_in_parallel_total;
+  unsigned long long partiton_reqs_in_parallel_util;
+  unsigned long long partiton_reqs_in_parallel_util_total;
+  unsigned long long gpu_sim_cycle_parition_util;
+  unsigned long long gpu_tot_sim_cycle_parition_util;
+  unsigned long long partiton_replys_in_parallel;
+  unsigned long long partiton_replys_in_parallel_total;
+
+  FuncCache get_cache_config(std::string kernel_name);
+  void set_cache_config(std::string kernel_name, FuncCache cacheConfig);
+  bool has_special_cache_config(std::string kernel_name);
+  void change_cache_config(FuncCache cache_config);
+  void set_cache_config(std::string kernel_name);
+
+  // Jin: functional simulation for CDP
+ private:
+  // set by stream operation every time a functoinal simulation is done
+  bool m_functional_sim;
+  kernel_info_t *m_functional_sim_kernel;
+
+ public:
+  bool is_functional_sim() { return m_functional_sim; }
+  kernel_info_t *get_functional_kernel() { return m_functional_sim_kernel; }
+  void functional_launch(kernel_info_t *k) {
+    m_functional_sim = true;
+    m_functional_sim_kernel = k;
+  }
+  void finish_functional_sim(kernel_info_t *k) {
+    assert(m_functional_sim);
+    assert(m_functional_sim_kernel == k);
+    m_functional_sim = false;
+    m_functional_sim_kernel = NULL;
+  }
 };
 
+class exec_gpgpu_sim : public gpgpu_sim {
+ public:
+  exec_gpgpu_sim(const gpgpu_sim_config &config, gpgpu_context *ctx, gem5::CudaGPU *cuda_gpu = NULL )
+      : gpgpu_sim(config, ctx, cuda_gpu) {
+    createSIMTCluster();
+  }
+
+  virtual void createSIMTCluster();
+};
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.cc
index 0edc3b7363..783b4d8007 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.cc
@@ -45,711 +45,831 @@
 #include "mem_latency_stat.h"
 #include "l2cache_trace.h"
 
-
-mem_fetch * partition_mf_allocator::alloc(new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const 
-{
-    assert( wr );
-    mem_access_t access( type, addr, size, wr );
-    mem_fetch *mf = new mem_fetch( access, 
-                                   NULL,
-                                   WRITE_PACKET_SIZE, 
-                                   -1, 
-                                   -1, 
-                                   -1,
-                                   m_memory_config );
-    return mf;
-}
-
-memory_partition_unit::memory_partition_unit( unsigned partition_id, 
-                                              const struct memory_config *config,
-                                              class memory_stats_t *stats )
-: m_id(partition_id), m_config(config), m_stats(stats), m_arbitration_metadata(config) 
-{
-    m_dram = new dram_t(m_id,m_config,m_stats,this);
-
-    m_sub_partition = new memory_sub_partition*[m_config->m_n_sub_partition_per_memory_channel]; 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        unsigned sub_partition_id = m_id * m_config->m_n_sub_partition_per_memory_channel + p; 
-        m_sub_partition[p] = new memory_sub_partition(sub_partition_id, m_config, stats); 
-    }
-}
-
-void memory_partition_unit::handle_memcpy_to_gpu( size_t addr, unsigned global_subpart_id, mem_access_sector_mask_t mask )
-{
-    unsigned p = global_sub_partition_id_to_local_id(global_subpart_id);
-    std::string mystring =
-        mask.to_string<char,std::string::traits_type,std::string::allocator_type>();
-    MEMPART_DPRINTF("Copy Engine Request Received For Address=%llx, local_subpart=%u, global_subpart=%u, sector_mask=%s \n", addr, p, global_subpart_id, mystring.c_str()); 
-    m_sub_partition[p]->force_l2_tag_update(addr,gpu_sim_cycle+gpu_tot_sim_cycle, mask);
-}
-
-memory_partition_unit::~memory_partition_unit() 
-{
-    delete m_dram; 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        delete m_sub_partition[p]; 
-    } 
-    delete[] m_sub_partition; 
-}
-
-memory_partition_unit::arbitration_metadata::arbitration_metadata(const struct memory_config *config) 
-: m_last_borrower(config->m_n_sub_partition_per_memory_channel - 1), 
-  m_private_credit(config->m_n_sub_partition_per_memory_channel, 0), 
-  m_shared_credit(0) 
-{
-    // each sub partition get at least 1 credit for forward progress 
-    // the rest is shared among with other partitions 
-    m_private_credit_limit = 1; 
-    m_shared_credit_limit = config->gpgpu_frfcfs_dram_sched_queue_size 
-                            + config->gpgpu_dram_return_queue_size 
-                            - (config->m_n_sub_partition_per_memory_channel - 1);
-    if(config->seperate_write_queue_enabled )
-    	m_shared_credit_limit += config->gpgpu_frfcfs_dram_write_queue_size;
-    if (config->gpgpu_frfcfs_dram_sched_queue_size == 0 
-        or config->gpgpu_dram_return_queue_size == 0) 
-    {
-        m_shared_credit_limit = 0; // no limit if either of the queue has no limit in size 
-    }
-    assert(m_shared_credit_limit >= 0); 
-}
-
-bool memory_partition_unit::arbitration_metadata::has_credits(int inner_sub_partition_id) const 
-{
-    int spid = inner_sub_partition_id; 
-    if (m_private_credit[spid] < m_private_credit_limit) {
-        return true; 
-    } else if (m_shared_credit_limit == 0 || m_shared_credit < m_shared_credit_limit) {
-        return true; 
-    } else {
-        return false; 
-    }
-}
-
-void memory_partition_unit::arbitration_metadata::borrow_credit(int inner_sub_partition_id) 
-{
-    int spid = inner_sub_partition_id; 
-    if (m_private_credit[spid] < m_private_credit_limit) {
-        m_private_credit[spid] += 1; 
-    } else if (m_shared_credit_limit == 0 || m_shared_credit < m_shared_credit_limit) {
-        m_shared_credit += 1; 
-    } else {
-        assert(0 && "DRAM arbitration error: Borrowing from depleted credit!"); 
-    }
-    m_last_borrower = spid; 
-}
-
-void memory_partition_unit::arbitration_metadata::return_credit(int inner_sub_partition_id) 
-{
-    int spid = inner_sub_partition_id; 
-    if (m_private_credit[spid] > 0) {
-        m_private_credit[spid] -= 1; 
-    } else {
-        m_shared_credit -= 1; 
-    } 
-    assert((m_shared_credit >= 0) && "DRAM arbitration error: Returning more than available credits!"); 
-}
-
-void memory_partition_unit::arbitration_metadata::print( FILE *fp ) const 
-{
-    fprintf(fp, "private_credit = "); 
-    for (unsigned p = 0; p < m_private_credit.size(); p++) {
-        fprintf(fp, "%d ", m_private_credit[p]); 
+mem_fetch *partition_mf_allocator::alloc(new_addr_type addr,
+                                         mem_access_type type, unsigned size,
+                                         bool wr,
+                                         unsigned long long cycle) const {
+  assert(wr);
+  mem_access_t access(type, addr, size, wr, m_memory_config->gpgpu_ctx);
+  mem_fetch *mf = new mem_fetch(access, NULL, WRITE_PACKET_SIZE, -1, -1, -1,
+                                m_memory_config, cycle);
+  return mf;
+}
+
+mem_fetch *partition_mf_allocator::alloc(
+    new_addr_type addr, mem_access_type type, const active_mask_t &active_mask,
+    const mem_access_byte_mask_t &byte_mask,
+    const mem_access_sector_mask_t &sector_mask, unsigned size, bool wr,
+    unsigned long long cycle, unsigned wid, unsigned sid, unsigned tpc,
+    mem_fetch *original_mf) const {
+  mem_access_t access(type, addr, size, wr, active_mask, byte_mask, sector_mask,
+                      m_memory_config->gpgpu_ctx);
+  mem_fetch *mf =
+      new mem_fetch(access, NULL, wr ? WRITE_PACKET_SIZE : READ_PACKET_SIZE,
+                    wid, sid, tpc, m_memory_config, cycle, original_mf);
+  return mf;
+}
+memory_partition_unit::memory_partition_unit(unsigned partition_id,
+                                             const memory_config *config,
+                                             class memory_stats_t *stats,
+                                             class gpgpu_sim *gpu)
+    : m_id(partition_id),
+      m_config(config),
+      m_stats(stats),
+      m_arbitration_metadata(config),
+      m_gpu(gpu) {
+  m_dram = new dram_t(m_id, m_config, m_stats, this, gpu);
+
+  m_sub_partition = new memory_sub_partition
+      *[m_config->m_n_sub_partition_per_memory_channel];
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    unsigned sub_partition_id =
+        m_id * m_config->m_n_sub_partition_per_memory_channel + p;
+    m_sub_partition[p] =
+        new memory_sub_partition(sub_partition_id, m_config, stats, gpu);
+  }
+}
+
+void memory_partition_unit::handle_memcpy_to_gpu(
+    size_t addr, unsigned global_subpart_id, mem_access_sector_mask_t mask) {
+  unsigned p = global_sub_partition_id_to_local_id(global_subpart_id);
+  std::string mystring = mask.to_string<char, std::string::traits_type,
+                                        std::string::allocator_type>();
+  MEMPART_DPRINTF(
+      "Copy Engine Request Received For Address=%zx, local_subpart=%u, "
+      "global_subpart=%u, sector_mask=%s \n",
+      addr, p, global_subpart_id, mystring.c_str());
+  m_sub_partition[p]->force_l2_tag_update(
+      addr, m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle, mask);
+}
+
+memory_partition_unit::~memory_partition_unit() {
+  delete m_dram;
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    delete m_sub_partition[p];
+  }
+  delete[] m_sub_partition;
+}
+
+memory_partition_unit::arbitration_metadata::arbitration_metadata(
+    const memory_config *config)
+    : m_last_borrower(config->m_n_sub_partition_per_memory_channel - 1),
+      m_private_credit(config->m_n_sub_partition_per_memory_channel, 0),
+      m_shared_credit(0) {
+  // each sub partition get at least 1 credit for forward progress
+  // the rest is shared among with other partitions
+  m_private_credit_limit = 1;
+  m_shared_credit_limit = config->gpgpu_frfcfs_dram_sched_queue_size +
+                          config->gpgpu_dram_return_queue_size -
+                          (config->m_n_sub_partition_per_memory_channel - 1);
+  if (config->seperate_write_queue_enabled)
+    m_shared_credit_limit += config->gpgpu_frfcfs_dram_write_queue_size;
+  if (config->gpgpu_frfcfs_dram_sched_queue_size == 0 or
+      config->gpgpu_dram_return_queue_size == 0) {
+    m_shared_credit_limit =
+        0;  // no limit if either of the queue has no limit in size
+  }
+  assert(m_shared_credit_limit >= 0);
+}
+
+bool memory_partition_unit::arbitration_metadata::has_credits(
+    int inner_sub_partition_id) const {
+  int spid = inner_sub_partition_id;
+  if (m_private_credit[spid] < m_private_credit_limit) {
+    return true;
+  } else if (m_shared_credit_limit == 0 ||
+             m_shared_credit < m_shared_credit_limit) {
+    return true;
+  } else {
+    return false;
+  }
+}
+
+void memory_partition_unit::arbitration_metadata::borrow_credit(
+    int inner_sub_partition_id) {
+  int spid = inner_sub_partition_id;
+  if (m_private_credit[spid] < m_private_credit_limit) {
+    m_private_credit[spid] += 1;
+  } else if (m_shared_credit_limit == 0 ||
+             m_shared_credit < m_shared_credit_limit) {
+    m_shared_credit += 1;
+  } else {
+    assert(0 && "DRAM arbitration error: Borrowing from depleted credit!");
+  }
+  m_last_borrower = spid;
+}
+
+void memory_partition_unit::arbitration_metadata::return_credit(
+    int inner_sub_partition_id) {
+  int spid = inner_sub_partition_id;
+  if (m_private_credit[spid] > 0) {
+    m_private_credit[spid] -= 1;
+  } else {
+    m_shared_credit -= 1;
+  }
+  assert((m_shared_credit >= 0) &&
+         "DRAM arbitration error: Returning more than available credits!");
+}
+
+void memory_partition_unit::arbitration_metadata::print(FILE *fp) const {
+  fprintf(fp, "private_credit = ");
+  for (unsigned p = 0; p < m_private_credit.size(); p++) {
+    fprintf(fp, "%d ", m_private_credit[p]);
+  }
+  fprintf(fp, "(limit = %d)\n", m_private_credit_limit);
+  fprintf(fp, "shared_credit = %d (limit = %d)\n", m_shared_credit,
+          m_shared_credit_limit);
+}
+
+bool memory_partition_unit::busy() const {
+  bool busy = false;
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    if (m_sub_partition[p]->busy()) {
+      busy = true;
     }
-    fprintf(fp, "(limit = %d)\n", m_private_credit_limit); 
-    fprintf(fp, "shared_credit = %d (limit = %d)\n", m_shared_credit, m_shared_credit_limit); 
-}
-
-bool memory_partition_unit::busy() const 
-{
-    bool busy = false; 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        if (m_sub_partition[p]->busy()) {
-            busy = true; 
+  }
+  return busy;
+}
+
+void memory_partition_unit::cache_cycle(unsigned cycle) {
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    m_sub_partition[p]->cache_cycle(cycle);
+  }
+}
+
+void memory_partition_unit::visualizer_print(gzFile visualizer_file) const {
+  m_dram->visualizer_print(visualizer_file);
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    m_sub_partition[p]->visualizer_print(visualizer_file);
+  }
+}
+
+// determine whether a given subpartition can issue to DRAM
+bool memory_partition_unit::can_issue_to_dram(int inner_sub_partition_id) {
+  int spid = inner_sub_partition_id;
+  bool sub_partition_contention = m_sub_partition[spid]->dram_L2_queue_full();
+  bool has_dram_resource = m_arbitration_metadata.has_credits(spid);
+
+  MEMPART_DPRINTF(
+      "sub partition %d sub_partition_contention=%c has_dram_resource=%c\n",
+      spid, (sub_partition_contention) ? 'T' : 'F',
+      (has_dram_resource) ? 'T' : 'F');
+
+  return (has_dram_resource && !sub_partition_contention);
+}
+
+int memory_partition_unit::global_sub_partition_id_to_local_id(
+    int global_sub_partition_id) const {
+  return (global_sub_partition_id -
+          m_id * m_config->m_n_sub_partition_per_memory_channel);
+}
+
+void memory_partition_unit::simple_dram_model_cycle() {
+  // pop completed memory request from dram and push it to dram-to-L2 queue
+  // of the original sub partition
+  if (!m_dram_latency_queue.empty() &&
+      ((m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle) >=
+       m_dram_latency_queue.front().ready_cycle)) {
+    mem_fetch *mf_return = m_dram_latency_queue.front().req;
+    if (mf_return->get_access_type() != L1_WRBK_ACC &&
+        mf_return->get_access_type() != L2_WRBK_ACC) {
+      mf_return->set_reply();
+
+      unsigned dest_global_spid = mf_return->get_sub_partition_id();
+      int dest_spid = global_sub_partition_id_to_local_id(dest_global_spid);
+      assert(m_sub_partition[dest_spid]->get_id() == dest_global_spid);
+      if (!m_sub_partition[dest_spid]->dram_L2_queue_full()) {
+        if (mf_return->get_access_type() == L1_WRBK_ACC) {
+          m_sub_partition[dest_spid]->set_done(mf_return);
+          delete mf_return;
+        } else {
+          m_sub_partition[dest_spid]->dram_L2_queue_push(mf_return);
+          mf_return->set_status(
+              IN_PARTITION_DRAM_TO_L2_QUEUE,
+              m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+          m_arbitration_metadata.return_credit(dest_spid);
+          MEMPART_DPRINTF(
+              "mem_fetch request %p return from dram to sub partition %d\n",
+              mf_return, dest_spid);
         }
-    }
-    return busy; 
-}
-
-void memory_partition_unit::cache_cycle(unsigned cycle) 
-{
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        m_sub_partition[p]->cache_cycle(cycle); 
-    }
-}
-
-void memory_partition_unit::visualizer_print( gzFile visualizer_file ) const 
-{
-    m_dram->visualizer_print(visualizer_file);
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        m_sub_partition[p]->visualizer_print(visualizer_file); 
-    }
-}
-
-// determine whether a given subpartition can issue to DRAM 
-bool memory_partition_unit::can_issue_to_dram(int inner_sub_partition_id) 
-{
-    int spid = inner_sub_partition_id; 
-    bool sub_partition_contention = m_sub_partition[spid]->dram_L2_queue_full(); 
-    bool has_dram_resource = m_arbitration_metadata.has_credits(spid); 
+        m_dram_latency_queue.pop_front();
+      }
 
-    MEMPART_DPRINTF("sub partition %d sub_partition_contention=%c has_dram_resource=%c\n", 
-                    spid, (sub_partition_contention)? 'T':'F', (has_dram_resource)? 'T':'F'); 
-
-    return (has_dram_resource && !sub_partition_contention); 
-}
-
-int memory_partition_unit::global_sub_partition_id_to_local_id(int global_sub_partition_id) const
-{
-    return (global_sub_partition_id - m_id * m_config->m_n_sub_partition_per_memory_channel); 
-}
-
-void memory_partition_unit::dram_cycle() 
-{ 
-    // pop completed memory request from dram and push it to dram-to-L2 queue 
-    // of the original sub partition 
-    mem_fetch* mf_return = m_dram->return_queue_top();
-    if (mf_return) {
-        unsigned dest_global_spid = mf_return->get_sub_partition_id(); 
-        int dest_spid = global_sub_partition_id_to_local_id(dest_global_spid); 
-        assert(m_sub_partition[dest_spid]->get_id() == dest_global_spid); 
-        if (!m_sub_partition[dest_spid]->dram_L2_queue_full()) {
-            if( mf_return->get_access_type() == L1_WRBK_ACC ) {
-                m_sub_partition[dest_spid]->set_done(mf_return); 
-                delete mf_return;
-            } else {
-                m_sub_partition[dest_spid]->dram_L2_queue_push(mf_return);
-                mf_return->set_status(IN_PARTITION_DRAM_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                m_arbitration_metadata.return_credit(dest_spid); 
-                MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf_return, dest_spid); 
-            }
-            m_dram->return_queue_pop(); 
-        }
     } else {
-        m_dram->return_queue_pop(); 
+      this->set_done(mf_return);
+      delete mf_return;
+      m_dram_latency_queue.pop_front();
     }
-    
-    m_dram->cycle(); 
-    m_dram->dram_log(SAMPLELOG);   
-
-   // mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
-    //if( !m_dram->full(mf->is_write()) ) {
-        // L2->DRAM queue to DRAM latency queue
-        // Arbitrate among multiple L2 subpartitions 
-        int last_issued_partition = m_arbitration_metadata.last_borrower(); 
-        for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-            int spid = (p + last_issued_partition + 1) % m_config->m_n_sub_partition_per_memory_channel; 
-            if (!m_sub_partition[spid]->L2_dram_queue_empty() && can_issue_to_dram(spid)) {
-                mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
-                if(m_dram->full(mf->is_write()) )
-                	break;
-
-                m_sub_partition[spid]->L2_dram_queue_pop();
-                MEMPART_DPRINTF("Issue mem_fetch request %p from sub partition %d to dram\n", mf, spid); 
-                dram_delay_t d;
-                d.req = mf;
-                d.ready_cycle = gpu_sim_cycle+gpu_tot_sim_cycle + m_config->dram_latency;
-                m_dram_latency_queue.push_back(d);
-                mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                m_arbitration_metadata.borrow_credit(spid); 
-                break;  // the DRAM should only accept one request per cycle 
-            }
-        }
-    //}
-
-    // DRAM latency queue
-
-    if( !m_dram_latency_queue.empty() && ( (gpu_sim_cycle+gpu_tot_sim_cycle) >= m_dram_latency_queue.front().ready_cycle ) && !m_dram->full(m_dram_latency_queue.front().req->is_write()) ) {
-    	mem_fetch* mf = m_dram_latency_queue.front().req;
-    	m_dram_latency_queue.pop_front();
-        m_dram->push(mf);
+  }
+
+  // mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
+  // if( !m_dram->full(mf->is_write()) ) {
+  // L2->DRAM queue to DRAM latency queue
+  // Arbitrate among multiple L2 subpartitions
+  int last_issued_partition = m_arbitration_metadata.last_borrower();
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    int spid = (p + last_issued_partition + 1) %
+               m_config->m_n_sub_partition_per_memory_channel;
+    if (!m_sub_partition[spid]->L2_dram_queue_empty() &&
+        can_issue_to_dram(spid)) {
+      mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
+      if (m_dram->full(mf->is_write())) break;
+
+      m_sub_partition[spid]->L2_dram_queue_pop();
+      MEMPART_DPRINTF(
+          "Issue mem_fetch request %p from sub partition %d to dram\n", mf,
+          spid);
+      dram_delay_t d;
+      d.req = mf;
+      d.ready_cycle = m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle +
+                      m_config->dram_latency;
+      m_dram_latency_queue.push_back(d);
+      mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE,
+                     m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+      m_arbitration_metadata.borrow_credit(spid);
+      break;  // the DRAM should only accept one request per cycle
     }
-}
-
-void memory_partition_unit::set_done( mem_fetch *mf )
-{
-    unsigned global_spid = mf->get_sub_partition_id(); 
-    int spid = global_sub_partition_id_to_local_id(global_spid); 
-    assert(m_sub_partition[spid]->get_id() == global_spid); 
-    if (mf->get_access_type() == L1_WRBK_ACC || mf->get_access_type() == L2_WRBK_ACC) {
-        m_arbitration_metadata.return_credit(spid); 
-        MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf, spid); 
+  }
+  //}
+}
+
+void memory_partition_unit::dram_cycle() {
+  // pop completed memory request from dram and push it to dram-to-L2 queue
+  // of the original sub partition
+  mem_fetch *mf_return = m_dram->return_queue_top();
+  if (mf_return) {
+    unsigned dest_global_spid = mf_return->get_sub_partition_id();
+    int dest_spid = global_sub_partition_id_to_local_id(dest_global_spid);
+    assert(m_sub_partition[dest_spid]->get_id() == dest_global_spid);
+    if (!m_sub_partition[dest_spid]->dram_L2_queue_full()) {
+      if (mf_return->get_access_type() == L1_WRBK_ACC) {
+        m_sub_partition[dest_spid]->set_done(mf_return);
+        delete mf_return;
+      } else {
+        m_sub_partition[dest_spid]->dram_L2_queue_push(mf_return);
+        mf_return->set_status(IN_PARTITION_DRAM_TO_L2_QUEUE,
+                              m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+        m_arbitration_metadata.return_credit(dest_spid);
+        MEMPART_DPRINTF(
+            "mem_fetch request %p return from dram to sub partition %d\n",
+            mf_return, dest_spid);
+      }
+      m_dram->return_queue_pop();
     }
-    m_sub_partition[spid]->set_done(mf); 
-}
-
-void memory_partition_unit::set_dram_power_stats(unsigned &n_cmd,
-                                                 unsigned &n_activity,
-                                                 unsigned &n_nop,
-                                                 unsigned &n_act,
-                                                 unsigned &n_pre,
-                                                 unsigned &n_rd,
-                                                 unsigned &n_wr,
-                                                 unsigned &n_req) const
-{
-    m_dram->set_dram_power_stats(n_cmd, n_activity, n_nop, n_act, n_pre, n_rd, n_wr, n_req);
-}
-
-void memory_partition_unit::print( FILE *fp ) const
-{
-    fprintf(fp, "Memory Partition %u: \n", m_id); 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        m_sub_partition[p]->print(fp); 
+  } else {
+    m_dram->return_queue_pop();
+  }
+
+  m_dram->cycle();
+  m_dram->dram_log(SAMPLELOG);
+
+  // mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
+  // if( !m_dram->full(mf->is_write()) ) {
+  // L2->DRAM queue to DRAM latency queue
+  // Arbitrate among multiple L2 subpartitions
+  int last_issued_partition = m_arbitration_metadata.last_borrower();
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    int spid = (p + last_issued_partition + 1) %
+               m_config->m_n_sub_partition_per_memory_channel;
+    if (!m_sub_partition[spid]->L2_dram_queue_empty() &&
+        can_issue_to_dram(spid)) {
+      mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
+      if (m_dram->full(mf->is_write())) break;
+
+      m_sub_partition[spid]->L2_dram_queue_pop();
+      MEMPART_DPRINTF(
+          "Issue mem_fetch request %p from sub partition %d to dram\n", mf,
+          spid);
+      dram_delay_t d;
+      d.req = mf;
+      d.ready_cycle = m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle +
+                      m_config->dram_latency;
+      m_dram_latency_queue.push_back(d);
+      mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE,
+                     m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+      m_arbitration_metadata.borrow_credit(spid);
+      break;  // the DRAM should only accept one request per cycle
     }
-    fprintf(fp, "In Dram Latency Queue (total = %zd): \n", m_dram_latency_queue.size()); 
-    for (std::list<dram_delay_t>::const_iterator mf_dlq = m_dram_latency_queue.begin(); 
-         mf_dlq != m_dram_latency_queue.end(); ++mf_dlq) {
-        mem_fetch *mf = mf_dlq->req; 
-        fprintf(fp, "Ready @ %llu - ", mf_dlq->ready_cycle); 
-        if (mf) 
-            mf->print(fp); 
-        else 
-            fprintf(fp, " <NULL mem_fetch?>\n"); 
+  }
+  //}
+
+  // DRAM latency queue
+  if (!m_dram_latency_queue.empty() &&
+      ((m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle) >=
+       m_dram_latency_queue.front().ready_cycle) &&
+      !m_dram->full(m_dram_latency_queue.front().req->is_write())) {
+    mem_fetch *mf = m_dram_latency_queue.front().req;
+    m_dram_latency_queue.pop_front();
+    m_dram->push(mf);
+  }
+}
+
+void memory_partition_unit::set_done(mem_fetch *mf) {
+  unsigned global_spid = mf->get_sub_partition_id();
+  int spid = global_sub_partition_id_to_local_id(global_spid);
+  assert(m_sub_partition[spid]->get_id() == global_spid);
+  if (mf->get_access_type() == L1_WRBK_ACC ||
+      mf->get_access_type() == L2_WRBK_ACC) {
+    m_arbitration_metadata.return_credit(spid);
+    MEMPART_DPRINTF(
+        "mem_fetch request %p return from dram to sub partition %d\n", mf,
+        spid);
+  }
+  m_sub_partition[spid]->set_done(mf);
+}
+
+void memory_partition_unit::set_dram_power_stats(
+    unsigned &n_cmd, unsigned &n_activity, unsigned &n_nop, unsigned &n_act,
+    unsigned &n_pre, unsigned &n_rd, unsigned &n_wr, unsigned &n_wr_WB, unsigned &n_req) const {
+  m_dram->set_dram_power_stats(n_cmd, n_activity, n_nop, n_act, n_pre, n_rd,
+                               n_wr, n_wr_WB, n_req);
+}
+
+void memory_partition_unit::print(FILE *fp) const {
+  fprintf(fp, "Memory Partition %u: \n", m_id);
+  for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel;
+       p++) {
+    m_sub_partition[p]->print(fp);
+  }
+  fprintf(fp, "In Dram Latency Queue (total = %zd): \n",
+          m_dram_latency_queue.size());
+  for (std::list<dram_delay_t>::const_iterator mf_dlq =
+           m_dram_latency_queue.begin();
+       mf_dlq != m_dram_latency_queue.end(); ++mf_dlq) {
+    mem_fetch *mf = mf_dlq->req;
+    fprintf(fp, "Ready @ %llu - ", mf_dlq->ready_cycle);
+    if (mf)
+      mf->print(fp);
+    else
+      fprintf(fp, " <NULL mem_fetch?>\n");
+  }
+  m_dram->print(fp);
+}
+
+memory_sub_partition::memory_sub_partition(unsigned sub_partition_id,
+                                           const memory_config *config,
+                                           class memory_stats_t *stats,
+                                           class gpgpu_sim *gpu) {
+  m_id = sub_partition_id;
+  m_config = config;
+  m_stats = stats;
+  m_gpu = gpu;
+  m_memcpy_cycle_offset = 0;
+
+  assert(m_id < m_config->m_n_mem_sub_partition);
+
+  char L2c_name[32];
+  snprintf(L2c_name, 32, "L2_bank_%03d", m_id);
+  m_L2interface = new L2interface(this);
+  m_mf_allocator = new partition_mf_allocator(config);
+
+  if (!m_config->m_L2_config.disabled())
+    m_L2cache =
+        new l2_cache(L2c_name, m_config->m_L2_config, -1, -1, m_L2interface,
+                     m_mf_allocator, IN_PARTITION_L2_MISS_QUEUE, gpu);
+
+  unsigned int icnt_L2;
+  unsigned int L2_dram;
+  unsigned int dram_L2;
+  unsigned int L2_icnt;
+  sscanf(m_config->gpgpu_L2_queue_config, "%u:%u:%u:%u", &icnt_L2, &L2_dram,
+         &dram_L2, &L2_icnt);
+  m_icnt_L2_queue = new fifo_pipeline<mem_fetch>("icnt-to-L2", 0, icnt_L2);
+  m_L2_dram_queue = new fifo_pipeline<mem_fetch>("L2-to-dram", 0, L2_dram);
+  m_dram_L2_queue = new fifo_pipeline<mem_fetch>("dram-to-L2", 0, dram_L2);
+  m_L2_icnt_queue = new fifo_pipeline<mem_fetch>("L2-to-icnt", 0, L2_icnt);
+  wb_addr = -1;
+}
+
+memory_sub_partition::~memory_sub_partition() {
+  delete m_icnt_L2_queue;
+  delete m_L2_dram_queue;
+  delete m_dram_L2_queue;
+  delete m_L2_icnt_queue;
+  delete m_L2cache;
+  delete m_L2interface;
+}
+
+void memory_sub_partition::cache_cycle(unsigned cycle) {
+  // L2 fill responses
+  if (!m_config->m_L2_config.disabled()) {
+    if (m_L2cache->access_ready() && !m_L2_icnt_queue->full()) {
+      mem_fetch *mf = m_L2cache->next_access();
+      if (mf->get_access_type() !=
+          L2_WR_ALLOC_R) {  // Don't pass write allocate read request back to
+                            // upper level cache
+        mf->set_reply();
+        mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,
+                       m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+        m_L2_icnt_queue->push(mf);
+      } else {
+        if (m_config->m_L2_config.m_write_alloc_policy == FETCH_ON_WRITE) {
+          mem_fetch *original_wr_mf = mf->get_original_wr_mf();
+          assert(original_wr_mf);
+          original_wr_mf->set_reply();
+          original_wr_mf->set_status(
+              IN_PARTITION_L2_TO_ICNT_QUEUE,
+              m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+          m_L2_icnt_queue->push(original_wr_mf);
+        }
+        m_request_tracker.erase(mf);
+        delete mf;
+      }
     }
-    m_dram->print(fp); 
-}
-
-memory_sub_partition::memory_sub_partition( unsigned sub_partition_id, 
-                                            const struct memory_config *config,
-                                            class memory_stats_t *stats )
-{
-    m_id = sub_partition_id;
-    m_config=config;
-    m_stats=stats;
-    m_memcpy_cycle_offset = 0;
-
-    assert(m_id < m_config->m_n_mem_sub_partition); 
-
-    char L2c_name[32];
-    snprintf(L2c_name, 32, "L2_bank_%03d", m_id);
-    m_L2interface = new L2interface(this);
-    m_mf_allocator = new partition_mf_allocator(config);
-
-    if(!m_config->m_L2_config.disabled())
-       m_L2cache = new l2_cache(L2c_name,m_config->m_L2_config,-1,-1,m_L2interface,m_mf_allocator,IN_PARTITION_L2_MISS_QUEUE);
-
-    unsigned int icnt_L2;
-    unsigned int L2_dram;
-    unsigned int dram_L2;
-    unsigned int L2_icnt;
-    sscanf(m_config->gpgpu_L2_queue_config,"%u:%u:%u:%u", &icnt_L2,&L2_dram,&dram_L2,&L2_icnt );
-    m_icnt_L2_queue = new fifo_pipeline<mem_fetch>("icnt-to-L2",0,icnt_L2); 
-    m_L2_dram_queue = new fifo_pipeline<mem_fetch>("L2-to-dram",0,L2_dram);
-    m_dram_L2_queue = new fifo_pipeline<mem_fetch>("dram-to-L2",0,dram_L2);
-    m_L2_icnt_queue = new fifo_pipeline<mem_fetch>("L2-to-icnt",0,L2_icnt);
-    wb_addr=-1;
-}
-
-memory_sub_partition::~memory_sub_partition()
-{
-    delete m_icnt_L2_queue;
-    delete m_L2_dram_queue;
-    delete m_dram_L2_queue;
-    delete m_L2_icnt_queue;
-    delete m_L2cache;
-    delete m_L2interface;
-}
-
-void memory_sub_partition::cache_cycle( unsigned cycle )
-{
-    // L2 fill responses
-    if( !m_config->m_L2_config.disabled()) {
-       if ( m_L2cache->access_ready() && !m_L2_icnt_queue->full() ) {
-           mem_fetch *mf = m_L2cache->next_access();
-           if(mf->get_access_type() != L2_WR_ALLOC_R){ // Don't pass write allocate read request back to upper level cache
-				mf->set_reply();
-				mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-				m_L2_icnt_queue->push(mf);
-           }else{
-        	    if(m_config->m_L2_config.m_write_alloc_policy == FETCH_ON_WRITE)
-        	    {
-        	    	mem_fetch* original_wr_mf = mf->get_original_wr_mf();
-					assert(original_wr_mf);
-					original_wr_mf->set_reply();
-					original_wr_mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-					m_L2_icnt_queue->push(original_wr_mf);
-        	    }
-				m_request_tracker.erase(mf);
-				delete mf;
-           }
-       }
+  }
+
+  // DRAM to L2 (texture) and icnt (not texture)
+  if (!m_dram_L2_queue->empty()) {
+    mem_fetch *mf = m_dram_L2_queue->top();
+    if (!m_config->m_L2_config.disabled() && m_L2cache->waiting_for_fill(mf)) {
+      if (m_L2cache->fill_port_free()) {
+        mf->set_status(IN_PARTITION_L2_FILL_QUEUE,
+                       m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+        m_L2cache->fill(mf, m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle +
+                                m_memcpy_cycle_offset);
+        m_dram_L2_queue->pop();
+      }
+    } else if (!m_L2_icnt_queue->full()) {
+      if (mf->is_write() && mf->get_type() == WRITE_ACK)
+        mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,
+                       m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+      m_L2_icnt_queue->push(mf);
+      m_dram_L2_queue->pop();
     }
-
-    // DRAM to L2 (texture) and icnt (not texture)
-    if ( !m_dram_L2_queue->empty() ) {
-        mem_fetch *mf = m_dram_L2_queue->top();
-        if ( !m_config->m_L2_config.disabled() && m_L2cache->waiting_for_fill(mf) ) {
-            if (m_L2cache->fill_port_free()) {
-                mf->set_status(IN_PARTITION_L2_FILL_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                m_L2cache->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle+m_memcpy_cycle_offset);
-                m_dram_L2_queue->pop();
+  }
+
+  // prior L2 misses inserted into m_L2_dram_queue here
+  if (!m_config->m_L2_config.disabled()) m_L2cache->cycle();
+
+  // new L2 texture accesses and/or non-texture accesses
+  if (!m_L2_dram_queue->full() && !m_icnt_L2_queue->empty()) {
+    mem_fetch *mf = m_icnt_L2_queue->top();
+    if (!m_config->m_L2_config.disabled() &&
+        ((m_config->m_L2_texure_only && mf->istexture()) ||
+         (!m_config->m_L2_texure_only))) {
+      // L2 is enabled and access is for L2
+      bool output_full = m_L2_icnt_queue->full();
+      bool port_free = m_L2cache->data_port_free();
+      if (!output_full && port_free) {
+        std::list<cache_event> events;
+        enum cache_request_status status =
+            m_L2cache->access(mf->get_addr(), mf,
+                              m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle +
+                                  m_memcpy_cycle_offset,
+                              events);
+        bool write_sent = was_write_sent(events);
+        bool read_sent = was_read_sent(events);
+        MEM_SUBPART_DPRINTF("Probing L2 cache Address=%llx, status=%u\n",
+                            mf->get_addr(), status);
+
+        if (status == HIT) {
+          if (!write_sent) {
+            // L2 cache replies
+            assert(!read_sent);
+            if (mf->get_access_type() == L1_WRBK_ACC) {
+              m_request_tracker.erase(mf);
+              delete mf;
+            } else {
+              mf->set_reply();
+              mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,
+                             m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+              m_L2_icnt_queue->push(mf);
             }
-        } else if ( !m_L2_icnt_queue->full() ) {
-        	if(mf->is_write() && mf->get_type() == WRITE_ACK)
-            mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-            m_L2_icnt_queue->push(mf);
-            m_dram_L2_queue->pop();
-        }
-    }
-
-    // prior L2 misses inserted into m_L2_dram_queue here
-    if( !m_config->m_L2_config.disabled() )
-       m_L2cache->cycle();
-
-    // new L2 texture accesses and/or non-texture accesses
-    if ( !m_L2_dram_queue->full() && !m_icnt_L2_queue->empty() ) {
-        mem_fetch *mf = m_icnt_L2_queue->top();
-        if ( !m_config->m_L2_config.disabled() &&
-              ( (m_config->m_L2_texure_only && mf->istexture()) || (!m_config->m_L2_texure_only) )
-           ) {
-            // L2 is enabled and access is for L2
-            bool output_full = m_L2_icnt_queue->full(); 
-            bool port_free = m_L2cache->data_port_free(); 
-            if ( !output_full && port_free ) {
-                std::list<cache_event> events;
-                enum cache_request_status status = m_L2cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle+m_memcpy_cycle_offset,events);
-                bool write_sent = was_write_sent(events);
-                bool read_sent = was_read_sent(events);
-                MEM_SUBPART_DPRINTF("Probing L2 cache Address=%llx, status=%u\n", mf->get_addr(), status); 
-
-                if ( status == HIT ) {
-                    if( !write_sent ) {
-                        // L2 cache replies
-                        assert(!read_sent);
-                        if( mf->get_access_type() == L1_WRBK_ACC ) {
-                            m_request_tracker.erase(mf);
-                            delete mf;
-                        } else {
-                            mf->set_reply();
-                            mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                            m_L2_icnt_queue->push(mf);
-                        }
-                        m_icnt_L2_queue->pop();
-                    } else {
-                        assert(write_sent);
-                        m_icnt_L2_queue->pop();
-                    }
-                } else if ( status != RESERVATION_FAIL ) {
-                	if(mf->is_write() && (m_config->m_L2_config.m_write_alloc_policy == FETCH_ON_WRITE || m_config->m_L2_config.m_write_alloc_policy == LAZY_FETCH_ON_READ) && !was_writeallocate_sent(events)) {
-                		mf->set_reply();
-                		mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                		m_L2_icnt_queue->push(mf);
-                	}
-                    // L2 cache accepted request
-                    m_icnt_L2_queue->pop();
-                } else {
-                    assert(!write_sent);
-                    assert(!read_sent);
-                    // L2 cache lock-up: will try again next cycle
-                }
+            m_icnt_L2_queue->pop();
+          } else {
+            assert(write_sent);
+            m_icnt_L2_queue->pop();
+          }
+        } else if (status != RESERVATION_FAIL) {
+          if (mf->is_write() &&
+              (m_config->m_L2_config.m_write_alloc_policy == FETCH_ON_WRITE ||
+               m_config->m_L2_config.m_write_alloc_policy ==
+                   LAZY_FETCH_ON_READ) &&
+              !was_writeallocate_sent(events)) {
+            if (mf->get_access_type() == L1_WRBK_ACC) {
+              m_request_tracker.erase(mf);
+              delete mf;
+            } else {
+              mf->set_reply();
+              mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,
+                             m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+              m_L2_icnt_queue->push(mf);
             }
+          }
+          // L2 cache accepted request
+          m_icnt_L2_queue->pop();
         } else {
-            // L2 is disabled or non-texture access to texture-only L2
-            mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-            m_L2_dram_queue->push(mf);
-            m_icnt_L2_queue->pop();
+          assert(!write_sent);
+          assert(!read_sent);
+          // L2 cache lock-up: will try again next cycle
         }
+      }
+    } else {
+      // L2 is disabled or non-texture access to texture-only L2
+      mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,
+                     m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+      m_L2_dram_queue->push(mf);
+      m_icnt_L2_queue->pop();
     }
+  }
 
-    // ROP delay queue
-    if( !m_rop.empty() && (cycle >= m_rop.front().ready_cycle) && !m_icnt_L2_queue->full() ) {
-        mem_fetch* mf = m_rop.front().req;
-        m_rop.pop();
-        m_icnt_L2_queue->push(mf);
-        mf->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-    }
+  // ROP delay queue
+  if (!m_rop.empty() && (cycle >= m_rop.front().ready_cycle) &&
+      !m_icnt_L2_queue->full()) {
+    mem_fetch *mf = m_rop.front().req;
+    m_rop.pop();
+    m_icnt_L2_queue->push(mf);
+    mf->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,
+                   m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+  }
 }
 
-bool memory_sub_partition::full() const
-{
-    return m_icnt_L2_queue->full();
-}
+bool memory_sub_partition::full() const { return m_icnt_L2_queue->full(); }
 
-bool memory_sub_partition::full(unsigned size) const
-{
-    return m_icnt_L2_queue->is_avilable_size(size);
+bool memory_sub_partition::full(unsigned size) const {
+  return m_icnt_L2_queue->is_avilable_size(size);
 }
 
-bool memory_sub_partition::L2_dram_queue_empty() const
-{
-   return m_L2_dram_queue->empty(); 
+bool memory_sub_partition::L2_dram_queue_empty() const {
+  return m_L2_dram_queue->empty();
 }
 
-class mem_fetch* memory_sub_partition::L2_dram_queue_top() const
-{
-   return m_L2_dram_queue->top(); 
+class mem_fetch *memory_sub_partition::L2_dram_queue_top() const {
+  return m_L2_dram_queue->top();
 }
 
-void memory_sub_partition::L2_dram_queue_pop() 
-{
-   m_L2_dram_queue->pop(); 
-}
+void memory_sub_partition::L2_dram_queue_pop() { m_L2_dram_queue->pop(); }
 
-bool memory_sub_partition::dram_L2_queue_full() const
-{
-   return m_dram_L2_queue->full(); 
+bool memory_sub_partition::dram_L2_queue_full() const {
+  return m_dram_L2_queue->full();
 }
 
-void memory_sub_partition::dram_L2_queue_push( class mem_fetch* mf )
-{
-   m_dram_L2_queue->push(mf); 
+void memory_sub_partition::dram_L2_queue_push(class mem_fetch *mf) {
+  m_dram_L2_queue->push(mf);
 }
 
-void memory_sub_partition::print_cache_stat(unsigned &accesses, unsigned &misses) const
-{
-    FILE *fp = stdout;
-    if( !m_config->m_L2_config.disabled() )
-       m_L2cache->print(fp,accesses,misses);
+void memory_sub_partition::print_cache_stat(unsigned &accesses,
+                                            unsigned &misses) const {
+  FILE *fp = stdout;
+  if (!m_config->m_L2_config.disabled()) m_L2cache->print(fp, accesses, misses);
 }
 
-void memory_sub_partition::print( FILE *fp ) const
-{
-    if ( !m_request_tracker.empty() ) {
-        fprintf(fp,"Memory Sub Parition %u: pending memory requests:\n", m_id);
-        for ( std::set<mem_fetch*>::const_iterator r=m_request_tracker.begin(); r != m_request_tracker.end(); ++r ) {
-            mem_fetch *mf = *r;
-            if ( mf )
-                mf->print(fp);
-            else
-                fprintf(fp," <NULL mem_fetch?>\n");
-        }
+void memory_sub_partition::print(FILE *fp) const {
+  if (!m_request_tracker.empty()) {
+    fprintf(fp, "Memory Sub Parition %u: pending memory requests:\n", m_id);
+    for (std::set<mem_fetch *>::const_iterator r = m_request_tracker.begin();
+         r != m_request_tracker.end(); ++r) {
+      mem_fetch *mf = *r;
+      if (mf)
+        mf->print(fp);
+      else
+        fprintf(fp, " <NULL mem_fetch?>\n");
     }
-    if( !m_config->m_L2_config.disabled() )
-       m_L2cache->display_state(fp);
-}
-
-void memory_stats_t::visualizer_print( gzFile visualizer_file )
-{
-   gzprintf(visualizer_file, "Ltwowritemiss: %d\n", L2_write_miss);
-   gzprintf(visualizer_file, "Ltwowritehit: %d\n",  L2_write_hit);
-   gzprintf(visualizer_file, "Ltworeadmiss: %d\n", L2_read_miss);
-   gzprintf(visualizer_file, "Ltworeadhit: %d\n", L2_read_hit);
-   clear_L2_stats_pw();
-
-   if (num_mfs)
-      gzprintf(visualizer_file, "averagemflatency: %lld\n", mf_total_lat/num_mfs);
-}
-
-void memory_stats_t::clear_L2_stats_pw(){
-    L2_write_miss = 0;
-    L2_write_hit = 0;
-    L2_read_miss = 0;
-    L2_read_hit = 0;
-}
-
-void gpgpu_sim::print_dram_stats(FILE *fout) const
-{
-	unsigned cmd=0;
-	unsigned activity=0;
-	unsigned nop=0;
-	unsigned act=0;
-	unsigned pre=0;
-	unsigned rd=0;
-	unsigned wr=0;
-	unsigned req=0;
-	unsigned tot_cmd=0;
-	unsigned tot_nop=0;
-	unsigned tot_act=0;
-	unsigned tot_pre=0;
-	unsigned tot_rd=0;
-	unsigned tot_wr=0;
-	unsigned tot_req=0;
-
-	for (unsigned i=0;i<m_memory_config->m_n_mem;i++){
-		m_memory_partition_unit[i]->set_dram_power_stats(cmd,activity,nop,act,pre,rd,wr,req);
-		tot_cmd+=cmd;
-		tot_nop+=nop;
-		tot_act+=act;
-		tot_pre+=pre;
-		tot_rd+=rd;
-		tot_wr+=wr;
-		tot_req+=req;
-	}
-    fprintf(fout,"gpgpu_n_dram_reads = %d\n",tot_rd );
-    fprintf(fout,"gpgpu_n_dram_writes = %d\n",tot_wr );
-    fprintf(fout,"gpgpu_n_dram_activate = %d\n",tot_act );
-    fprintf(fout,"gpgpu_n_dram_commands = %d\n",tot_cmd);
-    fprintf(fout,"gpgpu_n_dram_noops = %d\n",tot_nop );
-    fprintf(fout,"gpgpu_n_dram_precharges = %d\n",tot_pre );
-    fprintf(fout,"gpgpu_n_dram_requests = %d\n",tot_req );
-}
-
-unsigned memory_sub_partition::flushL2() 
-{ 
-    if (!m_config->m_L2_config.disabled()) {
-        m_L2cache->flush(); 
+  }
+  if (!m_config->m_L2_config.disabled()) m_L2cache->display_state(fp);
+}
+
+void memory_stats_t::visualizer_print(gzFile visualizer_file) {
+  gzprintf(visualizer_file, "Ltwowritemiss: %d\n", L2_write_miss);
+  gzprintf(visualizer_file, "Ltwowritehit: %d\n", L2_write_hit);
+  gzprintf(visualizer_file, "Ltworeadmiss: %d\n", L2_read_miss);
+  gzprintf(visualizer_file, "Ltworeadhit: %d\n", L2_read_hit);
+  clear_L2_stats_pw();
+
+  if (num_mfs)
+    gzprintf(visualizer_file, "averagemflatency: %lld\n",
+             mf_total_lat / num_mfs);
+}
+
+void memory_stats_t::clear_L2_stats_pw() {
+  L2_write_miss = 0;
+  L2_write_hit = 0;
+  L2_read_miss = 0;
+  L2_read_hit = 0;
+}
+
+void gpgpu_sim::print_dram_stats(FILE *fout) const {
+  unsigned cmd = 0;
+  unsigned activity = 0;
+  unsigned nop = 0;
+  unsigned act = 0;
+  unsigned pre = 0;
+  unsigned rd = 0;
+  unsigned wr = 0;
+  unsigned wr_WB = 0;
+  unsigned req = 0;
+  unsigned tot_cmd = 0;
+  unsigned tot_nop = 0;
+  unsigned tot_act = 0;
+  unsigned tot_pre = 0;
+  unsigned tot_rd = 0;
+  unsigned tot_wr = 0;
+  unsigned tot_req = 0;
+
+  for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
+    m_memory_partition_unit[i]->set_dram_power_stats(cmd, activity, nop, act,
+                                                     pre, rd, wr, wr_WB, req);
+    tot_cmd += cmd;
+    tot_nop += nop;
+    tot_act += act;
+    tot_pre += pre;
+    tot_rd += rd;
+    tot_wr += wr + wr_WB;
+    tot_req += req;
+  }
+  fprintf(fout, "gpgpu_n_dram_reads = %d\n", tot_rd);
+  fprintf(fout, "gpgpu_n_dram_writes = %d\n", tot_wr);
+  fprintf(fout, "gpgpu_n_dram_activate = %d\n", tot_act);
+  fprintf(fout, "gpgpu_n_dram_commands = %d\n", tot_cmd);
+  fprintf(fout, "gpgpu_n_dram_noops = %d\n", tot_nop);
+  fprintf(fout, "gpgpu_n_dram_precharges = %d\n", tot_pre);
+  fprintf(fout, "gpgpu_n_dram_requests = %d\n", tot_req);
+}
+
+unsigned memory_sub_partition::flushL2() {
+  if (!m_config->m_L2_config.disabled()) {
+    m_L2cache->flush();
+  }
+  return 0;  // TODO: write the flushed data to the main memory
+}
+
+unsigned memory_sub_partition::invalidateL2() {
+  if (!m_config->m_L2_config.disabled()) {
+    m_L2cache->invalidate();
+  }
+  return 0;
+}
+
+bool memory_sub_partition::busy() const { return !m_request_tracker.empty(); }
+
+std::vector<mem_fetch *>
+memory_sub_partition::breakdown_request_to_sector_requests(mem_fetch *mf) {
+  std::vector<mem_fetch *> result;
+  mem_access_sector_mask_t sector_mask = mf->get_access_sector_mask();
+  if (mf->get_data_size() == SECTOR_SIZE &&
+      mf->get_access_sector_mask().count() == 1) {
+    result.push_back(mf);
+  } else if (mf->get_data_size() == MAX_MEMORY_ACCESS_SIZE) {
+    // break down every sector
+    mem_access_byte_mask_t mask;
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; i++) {
+      for (unsigned k = i * SECTOR_SIZE; k < (i + 1) * SECTOR_SIZE; k++) {
+        mask.set(k);
+      }
+      mem_fetch *n_mf = m_mf_allocator->alloc(
+          mf->get_addr() + SECTOR_SIZE * i, mf->get_access_type(),
+          mf->get_access_warp_mask(), mf->get_access_byte_mask() & mask,
+          std::bitset<SECTOR_CHUNCK_SIZE>().set(i), SECTOR_SIZE, mf->is_write(),
+          m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, mf->get_wid(),
+          mf->get_sid(), mf->get_tpc(), mf);
+
+      result.push_back(n_mf);
     }
-    return 0;   //TODO: write the flushed data to the main memory
-}
-
-unsigned memory_sub_partition::invalidateL2()
-{
-    if (!m_config->m_L2_config.disabled()) {
-        m_L2cache->invalidate();
+    // This is for constant cache
+  } else if (mf->get_data_size() == 64 &&
+             (mf->get_access_sector_mask().all() ||
+              mf->get_access_sector_mask().none())) {
+    unsigned start;
+    if (mf->get_addr() % MAX_MEMORY_ACCESS_SIZE == 0)
+      start = 0;
+    else
+      start = 2;
+    mem_access_byte_mask_t mask;
+    for (unsigned i = start; i < start + 2; i++) {
+      for (unsigned k = i * SECTOR_SIZE; k < (i + 1) * SECTOR_SIZE; k++) {
+        mask.set(k);
+      }
+      mem_fetch *n_mf = m_mf_allocator->alloc(
+          mf->get_addr(), mf->get_access_type(), mf->get_access_warp_mask(),
+          mf->get_access_byte_mask() & mask,
+          std::bitset<SECTOR_CHUNCK_SIZE>().set(i), SECTOR_SIZE, mf->is_write(),
+          m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle, mf->get_wid(),
+          mf->get_sid(), mf->get_tpc(), mf);
+
+      result.push_back(n_mf);
     }
-    return 0;
-}
-
-bool memory_sub_partition::busy() const 
-{
-    return !m_request_tracker.empty();
-}
-
-std::vector<mem_fetch*> memory_sub_partition::breakdown_request_to_sector_requests(mem_fetch* mf)
-{
-	std::vector<mem_fetch*> result;
-
-	if(mf->get_data_size() == SECTOR_SIZE && mf->get_access_sector_mask().count() == 1) {
-		result.push_back(mf);
-	} else if (mf->get_data_size() == 128 || mf->get_data_size() == 64) {
-        //We only accept 32, 64 and 128 bytes reqs
-		unsigned start=0, end=0;
-		if(mf->get_data_size() == 128) {
-			start=0; end=3;
-		} else if (mf->get_data_size() == 64 && mf->get_access_sector_mask().to_string() == "1100") {
-			start=2; end=3;
-		} else if (mf->get_data_size() == 64 && mf->get_access_sector_mask().to_string() == "0011") {
-			start=0; end=1;
-		} else if (mf->get_data_size() == 64 && (mf->get_access_sector_mask().to_string() == "1111" || mf->get_access_sector_mask().to_string() == "0000")) {
-			if(mf->get_addr() % 128 == 0) {
-				start=0; end=1;
-			} else {
-				start=2; end=3;
-			}
-		} else
-			{
-			    printf("Invalid sector received, address = 0x%06x, sector mask = %s, data size = %d",
-			    		mf->get_addr(), mf->get_access_sector_mask(), mf->get_data_size());
-				assert(0 && "Undefined sector mask is received");
-			}
-
-		std::bitset<SECTOR_SIZE*SECTOR_CHUNCK_SIZE> byte_sector_mask;
-		byte_sector_mask.reset();
-		for(unsigned  k=start*SECTOR_SIZE; k< SECTOR_SIZE; ++k)
-			byte_sector_mask.set(k);
-
-		for(unsigned j=start, i=0; j<= end ; ++j, ++i){
-
-			const mem_access_t *ma = new  mem_access_t( mf->get_access_type(),
-									mf->get_addr() + SECTOR_SIZE*i,
-									SECTOR_SIZE,
-									mf->is_write(),
-									mf->get_access_warp_mask(),
-									mf->get_access_byte_mask() & byte_sector_mask,
-									std::bitset<SECTOR_CHUNCK_SIZE>().set(j));
-
-			 mem_fetch *n_mf = new mem_fetch( *ma,
-								NULL,
-								mf->get_ctrl_size(),
-								mf->get_wid(),
-								mf->get_sid(),
-								mf->get_tpc(),
-								mf->get_mem_config(),
-								mf);
-
-			 result.push_back(n_mf);
-			 byte_sector_mask <<= SECTOR_SIZE;
-		}
-	} else {
-		 printf("Invalid sector received, address = 0x%06x, sector mask = %d, byte mask = , data size = %d",
-					    		mf->get_addr(), mf->get_access_sector_mask().count(), mf->get_data_size());
-		 assert(0 && "Undefined data size is received");
-	}
-
-	return result;
-}
-
-void memory_sub_partition::push( mem_fetch* m_req, unsigned long long cycle )
-{
-    if (m_req) {
-    	m_stats->memlatstat_icnt2mem_pop(m_req);
-    	std::vector<mem_fetch*> reqs;
-    	if(m_config->m_L2_config.m_cache_type == SECTOR)
-    		reqs = breakdown_request_to_sector_requests(m_req);
-    	else
-    		reqs.push_back(m_req);
-
-    	for(unsigned i=0; i<reqs.size(); ++i) {
-    		mem_fetch* req = reqs[i];
-			m_request_tracker.insert(req);
-			if( req->istexture() ) {
-				m_icnt_L2_queue->push(req);
-				req->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-			} else {
-				rop_delay_t r;
-				r.req = req;
-				r.ready_cycle = cycle + m_config->rop_latency;
-				m_rop.push(r);
-				req->set_status(IN_PARTITION_ROP_DELAY,gpu_sim_cycle+gpu_tot_sim_cycle);
-			}
-    	}
+  } else {
+    for (unsigned i = 0; i < SECTOR_CHUNCK_SIZE; i++) {
+      if (sector_mask.test(i)) {
+        mem_access_byte_mask_t mask;
+        for (unsigned k = i * SECTOR_SIZE; k < (i + 1) * SECTOR_SIZE; k++) {
+          mask.set(k);
+        }
+        mem_fetch *n_mf = m_mf_allocator->alloc(
+            mf->get_addr() + SECTOR_SIZE * i, mf->get_access_type(),
+            mf->get_access_warp_mask(), mf->get_access_byte_mask() & mask,
+            std::bitset<SECTOR_CHUNCK_SIZE>().set(i), SECTOR_SIZE,
+            mf->is_write(), m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle,
+            mf->get_wid(), mf->get_sid(), mf->get_tpc(), mf);
+
+        result.push_back(n_mf);
+      }
     }
-}
-
-mem_fetch* memory_sub_partition::pop() 
-{
-    mem_fetch* mf = m_L2_icnt_queue->pop();
+  }
+  if (result.size() == 0) assert(0 && "no mf sent");
+  return result;
+}
+
+void memory_sub_partition::push(mem_fetch *m_req, unsigned long long cycle) {
+  if (m_req) {
+    m_stats->memlatstat_icnt2mem_pop(m_req);
+    std::vector<mem_fetch *> reqs;
+    if (m_config->m_L2_config.m_cache_type == SECTOR)
+      reqs = breakdown_request_to_sector_requests(m_req);
+    else
+      reqs.push_back(m_req);
+
+    for (unsigned i = 0; i < reqs.size(); ++i) {
+      mem_fetch *req = reqs[i];
+      m_request_tracker.insert(req);
+      if (req->istexture()) {
+        m_icnt_L2_queue->push(req);
+        req->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,
+                        m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+      } else {
+        rop_delay_t r;
+        r.req = req;
+        r.ready_cycle = cycle + m_config->rop_latency;
+        m_rop.push(r);
+        req->set_status(IN_PARTITION_ROP_DELAY,
+                        m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+      }
+    }
+  }
+}
+
+mem_fetch *memory_sub_partition::pop() {
+  mem_fetch *mf = m_L2_icnt_queue->pop();
+  m_request_tracker.erase(mf);
+  if (mf && mf->isatomic()) mf->do_atomic();
+  if (mf && (mf->get_access_type() == L2_WRBK_ACC ||
+             mf->get_access_type() == L1_WRBK_ACC)) {
+    delete mf;
+    mf = NULL;
+  }
+  return mf;
+}
+
+mem_fetch *memory_sub_partition::top() {
+  mem_fetch *mf = m_L2_icnt_queue->top();
+  if (mf && (mf->get_access_type() == L2_WRBK_ACC ||
+             mf->get_access_type() == L1_WRBK_ACC)) {
+    m_L2_icnt_queue->pop();
     m_request_tracker.erase(mf);
-    if ( mf && mf->isatomic() )
-        mf->do_atomic();
-    if( mf && (mf->get_access_type() == L2_WRBK_ACC || mf->get_access_type() == L1_WRBK_ACC) ) {
-        delete mf;
-        mf = NULL;
-    } 
-    return mf;
+    delete mf;
+    mf = NULL;
+  }
+  return mf;
 }
 
-mem_fetch* memory_sub_partition::top() 
-{
-    mem_fetch *mf = m_L2_icnt_queue->top();
-    if( mf && (mf->get_access_type() == L2_WRBK_ACC || mf->get_access_type() == L1_WRBK_ACC) ) {
-        m_L2_icnt_queue->pop();
-        m_request_tracker.erase(mf);
-        delete mf;
-        mf = NULL;
-    } 
-    return mf;
+void memory_sub_partition::set_done(mem_fetch *mf) {
+  m_request_tracker.erase(mf);
 }
 
-void memory_sub_partition::set_done( mem_fetch *mf )
-{
-    m_request_tracker.erase(mf);
+void memory_sub_partition::accumulate_L2cache_stats(
+    class cache_stats &l2_stats) const {
+  if (!m_config->m_L2_config.disabled()) {
+    l2_stats += m_L2cache->get_stats();
+  }
 }
 
-void memory_sub_partition::accumulate_L2cache_stats(class cache_stats &l2_stats) const {
-    if (!m_config->m_L2_config.disabled()) {
-        l2_stats += m_L2cache->get_stats();
-    }
+void memory_sub_partition::get_L2cache_sub_stats(
+    struct cache_sub_stats &css) const {
+  if (!m_config->m_L2_config.disabled()) {
+    m_L2cache->get_sub_stats(css);
+  }
 }
 
-void memory_sub_partition::get_L2cache_sub_stats(struct cache_sub_stats &css) const{
-    if (!m_config->m_L2_config.disabled()) {
-        m_L2cache->get_sub_stats(css);
-    }
-}
-
-void memory_sub_partition::get_L2cache_sub_stats_pw(struct cache_sub_stats_pw &css) const{
-    if (!m_config->m_L2_config.disabled()) {
-        m_L2cache->get_sub_stats_pw(css);
-    }
+void memory_sub_partition::get_L2cache_sub_stats_pw(
+    struct cache_sub_stats_pw &css) const {
+  if (!m_config->m_L2_config.disabled()) {
+    m_L2cache->get_sub_stats_pw(css);
+  }
 }
 
 void memory_sub_partition::clear_L2cache_stats_pw() {
-    if (!m_config->m_L2_config.disabled()) {
-        m_L2cache->clear_pw();
-    }
+  if (!m_config->m_L2_config.disabled()) {
+    m_L2cache->clear_pw();
+  }
 }
 
-void memory_sub_partition::visualizer_print( gzFile visualizer_file )
-{
-    // Support for L2 AerialVision stats
-    // Per-sub-partition stats would be trivial to extend from this
-    cache_sub_stats_pw temp_sub_stats;
-    get_L2cache_sub_stats_pw(temp_sub_stats);
+void memory_sub_partition::visualizer_print(gzFile visualizer_file) {
+  // Support for L2 AerialVision stats
+  // Per-sub-partition stats would be trivial to extend from this
+  cache_sub_stats_pw temp_sub_stats;
+  get_L2cache_sub_stats_pw(temp_sub_stats);
 
-    m_stats->L2_read_miss += temp_sub_stats.read_misses;
-    m_stats->L2_write_miss += temp_sub_stats.write_misses;
-    m_stats->L2_read_hit += temp_sub_stats.read_hits;
-    m_stats->L2_write_hit += temp_sub_stats.write_hits;
+  m_stats->L2_read_miss += temp_sub_stats.read_misses;
+  m_stats->L2_write_miss += temp_sub_stats.write_misses;
+  m_stats->L2_read_hit += temp_sub_stats.read_hits;
+  m_stats->L2_write_hit += temp_sub_stats.write_hits;
 
-    clear_L2cache_stats_pw();
+  clear_L2cache_stats_pw();
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.h
index beafdd3c31..77bacc9e05 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache.h
@@ -28,8 +28,8 @@
 #ifndef MC_PARTITION_INCLUDED
 #define MC_PARTITION_INCLUDED
 
-#include "dram.h"
 #include "../abstract_hardware_model.h"
+#include "dram.h"
 
 #include <list>
 #include <queue>
@@ -37,216 +37,228 @@
 class mem_fetch;
 
 class partition_mf_allocator : public mem_fetch_allocator {
-public:
-    partition_mf_allocator( const memory_config *config )
-    {
-        m_memory_config = config;
-    }
-    virtual mem_fetch * alloc(const class warp_inst_t &inst, const mem_access_t &access) const 
-    {
-        abort();
-        return NULL;
-    }
-    virtual mem_fetch * alloc(new_addr_type addr, mem_access_type type, unsigned size, bool wr) const;
-private:
-    const memory_config *m_memory_config;
+ public:
+  partition_mf_allocator(const memory_config *config) {
+    m_memory_config = config;
+  }
+  virtual mem_fetch *alloc(const class warp_inst_t &inst,
+                           const mem_access_t &access,
+                           unsigned long long cycle) const {
+    abort();
+    return NULL;
+  }
+  virtual mem_fetch *alloc(new_addr_type addr, mem_access_type type,
+                           unsigned size, bool wr,
+                           unsigned long long cycle) const;
+  virtual mem_fetch *alloc(new_addr_type addr, mem_access_type type,
+                           const active_mask_t &active_mask,
+                           const mem_access_byte_mask_t &byte_mask,
+                           const mem_access_sector_mask_t &sector_mask,
+                           unsigned size, bool wr, unsigned long long cycle,
+                           unsigned wid, unsigned sid, unsigned tpc,
+                           mem_fetch *original_mf) const;
+
+ private:
+  const memory_config *m_memory_config;
 };
 
-// Memory partition unit contains all the units assolcated with a single DRAM channel. 
-// - It arbitrates the DRAM channel among multiple sub partitions.  
-// - It does not connect directly with the interconnection network. 
-class memory_partition_unit
-{
-public: 
-   memory_partition_unit( unsigned partition_id, const struct memory_config *config, class memory_stats_t *stats );
-   ~memory_partition_unit(); 
-
-   bool busy() const;
-
-   void cache_cycle( unsigned cycle );
-   void dram_cycle();
-
-   void set_done( mem_fetch *mf );
-
-   void visualizer_print( gzFile visualizer_file ) const;
-   void print_stat( FILE *fp ) { m_dram->print_stat(fp); }
-   void visualize() const { m_dram->visualize(); }
-   void print( FILE *fp ) const;
-   void handle_memcpy_to_gpu( size_t dst_start_addr, unsigned subpart_id, mem_access_sector_mask_t mask );
-
-   class memory_sub_partition * get_sub_partition(int sub_partition_id) 
-   {
-      return m_sub_partition[sub_partition_id]; 
-   }
-
-   // Power model
-   void set_dram_power_stats(unsigned &n_cmd,
-                             unsigned &n_activity,
-                             unsigned &n_nop,
-                             unsigned &n_act,
-                             unsigned &n_pre,
-                             unsigned &n_rd,
-                             unsigned &n_wr,
-                             unsigned &n_req) const;
-
-   int global_sub_partition_id_to_local_id(int global_sub_partition_id) const; 
-
-   unsigned get_mpid() const { return m_id; }
-
-private: 
-
-   unsigned m_id;
-   const struct memory_config *m_config;
-   class memory_stats_t *m_stats;
-   class memory_sub_partition **m_sub_partition; 
-   class dram_t *m_dram;
-
-   class arbitration_metadata
-   {
-   public: 
-      arbitration_metadata(const struct memory_config *config); 
-
-      // check if a subpartition still has credit 
-      bool has_credits(int inner_sub_partition_id) const; 
-      // borrow a credit for a subpartition 
-      void borrow_credit(int inner_sub_partition_id); 
-      // return a credit from a subpartition 
-      void return_credit(int inner_sub_partition_id); 
-
-      // return the last subpartition that borrowed credit 
-      int last_borrower() const { return m_last_borrower; } 
-
-      void print( FILE *fp ) const; 
-   private: 
-      // id of the last subpartition that borrowed credit 
-      int m_last_borrower; 
-
-      int m_shared_credit_limit; 
-      int m_private_credit_limit; 
-
-      // credits borrowed by the subpartitions
-      std::vector<int> m_private_credit; 
-      int m_shared_credit; 
-   }; 
-   arbitration_metadata m_arbitration_metadata; 
-
-   // determine wheither a given subpartition can issue to DRAM 
-   bool can_issue_to_dram(int inner_sub_partition_id); 
-
-   // model DRAM access scheduler latency (fixed latency between L2 and DRAM)
-   struct dram_delay_t
-   {
-      unsigned long long ready_cycle;
-      class mem_fetch* req;
-   };
-   std::list<dram_delay_t> m_dram_latency_queue;
+// Memory partition unit contains all the units assolcated with a single DRAM
+// channel.
+// - It arbitrates the DRAM channel among multiple sub partitions.
+// - It does not connect directly with the interconnection network.
+class memory_partition_unit {
+ public:
+  memory_partition_unit(unsigned partition_id, const memory_config *config,
+                        class memory_stats_t *stats, class gpgpu_sim *gpu);
+  ~memory_partition_unit();
+
+  bool busy() const;
+
+  void cache_cycle(unsigned cycle);
+  void dram_cycle();
+  void simple_dram_model_cycle();
+
+  void set_done(mem_fetch *mf);
+
+  void visualizer_print(gzFile visualizer_file) const;
+  void print_stat(FILE *fp) { m_dram->print_stat(fp); }
+  void visualize() const { m_dram->visualize(); }
+  void print(FILE *fp) const;
+  void handle_memcpy_to_gpu(size_t dst_start_addr, unsigned subpart_id,
+                            mem_access_sector_mask_t mask);
+
+  class memory_sub_partition *get_sub_partition(int sub_partition_id) {
+    return m_sub_partition[sub_partition_id];
+  }
+
+  // Power model
+  void set_dram_power_stats(unsigned &n_cmd, unsigned &n_activity,
+                            unsigned &n_nop, unsigned &n_act, unsigned &n_pre,
+                            unsigned &n_rd, unsigned &n_wr, unsigned &n_wr_WB,
+                            unsigned &n_req) const;
+
+  int global_sub_partition_id_to_local_id(int global_sub_partition_id) const;
+
+  unsigned get_mpid() const { return m_id; }
+
+  class gpgpu_sim *get_mgpu() const {
+    return m_gpu;
+  }
+
+ private:
+  unsigned m_id;
+  const memory_config *m_config;
+  class memory_stats_t *m_stats;
+  class memory_sub_partition **m_sub_partition;
+  class dram_t *m_dram;
+
+  class arbitration_metadata {
+   public:
+    arbitration_metadata(const memory_config *config);
+
+    // check if a subpartition still has credit
+    bool has_credits(int inner_sub_partition_id) const;
+    // borrow a credit for a subpartition
+    void borrow_credit(int inner_sub_partition_id);
+    // return a credit from a subpartition
+    void return_credit(int inner_sub_partition_id);
+
+    // return the last subpartition that borrowed credit
+    int last_borrower() const { return m_last_borrower; }
+
+    void print(FILE *fp) const;
+
+   private:
+    // id of the last subpartition that borrowed credit
+    int m_last_borrower;
+
+    int m_shared_credit_limit;
+    int m_private_credit_limit;
+
+    // credits borrowed by the subpartitions
+    std::vector<int> m_private_credit;
+    int m_shared_credit;
+  };
+  arbitration_metadata m_arbitration_metadata;
+
+  // determine wheither a given subpartition can issue to DRAM
+  bool can_issue_to_dram(int inner_sub_partition_id);
+
+  // model DRAM access scheduler latency (fixed latency between L2 and DRAM)
+  struct dram_delay_t {
+    unsigned long long ready_cycle;
+    class mem_fetch *req;
+  };
+  std::list<dram_delay_t> m_dram_latency_queue;
+
+  class gpgpu_sim *m_gpu;
 };
 
-class memory_sub_partition
-{
-public:
-   memory_sub_partition( unsigned sub_partition_id, const struct memory_config *config, class memory_stats_t *stats );
-   ~memory_sub_partition(); 
-
-   unsigned get_id() const { return m_id; } 
-
-   bool busy() const;
-
-   void cache_cycle( unsigned cycle );
-
-   bool full() const;
-   bool full(unsigned size) const;
-   void push( class mem_fetch* mf, unsigned long long clock_cycle );
-   class mem_fetch* pop(); 
-   class mem_fetch* top();
-   void set_done( mem_fetch *mf );
-
-   unsigned flushL2();
-   unsigned invalidateL2();
-
-   // interface to L2_dram_queue
-   bool L2_dram_queue_empty() const; 
-   class mem_fetch* L2_dram_queue_top() const; 
-   void L2_dram_queue_pop(); 
-
-   // interface to dram_L2_queue
-   bool dram_L2_queue_full() const; 
-   void dram_L2_queue_push( class mem_fetch* mf ); 
-
-   void visualizer_print( gzFile visualizer_file );
-   void print_cache_stat(unsigned &accesses, unsigned &misses) const;
-   void print( FILE *fp ) const;
-
-   void accumulate_L2cache_stats(class cache_stats &l2_stats) const;
-   void get_L2cache_sub_stats(struct cache_sub_stats &css) const;
-
-   // Support for getting per-window L2 stats for AerialVision
-   void get_L2cache_sub_stats_pw(struct cache_sub_stats_pw &css) const;
-   void clear_L2cache_stats_pw();
-
-   void force_l2_tag_update(new_addr_type addr, unsigned time, mem_access_sector_mask_t mask)
-   {
-        m_L2cache->force_tag_access( addr, m_memcpy_cycle_offset + time, mask );
-        m_memcpy_cycle_offset += 1;
-   }
-
-private:
-// data
-   unsigned m_id;  //< the global sub partition ID
-   const struct memory_config *m_config;
-   class l2_cache *m_L2cache;
-   class L2interface *m_L2interface;
-   partition_mf_allocator *m_mf_allocator;
-
-   // model delay of ROP units with a fixed latency
-   struct rop_delay_t
-   {
-    	unsigned long long ready_cycle;
-    	class mem_fetch* req;
-   };
-   std::queue<rop_delay_t> m_rop;
-
-   // these are various FIFOs between units within a memory partition
-   fifo_pipeline<mem_fetch> *m_icnt_L2_queue;
-   fifo_pipeline<mem_fetch> *m_L2_dram_queue;
-   fifo_pipeline<mem_fetch> *m_dram_L2_queue;
-   fifo_pipeline<mem_fetch> *m_L2_icnt_queue; // L2 cache hit response queue
-
-   class mem_fetch *L2dramout; 
-   unsigned long long int wb_addr;
-
-   class memory_stats_t *m_stats;
-
-   std::set<mem_fetch*> m_request_tracker;
-
-   friend class L2interface;
-
-   std::vector<mem_fetch*> breakdown_request_to_sector_requests(mem_fetch* mf);
-
-   // This is a cycle offset that has to be applied to the l2 accesses to account for
-   // the cudamemcpy read/writes. We want GPGPU-Sim to only count cycles for kernel execution
-   // but we want cudamemcpy to go through the L2. Everytime an access is made from cudamemcpy
-   // this counter is incremented, and when the l2 is accessed (in both cudamemcpyies and otherwise)
-   // this value is added to the gpgpu-sim cycle counters.
-   unsigned m_memcpy_cycle_offset;
+class memory_sub_partition {
+ public:
+  memory_sub_partition(unsigned sub_partition_id, const memory_config *config,
+                       class memory_stats_t *stats, class gpgpu_sim *gpu);
+  ~memory_sub_partition();
+
+  unsigned get_id() const { return m_id; }
+
+  bool busy() const;
+
+  void cache_cycle(unsigned cycle);
+
+  bool full() const;
+  bool full(unsigned size) const;
+  void push(class mem_fetch *mf, unsigned long long clock_cycle);
+  class mem_fetch *pop();
+  class mem_fetch *top();
+  void set_done(mem_fetch *mf);
+
+  unsigned flushL2();
+  unsigned invalidateL2();
+
+  // interface to L2_dram_queue
+  bool L2_dram_queue_empty() const;
+  class mem_fetch *L2_dram_queue_top() const;
+  void L2_dram_queue_pop();
+
+  // interface to dram_L2_queue
+  bool dram_L2_queue_full() const;
+  void dram_L2_queue_push(class mem_fetch *mf);
+
+  void visualizer_print(gzFile visualizer_file);
+  void print_cache_stat(unsigned &accesses, unsigned &misses) const;
+  void print(FILE *fp) const;
+
+  void accumulate_L2cache_stats(class cache_stats &l2_stats) const;
+  void get_L2cache_sub_stats(struct cache_sub_stats &css) const;
+
+  // Support for getting per-window L2 stats for AerialVision
+  void get_L2cache_sub_stats_pw(struct cache_sub_stats_pw &css) const;
+  void clear_L2cache_stats_pw();
+
+  void force_l2_tag_update(new_addr_type addr, unsigned time,
+                           mem_access_sector_mask_t mask) {
+    m_L2cache->force_tag_access(addr, m_memcpy_cycle_offset + time, mask);
+    m_memcpy_cycle_offset += 1;
+  }
+
+ private:
+  // data
+  unsigned m_id;  //< the global sub partition ID
+  const memory_config *m_config;
+  class l2_cache *m_L2cache;
+  class L2interface *m_L2interface;
+  class gpgpu_sim *m_gpu;
+  partition_mf_allocator *m_mf_allocator;
+
+  // model delay of ROP units with a fixed latency
+  struct rop_delay_t {
+    unsigned long long ready_cycle;
+    class mem_fetch *req;
+  };
+  std::queue<rop_delay_t> m_rop;
+
+  // these are various FIFOs between units within a memory partition
+  fifo_pipeline<mem_fetch> *m_icnt_L2_queue;
+  fifo_pipeline<mem_fetch> *m_L2_dram_queue;
+  fifo_pipeline<mem_fetch> *m_dram_L2_queue;
+  fifo_pipeline<mem_fetch> *m_L2_icnt_queue;  // L2 cache hit response queue
+
+  class mem_fetch *L2dramout;
+  unsigned long long int wb_addr;
+
+  class memory_stats_t *m_stats;
+
+  std::set<mem_fetch *> m_request_tracker;
+
+  friend class L2interface;
+
+  std::vector<mem_fetch *> breakdown_request_to_sector_requests(mem_fetch *mf);
+
+  // This is a cycle offset that has to be applied to the l2 accesses to account
+  // for the cudamemcpy read/writes. We want GPGPU-Sim to only count cycles for
+  // kernel execution but we want cudamemcpy to go through the L2. Everytime an
+  // access is made from cudamemcpy this counter is incremented, and when the l2
+  // is accessed (in both cudamemcpyies and otherwise) this value is added to
+  // the gpgpu-sim cycle counters.
+  unsigned m_memcpy_cycle_offset;
 };
 
 class L2interface : public mem_fetch_interface {
-public:
-    L2interface( memory_sub_partition *unit ) { m_unit=unit; }
-    virtual ~L2interface() {}
-    virtual bool full( unsigned size, bool write) const 
-    {
-        // assume read and write packets all same size
-        return m_unit->m_L2_dram_queue->full();
-    }
-    virtual void push(mem_fetch *mf) 
-    {
-        mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,0/*FIXME*/);
-        m_unit->m_L2_dram_queue->push(mf);
-    }
-private:
-    memory_sub_partition *m_unit;
+ public:
+  L2interface(memory_sub_partition *unit) { m_unit = unit; }
+  virtual ~L2interface() {}
+  virtual bool full(unsigned size, bool write) const {
+    // assume read and write packets all same size
+    return m_unit->m_L2_dram_queue->full();
+  }
+  virtual void push(mem_fetch *mf) {
+    mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE, 0 /*FIXME*/);
+    m_unit->m_L2_dram_queue->push(mf);
+  }
+
+ private:
+  memory_sub_partition *m_unit;
 };
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache_trace.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache_trace.h
index 10e0b62d57..a595cc07ea 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache_trace.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/l2cache_trace.h
@@ -39,25 +39,25 @@
 
 // Intended to be called from inside components of a memory partition
 // Depends on a get_mpid() function
-#define MEMPART_DPRINTF(...) do {\
-    if (MEMPART_DTRACE(MEMORY_PARTITION_UNIT)) {\
-        printf( MEMPART_PRINT_STR,\
-                gpu_sim_cycle + gpu_tot_sim_cycle,\
-                Trace_gpgpu::trace_streams_str[Trace_gpgpu::MEMORY_PARTITION_UNIT],\
-                get_mpid() );\
-        printf(__VA_ARGS__);\
-    }\
-} while (0)
+#define MEMPART_DPRINTF(...)                                                   \
+  do {                                                                         \
+    if (MEMPART_DTRACE(MEMORY_PARTITION_UNIT)) {                               \
+      printf(                                                                  \
+          MEMPART_PRINT_STR, m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle,  \
+          Trace_gpgpu::trace_streams_str[Trace_gpgpu::MEMORY_PARTITION_UNIT], get_mpid()); \
+      printf(__VA_ARGS__);                                                     \
+    }                                                                          \
+  } while (0)
 
-#define MEM_SUBPART_DPRINTF(...) do {\
-    if (MEM_SUBPART_DTRACE(MEMORY_PARTITION_UNIT)) {\
-        printf( MEM_SUBPART_PRINT_STR,\
-                gpu_sim_cycle + gpu_tot_sim_cycle,\
-                Trace_gpgpu::trace_streams_str[Trace_gpgpu::MEMORY_SUBPARTITION_UNIT],\
-                m_id );\
-        printf(__VA_ARGS__);\
-    }\
-} while (0)
+#define MEM_SUBPART_DPRINTF(...)                                               \
+  do {                                                                         \
+    if (MEM_SUBPART_DTRACE(MEMORY_PARTITION_UNIT)) {                           \
+      printf(MEM_SUBPART_PRINT_STR,                                            \
+             m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle,                  \
+             Trace_gpgpu::trace_streams_str[Trace_gpgpu::MEMORY_SUBPARTITION_UNIT], m_id); \
+      printf(__VA_ARGS__);                                                     \
+    }                                                                          \
+  } while (0)
 
 #else
 
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.cc
index a260a3580e..21abedcd6d 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.cc
@@ -26,117 +26,115 @@
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #include "mem_fetch.h"
+#include "gpu-sim.h"
 #include "mem_latency_stat.h"
 #include "shader.h"
 #include "visualizer.h"
-#include "gpu-sim.h"
 
-unsigned mem_fetch::sm_next_mf_request_uid=1;
+unsigned mem_fetch::sm_next_mf_request_uid = 1;
 
-mem_fetch::mem_fetch( const mem_access_t &access, 
-                      const warp_inst_t *inst,
-                      unsigned ctrl_size, 
-                      unsigned wid,
-                      unsigned sid, 
-                      unsigned tpc, 
-                      const struct memory_config *config,
-					  mem_fetch *m_original_mf,
-					  mem_fetch *m_original_wr_mf)
+mem_fetch::mem_fetch(const mem_access_t &access, const warp_inst_t *inst,
+                     unsigned ctrl_size, unsigned wid, unsigned sid,
+                     unsigned tpc, const memory_config *config,
+                     unsigned long long cycle, mem_fetch *m_original_mf,
+                     mem_fetch *m_original_wr_mf)
+    : m_access(access)
 
 {
-   m_request_uid = sm_next_mf_request_uid++;
-   m_access = access;
-   if( inst ) { 
-       m_inst = *inst;
-       assert( wid == m_inst.warp_id() );
-   }
-   m_data_size = access.get_size();
-   m_ctrl_size = ctrl_size;
-   m_sid = sid;
-   m_tpc = tpc;
-   m_wid = wid;
-   config->m_address_mapping.addrdec_tlx(access.get_addr(),&m_raw_addr);
-   m_partition_addr = config->m_address_mapping.partition_address(access.get_addr());
-   m_type = m_access.is_write()?WRITE_REQUEST:READ_REQUEST;
-   m_timestamp = gpu_sim_cycle + gpu_tot_sim_cycle;
-   m_timestamp2 = 0;
-   m_status = MEM_FETCH_INITIALIZED;
-   m_status_change = gpu_sim_cycle + gpu_tot_sim_cycle;
-   m_mem_config = config;
-   icnt_flit_size = config->icnt_flit_size;
-   original_mf = m_original_mf;
-   original_wr_mf = m_original_wr_mf;
+  m_request_uid = sm_next_mf_request_uid++;
+  m_access = access;
+  if (inst) {
+    m_inst = *inst;
+    assert(wid == m_inst.warp_id());
+  }
+  m_data_size = access.get_size();
+  m_ctrl_size = ctrl_size;
+  m_sid = sid;
+  m_tpc = tpc;
+  m_wid = wid;
+  config->m_address_mapping.addrdec_tlx(access.get_addr(), &m_raw_addr);
+  m_partition_addr =
+      config->m_address_mapping.partition_address(access.get_addr());
+  m_type = m_access.is_write() ? WRITE_REQUEST : READ_REQUEST;
+  m_timestamp = cycle;
+  m_timestamp2 = 0;
+  m_status = MEM_FETCH_INITIALIZED;
+  m_status_change = cycle;
+  m_mem_config = config;
+  icnt_flit_size = config->icnt_flit_size;
+  original_mf = m_original_mf;
+  original_wr_mf = m_original_wr_mf;
+  if (m_original_mf) {
+    m_raw_addr.chip = m_original_mf->get_tlx_addr().chip;
+    m_raw_addr.sub_partition = m_original_mf->get_tlx_addr().sub_partition;
+  }
 }
 
-mem_fetch::~mem_fetch()
-{
-    m_status = MEM_FETCH_DELETED;
-}
+mem_fetch::~mem_fetch() { m_status = MEM_FETCH_DELETED; }
 
-#define MF_TUP_BEGIN(X) static const char* Status_str[] = {
+#define MF_TUP_BEGIN(X) static const char *Status_str[] = {
 #define MF_TUP(X) #X
-#define MF_TUP_END(X) };
+#define MF_TUP_END(X) \
+  }                   \
+  ;
 #include "mem_fetch_status.tup"
 #undef MF_TUP_BEGIN
 #undef MF_TUP
 #undef MF_TUP_END
 
-void mem_fetch::print( FILE *fp, bool print_inst ) const
-{
-    if( this == NULL ) {
-        fprintf(fp," <NULL mem_fetch pointer>\n");
-        return;
-    }
-    fprintf(fp,"  mf: uid=%6u, sid%02u:w%02u, part=%u, ", m_request_uid, m_sid, m_wid, m_raw_addr.chip );
-    m_access.print(fp);
-    if( (unsigned)m_status < NUM_MEM_REQ_STAT ) 
-       fprintf(fp," status = %s (%llu), ", Status_str[m_status], m_status_change );
-    else
-       fprintf(fp," status = %u??? (%llu), ", m_status, m_status_change );
-    if( !m_inst.empty() && print_inst ) m_inst.print(fp);
-    else fprintf(fp,"\n");
+void mem_fetch::print(FILE *fp, bool print_inst) const {
+  if (this == NULL) {
+    fprintf(fp, " <NULL mem_fetch pointer>\n");
+    return;
+  }
+  fprintf(fp, "  mf: uid=%6u, sid%02u:w%02u, part=%u, ", m_request_uid, m_sid,
+          m_wid, m_raw_addr.chip);
+  m_access.print(fp);
+  if ((unsigned)m_status < NUM_MEM_REQ_STAT)
+    fprintf(fp, " status = %s (%llu), ", Status_str[m_status], m_status_change);
+  else
+    fprintf(fp, " status = %u??? (%llu), ", m_status, m_status_change);
+  if (!m_inst.empty() && print_inst)
+    m_inst.print(fp);
+  else
+    fprintf(fp, "\n");
 }
 
-void mem_fetch::set_status( enum mem_fetch_status status, unsigned long long cycle ) 
-{
-    m_status = status;
-    m_status_change = cycle;
+void mem_fetch::set_status(enum mem_fetch_status status,
+                           unsigned long long cycle) {
+  m_status = status;
+  m_status_change = cycle;
 }
 
-bool mem_fetch::isatomic() const
-{
-   if( m_inst.empty() ) return false;
-   return m_inst.isatomic();
+bool mem_fetch::isatomic() const {
+  if (m_inst.empty()) return false;
+  return m_inst.isatomic();
 }
 
-void mem_fetch::do_atomic()
-{
-    m_inst.do_atomic( m_access.get_warp_mask() );
-}
+void mem_fetch::do_atomic() { m_inst.do_atomic(m_access.get_warp_mask()); }
 
-bool mem_fetch::istexture() const
-{
-    if( m_inst.empty() ) return false;
-    return m_inst.space.get_type() == tex_space;
+bool mem_fetch::istexture() const {
+  if (m_inst.empty()) return false;
+  return m_inst.space.get_type() == tex_space;
 }
 
-bool mem_fetch::isconst() const
-{ 
-    if( m_inst.empty() ) return false;
-    return (m_inst.space.get_type() == const_space) || (m_inst.space.get_type() == param_space_kernel);
+bool mem_fetch::isconst() const {
+  if (m_inst.empty()) return false;
+  return (m_inst.space.get_type() == const_space) ||
+         (m_inst.space.get_type() == param_space_kernel);
 }
 
-/// Returns number of flits traversing interconnect. simt_to_mem specifies the direction
-unsigned mem_fetch::get_num_flits(bool simt_to_mem){
-	unsigned sz=0;
-	// If atomic, write going to memory, or read coming back from memory, size = ctrl + data. Else, only ctrl
-	if( isatomic() || (simt_to_mem && get_is_write()) || !(simt_to_mem || get_is_write()) )
-		sz = size();
-	else
-		sz = get_ctrl_size();
-
-	return (sz/icnt_flit_size) + ( (sz % icnt_flit_size)? 1:0);
+/// Returns number of flits traversing interconnect. simt_to_mem specifies the
+/// direction
+unsigned mem_fetch::get_num_flits(bool simt_to_mem) {
+  unsigned sz = 0;
+  // If atomic, write going to memory, or read coming back from memory, size =
+  // ctrl + data. Else, only ctrl
+  if (isatomic() || (simt_to_mem && get_is_write()) ||
+      !(simt_to_mem || get_is_write()))
+    sz = size();
+  else
+    sz = get_ctrl_size();
+
+  return (sz / icnt_flit_size) + ((sz % icnt_flit_size) ? 1 : 0);
 }
-
-
-
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.h
index e5efffd4c0..75fd6398c1 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_fetch.h
@@ -7,153 +7,173 @@
 //
 // Redistributions of source code must retain the above copyright notice, this
 // list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// Redistributions in binary form must reproduce the above copyright notice,
+// this list of conditions and the following disclaimer in the documentation
+// and/or other materials provided with the distribution. Neither the name of
+// The University of British Columbia nor the names of its contributors may be
+// used to endorse or promote products derived from this software without
+// specific prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
 
 #ifndef MEM_FETCH_H
 #define MEM_FETCH_H
 
-#include "addrdec.h"
-#include "../abstract_hardware_model.h"
 #include <bitset>
+#include "../abstract_hardware_model.h"
+#include "addrdec.h"
 
 enum mf_type {
-   READ_REQUEST = 0,
-   WRITE_REQUEST,
-   READ_REPLY, // send to shader
-   WRITE_ACK
+  READ_REQUEST = 0,
+  WRITE_REQUEST,
+  READ_REPLY,  // send to shader
+  WRITE_ACK
 };
 
 #define MF_TUP_BEGIN(X) enum X {
 #define MF_TUP(X) X
-#define MF_TUP_END(X) };
+#define MF_TUP_END(X) \
+  }                   \
+  ;
 #include "mem_fetch_status.tup"
 #undef MF_TUP_BEGIN
 #undef MF_TUP
 #undef MF_TUP_END
 
+class memory_config;
 class mem_fetch {
-public:
-    mem_fetch( const mem_access_t &access, 
-               const warp_inst_t *inst,
-               unsigned ctrl_size, 
-               unsigned wid,
-               unsigned sid, 
-               unsigned tpc, 
-               const struct memory_config *config,
-			   mem_fetch *original_mf = NULL,
-			   mem_fetch *original_wr_mf = NULL);
-   ~mem_fetch();
-
-   void set_status( enum mem_fetch_status status, unsigned long long cycle );
-   void set_reply() 
-   { 
-       assert( m_access.get_type() != L1_WRBK_ACC && m_access.get_type() != L2_WRBK_ACC );
-       if( m_type==READ_REQUEST ) {
-           assert( !get_is_write() );
-           m_type = READ_REPLY;
-       } else if( m_type == WRITE_REQUEST ) {
-           assert( get_is_write() );
-           m_type = WRITE_ACK;
-       }
-   }
-   void do_atomic();
-
-   void print( FILE *fp, bool print_inst = true ) const;
-
-   const addrdec_t &get_tlx_addr() const { return m_raw_addr; }
-   unsigned get_data_size() const { return m_data_size; }
-   void     set_data_size( unsigned size ) { m_data_size=size; }
-   unsigned get_ctrl_size() const { return m_ctrl_size; }
-   unsigned size() const { return m_data_size+m_ctrl_size; }
-   bool is_write() {return m_access.is_write();}
-   void set_addr(new_addr_type addr) { m_access.set_addr(addr); }
-   new_addr_type get_addr() const { return m_access.get_addr(); }
-   unsigned get_access_size() const { return m_access.get_size(); }
-   new_addr_type get_partition_addr() const { return m_partition_addr; }
-   unsigned get_sub_partition_id() const { return m_raw_addr.sub_partition; }
-   bool     get_is_write() const { return m_access.is_write(); }
-   unsigned get_request_uid() const { return m_request_uid; }
-   unsigned get_sid() const { return m_sid; }
-   unsigned get_tpc() const { return m_tpc; }
-   unsigned get_wid() const { return m_wid; }
-   bool istexture() const;
-   bool isconst() const;
-   enum mf_type get_type() const { return m_type; }
-   bool isatomic() const;
-
-   void set_return_timestamp( unsigned t ) { m_timestamp2=t; }
-   void set_icnt_receive_time( unsigned t ) { m_icnt_receive_time=t; }
-   unsigned get_timestamp() const { return m_timestamp; }
-   unsigned get_return_timestamp() const { return m_timestamp2; }
-   unsigned get_icnt_receive_time() const { return m_icnt_receive_time; }
-
-   enum mem_access_type get_access_type() const { return m_access.get_type(); }
-   const active_mask_t& get_access_warp_mask() const { return m_access.get_warp_mask(); }
-   mem_access_byte_mask_t get_access_byte_mask() const { return m_access.get_byte_mask(); }
-   mem_access_sector_mask_t get_access_sector_mask() const { return m_access.get_sector_mask(); }
-
-   address_type get_pc() const { return m_inst.empty()?-1:m_inst.pc; }
-   const warp_inst_t &get_inst() { return m_inst; }
-   enum mem_fetch_status get_status() const { return m_status; }
-
-   const memory_config *get_mem_config(){return m_mem_config;}
-
-   unsigned get_num_flits(bool simt_to_mem);
-
-   mem_fetch* get_original_mf() { return original_mf; }
-   mem_fetch* get_original_wr_mf()  { return original_wr_mf; }
+ public:
+  mem_fetch(const mem_access_t &access, const warp_inst_t *inst,
+            unsigned ctrl_size, unsigned wid, unsigned sid, unsigned tpc,
+            const memory_config *config, unsigned long long cycle,
+            mem_fetch *original_mf = NULL, mem_fetch *original_wr_mf = NULL);
+  ~mem_fetch();
+
+  void set_status(enum mem_fetch_status status, unsigned long long cycle);
+  void set_reply() {
+    assert(m_access.get_type() != L1_WRBK_ACC &&
+           m_access.get_type() != L2_WRBK_ACC);
+    if (m_type == READ_REQUEST) {
+      assert(!get_is_write());
+      m_type = READ_REPLY;
+    } else if (m_type == WRITE_REQUEST) {
+      assert(get_is_write());
+      m_type = WRITE_ACK;
+    }
+  }
+  void do_atomic();
+
+  void print(FILE *fp, bool print_inst = true) const;
+
+  const addrdec_t &get_tlx_addr() const { return m_raw_addr; }
+  void set_chip(unsigned chip_id) { m_raw_addr.chip = chip_id; }
+  void set_parition(unsigned sub_partition_id) {
+    m_raw_addr.sub_partition = sub_partition_id;
+  }
+  unsigned get_data_size() const { return m_data_size; }
+  void set_data_size(unsigned size) { m_data_size = size; }
+  unsigned get_ctrl_size() const { return m_ctrl_size; }
+  unsigned size() const { return m_data_size + m_ctrl_size; }
+  bool is_write() { return m_access.is_write(); }
+  void set_addr(new_addr_type addr) { m_access.set_addr(addr); }
+  new_addr_type get_addr() const { return m_access.get_addr(); }
+  unsigned get_access_size() const { return m_access.get_size(); }
+  new_addr_type get_partition_addr() const { return m_partition_addr; }
+  unsigned get_sub_partition_id() const { return m_raw_addr.sub_partition; }
+  bool get_is_write() const { return m_access.is_write(); }
+  unsigned get_request_uid() const { return m_request_uid; }
+  unsigned get_sid() const { return m_sid; }
+  unsigned get_tpc() const { return m_tpc; }
+  unsigned get_wid() const { return m_wid; }
+  bool istexture() const;
+  bool isconst() const;
+  enum mf_type get_type() const { return m_type; }
+  bool isatomic() const;
+
+  void set_return_timestamp(unsigned t) { m_timestamp2 = t; }
+  void set_icnt_receive_time(unsigned t) { m_icnt_receive_time = t; }
+  unsigned get_timestamp() const { return m_timestamp; }
+  unsigned get_return_timestamp() const { return m_timestamp2; }
+  unsigned get_icnt_receive_time() const { return m_icnt_receive_time; }
+
+  enum mem_access_type get_access_type() const { return m_access.get_type(); }
+  const active_mask_t &get_access_warp_mask() const {
+    return m_access.get_warp_mask();
+  }
+  mem_access_byte_mask_t get_access_byte_mask() const {
+    return m_access.get_byte_mask();
+  }
+  mem_access_sector_mask_t get_access_sector_mask() const {
+    return m_access.get_sector_mask();
+  }
+
+  address_type get_pc() const { return m_inst.empty() ? -1 : m_inst.pc; }
+  const warp_inst_t &get_inst() { return m_inst; }
+  enum mem_fetch_status get_status() const { return m_status; }
+
+  const memory_config *get_mem_config() { return m_mem_config; }
+
+  unsigned get_num_flits(bool simt_to_mem);
+
+  mem_fetch *get_original_mf() { return original_mf; }
+  mem_fetch *get_original_wr_mf() { return original_wr_mf; }
 
 private:
-   // request source information
-   unsigned m_request_uid;
-   unsigned m_sid;
-   unsigned m_tpc;
-   unsigned m_wid;
-
-   // where is this request now?
-   enum mem_fetch_status m_status;
-   unsigned long long m_status_change;
-
-   // request type, address, size, mask
-   mem_access_t m_access;
-   unsigned m_data_size; // how much data is being written
-   unsigned m_ctrl_size; // how big would all this meta data be in hardware (does not necessarily match actual size of mem_fetch)
-   new_addr_type m_partition_addr; // linear physical address *within* dram partition (partition bank select bits squeezed out)
-   addrdec_t m_raw_addr; // raw physical address (i.e., decoded DRAM chip-row-bank-column address)
-   enum mf_type m_type;
-
-   // statistics
-   unsigned m_timestamp;  // set to gpu_sim_cycle+gpu_tot_sim_cycle at struct creation
-   unsigned m_timestamp2; // set to gpu_sim_cycle+gpu_tot_sim_cycle when pushed onto icnt to shader; only used for reads
-   unsigned m_icnt_receive_time; // set to gpu_sim_cycle + interconnect_latency when fixed icnt latency mode is enabled
-
-   // requesting instruction (put last so mem_fetch prints nicer in gdb)
-   warp_inst_t m_inst;
-
-   static unsigned sm_next_mf_request_uid;
-
-   const struct memory_config *m_mem_config;
-   unsigned icnt_flit_size;
-
-   mem_fetch* original_mf;  //this pointer is set up when a request is divided into sector requests at L2 cache (if the req size > L2 sector size), so the pointer refers to the original request
-   mem_fetch* original_wr_mf;  //this pointer refers to the original write req, when fetch-on-write policy is used
-
+  // request source information
+  unsigned m_request_uid;
+  unsigned m_sid;
+  unsigned m_tpc;
+  unsigned m_wid;
+
+  // where is this request now?
+  enum mem_fetch_status m_status;
+  unsigned long long m_status_change;
+
+  // request type, address, size, mask
+  mem_access_t m_access;
+  unsigned m_data_size;  // how much data is being written
+  unsigned
+      m_ctrl_size;  // how big would all this meta data be in hardware (does not
+                    // necessarily match actual size of mem_fetch)
+  new_addr_type
+      m_partition_addr;  // linear physical address *within* dram partition
+                         // (partition bank select bits squeezed out)
+  addrdec_t m_raw_addr;  // raw physical address (i.e., decoded DRAM
+                         // chip-row-bank-column address)
+  enum mf_type m_type;
+
+  // statistics
+  unsigned
+      m_timestamp;  // set to gpu_sim_cycle+gpu_tot_sim_cycle at struct creation
+  unsigned m_timestamp2;  // set to gpu_sim_cycle+gpu_tot_sim_cycle when pushed
+                          // onto icnt to shader; only used for reads
+  unsigned m_icnt_receive_time;  // set to gpu_sim_cycle + interconnect_latency
+                                 // when fixed icnt latency mode is enabled
+
+  // requesting instruction (put last so mem_fetch prints nicer in gdb)
+  warp_inst_t m_inst;
+
+  static unsigned sm_next_mf_request_uid;
+
+  const memory_config *m_mem_config;
+  unsigned icnt_flit_size;
+
+  mem_fetch
+      *original_mf;  // this pointer is set up when a request is divided into
+                     // sector requests at L2 cache (if the req size > L2 sector
+                     // size), so the pointer refers to the original request
+  mem_fetch *original_wr_mf;  // this pointer refers to the original write req,
+                              // when fetch-on-write policy is used
 };
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.cc
index 11624f46cb..d1aca6cf08 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.cc
@@ -41,439 +41,483 @@
 #include <string.h>
 #include <stdlib.h>
 #include <stdio.h>
-
-memory_stats_t::memory_stats_t( unsigned n_shader, const struct shader_core_config *shader_config, const struct memory_config *mem_config )
-{
-   assert( mem_config->m_valid );
-   assert( shader_config->m_valid );
-
-   unsigned i,j;
-
-
-   concurrent_row_access = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-   num_activates = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-   row_access = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-   max_conc_access2samerow = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-   max_servicetime2samerow = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-
-   for (unsigned i=0;i<mem_config->m_n_mem ;i++ ) {
-      concurrent_row_access[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      row_access[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      num_activates[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      max_conc_access2samerow[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      max_servicetime2samerow[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-   }
-
-
-   m_n_shader=n_shader;
-   m_memory_config=mem_config;
-   total_n_access=0;
-   total_n_reads=0;
-   total_n_writes=0;
-   max_mrq_latency = 0;
-   max_dq_latency = 0;
-   max_mf_latency = 0;
-   max_icnt2mem_latency = 0;
-   max_icnt2sh_latency = 0;
-   tot_icnt2mem_latency = 0;
-   tot_icnt2sh_latency = 0;
-   tot_mrq_num = 0;
-   tot_mrq_latency = 0;
-   memset(mrq_lat_table, 0, sizeof(unsigned)*32);
-   memset(dq_lat_table, 0, sizeof(unsigned)*32);
-   memset(mf_lat_table, 0, sizeof(unsigned)*32);
-   memset(icnt2mem_lat_table, 0, sizeof(unsigned)*24);
-   memset(icnt2sh_lat_table, 0, sizeof(unsigned)*24);
-   memset(mf_lat_pw_table, 0, sizeof(unsigned)*32);
-   mf_num_lat_pw = 0;
-   max_warps = n_shader * (shader_config->n_thread_per_shader / shader_config->warp_size+1);
-   mf_tot_lat_pw = 0; //total latency summed up per window. divide by mf_num_lat_pw to obtain average latency Per Window
-   mf_total_lat = 0;
-   num_mfs = 0;
-   printf("*** Initializing Memory Statistics ***\n");
-   totalbankreads = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-   totalbankwrites = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-   totalbankaccesses = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-   mf_total_lat_table = (unsigned long long int **) calloc(mem_config->m_n_mem, sizeof(unsigned long long *));
-   mf_max_lat_table = (unsigned **) calloc(mem_config->m_n_mem, sizeof(unsigned *));
-   bankreads = (unsigned int***) calloc(n_shader, sizeof(unsigned int**));
-   bankwrites = (unsigned int***) calloc(n_shader, sizeof(unsigned int**));
-   num_MCBs_accessed = (unsigned int*) calloc(mem_config->m_n_mem*mem_config->nbk, sizeof(unsigned int));
-   if (mem_config->gpgpu_frfcfs_dram_sched_queue_size) {
-      position_of_mrq_chosen = (unsigned int*) calloc(mem_config->gpgpu_frfcfs_dram_sched_queue_size, sizeof(unsigned int));
-   } else
-      position_of_mrq_chosen = (unsigned int*) calloc(1024, sizeof(unsigned int));
-   for (i=0;i<n_shader ;i++ ) {
-      bankreads[i] = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-      bankwrites[i] = (unsigned int**) calloc(mem_config->m_n_mem, sizeof(unsigned int*));
-      for (j=0;j<mem_config->m_n_mem ;j++ ) {
-         bankreads[i][j] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-         bankwrites[i][j] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      }
-   }
-
-   for (i=0;i<mem_config->m_n_mem ;i++ ) {
-      totalbankreads[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      totalbankwrites[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      totalbankaccesses[i] = (unsigned int*) calloc(mem_config->nbk, sizeof(unsigned int));
-      mf_total_lat_table[i] = (unsigned long long int*) calloc(mem_config->nbk, sizeof(unsigned long long int));
-      mf_max_lat_table[i] = (unsigned *) calloc(mem_config->nbk, sizeof(unsigned));
-   }
-
-   mem_access_type_stats = (unsigned ***) malloc(NUM_MEM_ACCESS_TYPE * sizeof(unsigned **));
-   for (i = 0; i < NUM_MEM_ACCESS_TYPE; i++) {
-      int j;
-      mem_access_type_stats[i] = (unsigned **) calloc(mem_config->m_n_mem, sizeof(unsigned*));
-      for (j=0; (unsigned) j< mem_config->m_n_mem; j++) {
-         mem_access_type_stats[i][j] = (unsigned *) calloc((mem_config->nbk+1), sizeof(unsigned*));
-      }
-   }
-
-   // AerialVision L2 stats
-   L2_read_miss = 0;
-   L2_write_miss = 0;
-   L2_read_hit = 0;
-   L2_write_hit = 0;
-
-   L2_cbtoL2length = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
-   L2_cbtoL2writelength = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
-   L2_L2tocblength = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
-   L2_dramtoL2length = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
-   L2_dramtoL2writelength = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
-   L2_L2todramlength = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
+#include "../../libcuda_sim/gpgpu_context.h"
+
+memory_stats_t::memory_stats_t(unsigned n_shader,
+                               const shader_core_config *shader_config,
+                               const memory_config *mem_config,
+                               const class gpgpu_sim *gpu) {
+  assert(mem_config->m_valid);
+  assert(shader_config->m_valid);
+
+  unsigned i, j;
+
+  concurrent_row_access =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+  num_activates =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+  row_access =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+  max_conc_access2samerow =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+  max_servicetime2samerow =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+
+  for (unsigned i = 0; i < mem_config->m_n_mem; i++) {
+    concurrent_row_access[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    row_access[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    num_activates[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    max_conc_access2samerow[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    max_servicetime2samerow[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+  }
+
+  m_n_shader = n_shader;
+  m_memory_config = mem_config;
+  m_gpu = gpu;
+  total_n_access = 0;
+  total_n_reads = 0;
+  total_n_writes = 0;
+  max_mrq_latency = 0;
+  max_dq_latency = 0;
+  max_mf_latency = 0;
+  max_icnt2mem_latency = 0;
+  max_icnt2sh_latency = 0;
+  tot_icnt2mem_latency = 0;
+  tot_icnt2sh_latency = 0;
+  tot_mrq_num = 0;
+  tot_mrq_latency = 0;
+  memset(mrq_lat_table, 0, sizeof(unsigned) * 32);
+  memset(dq_lat_table, 0, sizeof(unsigned) * 32);
+  memset(mf_lat_table, 0, sizeof(unsigned) * 32);
+  memset(icnt2mem_lat_table, 0, sizeof(unsigned) * 24);
+  memset(icnt2sh_lat_table, 0, sizeof(unsigned) * 24);
+  memset(mf_lat_pw_table, 0, sizeof(unsigned) * 32);
+  mf_num_lat_pw = 0;
+  max_warps =
+      n_shader *
+      (shader_config->n_thread_per_shader / shader_config->warp_size + 1);
+  mf_tot_lat_pw = 0;  // total latency summed up per window. divide by
+                      // mf_num_lat_pw to obtain average latency Per Window
+  mf_total_lat = 0;
+  num_mfs = 0;
+  printf("*** Initializing Memory Statistics ***\n");
+  totalbankreads =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+  totalbankwrites =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+  totalbankaccesses =
+      (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+  mf_total_lat_table = (unsigned long long int **)calloc(
+      mem_config->m_n_mem, sizeof(unsigned long long *));
+  mf_max_lat_table =
+      (unsigned **)calloc(mem_config->m_n_mem, sizeof(unsigned *));
+  bankreads = (unsigned int ***)calloc(n_shader, sizeof(unsigned int **));
+  bankwrites = (unsigned int ***)calloc(n_shader, sizeof(unsigned int **));
+  num_MCBs_accessed = (unsigned int *)calloc(
+      mem_config->m_n_mem * mem_config->nbk, sizeof(unsigned int));
+  if (mem_config->gpgpu_frfcfs_dram_sched_queue_size) {
+    position_of_mrq_chosen = (unsigned int *)calloc(
+        mem_config->gpgpu_frfcfs_dram_sched_queue_size, sizeof(unsigned int));
+  } else
+    position_of_mrq_chosen = (unsigned int *)calloc(1024, sizeof(unsigned int));
+  for (i = 0; i < n_shader; i++) {
+    bankreads[i] =
+        (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+    bankwrites[i] =
+        (unsigned int **)calloc(mem_config->m_n_mem, sizeof(unsigned int *));
+    for (j = 0; j < mem_config->m_n_mem; j++) {
+      bankreads[i][j] =
+          (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+      bankwrites[i][j] =
+          (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    }
+  }
+
+  for (i = 0; i < mem_config->m_n_mem; i++) {
+    totalbankreads[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    totalbankwrites[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    totalbankaccesses[i] =
+        (unsigned int *)calloc(mem_config->nbk, sizeof(unsigned int));
+    mf_total_lat_table[i] = (unsigned long long int *)calloc(
+        mem_config->nbk, sizeof(unsigned long long int));
+    mf_max_lat_table[i] = (unsigned *)calloc(mem_config->nbk, sizeof(unsigned));
+  }
+
+  mem_access_type_stats =
+      (unsigned ***)malloc(NUM_MEM_ACCESS_TYPE * sizeof(unsigned **));
+  for (i = 0; i < NUM_MEM_ACCESS_TYPE; i++) {
+    int j;
+    mem_access_type_stats[i] =
+        (unsigned **)calloc(mem_config->m_n_mem, sizeof(unsigned *));
+    for (j = 0; (unsigned)j < mem_config->m_n_mem; j++) {
+      mem_access_type_stats[i][j] =
+          (unsigned *)calloc((mem_config->nbk + 1), sizeof(unsigned *));
+    }
+  }
+
+  // AerialVision L2 stats
+  L2_read_miss = 0;
+  L2_write_miss = 0;
+  L2_read_hit = 0;
+  L2_write_hit = 0;
+
+  L2_cbtoL2length =
+      (unsigned int *)calloc(mem_config->m_n_mem, sizeof(unsigned int));
+  L2_cbtoL2writelength =
+      (unsigned int *)calloc(mem_config->m_n_mem, sizeof(unsigned int));
+  L2_L2tocblength =
+      (unsigned int *)calloc(mem_config->m_n_mem, sizeof(unsigned int));
+  L2_dramtoL2length =
+      (unsigned int *)calloc(mem_config->m_n_mem, sizeof(unsigned int));
+  L2_dramtoL2writelength =
+      (unsigned int *)calloc(mem_config->m_n_mem, sizeof(unsigned int));
+  L2_L2todramlength =
+      (unsigned int *)calloc(mem_config->m_n_mem, sizeof(unsigned int));
 }
 
 // record the total latency
-unsigned memory_stats_t::memlatstat_done(mem_fetch *mf )
-{
-   unsigned mf_latency;
-   mf_latency = (gpu_sim_cycle+gpu_tot_sim_cycle) - mf->get_timestamp();
-   mf_num_lat_pw++;
-   mf_tot_lat_pw += mf_latency;
-   unsigned idx = LOGB2(mf_latency);
-   assert(idx<32);
-   mf_lat_table[idx]++;
-   shader_mem_lat_log(mf->get_sid(), mf_latency);
-   mf_total_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk] += mf_latency;
-   if (mf_latency > max_mf_latency)
-      max_mf_latency = mf_latency;
-   return mf_latency;
+unsigned memory_stats_t::memlatstat_done(mem_fetch *mf) {
+  unsigned mf_latency;
+  mf_latency =
+      (m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle) - mf->get_timestamp();
+  mf_num_lat_pw++;
+  mf_tot_lat_pw += mf_latency;
+  unsigned idx = LOGB2(mf_latency);
+  assert(idx < 32);
+  mf_lat_table[idx]++;
+  shader_mem_lat_log(mf->get_sid(), mf_latency);
+  mf_total_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk] +=
+      mf_latency;
+  if (mf_latency > max_mf_latency) max_mf_latency = mf_latency;
+  return mf_latency;
 }
 
-void memory_stats_t::memlatstat_read_done(mem_fetch *mf)
-{
-   if (m_memory_config->gpgpu_memlatency_stat) {
-      unsigned mf_latency = memlatstat_done(mf);
-      if (mf_latency > mf_max_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk]) 
-         mf_max_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk] = mf_latency;
-      unsigned icnt2sh_latency;
-      icnt2sh_latency = (gpu_tot_sim_cycle+gpu_sim_cycle) - mf->get_return_timestamp();
-      tot_icnt2sh_latency += icnt2sh_latency;
-      icnt2sh_lat_table[LOGB2(icnt2sh_latency)]++;
-      if (icnt2sh_latency > max_icnt2sh_latency)
-         max_icnt2sh_latency = icnt2sh_latency;
-   }
+void memory_stats_t::memlatstat_read_done(mem_fetch *mf) {
+  if (m_memory_config->gpgpu_memlatency_stat) {
+    unsigned mf_latency = memlatstat_done(mf);
+    if (mf_latency >
+        mf_max_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk])
+      mf_max_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk] =
+          mf_latency;
+    unsigned icnt2sh_latency;
+    icnt2sh_latency = (m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle) -
+                      mf->get_return_timestamp();
+    tot_icnt2sh_latency += icnt2sh_latency;
+    icnt2sh_lat_table[LOGB2(icnt2sh_latency)]++;
+    if (icnt2sh_latency > max_icnt2sh_latency)
+      max_icnt2sh_latency = icnt2sh_latency;
+  }
 }
 
-void memory_stats_t::memlatstat_dram_access(mem_fetch *mf)
-{
-   unsigned dram_id = mf->get_tlx_addr().chip;
-   unsigned bank = mf->get_tlx_addr().bk;
-   if (m_memory_config->gpgpu_memlatency_stat) { 
-      if (mf->get_is_write()) {
-         if ( mf->get_sid() < m_n_shader  ) {   //do not count L2_writebacks here 
-            bankwrites[mf->get_sid()][dram_id][bank]++;
-            shader_mem_acc_log( mf->get_sid(), dram_id, bank, 'w');
-         }
-         totalbankwrites[dram_id][bank]++;
-      } else {
-         bankreads[mf->get_sid()][dram_id][bank]++;
-         shader_mem_acc_log( mf->get_sid(), dram_id, bank, 'r');
-         totalbankreads[dram_id][bank]++;
+void memory_stats_t::memlatstat_dram_access(mem_fetch *mf) {
+  unsigned dram_id = mf->get_tlx_addr().chip;
+  unsigned bank = mf->get_tlx_addr().bk;
+  if (m_memory_config->gpgpu_memlatency_stat) {
+    if (mf->get_is_write()) {
+      if (mf->get_sid() < m_n_shader) {  // do not count L2_writebacks here
+        bankwrites[mf->get_sid()][dram_id][bank]++;
+        shader_mem_acc_log(mf->get_sid(), dram_id, bank, 'w');
       }
-      mem_access_type_stats[mf->get_access_type()][dram_id][bank]++;
-   }
-   if (mf->get_pc() != (unsigned)-1) 
-      ptx_file_line_stats_add_dram_traffic(mf->get_pc(), mf->get_data_size());
+      totalbankwrites[dram_id][bank] +=
+          ceil(mf->get_data_size() / m_memory_config->dram_atom_size);
+    } else {
+      bankreads[mf->get_sid()][dram_id][bank]++;
+      shader_mem_acc_log(mf->get_sid(), dram_id, bank, 'r');
+      totalbankreads[dram_id][bank] +=
+          ceil(mf->get_data_size() / m_memory_config->dram_atom_size);
+    }
+    mem_access_type_stats[mf->get_access_type()][dram_id][bank] +=
+        ceil(mf->get_data_size() / m_memory_config->dram_atom_size);
+  }
+
+  if (mf->get_pc() != (unsigned)-1)
+    m_gpu->gpgpu_ctx->stats->ptx_file_line_stats_add_dram_traffic(
+        mf->get_pc(), mf->get_data_size());
 }
 
-void memory_stats_t::memlatstat_icnt2mem_pop(mem_fetch *mf)
-{
-   if (m_memory_config->gpgpu_memlatency_stat) {
-      unsigned icnt2mem_latency;
-      icnt2mem_latency = (gpu_tot_sim_cycle+gpu_sim_cycle) - mf->get_timestamp();
-      tot_icnt2mem_latency += icnt2mem_latency;
-      icnt2mem_lat_table[LOGB2(icnt2mem_latency)]++;
-      if (icnt2mem_latency > max_icnt2mem_latency)
-         max_icnt2mem_latency = icnt2mem_latency;
-   }
+void memory_stats_t::memlatstat_icnt2mem_pop(mem_fetch *mf) {
+  if (m_memory_config->gpgpu_memlatency_stat) {
+    unsigned icnt2mem_latency;
+    icnt2mem_latency =
+        (m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle) - mf->get_timestamp();
+    tot_icnt2mem_latency += icnt2mem_latency;
+    icnt2mem_lat_table[LOGB2(icnt2mem_latency)]++;
+    if (icnt2mem_latency > max_icnt2mem_latency)
+      max_icnt2mem_latency = icnt2mem_latency;
+  }
 }
 
-void memory_stats_t::memlatstat_lat_pw()
-{
-   if (mf_num_lat_pw && m_memory_config->gpgpu_memlatency_stat) {
-      assert(mf_tot_lat_pw);
-      mf_total_lat += mf_tot_lat_pw;
-      num_mfs += mf_num_lat_pw;
-      mf_lat_pw_table[LOGB2(mf_tot_lat_pw/mf_num_lat_pw)]++;
-      mf_tot_lat_pw = 0;
-      mf_num_lat_pw = 0;
-   }
+void memory_stats_t::memlatstat_lat_pw() {
+  if (mf_num_lat_pw && m_memory_config->gpgpu_memlatency_stat) {
+    assert(mf_tot_lat_pw);
+    mf_total_lat += mf_tot_lat_pw;
+    num_mfs += mf_num_lat_pw;
+    mf_lat_pw_table[LOGB2(mf_tot_lat_pw / mf_num_lat_pw)]++;
+    mf_tot_lat_pw = 0;
+    mf_num_lat_pw = 0;
+  }
 }
 
-
-void memory_stats_t::memlatstat_print( unsigned n_mem, unsigned gpu_mem_n_bk )
-{
-   unsigned i,j,k,l,m;
-   unsigned max_bank_accesses, min_bank_accesses, max_chip_accesses, min_chip_accesses;
-
-   if (m_memory_config->gpgpu_memlatency_stat) {
-	   printf("maxmflatency = %d \n", max_mf_latency);
-	   printf("max_icnt2mem_latency = %d \n", max_icnt2mem_latency);
-      printf("maxmrqlatency = %d \n", max_mrq_latency);
-      //printf("maxdqlatency = %d \n", max_dq_latency);
-      printf("max_icnt2sh_latency = %d \n", max_icnt2sh_latency);
-      if (num_mfs) {
-         printf("averagemflatency = %lld \n", mf_total_lat/num_mfs);
-         printf("avg_icnt2mem_latency = %lld \n", tot_icnt2mem_latency/num_mfs);
-         if(tot_mrq_num)
-        	 printf("avg_mrq_latency = %lld \n", tot_mrq_latency/tot_mrq_num);
-
-         printf("avg_icnt2sh_latency = %lld \n", tot_icnt2sh_latency/num_mfs);
-      }
-      printf("mrq_lat_table:");
-      for (i=0; i< 32; i++) {
-         printf("%d \t", mrq_lat_table[i]);
-      }
-      printf("\n");
-      printf("dq_lat_table:");
-      for (i=0; i< 32; i++) {
-         printf("%d \t", dq_lat_table[i]);
+void memory_stats_t::memlatstat_print(unsigned n_mem, unsigned gpu_mem_n_bk) {
+  unsigned i, j, k, l, m;
+  unsigned max_bank_accesses, min_bank_accesses, max_chip_accesses,
+      min_chip_accesses;
+
+  if (m_memory_config->gpgpu_memlatency_stat) {
+    printf("maxmflatency = %d \n", max_mf_latency);
+    printf("max_icnt2mem_latency = %d \n", max_icnt2mem_latency);
+    printf("maxmrqlatency = %d \n", max_mrq_latency);
+    // printf("maxdqlatency = %d \n", max_dq_latency);
+    printf("max_icnt2sh_latency = %d \n", max_icnt2sh_latency);
+    if (num_mfs) {
+      printf("averagemflatency = %lld \n", mf_total_lat / num_mfs);
+      printf("avg_icnt2mem_latency = %lld \n", tot_icnt2mem_latency / num_mfs);
+      if (tot_mrq_num)
+        printf("avg_mrq_latency = %lld \n", tot_mrq_latency / tot_mrq_num);
+
+      printf("avg_icnt2sh_latency = %lld \n", tot_icnt2sh_latency / num_mfs);
+    }
+    printf("mrq_lat_table:");
+    for (i = 0; i < 32; i++) {
+      printf("%d \t", mrq_lat_table[i]);
+    }
+    printf("\n");
+    printf("dq_lat_table:");
+    for (i = 0; i < 32; i++) {
+      printf("%d \t", dq_lat_table[i]);
+    }
+    printf("\n");
+    printf("mf_lat_table:");
+    for (i = 0; i < 32; i++) {
+      printf("%d \t", mf_lat_table[i]);
+    }
+    printf("\n");
+    printf("icnt2mem_lat_table:");
+    for (i = 0; i < 24; i++) {
+      printf("%d \t", icnt2mem_lat_table[i]);
+    }
+    printf("\n");
+    printf("icnt2sh_lat_table:");
+    for (i = 0; i < 24; i++) {
+      printf("%d \t", icnt2sh_lat_table[i]);
+    }
+    printf("\n");
+    printf("mf_lat_pw_table:");
+    for (i = 0; i < 32; i++) {
+      printf("%d \t", mf_lat_pw_table[i]);
+    }
+    printf("\n");
+
+    /*MAXIMUM CONCURRENT ACCESSES TO SAME ROW*/
+    printf("maximum concurrent accesses to same row:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        printf("%9d ", max_conc_access2samerow[i][j]);
       }
       printf("\n");
-      printf("mf_lat_table:");
-      for (i=0; i< 32; i++) {
-         printf("%d \t", mf_lat_table[i]);
+    }
+
+    /*MAXIMUM SERVICE TIME TO SAME ROW*/
+    printf("maximum service time to same row:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        printf("%9d ", max_servicetime2samerow[i][j]);
       }
       printf("\n");
-      printf("icnt2mem_lat_table:");
-      for (i=0; i< 24; i++) {
-         printf("%d \t", icnt2mem_lat_table[i]);
+    }
+
+    /*AVERAGE ROW ACCESSES PER ACTIVATE*/
+    int total_row_accesses = 0;
+    int total_num_activates = 0;
+    printf("average row accesses per activate:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        total_row_accesses += row_access[i][j];
+        total_num_activates += num_activates[i][j];
+        printf("%9f ", (float)row_access[i][j] / num_activates[i][j]);
       }
       printf("\n");
-      printf("icnt2sh_lat_table:");
-      for (i=0; i< 24; i++) {
-         printf("%d \t", icnt2sh_lat_table[i]);
+    }
+    printf("average row locality = %d/%d = %f\n", total_row_accesses,
+           total_num_activates,
+           (float)total_row_accesses / total_num_activates);
+    /*MEMORY ACCESSES*/
+    k = 0;
+    l = 0;
+    m = 0;
+    max_bank_accesses = 0;
+    max_chip_accesses = 0;
+    min_bank_accesses = 0xFFFFFFFF;
+    min_chip_accesses = 0xFFFFFFFF;
+    printf("number of total memory accesses made:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        l = totalbankaccesses[i][j];
+        if (l < min_bank_accesses) min_bank_accesses = l;
+        if (l > max_bank_accesses) max_bank_accesses = l;
+        k += l;
+        m += l;
+        printf("%9d ", l);
       }
-      printf("\n");
-      printf("mf_lat_pw_table:");
-      for (i=0; i< 32; i++) {
-         printf("%d \t", mf_lat_pw_table[i]);
-      }
-      printf("\n");
-
-      /*MAXIMUM CONCURRENT ACCESSES TO SAME ROW*/
-      printf("maximum concurrent accesses to same row:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            printf("%9d ",max_conc_access2samerow[i][j]);
-         }
-         printf("\n");
-      }
-
-      /*MAXIMUM SERVICE TIME TO SAME ROW*/
-      printf("maximum service time to same row:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            printf("%9d ",max_servicetime2samerow[i][j]);
-         }
-         printf("\n");
-      }
-
-      /*AVERAGE ROW ACCESSES PER ACTIVATE*/
-      int total_row_accesses = 0;
-      int total_num_activates = 0;
-      printf("average row accesses per activate:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            total_row_accesses += row_access[i][j];
-            total_num_activates += num_activates[i][j];
-            printf("%9f ",(float) row_access[i][j]/num_activates[i][j]);
-         }
-         printf("\n");
-      }
-      printf("average row locality = %d/%d = %f\n", total_row_accesses, total_num_activates, (float)total_row_accesses/total_num_activates);
-      /*MEMORY ACCESSES*/
-      k = 0;
-      l = 0;
+      if (m < min_chip_accesses) min_chip_accesses = m;
+      if (m > max_chip_accesses) max_chip_accesses = m;
       m = 0;
-      max_bank_accesses = 0;
-      max_chip_accesses = 0;
-      min_bank_accesses = 0xFFFFFFFF;
-      min_chip_accesses = 0xFFFFFFFF;
-      printf("number of total memory accesses made:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            l = totalbankaccesses[i][j];
-            if (l < min_bank_accesses)
-               min_bank_accesses = l;
-            if (l > max_bank_accesses)
-               max_bank_accesses = l;
-            k += l;
-            m += l;
-            printf("%9d ",l);
-         }
-         if (m < min_chip_accesses)
-            min_chip_accesses = m;
-         if (m > max_chip_accesses)
-            max_chip_accesses = m;
-         m = 0;
-         printf("\n");
+      printf("\n");
+    }
+    printf("total accesses: %d\n", k);
+    if (min_bank_accesses)
+      printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses,
+             (float)max_bank_accesses / min_bank_accesses);
+    else
+      printf("min_bank_accesses = 0!\n");
+    if (min_chip_accesses)
+      printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses,
+             (float)max_chip_accesses / min_chip_accesses);
+    else
+      printf("min_chip_accesses = 0!\n");
+
+    /*READ ACCESSES*/
+    k = 0;
+    l = 0;
+    m = 0;
+    max_bank_accesses = 0;
+    max_chip_accesses = 0;
+    min_bank_accesses = 0xFFFFFFFF;
+    min_chip_accesses = 0xFFFFFFFF;
+    printf("number of total read accesses:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        l = totalbankreads[i][j];
+        if (l < min_bank_accesses) min_bank_accesses = l;
+        if (l > max_bank_accesses) max_bank_accesses = l;
+        k += l;
+        m += l;
+        printf("%9d ", l);
       }
-      printf("total accesses: %d\n", k);
-      if (min_bank_accesses)
-         printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses/min_bank_accesses);
-      else
-         printf("min_bank_accesses = 0!\n");
-      if (min_chip_accesses)
-         printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses/min_chip_accesses);
-      else
-         printf("min_chip_accesses = 0!\n");
-
-      /*READ ACCESSES*/
-      k = 0;
-      l = 0;
+      if (m < min_chip_accesses) min_chip_accesses = m;
+      if (m > max_chip_accesses) max_chip_accesses = m;
       m = 0;
-      max_bank_accesses = 0;
-      max_chip_accesses = 0;
-      min_bank_accesses = 0xFFFFFFFF;
-      min_chip_accesses = 0xFFFFFFFF;
-      printf("number of total read accesses:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            l = totalbankreads[i][j];
-            if (l < min_bank_accesses)
-               min_bank_accesses = l;
-            if (l > max_bank_accesses)
-               max_bank_accesses = l;
-            k += l;
-            m += l;
-            printf("%9d ",l);
-         }
-         if (m < min_chip_accesses)
-            min_chip_accesses = m;
-         if (m > max_chip_accesses)
-            max_chip_accesses = m;
-         m = 0;
-         printf("\n");
+      printf("\n");
+    }
+    printf("total dram reads = %d\n", k);
+    if (min_bank_accesses)
+      printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses,
+             (float)max_bank_accesses / min_bank_accesses);
+    else
+      printf("min_bank_accesses = 0!\n");
+    if (min_chip_accesses)
+      printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses,
+             (float)max_chip_accesses / min_chip_accesses);
+    else
+      printf("min_chip_accesses = 0!\n");
+
+    /*WRITE ACCESSES*/
+    k = 0;
+    l = 0;
+    m = 0;
+    max_bank_accesses = 0;
+    max_chip_accesses = 0;
+    min_bank_accesses = 0xFFFFFFFF;
+    min_chip_accesses = 0xFFFFFFFF;
+    printf("number of total write accesses:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        l = totalbankwrites[i][j];
+        if (l < min_bank_accesses) min_bank_accesses = l;
+        if (l > max_bank_accesses) max_bank_accesses = l;
+        k += l;
+        m += l;
+        printf("%9d ", l);
       }
-      printf("total dram reads = %d\n", k);
-      if (min_bank_accesses)
-         printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses/min_bank_accesses);
-      else
-         printf("min_bank_accesses = 0!\n");
-      if (min_chip_accesses)
-         printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses/min_chip_accesses);
-      else
-         printf("min_chip_accesses = 0!\n");
-
-      /*WRITE ACCESSES*/
-      k = 0;
-      l = 0;
+      if (m < min_chip_accesses) min_chip_accesses = m;
+      if (m > max_chip_accesses) max_chip_accesses = m;
       m = 0;
-      max_bank_accesses = 0;
-      max_chip_accesses = 0;
-      min_bank_accesses = 0xFFFFFFFF;
-      min_chip_accesses = 0xFFFFFFFF;
-      printf("number of total write accesses:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            l = totalbankwrites[i][j];
-            if (l < min_bank_accesses)
-               min_bank_accesses = l;
-            if (l > max_bank_accesses)
-               max_bank_accesses = l;
-            k += l;
-            m += l;
-            printf("%9d ",l);
-         }
-         if (m < min_chip_accesses)
-            min_chip_accesses = m;
-         if (m > max_chip_accesses)
-            max_chip_accesses = m;
-         m = 0;
-         printf("\n");
-      }
-      printf("total dram writes = %d\n", k);
-      if (min_bank_accesses)
-         printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses/min_bank_accesses);
-      else
-         printf("min_bank_accesses = 0!\n");
-      if (min_chip_accesses)
-         printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses/min_chip_accesses);
-      else
-         printf("min_chip_accesses = 0!\n");
-
-
-      /*AVERAGE MF LATENCY PER BANK*/
-      printf("average mf latency per bank:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            k = totalbankwrites[i][j] + totalbankreads[i][j];
-            if (k)
-               printf("%10lld", mf_total_lat_table[i][j] / k);
-            else
-               printf("    none  ");
-         }
-         printf("\n");
-      }
-
-      /*MAXIMUM MF LATENCY PER BANK*/
-      printf("maximum mf latency per bank:\n");
-      for (i=0;i<n_mem ;i++ ) {
-         printf("dram[%d]: ", i);
-         for (j=0;j<gpu_mem_n_bk;j++ ) {
-            printf("%10d", mf_max_lat_table[i][j]);
-         }
-         printf("\n");
-      }
-   }
-
-   if (m_memory_config->gpgpu_memlatency_stat & GPU_MEMLATSTAT_MC) {
-      printf("\nNumber of Memory Banks Accessed per Memory Operation per Warp (from 0):\n");
-      unsigned long long accum_MCBs_accessed = 0;
-      unsigned long long tot_mem_ops_per_warp = 0;
-      for (i=0;i< n_mem*gpu_mem_n_bk ; i++ ) {
-         accum_MCBs_accessed += i*num_MCBs_accessed[i];
-         tot_mem_ops_per_warp += num_MCBs_accessed[i];
-         printf("%d\t", num_MCBs_accessed[i]);
+      printf("\n");
+    }
+    printf("total dram writes = %d\n", k);
+    if (min_bank_accesses)
+      printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses,
+             (float)max_bank_accesses / min_bank_accesses);
+    else
+      printf("min_bank_accesses = 0!\n");
+    if (min_chip_accesses)
+      printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses,
+             (float)max_chip_accesses / min_chip_accesses);
+    else
+      printf("min_chip_accesses = 0!\n");
+
+    /*AVERAGE MF LATENCY PER BANK*/
+    printf("average mf latency per bank:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        k = totalbankwrites[i][j] + totalbankreads[i][j];
+        if (k)
+          printf("%10lld", mf_total_lat_table[i][j] / k);
+        else
+          printf("    none  ");
       }
-
-      printf("\nAverage # of Memory Banks Accessed per Memory Operation per Warp=%f\n", (float)accum_MCBs_accessed/tot_mem_ops_per_warp);
-
-      //printf("\nAverage Difference Between First and Last Response from Memory System per warp = ");
-
-
-      printf("\nposition of mrq chosen\n");
-
-      if (!m_memory_config->gpgpu_frfcfs_dram_sched_queue_size)
-         j = 1024;
-      else
-         j = m_memory_config->gpgpu_frfcfs_dram_sched_queue_size;
-      k=0;l=0;
-      for (i=0;i< j; i++ ) {
-         printf("%d\t", position_of_mrq_chosen[i]);
-         k += position_of_mrq_chosen[i];
-         l += i*position_of_mrq_chosen[i];
+      printf("\n");
+    }
+
+    /*MAXIMUM MF LATENCY PER BANK*/
+    printf("maximum mf latency per bank:\n");
+    for (i = 0; i < n_mem; i++) {
+      printf("dram[%d]: ", i);
+      for (j = 0; j < gpu_mem_n_bk; j++) {
+        printf("%10d", mf_max_lat_table[i][j]);
       }
       printf("\n");
-      printf("\naverage position of mrq chosen = %f\n", (float)l/k);
-   }
+    }
+  }
+
+  if (m_memory_config->gpgpu_memlatency_stat & GPU_MEMLATSTAT_MC) {
+    printf(
+        "\nNumber of Memory Banks Accessed per Memory Operation per Warp (from "
+        "0):\n");
+    unsigned long long accum_MCBs_accessed = 0;
+    unsigned long long tot_mem_ops_per_warp = 0;
+    for (i = 0; i < n_mem * gpu_mem_n_bk; i++) {
+      accum_MCBs_accessed += i * num_MCBs_accessed[i];
+      tot_mem_ops_per_warp += num_MCBs_accessed[i];
+      printf("%d\t", num_MCBs_accessed[i]);
+    }
+
+    printf(
+        "\nAverage # of Memory Banks Accessed per Memory Operation per "
+        "Warp=%f\n",
+        (float)accum_MCBs_accessed / tot_mem_ops_per_warp);
+
+    // printf("\nAverage Difference Between First and Last Response from Memory
+    // System per warp = ");
+
+    printf("\nposition of mrq chosen\n");
+
+    if (!m_memory_config->gpgpu_frfcfs_dram_sched_queue_size)
+      j = 1024;
+    else
+      j = m_memory_config->gpgpu_frfcfs_dram_sched_queue_size;
+    k = 0;
+    l = 0;
+    for (i = 0; i < j; i++) {
+      printf("%d\t", position_of_mrq_chosen[i]);
+      k += position_of_mrq_chosen[i];
+      l += i * position_of_mrq_chosen[i];
+    }
+    printf("\n");
+    printf("\naverage position of mrq chosen = %f\n", (float)l / k);
+  }
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.h
index 982b9ae22c..241c8fdea8 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/mem_latency_stat.h
@@ -32,16 +32,17 @@
 #include <zlib.h>
 #include <map>
 
+class memory_config;
 class memory_stats_t {
 public:
-   memory_stats_t( unsigned n_shader, 
-                   const struct shader_core_config *shader_config, 
-                   const struct memory_config *mem_config );
+  memory_stats_t(unsigned n_shader,
+                 const class shader_core_config *shader_config,
+                 const memory_config *mem_config, const class gpgpu_sim *gpu);
 
-   unsigned memlatstat_done( class mem_fetch *mf );
-   void memlatstat_read_done( class mem_fetch *mf );
-   void memlatstat_dram_access( class mem_fetch *mf );
-   void memlatstat_icnt2mem_pop( class mem_fetch *mf);
+  unsigned memlatstat_done(class mem_fetch *mf);
+  void memlatstat_read_done(class mem_fetch *mf);
+  void memlatstat_dram_access(class mem_fetch *mf);
+  void memlatstat_icnt2mem_pop(class mem_fetch *mf);
    void memlatstat_lat_pw();
    void memlatstat_print(unsigned n_mem, unsigned gpu_mem_n_bk);
 
@@ -54,6 +55,7 @@ public:
 
    const struct shader_core_config *m_shader_config;
    const struct memory_config *m_memory_config;
+  const class gpgpu_sim *m_gpu;
 
    unsigned max_mrq_latency;
    unsigned max_dq_latency;
@@ -69,49 +71,57 @@ public:
    unsigned mf_lat_table[32];
    unsigned icnt2mem_lat_table[24];
    unsigned icnt2sh_lat_table[24];
-   unsigned mf_lat_pw_table[32]; //table storing values of mf latency Per Window
-   unsigned mf_num_lat_pw;
-   unsigned max_warps;
-   unsigned mf_tot_lat_pw; //total latency summed up per window. divide by mf_num_lat_pw to obtain average latency Per Window
-   unsigned long long int mf_total_lat;
-   unsigned long long int ** mf_total_lat_table; //mf latency sums[dram chip id][bank id]
-   unsigned ** mf_max_lat_table; //mf latency sums[dram chip id][bank id]
-   unsigned num_mfs;
-   unsigned int ***bankwrites; //bankwrites[shader id][dram chip id][bank id]
-   unsigned int ***bankreads; //bankreads[shader id][dram chip id][bank id]
-   unsigned int **totalbankwrites; //bankwrites[dram chip id][bank id]
-   unsigned int **totalbankreads; //bankreads[dram chip id][bank id]
-   unsigned int **totalbankaccesses; //bankaccesses[dram chip id][bank id]
-   unsigned int *num_MCBs_accessed; //tracks how many memory controllers are accessed whenever any thread in a warp misses in cache
-   unsigned int *position_of_mrq_chosen; //position of mrq in m_queue chosen 
-   
-   unsigned ***mem_access_type_stats; // dram access type classification
+  unsigned mf_lat_pw_table[32];  // table storing values of mf latency Per
+                                 // Window
+  unsigned mf_num_lat_pw;
+  unsigned max_warps;
+  unsigned mf_tot_lat_pw;  // total latency summed up per window. divide by
+                           // mf_num_lat_pw to obtain average latency Per Window
+  unsigned long long int mf_total_lat;
+  unsigned long long int *
+      *mf_total_lat_table;      // mf latency sums[dram chip id][bank id]
+  unsigned **mf_max_lat_table;  // mf latency sums[dram chip id][bank id]
+  unsigned num_mfs;
+  unsigned int ***bankwrites;  // bankwrites[shader id][dram chip id][bank id]
+  unsigned int ***bankreads;   // bankreads[shader id][dram chip id][bank id]
+  unsigned int **totalbankwrites;    // bankwrites[dram chip id][bank id]
+  unsigned int **totalbankreads;     // bankreads[dram chip id][bank id]
+  unsigned int **totalbankaccesses;  // bankaccesses[dram chip id][bank id]
+  unsigned int
+      *num_MCBs_accessed;  // tracks how many memory controllers are accessed
+                           // whenever any thread in a warp misses in cache
+  unsigned int *position_of_mrq_chosen;  // position of mrq in m_queue chosen
 
-   // AerialVision L2 stats
-   unsigned L2_read_miss;
-   unsigned L2_write_miss;
-   unsigned L2_read_hit;
-   unsigned L2_write_hit;
+  unsigned ***mem_access_type_stats;  // dram access type classification
 
-   // L2 cache stats
-   unsigned int *L2_cbtoL2length;
-   unsigned int *L2_cbtoL2writelength;
-   unsigned int *L2_L2tocblength;
-   unsigned int *L2_dramtoL2length;
-   unsigned int *L2_dramtoL2writelength;
-   unsigned int *L2_L2todramlength;
+  // AerialVision L2 stats
+  unsigned L2_read_miss;
+  unsigned L2_write_miss;
+  unsigned L2_read_hit;
+  unsigned L2_write_hit;
 
-   // DRAM access row locality stats 
-   unsigned int **concurrent_row_access; //concurrent_row_access[dram chip id][bank id]
-   unsigned int **num_activates; //num_activates[dram chip id][bank id]
-   unsigned int **row_access; //row_access[dram chip id][bank id]
-   unsigned int **max_conc_access2samerow; //max_conc_access2samerow[dram chip id][bank id]
-   unsigned int **max_servicetime2samerow; //max_servicetime2samerow[dram chip id][bank id]
+  // L2 cache stats
+  unsigned int *L2_cbtoL2length;
+  unsigned int *L2_cbtoL2writelength;
+  unsigned int *L2_L2tocblength;
+  unsigned int *L2_dramtoL2length;
+  unsigned int *L2_dramtoL2writelength;
+  unsigned int *L2_L2todramlength;
 
-   // Power stats
-   unsigned total_n_access;
-   unsigned total_n_reads;
-   unsigned total_n_writes;
+  // DRAM access row locality stats
+  unsigned int *
+      *concurrent_row_access;    // concurrent_row_access[dram chip id][bank id]
+  unsigned int **num_activates;  // num_activates[dram chip id][bank id]
+  unsigned int **row_access;     // row_access[dram chip id][bank id]
+  unsigned int **max_conc_access2samerow;  // max_conc_access2samerow[dram chip
+                                           // id][bank id]
+  unsigned int **max_servicetime2samerow;  // max_servicetime2samerow[dram chip
+                                           // id][bank id]
+
+  // Power stats
+  unsigned total_n_access;
+  unsigned total_n_reads;
+  unsigned total_n_writes;
 };
 
 #endif /*MEM_LATENCY_STAT_H*/
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.cc
index 3861b6a80f..c19c973fad 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.cc
@@ -27,99 +27,495 @@
 
 #include "power_interface.h"
 
-void init_mcpat(const gpgpu_sim_config &config, class gpgpu_sim_wrapper *wrapper, unsigned stat_sample_freq, unsigned tot_inst, unsigned inst){
-
-	wrapper->init_mcpat(config.g_power_config_name, config.g_power_filename, config.g_power_trace_filename,
-	    			config.g_metric_trace_filename,config.g_steady_state_tracking_filename,config.g_power_simulation_enabled,
-	    			config.g_power_trace_enabled,config.g_steady_power_levels_enabled,config.g_power_per_cycle_dump,
-	    			config.gpu_steady_power_deviation,config.gpu_steady_min_period,config.g_power_trace_zlevel,
-	    			tot_inst+inst,stat_sample_freq
-	    			);
 
+void init_mcpat(const gpgpu_sim_config &config,
+                class gpgpu_sim_wrapper *wrapper, unsigned stat_sample_freq,
+                unsigned tot_inst, unsigned inst) {
+  wrapper->init_mcpat(
+      config.g_power_config_name, config.g_power_filename,
+      config.g_power_trace_filename, config.g_metric_trace_filename,
+      config.g_steady_state_tracking_filename,
+      config.g_power_simulation_enabled, config.g_power_trace_enabled,
+      config.g_steady_power_levels_enabled, config.g_power_per_cycle_dump,
+      config.gpu_steady_power_deviation, config.gpu_steady_min_period,
+      config.g_power_trace_zlevel, tot_inst + inst, stat_sample_freq,  
+      config.g_power_simulation_mode, 
+      config.g_dvfs_enabled,
+      config.get_core_freq()/1000000,
+      config.num_shader());
 }
 
-void mcpat_cycle(const gpgpu_sim_config &config, const struct shader_core_config *shdr_config, class gpgpu_sim_wrapper *wrapper, class power_stat_t *power_stats, unsigned stat_sample_freq, unsigned tot_cycle, unsigned cycle, unsigned tot_inst, unsigned inst){
+void mcpat_cycle(const gpgpu_sim_config &config,
+                 const shader_core_config *shdr_config,
+                 class gpgpu_sim_wrapper *wrapper,
+                 class power_stat_t *power_stats, unsigned stat_sample_freq,
+                 unsigned tot_cycle, unsigned cycle, unsigned tot_inst,
+                 unsigned inst, bool dvfs_enabled) {
+  static bool mcpat_init = true;
+
+  if (mcpat_init) {  // If first cycle, don't have any power numbers yet
+    mcpat_init = false;
+    return;
+  }
+
+  if ((tot_cycle + cycle) % stat_sample_freq == 0) {
+    if(dvfs_enabled){
+      wrapper->set_model_voltage(1); //performance model needs to support this.
+    }
+
+    wrapper->set_inst_power(
+        shdr_config->gpgpu_clock_gated_lanes, stat_sample_freq,
+        stat_sample_freq, power_stats->get_total_inst(0),
+        power_stats->get_total_int_inst(0), power_stats->get_total_fp_inst(0),
+        power_stats->get_l1d_read_accesses(0),
+        power_stats->get_l1d_write_accesses(0),
+        power_stats->get_committed_inst(0));
 
-	static bool mcpat_init=true;
+    // Single RF for both int and fp ops
+    wrapper->set_regfile_power(power_stats->get_regfile_reads(0),
+                               power_stats->get_regfile_writes(0),
+                               power_stats->get_non_regfile_operands(0));
 
-	if(mcpat_init){ // If first cycle, don't have any power numbers yet
-		mcpat_init=false;
-		return;
-	}
+    // Instruction cache stats
+    wrapper->set_icache_power(power_stats->get_inst_c_hits(0),
+                              power_stats->get_inst_c_misses(0));
 
-	if ((tot_cycle+cycle) % stat_sample_freq == 0) {
+    // Constant Cache, shared memory, texture cache
+    wrapper->set_ccache_power(power_stats->get_const_accessess(0), 0); //assuming all HITS in constant cache for now
+    wrapper->set_tcache_power(power_stats->get_texture_c_hits(),
+                              power_stats->get_texture_c_misses());
+    wrapper->set_shrd_mem_power(power_stats->get_shmem_access(0));
 
-		wrapper->set_inst_power(shdr_config->gpgpu_clock_gated_lanes,
-				stat_sample_freq, stat_sample_freq,
-				power_stats->get_total_inst(), power_stats->get_total_int_inst(),
-				power_stats->get_total_fp_inst(), power_stats->get_l1d_read_accesses(),
-				power_stats->get_l1d_write_accesses(), power_stats->get_committed_inst());
+    wrapper->set_l1cache_power(
+        power_stats->get_l1d_read_hits(0), power_stats->get_l1d_read_misses(0),
+        power_stats->get_l1d_write_hits(0), power_stats->get_l1d_write_misses(0));
 
-		// Single RF for both int and fp ops
-		wrapper->set_regfile_power(power_stats->get_regfile_reads(), power_stats->get_regfile_writes(), power_stats->get_non_regfile_operands());
+    wrapper->set_l2cache_power(
+        power_stats->get_l2_read_hits(0), power_stats->get_l2_read_misses(0),
+        power_stats->get_l2_write_hits(0), power_stats->get_l2_write_misses(0));
 
-		//Instruction cache stats
-		wrapper->set_icache_power(power_stats->get_inst_c_hits(), power_stats->get_inst_c_misses());
+    float active_sms = (*power_stats->m_active_sms) / stat_sample_freq;
+    float num_cores = shdr_config->num_shader();
+    float num_idle_core = num_cores - active_sms;
+    wrapper->set_num_cores(num_cores);
+    wrapper->set_idle_core_power(num_idle_core);
 
-		//Constant Cache, shared memory, texture cache
-		wrapper->set_ccache_power(power_stats->get_constant_c_hits(), power_stats->get_constant_c_misses());
-		wrapper->set_tcache_power(power_stats->get_texture_c_hits(), power_stats->get_texture_c_misses());
-		wrapper->set_shrd_mem_power(power_stats->get_shmem_read_access());
+    // pipeline power - pipeline_duty_cycle *= percent_active_sms;
+    float pipeline_duty_cycle =
+        ((*power_stats->m_average_pipeline_duty_cycle / (stat_sample_freq)) <
+         0.8)
+            ? ((*power_stats->m_average_pipeline_duty_cycle) / stat_sample_freq)
+            : 0.8;
+    wrapper->set_duty_cycle_power(pipeline_duty_cycle);
 
-		wrapper->set_l1cache_power(power_stats->get_l1d_read_hits(), power_stats->get_l1d_read_misses(),
-				power_stats->get_l1d_write_hits(), power_stats->get_l1d_write_misses());
+    // Memory Controller
+    wrapper->set_mem_ctrl_power(power_stats->get_dram_rd(0),
+                                power_stats->get_dram_wr(0),
+                                power_stats->get_dram_pre(0));
 
+    // Execution pipeline accesses
+    // FPU (SP) accesses, Integer ALU (not present in Tesla), Sfu accesses
 
-		wrapper->set_l2cache_power(power_stats->get_l2_read_hits(), power_stats->get_l2_read_misses(),
-				power_stats->get_l2_write_hits(), power_stats->get_l2_write_misses());
+    wrapper->set_int_accesses(power_stats->get_ialu_accessess(0), 
+                              power_stats->get_intmul24_accessess(0), 
+                              power_stats->get_intmul32_accessess(0), 
+                              power_stats->get_intmul_accessess(0), 
+                              power_stats->get_intdiv_accessess(0));
 
+    wrapper->set_dp_accesses(power_stats->get_dp_accessess(0), 
+                              power_stats->get_dpmul_accessess(0), 
+                              power_stats->get_dpdiv_accessess(0));
 
-		float active_sms=(*power_stats->m_active_sms)/stat_sample_freq;
-		float num_cores = shdr_config->num_shader();
-		float num_idle_core = num_cores - active_sms;
-		wrapper->set_idle_core_power(num_idle_core);
+    wrapper->set_fp_accesses(power_stats->get_fp_accessess(0), 
+                            power_stats->get_fpmul_accessess(0), 
+                            power_stats->get_fpdiv_accessess(0));
 
-		//pipeline power - pipeline_duty_cycle *= percent_active_sms;
-		float pipeline_duty_cycle=((*power_stats->m_average_pipeline_duty_cycle/( stat_sample_freq)) < 0.8)?((*power_stats->m_average_pipeline_duty_cycle)/stat_sample_freq):0.8;
-		wrapper->set_duty_cycle_power(pipeline_duty_cycle);
+    wrapper->set_trans_accesses(power_stats->get_sqrt_accessess(0), 
+                                power_stats->get_log_accessess(0), 
+                                power_stats->get_sin_accessess(0), 
+                                power_stats->get_exp_accessess(0));
 
-		//Memory Controller
-		wrapper->set_mem_ctrl_power(power_stats->get_dram_rd(), power_stats->get_dram_wr(), power_stats->get_dram_pre());
+    wrapper->set_tensor_accesses(power_stats->get_tensor_accessess(0));
 
-		//Execution pipeline accesses
-		//FPU (SP) accesses, Integer ALU (not present in Tesla), Sfu accesses
-		wrapper->set_exec_unit_power(power_stats->get_tot_fpu_accessess(), power_stats->get_ialu_accessess(), power_stats->get_tot_sfu_accessess());
+    wrapper->set_tex_accesses(power_stats->get_tex_accessess(0));
 
-		//Average active lanes for sp and sfu pipelines
-		float avg_sp_active_lanes=(power_stats->get_sp_active_lanes())/stat_sample_freq;
-		float avg_sfu_active_lanes=(power_stats->get_sfu_active_lanes())/stat_sample_freq;
-		assert(avg_sp_active_lanes<=32);
-		assert(avg_sfu_active_lanes<=32);
-		wrapper->set_active_lanes_power((power_stats->get_sp_active_lanes())/stat_sample_freq,
-				(power_stats->get_sfu_active_lanes())/stat_sample_freq);
+    wrapper->set_exec_unit_power(power_stats->get_tot_fpu_accessess(0),
+                                 power_stats->get_ialu_accessess(0),
+                                 power_stats->get_tot_sfu_accessess(0));
 
+    wrapper->set_avg_active_threads(power_stats->get_active_threads(0));
 
-		double n_icnt_simt_to_mem = (double)power_stats->get_icnt_simt_to_mem(); // # flits from SIMT clusters to memory partitions
-		double n_icnt_mem_to_simt = (double)power_stats->get_icnt_mem_to_simt(); // # flits from memory partitions to SIMT clusters
-		wrapper->set_NoC_power(n_icnt_mem_to_simt, n_icnt_simt_to_mem); // Number of flits traversing the interconnect
+    // Average active lanes for sp and sfu pipelines
+    float avg_sp_active_lanes =
+        (power_stats->get_sp_active_lanes()) / stat_sample_freq;
+    float avg_sfu_active_lanes =
+        (power_stats->get_sfu_active_lanes()) / stat_sample_freq;
+    if(avg_sp_active_lanes >32.0 )
+      avg_sp_active_lanes = 32.0;
+    if(avg_sfu_active_lanes >32.0 )
+      avg_sfu_active_lanes = 32.0;
+    assert(avg_sp_active_lanes <= 32);
+    assert(avg_sfu_active_lanes <= 32);
+    wrapper->set_active_lanes_power(avg_sp_active_lanes, avg_sfu_active_lanes);
 
-		wrapper->compute();
+    double n_icnt_simt_to_mem =
+        (double)
+            power_stats->get_icnt_simt_to_mem(0);  // # flits from SIMT clusters
+                                                  // to memory partitions
+    double n_icnt_mem_to_simt =
+        (double)
+            power_stats->get_icnt_mem_to_simt(0);  // # flits from memory
+                                                  // partitions to SIMT clusters
+    wrapper->set_NoC_power(n_icnt_mem_to_simt + n_icnt_simt_to_mem);  // Number of flits traversing the interconnect
 
+    wrapper->compute();
 
-		wrapper->update_components_power();
-		wrapper->print_trace_files();
-		power_stats->save_stats();
+    wrapper->update_components_power();
+    wrapper->print_trace_files();
+    power_stats->save_stats();
 
-		wrapper->detect_print_steady_state(0,tot_inst+inst);
+    wrapper->detect_print_steady_state(0, tot_inst + inst);
 
-		wrapper->power_metrics_calculations();
+    wrapper->power_metrics_calculations();
 
+    wrapper->dump();
+  }
+  // wrapper->close_files();
+}
+
+void mcpat_reset_perf_count(class gpgpu_sim_wrapper *wrapper) {
+  wrapper->reset_counters();
+}
 
-		wrapper->dump();
-	}
-	//wrapper->close_files();
+bool parse_hw_file(char* hwpowerfile, bool find_target_kernel, vector<string> &hw_data, char* benchname, std::string executed_kernelname){
+  fstream hw_file;
+  hw_file.open(hwpowerfile, ios::in);
+  string line, word, temp;
+  while(!hw_file.eof()){
+    hw_data.clear();
+    getline(hw_file, line);
+    stringstream s(line);
+    while (getline(s,word,',')){
+      hw_data.push_back(word);
+    }
+    if(hw_data[HW_BENCH_NAME] == std::string(benchname)){
+      if(find_target_kernel){
+        if(hw_data[HW_KERNEL_NAME] == ""){
+          hw_file.close();
+          return true;
+        }
+        else{
+          if(hw_data[HW_KERNEL_NAME] == executed_kernelname){
+            hw_file.close();
+            return true;
+          }
+        }
+      }
+      else{
+        hw_file.close();
+        return true;
+      }
+    } 
+  }
+  hw_file.close();
+  return false;
 }
 
-void mcpat_reset_perf_count(class gpgpu_sim_wrapper *wrapper){
-	wrapper->reset_counters();
+
+void calculate_hw_mcpat(const gpgpu_sim_config &config,
+                 const shader_core_config *shdr_config,
+                 class gpgpu_sim_wrapper *wrapper,
+                 class power_stat_t *power_stats, unsigned stat_sample_freq,
+                 unsigned tot_cycle, unsigned cycle, unsigned tot_inst,
+                 unsigned inst, int power_simulation_mode, bool dvfs_enabled, char* hwpowerfile, 
+                 char* benchname, std::string executed_kernelname, 
+                 const bool *accelwattch_hybrid_configuration, bool aggregate_power_stats){
+
+  /* Reading HW data from CSV file */
+
+  vector<string> hw_data;
+  bool kernel_found = false;
+  kernel_found = parse_hw_file(hwpowerfile, true, hw_data, benchname, executed_kernelname); //Searching for matching executed_kernelname.
+  if(!kernel_found)
+    kernel_found = parse_hw_file(hwpowerfile, false, hw_data, benchname, executed_kernelname); //Searching for any kernel with same benchname. 
+  assert("Could not find perf stats for the target benchmark in hwpowerfile.\n" && (kernel_found));
+  unsigned perf_cycles = static_cast<unsigned int>(std::stod(hw_data[HW_CYCLES]) + 0.5);
+  if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_CYCLES]))
+    perf_cycles = cycle;
+  wrapper->init_mcpat_hw_mode(perf_cycles); //total PERF MODEL cycles for current kernel
+
+  if(dvfs_enabled){
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_VOLTAGE])) 
+      wrapper->set_model_voltage(1); //performance model needs to support this
+    else  
+      wrapper->set_model_voltage(std::stod(hw_data[HW_VOLTAGE])); //performance model needs to support this
+  }
+
+  double l1_read_hits = std::stod(hw_data[HW_L1_RH]);
+  double l1_read_misses = std::stod(hw_data[HW_L1_RM]);
+  double l1_write_hits = std::stod(hw_data[HW_L1_WH]);
+  double l1_write_misses = std::stod(hw_data[HW_L1_WM]);
+
+  if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L1_RH]))
+    l1_read_hits = power_stats->get_l1d_read_hits(1) - power_stats->l1r_hits_kernel;
+  if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L1_RM]))
+    l1_read_misses = power_stats->get_l1d_read_misses(1) - power_stats->l1r_misses_kernel;
+  if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L1_WH]))
+    l1_write_hits = power_stats->get_l1d_write_hits(1) - power_stats->l1w_hits_kernel;
+  if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L1_WM]))
+    l1_write_misses = power_stats->get_l1d_write_misses(1) - power_stats->l1w_misses_kernel;
+
+    if(aggregate_power_stats){
+      power_stats->tot_inst_execution += power_stats->get_total_inst(1);
+      power_stats->tot_int_inst_execution +=  power_stats->get_total_int_inst(1);
+      power_stats->tot_fp_inst_execution +=  power_stats->get_total_fp_inst(1);
+      power_stats->commited_inst_execution += power_stats->get_committed_inst(1);
+      wrapper->set_inst_power(
+        shdr_config->gpgpu_clock_gated_lanes, cycle, //TODO: core.[0] cycles counts don't matter, remove this
+        cycle, power_stats->tot_inst_execution,
+        power_stats->tot_int_inst_execution, power_stats->tot_fp_inst_execution,
+        l1_read_hits + l1_read_misses,
+        l1_write_hits + l1_write_misses,
+        power_stats->commited_inst_execution);
+    }
+    else{
+    wrapper->set_inst_power(
+        shdr_config->gpgpu_clock_gated_lanes, cycle, //TODO: core.[0] cycles counts don't matter, remove this
+        cycle, power_stats->get_total_inst(1),
+        power_stats->get_total_int_inst(1), power_stats->get_total_fp_inst(1),
+        l1_read_hits + l1_read_misses,
+        l1_write_hits + l1_write_misses,
+        power_stats->get_committed_inst(1));
+    }
+
+    // Single RF for both int and fp ops -- activity factor set to 0 for Accelwattch HW and Accelwattch Hybrid because no HW Perf Stats for register files
+    wrapper->set_regfile_power(power_stats->get_regfile_reads(1),
+                               power_stats->get_regfile_writes(1),
+                               power_stats->get_non_regfile_operands(1));
+
+    // Instruction cache stats -- activity factor set to 0 for Accelwattch HW and Accelwattch Hybrid because no HW Perf Stats for instruction cache
+    wrapper->set_icache_power(power_stats->get_inst_c_hits(1) - power_stats->l1i_hits_kernel,
+                              power_stats->get_inst_c_misses(1) - power_stats->l1i_misses_kernel);
+
+    // Constant Cache, shared memory, texture cache
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_CC_ACC]))
+      wrapper->set_ccache_power(power_stats->get_const_accessess(1) - power_stats->cc_accesses_kernel, 0); //assuming all HITS in constant cache for now
+    else  
+      wrapper->set_ccache_power(std::stod(hw_data[HW_CC_ACC]), 0); //assuming all HITS in constant cache for now
+
+    
+    // wrapper->set_tcache_power(power_stats->get_texture_c_hits(),
+    //                           power_stats->get_texture_c_misses());
+
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_SHRD_ACC]))
+      wrapper->set_shrd_mem_power(power_stats->get_shmem_access(1) - power_stats->shared_accesses_kernel);
+    else  
+      wrapper->set_shrd_mem_power(std::stod(hw_data[HW_SHRD_ACC]));
+
+    wrapper->set_l1cache_power( l1_read_hits,  l1_read_misses, l1_write_hits,  l1_write_misses);
+
+    double l2_read_hits = std::stod(hw_data[HW_L2_RH]);
+    double l2_read_misses = std::stod(hw_data[HW_L2_RM]);
+    double l2_write_hits = std::stod(hw_data[HW_L2_WH]);
+    double l2_write_misses = std::stod(hw_data[HW_L2_WM]);
+
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L2_RH]))
+      l2_read_hits = power_stats->get_l2_read_hits(1) - power_stats->l2r_hits_kernel;
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L2_RM]))
+      l2_read_misses = power_stats->get_l2_read_misses(1)  - power_stats->l2r_misses_kernel;
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L2_WH]))
+      l2_write_hits = power_stats->get_l2_write_hits(1) - power_stats->l2w_hits_kernel;
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_L2_WM]))
+      l2_write_misses = power_stats->get_l2_write_misses(1) - power_stats->l2w_misses_kernel;
+
+    wrapper->set_l2cache_power(l2_read_hits, l2_read_misses, l2_write_hits, l2_write_misses);
+    
+    float active_sms = (*power_stats->m_active_sms) / stat_sample_freq;
+    float num_cores = shdr_config->num_shader();
+    float num_idle_core = num_cores - active_sms;
+    wrapper->set_num_cores(num_cores);
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_NUM_SM_IDLE]))
+      wrapper->set_idle_core_power(num_idle_core);
+    else 
+      wrapper->set_idle_core_power(std::stod(hw_data[HW_NUM_SM_IDLE])); 
+
+    float pipeline_duty_cycle =
+        ((*power_stats->m_average_pipeline_duty_cycle / (stat_sample_freq)) <
+         0.8)
+            ? ((*power_stats->m_average_pipeline_duty_cycle) / stat_sample_freq)
+            : 0.8;
+    
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_PIPE_DUTY]))
+      wrapper->set_duty_cycle_power(pipeline_duty_cycle);
+    else
+      wrapper->set_duty_cycle_power(std::stod(hw_data[HW_PIPE_DUTY]));
+
+    // Memory Controller
+  
+    double dram_reads = std::stod(hw_data[HW_DRAM_RD]);
+    double dram_writes = std::stod(hw_data[HW_DRAM_WR]);
+    double dram_pre = 0;
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_DRAM_RD]))
+      dram_reads = power_stats->get_dram_rd(1) - power_stats->dram_rd_kernel;
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_DRAM_WR]))
+      dram_writes = power_stats->get_dram_wr(1) - power_stats->dram_wr_kernel;
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_DRAM_RD]))
+      dram_pre = power_stats->get_dram_pre(1) - power_stats->dram_pre_kernel;
+
+
+    wrapper->set_mem_ctrl_power(dram_reads, dram_writes, dram_pre);
+
+    if(aggregate_power_stats){
+      power_stats->ialu_acc_execution += power_stats->get_ialu_accessess(1);
+      power_stats->imul24_acc_execution += power_stats->get_intmul24_accessess(1);
+      power_stats->imul32_acc_execution += power_stats->get_intmul32_accessess(1);
+      power_stats->imul_acc_execution += power_stats->get_intmul_accessess(1);
+      power_stats->idiv_acc_execution += power_stats->get_intdiv_accessess(1);
+      power_stats->dp_acc_execution += power_stats->get_dp_accessess(1);
+      power_stats->dpmul_acc_execution += power_stats->get_dpmul_accessess(1);
+      power_stats->dpdiv_acc_execution += power_stats->get_dpdiv_accessess(1);
+      power_stats->fp_acc_execution += power_stats->get_fp_accessess(1);
+      power_stats->fpmul_acc_execution += power_stats->get_fpmul_accessess(1);
+      power_stats->fpdiv_acc_execution += power_stats->get_fpdiv_accessess(1);
+      power_stats->sqrt_acc_execution += power_stats->get_sqrt_accessess(1);
+      power_stats->log_acc_execution += power_stats->get_log_accessess(1);
+      power_stats->sin_acc_execution += power_stats->get_sin_accessess(1);
+      power_stats->exp_acc_execution += power_stats->get_exp_accessess(1);
+      power_stats->tensor_acc_execution += power_stats->get_tensor_accessess(1);
+      power_stats->tex_acc_execution += power_stats->get_tex_accessess(1);
+      power_stats->tot_fpu_acc_execution += power_stats->get_tot_fpu_accessess(1);
+      power_stats->tot_sfu_acc_execution += power_stats->get_tot_sfu_accessess(1);
+      power_stats->tot_threads_acc_execution += power_stats->get_tot_threads_kernel(1);
+      power_stats->tot_warps_acc_execution += power_stats->get_tot_warps_kernel(1);
+      
+      power_stats->sp_active_lanes_execution += (power_stats->get_sp_active_lanes() * shdr_config->num_shader() * shdr_config->gpgpu_num_sp_units);
+      power_stats->sfu_active_lanes_execution += (power_stats->get_sfu_active_lanes() * shdr_config->num_shader() * shdr_config->gpgpu_num_sp_units);
+
+      wrapper->set_int_accesses(power_stats->ialu_acc_execution, 
+                                power_stats->imul24_acc_execution, 
+                                power_stats->imul32_acc_execution, 
+                                power_stats->imul_acc_execution, 
+                                power_stats->idiv_acc_execution);
+
+      wrapper->set_dp_accesses(power_stats->dp_acc_execution, 
+                                power_stats->dpmul_acc_execution, 
+                                power_stats->dpdiv_acc_execution);
+
+      wrapper->set_fp_accesses(power_stats->fp_acc_execution, 
+                              power_stats->fpmul_acc_execution, 
+                              power_stats->fpdiv_acc_execution);
+
+      wrapper->set_trans_accesses(power_stats->sqrt_acc_execution, 
+                                  power_stats->log_acc_execution, 
+                                  power_stats->sin_acc_execution, 
+                                  power_stats->exp_acc_execution);
+
+      wrapper->set_tensor_accesses(power_stats->tensor_acc_execution);
+
+      wrapper->set_tex_accesses(power_stats->tex_acc_execution);
+
+      wrapper->set_exec_unit_power(power_stats->ialu_acc_execution,
+                                   power_stats->tot_fpu_acc_execution,
+                                   power_stats->tot_sfu_acc_execution);
+
+      wrapper->set_avg_active_threads((double)((double)power_stats->tot_threads_acc_execution / (double)power_stats->tot_warps_acc_execution));
+
+      // Average active lanes for sp and sfu pipelines
+      float avg_sp_active_lanes =
+          (power_stats->sp_active_lanes_execution) / shdr_config->num_shader() / shdr_config->gpgpu_num_sp_units / stat_sample_freq;
+      float avg_sfu_active_lanes =
+          (power_stats->sfu_active_lanes_execution) / shdr_config->num_shader() / shdr_config->gpgpu_num_sp_units / stat_sample_freq;
+      if(avg_sp_active_lanes >32.0 )
+        avg_sp_active_lanes = 32.0;
+      if(avg_sfu_active_lanes >32.0 )
+        avg_sfu_active_lanes = 32.0;
+      assert(avg_sp_active_lanes <= 32);
+      assert(avg_sfu_active_lanes <= 32);
+      wrapper->set_active_lanes_power(avg_sp_active_lanes, avg_sfu_active_lanes);
+    }
+    else{
+      wrapper->set_int_accesses(power_stats->get_ialu_accessess(1), 
+                                power_stats->get_intmul24_accessess(1), 
+                                power_stats->get_intmul32_accessess(1), 
+                                power_stats->get_intmul_accessess(1), 
+                                power_stats->get_intdiv_accessess(1));
+
+      wrapper->set_dp_accesses(power_stats->get_dp_accessess(1), 
+                                power_stats->get_dpmul_accessess(1), 
+                                power_stats->get_dpdiv_accessess(1));
+
+      wrapper->set_fp_accesses(power_stats->get_fp_accessess(1), 
+                              power_stats->get_fpmul_accessess(1), 
+                              power_stats->get_fpdiv_accessess(1));
+
+      wrapper->set_trans_accesses(power_stats->get_sqrt_accessess(1), 
+                                  power_stats->get_log_accessess(1), 
+                                  power_stats->get_sin_accessess(1), 
+                                  power_stats->get_exp_accessess(1));
+
+      wrapper->set_tensor_accesses(power_stats->get_tensor_accessess(1));
+
+      wrapper->set_tex_accesses(power_stats->get_tex_accessess(1));
+
+      wrapper->set_exec_unit_power(power_stats->get_tot_fpu_accessess(1),
+                                   power_stats->get_ialu_accessess(1),
+                                   power_stats->get_tot_sfu_accessess(1));
+
+      wrapper->set_avg_active_threads(power_stats->get_active_threads(1));
+
+      // Average active lanes for sp and sfu pipelines
+      float avg_sp_active_lanes =
+          (power_stats->get_sp_active_lanes()) / stat_sample_freq;
+      float avg_sfu_active_lanes =
+          (power_stats->get_sfu_active_lanes()) / stat_sample_freq;
+      if(avg_sp_active_lanes >32.0 )
+        avg_sp_active_lanes = 32.0;
+      if(avg_sfu_active_lanes >32.0 )
+        avg_sfu_active_lanes = 32.0;
+      assert(avg_sp_active_lanes <= 32);
+      assert(avg_sfu_active_lanes <= 32);
+      wrapper->set_active_lanes_power(avg_sp_active_lanes, avg_sfu_active_lanes);
+    }
+
+  
+    double n_icnt_simt_to_mem =
+      (double)
+          (power_stats->get_icnt_simt_to_mem(1) - power_stats->noc_tr_kernel);  // # flits from SIMT clusters
+                                                // to memory partitions
+    double n_icnt_mem_to_simt =
+      (double)
+          (power_stats->get_icnt_mem_to_simt(1)- power_stats->noc_rc_kernel);  // # flits from memory
+                                                // partitions to SIMT clusters
+    if((power_simulation_mode == 2) && (accelwattch_hybrid_configuration[HW_NOC]))   
+      wrapper->set_NoC_power(n_icnt_mem_to_simt + n_icnt_simt_to_mem);  // Number of flits traversing the interconnect from Accel-Sim
+    else
+      wrapper->set_NoC_power(std::stod(hw_data[HW_NOC]));  // Number of flits traversing the interconnect from HW
+   
+    wrapper->compute();
+
+    wrapper->update_components_power();
+
+    wrapper->power_metrics_calculations();
+
+    wrapper->dump();
+    power_stats->l1r_hits_kernel = power_stats->get_l1d_read_hits(1);
+    power_stats->l1r_misses_kernel = power_stats->get_l1d_read_misses(1);
+    power_stats->l1w_hits_kernel = power_stats->get_l1d_write_hits(1);
+    power_stats->l1w_misses_kernel = power_stats->get_l1d_write_misses(1);
+    power_stats->shared_accesses_kernel = power_stats->get_const_accessess(1);
+    power_stats->cc_accesses_kernel = power_stats->get_shmem_access(1);
+    power_stats->dram_rd_kernel = power_stats->get_dram_rd(1);
+    power_stats->dram_wr_kernel = power_stats->get_dram_wr(1);
+    power_stats->dram_pre_kernel = power_stats->get_dram_pre(1);
+    power_stats->l1i_hits_kernel = power_stats->get_inst_c_hits(1);
+    power_stats->l1i_misses_kernel = power_stats->get_inst_c_misses(1);
+    power_stats->l2r_hits_kernel = power_stats->get_l2_read_hits(1);
+    power_stats->l2r_misses_kernel = power_stats->get_l2_read_misses(1);
+    power_stats->l2w_hits_kernel =  power_stats->get_l2_write_hits(1); 
+    power_stats->l2w_misses_kernel = power_stats->get_l2_write_misses(1);
+    power_stats->noc_tr_kernel = power_stats->get_icnt_simt_to_mem(1);
+    power_stats->noc_rc_kernel =  power_stats->get_icnt_mem_to_simt(1);
+
+
+    power_stats->clear();
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.h
index afac22b836..1f0a923d84 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_interface.h
@@ -32,12 +32,29 @@
 #include "power_stat.h"
 #include "shader.h"
 
-
 #include "gpgpu_sim_wrapper.h"
 
-void init_mcpat(const gpgpu_sim_config &config, class gpgpu_sim_wrapper *wrapper, unsigned stat_sample_freq, unsigned tot_inst, unsigned inst);
-void mcpat_cycle(const gpgpu_sim_config &config, const struct shader_core_config *shdr_config, class gpgpu_sim_wrapper *wrapper, class power_stat_t *power_stats,
-        unsigned stat_sample_freq, unsigned tot_cycle, unsigned cycle, unsigned tot_inst, unsigned inst);
+void init_mcpat(const gpgpu_sim_config &config,
+                class gpgpu_sim_wrapper *wrapper, unsigned stat_sample_freq,
+                unsigned tot_inst, unsigned inst);
+void mcpat_cycle(const gpgpu_sim_config &config,
+                 const shader_core_config *shdr_config,
+                 class gpgpu_sim_wrapper *wrapper,
+                 class power_stat_t *power_stats, unsigned stat_sample_freq,
+                 unsigned tot_cycle, unsigned cycle, unsigned tot_inst,
+                 unsigned inst, bool dvfs_enabled);
+
+void calculate_hw_mcpat(const gpgpu_sim_config &config,
+                 const shader_core_config *shdr_config,
+                 class gpgpu_sim_wrapper *wrapper,
+                 class power_stat_t *power_stats, unsigned stat_sample_freq,
+                 unsigned tot_cycle, unsigned cycle, unsigned tot_inst,
+                 unsigned inst, int power_simulation_mode, bool dvfs_enabled, 
+                 char* hwpowerfile, char* benchname, std::string executed_kernelname, 
+                 const bool *accelwattch_hybrid_configuration, bool aggregate_power_stats);
+
+bool parse_hw_file(char* hwpowerfile, bool find_target_kernel, vector<string> &hw_data, char* benchname, std::string executed_kernelname);
+
 void mcpat_reset_perf_count(class gpgpu_sim_wrapper *wrapper);
 
 #endif /* POWER_INTERFACE_H_ */
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.cc
index 4c995e94ce..573c17dcac 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.cc
@@ -25,128 +25,184 @@
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-#include "../abstract_hardware_model.h"
 #include "power_stat.h"
-#include "gpu-sim.h"
+#include "../abstract_hardware_model.h"
+#include "../cuda-sim/ptx-stats.h"
+#include "dram.h"
 #include "gpu-misc.h"
-#include "shader.h"
+#include "gpu-sim.h"
 #include "mem_fetch.h"
+#include "shader.h"
 #include "stat-tool.h"
-#include "../cuda-sim/ptx-stats.h"
 #include "visualizer.h"
-#include "dram.h"
 
+#include <stdio.h>
 #include <string.h>
 #include <stdlib.h>
-#include <stdio.h>
 
 
+power_mem_stat_t::power_mem_stat_t(const memory_config *mem_config,
+                                   const shader_core_config *shdr_config,
+                                   memory_stats_t *mem_stats,
+                                   shader_core_stats *shdr_stats) {
+  assert(mem_config->m_valid);
+  m_mem_stats = mem_stats;
+  m_config = mem_config;
+  m_core_stats = shdr_stats;
+  m_core_config = shdr_config;
 
-power_mem_stat_t::power_mem_stat_t(const struct memory_config *mem_config, const struct shader_core_config *shdr_config, memory_stats_t *mem_stats, shader_core_stats *shdr_stats){
-	   assert( mem_config->m_valid );
-	   m_mem_stats = mem_stats;
-	   m_config = mem_config;
-	   m_core_stats = shdr_stats;
-	   m_core_config = shdr_config;
-
-	   init();
+  init();
 }
 
-void power_mem_stat_t::init(){
-
-    shmem_read_access[CURRENT_STAT_IDX] = m_core_stats->gpgpu_n_shmem_bank_access; 	// Shared memory access
-    shmem_read_access[PREV_STAT_IDX] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
-
-    for(unsigned i=0; i<NUM_STAT_IDX; ++i){
-        core_cache_stats[i].clear();
-        l2_cache_stats[i].clear();
-
-        n_cmd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_activity[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_nop[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_act[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_pre[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_rd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_wr[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_req[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-
-        // Interconnect stats
-        n_mem_to_simt[i] = (long *)calloc(m_core_config->n_simt_clusters,sizeof(long)); // Counted at SM
-        n_simt_to_mem[i] = (long *)calloc(m_core_config->n_simt_clusters,sizeof(long)); // Counted at SM
+void power_stat_t::clear(){
+  for(unsigned i=0; i< NUM_STAT_IDX; ++i){
+    pwr_mem_stat->core_cache_stats[i].clear();
+    pwr_mem_stat->l2_cache_stats[i].clear();
+    for(unsigned j=0; j<m_config->num_shader(); ++j){
+      pwr_core_stat->m_pipeline_duty_cycle[i][j]=0;                
+      pwr_core_stat->m_num_decoded_insn[i][j]=0;
+      pwr_core_stat->m_num_FPdecoded_insn[i][j]=0;
+      pwr_core_stat->m_num_INTdecoded_insn[i][j]=0;
+      pwr_core_stat->m_num_storequeued_insn[i][j]=0;
+      pwr_core_stat->m_num_loadqueued_insn[i][j]=0;
+      pwr_core_stat->m_num_tex_inst[i][j]=0;
+      pwr_core_stat->m_num_ialu_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_fp_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_imul_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_imul24_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_imul32_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_fpmul_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_idiv_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_fpdiv_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_dp_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_dpmul_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_dpdiv_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_tensor_core_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_const_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_tex_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_sp_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_sfu_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_sqrt_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_log_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_sin_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_exp_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_mem_acesses[i][j]=0;                   
+      pwr_core_stat->m_num_sp_committed[i][j]=0;
+      pwr_core_stat->m_num_sfu_committed[i][j]=0;
+      pwr_core_stat->m_num_mem_committed[i][j]=0;
+      pwr_core_stat->m_read_regfile_acesses[i][j]=0;
+      pwr_core_stat->m_write_regfile_acesses[i][j]=0;
+      pwr_core_stat->m_non_rf_operands[i][j]=0;
+      pwr_core_stat->m_active_sp_lanes[i][j]=0;
+      pwr_core_stat->m_active_sfu_lanes[i][j]=0;
+      pwr_core_stat->m_active_exu_threads[i][j]=0;                   
+      pwr_core_stat->m_active_exu_warps[i][j]=0;
+    }
+    for (unsigned j = 0; j < m_mem_config->m_n_mem; ++j) {
+      pwr_mem_stat->n_rd[i][j]=0;
+      pwr_mem_stat->n_wr[i][j]=0;
+      pwr_mem_stat->n_pre[i][j]=0;
     }
+  }
 }
 
-void power_mem_stat_t::save_stats(){
 
-    core_cache_stats[PREV_STAT_IDX] = core_cache_stats[CURRENT_STAT_IDX];
-    l2_cache_stats[PREV_STAT_IDX] = l2_cache_stats[CURRENT_STAT_IDX];
 
-    for(unsigned i=0; i<m_core_config->num_shader(); ++i){
-        shmem_read_access[PREV_STAT_IDX][i] = shmem_read_access[CURRENT_STAT_IDX][i] ; 	// Shared memory access
-    }
-
-    for(unsigned i=0; i<m_config->m_n_mem; ++i){
-        n_cmd[PREV_STAT_IDX][i] = n_cmd[CURRENT_STAT_IDX][i];
-        n_activity[PREV_STAT_IDX][i] = n_activity[CURRENT_STAT_IDX][i];
-        n_nop[PREV_STAT_IDX][i] = n_nop[CURRENT_STAT_IDX][i];
-        n_act[PREV_STAT_IDX][i] = n_act[CURRENT_STAT_IDX][i];
-        n_pre[PREV_STAT_IDX][i] = n_pre[CURRENT_STAT_IDX][i];
-        n_rd[PREV_STAT_IDX][i] = n_rd[CURRENT_STAT_IDX][i];
-        n_wr[PREV_STAT_IDX][i] = n_wr[CURRENT_STAT_IDX][i];
-        n_req[PREV_STAT_IDX][i] = n_req[CURRENT_STAT_IDX][i];
-    }
-
-    for(unsigned i=0; i<m_core_config->n_simt_clusters;i++){
-        n_simt_to_mem[PREV_STAT_IDX][i] = n_simt_to_mem[CURRENT_STAT_IDX][i]; // Interconnect
-        n_mem_to_simt[PREV_STAT_IDX][i] = n_mem_to_simt[CURRENT_STAT_IDX][i]; // Interconnect
-    }
+void power_mem_stat_t::init() {
+  shmem_access[CURRENT_STAT_IDX] =
+      m_core_stats->gpgpu_n_shmem_bank_access;  // Shared memory access
+  shmem_access[PREV_STAT_IDX] =
+      (unsigned *)calloc(m_core_config->num_shader(), sizeof(unsigned));
+
+  for (unsigned i = 0; i < NUM_STAT_IDX; ++i) {
+    core_cache_stats[i].clear();
+    l2_cache_stats[i].clear();
+
+    n_cmd[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_activity[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_nop[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_act[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_pre[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_rd[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_wr[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_wr_WB[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+    n_req[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
+
+    // Interconnect stats
+    n_mem_to_simt[i] = (long *)calloc(m_core_config->n_simt_clusters,
+                                      sizeof(long));  // Counted at SM
+    n_simt_to_mem[i] = (long *)calloc(m_core_config->n_simt_clusters,
+                                      sizeof(long));  // Counted at SM
+  }
 }
 
-void power_mem_stat_t::visualizer_print( gzFile power_visualizer_file ){
-
+void power_mem_stat_t::save_stats() {
+  core_cache_stats[PREV_STAT_IDX] = core_cache_stats[CURRENT_STAT_IDX];
+  l2_cache_stats[PREV_STAT_IDX] = l2_cache_stats[CURRENT_STAT_IDX];
+
+  for (unsigned i = 0; i < m_core_config->num_shader(); ++i) {
+    shmem_access[PREV_STAT_IDX][i] =
+        shmem_access[CURRENT_STAT_IDX][i];  // Shared memory access
+  }
+
+  for (unsigned i = 0; i < m_config->m_n_mem; ++i) {
+    n_cmd[PREV_STAT_IDX][i] = n_cmd[CURRENT_STAT_IDX][i];
+    n_activity[PREV_STAT_IDX][i] = n_activity[CURRENT_STAT_IDX][i];
+    n_nop[PREV_STAT_IDX][i] = n_nop[CURRENT_STAT_IDX][i];
+    n_act[PREV_STAT_IDX][i] = n_act[CURRENT_STAT_IDX][i];
+    n_pre[PREV_STAT_IDX][i] = n_pre[CURRENT_STAT_IDX][i];
+    n_rd[PREV_STAT_IDX][i] = n_rd[CURRENT_STAT_IDX][i];
+    n_wr[PREV_STAT_IDX][i] = n_wr[CURRENT_STAT_IDX][i];
+    n_wr_WB[PREV_STAT_IDX][i] = n_wr_WB[CURRENT_STAT_IDX][i];
+    n_req[PREV_STAT_IDX][i] = n_req[CURRENT_STAT_IDX][i];
+  }
+
+  for (unsigned i = 0; i < m_core_config->n_simt_clusters; i++) {
+    n_simt_to_mem[PREV_STAT_IDX][i] =
+        n_simt_to_mem[CURRENT_STAT_IDX][i];  // Interconnect
+    n_mem_to_simt[PREV_STAT_IDX][i] =
+        n_mem_to_simt[CURRENT_STAT_IDX][i];  // Interconnect
+  }
 }
 
-void power_mem_stat_t::print (FILE *fout) const {
-	fprintf(fout, "\n\n==========Power Metrics -- Memory==========\n");
-    unsigned total_mem_reads=0;
-    unsigned total_mem_writes=0;
-    for(unsigned i=0; i<m_config->m_n_mem; ++i){
-        total_mem_reads += n_rd[CURRENT_STAT_IDX][i];
-        total_mem_writes += n_wr[CURRENT_STAT_IDX][i];
-    }
-    fprintf(fout, "Total memory controller accesses: %u\n", total_mem_reads+total_mem_writes);
-    fprintf(fout, "Total memory controller reads: %u\n", total_mem_reads);
-    fprintf(fout, "Total memory controller writes: %u\n", total_mem_writes);
-
-    fprintf(fout, "Core cache stats:\n");
-    core_cache_stats->print_stats(fout);
-    fprintf(fout, "L2 cache stats:\n");
-    l2_cache_stats->print_stats(fout);
+void power_mem_stat_t::visualizer_print(gzFile power_visualizer_file) {}
+
+void power_mem_stat_t::print(FILE *fout) const {
+  fprintf(fout, "\n\n==========Power Metrics -- Memory==========\n");
+  unsigned total_mem_reads = 0;
+  unsigned total_mem_writes = 0;
+  for (unsigned i = 0; i < m_config->m_n_mem; ++i) {
+    total_mem_reads += n_rd[CURRENT_STAT_IDX][i];
+    total_mem_writes += n_wr[CURRENT_STAT_IDX][i] + n_wr_WB[CURRENT_STAT_IDX][i];
+  }
+  fprintf(fout, "Total memory controller accesses: %u\n",
+          total_mem_reads + total_mem_writes);
+  fprintf(fout, "Total memory controller reads: %u\n", total_mem_reads);
+  fprintf(fout, "Total memory controller writes: %u\n", total_mem_writes);
+
+  fprintf(fout, "Core cache stats:\n");
+  core_cache_stats->print_stats(fout);
+  fprintf(fout, "L2 cache stats:\n");
+  l2_cache_stats->print_stats(fout);
 }
 
+power_core_stat_t::power_core_stat_t(const shader_core_config *shader_config,
+                                     shader_core_stats *core_stats) {
+  assert(shader_config->m_valid);
+  m_config = shader_config;
+  shader_core_power_stats_pod *pod = this;
+  memset(pod, 0, sizeof(shader_core_power_stats_pod));
+  m_core_stats = core_stats;
 
-power_core_stat_t::power_core_stat_t( const struct shader_core_config *shader_config, shader_core_stats *core_stats )
-{
-     	assert( shader_config->m_valid );
-        m_config = shader_config;
-        shader_core_power_stats_pod *pod = this;
-        memset(pod,0,sizeof(shader_core_power_stats_pod));
-        m_core_stats=core_stats;
-
-        init();
-
+  init();
 }
 
-void power_core_stat_t::visualizer_print( gzFile visualizer_file )
-{
+void power_core_stat_t::visualizer_print(gzFile visualizer_file) {}
 
-}
-
-void power_core_stat_t::print (FILE *fout)
-{
-	// per core statistics
-    fprintf(fout,"Power Metrics: \n");
-    for(unsigned i=0; i<m_config->num_shader();i++){
+void power_core_stat_t::print(FILE *fout) {
+  // per core statistics
+  fprintf(fout, "Power Metrics: \n");
+  for (unsigned i = 0; i < m_config->num_shader(); i++) {
         fprintf(fout,"core %u:\n",i);
         fprintf(fout,"\tpipeline duty cycle =%f\n",m_pipeline_duty_cycle[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal Deocded Instructions=%u\n",m_num_decoded_insn[CURRENT_STAT_IDX][i]);
@@ -154,28 +210,36 @@ void power_core_stat_t::print (FILE *fout)
         fprintf(fout,"\tTotal INT Deocded Instructions=%u\n",m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal LOAD Queued Instructions=%u\n",m_num_loadqueued_insn[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal STORE Queued Instructions=%u\n",m_num_storequeued_insn[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IALU Acesses=%u\n",m_num_ialu_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal FP Acesses=%u\n",m_num_fp_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IMUL Acesses=%u\n",m_num_imul_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IMUL24 Acesses=%u\n",m_num_imul24_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IMUL32 Acesses=%u\n",m_num_imul32_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IDIV Acesses=%u\n",m_num_idiv_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal FPMUL Acesses=%u\n",m_num_fpmul_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_trans_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal FPDIV Acesses=%u\n",m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_sfu_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SP Acesses=%u\n",m_num_sp_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal MEM Acesses=%u\n",m_num_mem_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal IALU Acesses=%f\n",m_num_ialu_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal FP Acesses=%f\n",m_num_fp_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal DP Acesses=%f\n",m_num_dp_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal IMUL Acesses=%f\n",m_num_imul_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal IMUL24 Acesses=%f\n",m_num_imul24_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal IMUL32 Acesses=%f\n",m_num_imul32_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal IDIV Acesses=%f\n",m_num_idiv_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal FPMUL Acesses=%f\n",m_num_fpmul_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal DPMUL Acesses=%f\n",m_num_dpmul_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal SQRT Acesses=%f\n",m_num_sqrt_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal LOG Acesses=%f\n",m_num_log_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal SIN Acesses=%f\n",m_num_sin_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal EXP Acesses=%f\n",m_num_exp_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal FPDIV Acesses=%f\n",m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal DPDIV Acesses=%f\n",m_num_dpdiv_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal TENSOR Acesses=%f\n",m_num_tensor_core_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal CONST Acesses=%f\n",m_num_const_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal TEX Acesses=%f\n",m_num_tex_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal SFU Acesses=%f\n",m_num_sfu_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal SP Acesses=%f\n",m_num_sp_acesses[CURRENT_STAT_IDX][i]);
+        fprintf(fout,"\tTotal MEM Acesses=%f\n",m_num_mem_acesses[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal SFU Commissions=%u\n",m_num_sfu_committed[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal SP Commissions=%u\n",m_num_sp_committed[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal MEM Commissions=%u\n",m_num_mem_committed[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal REG Reads=%u\n",m_read_regfile_acesses[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal REG Writes=%u\n",m_write_regfile_acesses[CURRENT_STAT_IDX][i]);
         fprintf(fout,"\tTotal NON REG=%u\n",m_non_rf_operands[CURRENT_STAT_IDX][i]);
-    }
+  }
 }
-void power_core_stat_t::init()
-{
+void power_core_stat_t::init() {
     m_pipeline_duty_cycle[CURRENT_STAT_IDX]=m_core_stats->m_pipeline_duty_cycle;
     m_num_decoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_decoded_insn;
     m_num_FPdecoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_FPdecoded_insn;
@@ -190,9 +254,18 @@ void power_core_stat_t::init()
     m_num_fpmul_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fpmul_acesses;
     m_num_idiv_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_idiv_acesses;
     m_num_fpdiv_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fpdiv_acesses;
+    m_num_dp_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_dp_acesses;
+    m_num_dpmul_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_dpmul_acesses;
+    m_num_dpdiv_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_dpdiv_acesses;
     m_num_sp_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sp_acesses;
     m_num_sfu_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sfu_acesses;
-    m_num_trans_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_trans_acesses;
+    m_num_sqrt_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sqrt_acesses;
+    m_num_log_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_log_acesses;
+    m_num_sin_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sin_acesses;
+    m_num_exp_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_exp_acesses;
+    m_num_tensor_core_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_tensor_core_acesses;
+    m_num_const_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_const_acesses;
+    m_num_tex_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_tex_acesses;
     m_num_mem_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_mem_acesses;
     m_num_sp_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_sp_committed;
     m_num_sfu_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_sfu_committed;
@@ -202,28 +275,39 @@ void power_core_stat_t::init()
     m_non_rf_operands[CURRENT_STAT_IDX]=m_core_stats->m_non_rf_operands;
     m_active_sp_lanes[CURRENT_STAT_IDX]=m_core_stats->m_active_sp_lanes;
     m_active_sfu_lanes[CURRENT_STAT_IDX]=m_core_stats->m_active_sfu_lanes;
+    m_active_exu_threads[CURRENT_STAT_IDX]=m_core_stats->m_active_exu_threads;
+    m_active_exu_warps[CURRENT_STAT_IDX]=m_core_stats->m_active_exu_warps;
     m_num_tex_inst[CURRENT_STAT_IDX]=m_core_stats->m_num_tex_inst;
 
-
     m_pipeline_duty_cycle[PREV_STAT_IDX]=(float*)calloc(m_config->num_shader(),sizeof(float));
     m_num_decoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_num_FPdecoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_num_INTdecoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_num_storequeued_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_num_loadqueued_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_ialu_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_fp_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_num_tex_inst[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_imul_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_imul24_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_imul32_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_fpmul_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_idiv_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_fpdiv_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_sp_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_sfu_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_trans_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_mem_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+
+    m_num_ialu_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_fp_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_imul_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_imul24_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_imul32_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_fpmul_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_idiv_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_fpdiv_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_dp_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_dpmul_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_dpdiv_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_tensor_core_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_const_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_tex_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_sp_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_sfu_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_sqrt_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_log_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_sin_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_exp_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_num_mem_acesses[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
     m_num_sp_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_num_sfu_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_num_mem_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
@@ -232,12 +316,16 @@ void power_core_stat_t::init()
     m_non_rf_operands[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_active_sp_lanes[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
     m_active_sfu_lanes[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_active_exu_threads[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+    m_active_exu_warps[PREV_STAT_IDX]=(double *)calloc(m_config->num_shader(),sizeof(double));
+
+
 }
 
-void power_core_stat_t::save_stats(){
-for(unsigned i=0; i<m_config->num_shader(); ++i){
+void power_core_stat_t::save_stats() {
+  for (unsigned i = 0; i < m_config->num_shader(); ++i) {
     m_pipeline_duty_cycle[PREV_STAT_IDX][i]=m_pipeline_duty_cycle[CURRENT_STAT_IDX][i];
-    m_num_decoded_insn[PREV_STAT_IDX][i]=	m_num_decoded_insn[CURRENT_STAT_IDX][i];
+    m_num_decoded_insn[PREV_STAT_IDX][i]= m_num_decoded_insn[CURRENT_STAT_IDX][i];
     m_num_FPdecoded_insn[PREV_STAT_IDX][i]=m_num_FPdecoded_insn[CURRENT_STAT_IDX][i];
     m_num_INTdecoded_insn[PREV_STAT_IDX][i]=m_num_INTdecoded_insn[CURRENT_STAT_IDX][i];
     m_num_storequeued_insn[PREV_STAT_IDX][i]=m_num_storequeued_insn[CURRENT_STAT_IDX][i];
@@ -253,7 +341,16 @@ for(unsigned i=0; i<m_config->num_shader(); ++i){
     m_num_fpdiv_acesses[PREV_STAT_IDX][i]=m_num_fpdiv_acesses[CURRENT_STAT_IDX][i];
     m_num_sp_acesses[PREV_STAT_IDX][i]=m_num_sp_acesses[CURRENT_STAT_IDX][i];
     m_num_sfu_acesses[PREV_STAT_IDX][i]=m_num_sfu_acesses[CURRENT_STAT_IDX][i];
-    m_num_trans_acesses[PREV_STAT_IDX][i]=m_num_trans_acesses[CURRENT_STAT_IDX][i];
+    m_num_sqrt_acesses[PREV_STAT_IDX][i]=m_num_sqrt_acesses[CURRENT_STAT_IDX][i];
+    m_num_log_acesses[PREV_STAT_IDX][i]=m_num_log_acesses[CURRENT_STAT_IDX][i];
+    m_num_sin_acesses[PREV_STAT_IDX][i]=m_num_sin_acesses[CURRENT_STAT_IDX][i];
+    m_num_exp_acesses[PREV_STAT_IDX][i]=m_num_exp_acesses[CURRENT_STAT_IDX][i];
+    m_num_dp_acesses[PREV_STAT_IDX][i]=m_num_dp_acesses[CURRENT_STAT_IDX][i];
+    m_num_dpmul_acesses[PREV_STAT_IDX][i]=m_num_dpmul_acesses[CURRENT_STAT_IDX][i];
+    m_num_dpdiv_acesses[PREV_STAT_IDX][i]=m_num_dpdiv_acesses[CURRENT_STAT_IDX][i];
+    m_num_tensor_core_acesses[PREV_STAT_IDX][i]=m_num_tensor_core_acesses[CURRENT_STAT_IDX][i];
+    m_num_const_acesses[PREV_STAT_IDX][i]=m_num_const_acesses[CURRENT_STAT_IDX][i];
+    m_num_tex_acesses[PREV_STAT_IDX][i]=m_num_tex_acesses[CURRENT_STAT_IDX][i];
     m_num_mem_acesses[PREV_STAT_IDX][i]=m_num_mem_acesses[CURRENT_STAT_IDX][i];
     m_num_sp_committed[PREV_STAT_IDX][i]=m_num_sp_committed[CURRENT_STAT_IDX][i];
     m_num_sfu_committed[PREV_STAT_IDX][i]=m_num_sfu_committed[CURRENT_STAT_IDX][i];
@@ -263,31 +360,80 @@ for(unsigned i=0; i<m_config->num_shader(); ++i){
     m_non_rf_operands[PREV_STAT_IDX][i]=m_non_rf_operands[CURRENT_STAT_IDX][i];
     m_active_sp_lanes[PREV_STAT_IDX][i]=m_active_sp_lanes[CURRENT_STAT_IDX][i];
     m_active_sfu_lanes[PREV_STAT_IDX][i]=m_active_sfu_lanes[CURRENT_STAT_IDX][i];
-    }
+    m_active_exu_threads[PREV_STAT_IDX][i]=m_active_exu_threads[CURRENT_STAT_IDX][i];
+    m_active_exu_warps[PREV_STAT_IDX][i]=m_active_exu_warps[CURRENT_STAT_IDX][i];
+  }
 }
 
-power_stat_t::power_stat_t( const struct shader_core_config *shader_config,float * average_pipeline_duty_cycle,float *active_sms,shader_core_stats * shader_stats, const struct memory_config *mem_config,memory_stats_t * memory_stats)
-{
-	assert( shader_config->m_valid );
-	assert( mem_config->m_valid );
-	pwr_core_stat= new power_core_stat_t(shader_config,shader_stats);
-	pwr_mem_stat= new power_mem_stat_t(mem_config,shader_config, memory_stats, shader_stats);
-	m_average_pipeline_duty_cycle=average_pipeline_duty_cycle;
-	m_active_sms=active_sms;
-	m_config = shader_config;
-	m_mem_config = mem_config;
+power_stat_t::power_stat_t(const shader_core_config *shader_config,
+                           float *average_pipeline_duty_cycle,
+                           float *active_sms, shader_core_stats *shader_stats,
+                           const memory_config *mem_config,
+                           memory_stats_t *memory_stats) {
+  assert(shader_config->m_valid);
+  assert(mem_config->m_valid);
+  pwr_core_stat = new power_core_stat_t(shader_config, shader_stats);
+  pwr_mem_stat = new power_mem_stat_t(mem_config, shader_config, memory_stats,
+                                      shader_stats);
+  m_average_pipeline_duty_cycle = average_pipeline_duty_cycle;
+  m_active_sms = active_sms;
+  m_config = shader_config;
+  m_mem_config = mem_config;
+  l1r_hits_kernel = 0;
+  l1r_misses_kernel = 0;
+  l1w_hits_kernel = 0;
+  l1w_misses_kernel = 0;
+  shared_accesses_kernel = 0;
+  cc_accesses_kernel = 0;
+  dram_rd_kernel = 0;
+  dram_wr_kernel = 0;
+  dram_pre_kernel = 0;
+  l1i_hits_kernel =0;
+  l1i_misses_kernel =0;
+  l2r_hits_kernel =0;
+  l2r_misses_kernel =0;
+  l2w_hits_kernel =0;
+  l2w_misses_kernel =0;
+  noc_tr_kernel = 0;
+  noc_rc_kernel = 0;
+
+  tot_inst_execution = 0;
+  tot_int_inst_execution = 0;
+  tot_fp_inst_execution = 0;
+  commited_inst_execution = 0;
+  ialu_acc_execution = 0;
+  imul24_acc_execution = 0;
+  imul32_acc_execution = 0;
+  imul_acc_execution = 0;
+  idiv_acc_execution = 0;
+  dp_acc_execution = 0;
+  dpmul_acc_execution = 0;
+  dpdiv_acc_execution = 0;
+  fp_acc_execution = 0;
+  fpmul_acc_execution = 0;
+  fpdiv_acc_execution = 0;
+  sqrt_acc_execution = 0;
+  log_acc_execution = 0;
+  sin_acc_execution = 0;
+  exp_acc_execution = 0;
+  tensor_acc_execution = 0;
+  tex_acc_execution = 0;
+  tot_fpu_acc_execution = 0;
+  tot_sfu_acc_execution = 0;
+  tot_threads_acc_execution = 0;
+  tot_warps_acc_execution = 0;
+  sp_active_lanes_execution = 0;
+  sfu_active_lanes_execution = 0;
 }
 
-void power_stat_t::visualizer_print( gzFile visualizer_file )
-{
-	pwr_core_stat->visualizer_print(visualizer_file);
-	pwr_mem_stat->visualizer_print(visualizer_file);
+void power_stat_t::visualizer_print(gzFile visualizer_file) {
+  pwr_core_stat->visualizer_print(visualizer_file);
+  pwr_mem_stat->visualizer_print(visualizer_file);
 }
 
-void power_stat_t::print (FILE *fout) const
-{
-	fprintf(fout,"average_pipeline_duty_cycle=%f\n",*m_average_pipeline_duty_cycle);
-	pwr_core_stat->print(fout);
-	pwr_mem_stat->print(fout);
+void power_stat_t::print(FILE *fout) const {
+  fprintf(fout, "average_pipeline_duty_cycle=%f\n",
+          *m_average_pipeline_duty_cycle);
+  pwr_core_stat->print(fout);
+  pwr_mem_stat->print(fout);
 }
-
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.h
index 20af2e58e9..394c5b3e95 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/power_stat.h
@@ -30,599 +30,1147 @@
 
 #include <stdio.h>
 #include <zlib.h>
-#include "mem_latency_stat.h"
 #include "gpu-sim.h"
+#include "mem_latency_stat.h"
 
-typedef enum _stat_idx{
-    CURRENT_STAT_IDX = 0,    // Current activity count
-    PREV_STAT_IDX,           // Previous sample activity count
-    NUM_STAT_IDX     // Total number of samples
-}stat_idx;
-
+typedef enum _stat_idx {
+  CURRENT_STAT_IDX = 0,  // Current activity count
+  PREV_STAT_IDX,         // Previous sample activity count
+  NUM_STAT_IDX           // Total number of samples
+} stat_idx;
 
 struct shader_core_power_stats_pod {
-    // [CURRENT_STAT_IDX] = CURRENT_STAT_IDX stat, [PREV_STAT_IDX] = last reading
-    float *m_pipeline_duty_cycle[NUM_STAT_IDX];
-    unsigned *m_num_decoded_insn[NUM_STAT_IDX]; // number of instructions committed by this shader core
-    unsigned *m_num_FPdecoded_insn[NUM_STAT_IDX]; // number of instructions committed by this shader core
-    unsigned *m_num_INTdecoded_insn[NUM_STAT_IDX]; // number of instructions committed by this shader core
+  // [CURRENT_STAT_IDX] = CURRENT_STAT_IDX stat, [PREV_STAT_IDX] = last reading
+  float *m_pipeline_duty_cycle[NUM_STAT_IDX];
+  unsigned *m_num_decoded_insn[NUM_STAT_IDX];  // number of instructions
+                                               // committed by this shader core
+  unsigned
+      *m_num_FPdecoded_insn[NUM_STAT_IDX];  // number of instructions committed
+                                            // by this shader core
+  unsigned
+      *m_num_INTdecoded_insn[NUM_STAT_IDX];  // number of instructions committed
+                                             // by this shader core
     unsigned *m_num_storequeued_insn[NUM_STAT_IDX];
     unsigned *m_num_loadqueued_insn[NUM_STAT_IDX];
-    unsigned *m_num_ialu_acesses[NUM_STAT_IDX];
-    unsigned *m_num_fp_acesses[NUM_STAT_IDX];
     unsigned *m_num_tex_inst[NUM_STAT_IDX];
-    unsigned *m_num_imul_acesses[NUM_STAT_IDX];
-    unsigned *m_num_imul32_acesses[NUM_STAT_IDX];
-    unsigned *m_num_imul24_acesses[NUM_STAT_IDX];
-    unsigned *m_num_fpmul_acesses[NUM_STAT_IDX];
-    unsigned *m_num_idiv_acesses[NUM_STAT_IDX];
-    unsigned *m_num_fpdiv_acesses[NUM_STAT_IDX];
-    unsigned *m_num_sp_acesses[NUM_STAT_IDX];
-    unsigned *m_num_sfu_acesses[NUM_STAT_IDX];
-    unsigned *m_num_trans_acesses[NUM_STAT_IDX];
-    unsigned *m_num_mem_acesses[NUM_STAT_IDX];
+    double *m_num_ialu_acesses[NUM_STAT_IDX];
+    double *m_num_fp_acesses[NUM_STAT_IDX];
+    double *m_num_imul_acesses[NUM_STAT_IDX];
+    double *m_num_imul32_acesses[NUM_STAT_IDX];
+    double *m_num_imul24_acesses[NUM_STAT_IDX];
+    double *m_num_fpmul_acesses[NUM_STAT_IDX];
+    double *m_num_idiv_acesses[NUM_STAT_IDX];
+    double *m_num_fpdiv_acesses[NUM_STAT_IDX];
+    double *m_num_dp_acesses[NUM_STAT_IDX];
+    double *m_num_dpmul_acesses[NUM_STAT_IDX];
+    double *m_num_dpdiv_acesses[NUM_STAT_IDX];
+    double *m_num_sp_acesses[NUM_STAT_IDX];
+    double *m_num_sfu_acesses[NUM_STAT_IDX];
+    double *m_num_sqrt_acesses[NUM_STAT_IDX];
+    double *m_num_log_acesses[NUM_STAT_IDX];
+    double *m_num_sin_acesses[NUM_STAT_IDX];
+    double *m_num_exp_acesses[NUM_STAT_IDX];
+    double *m_num_tensor_core_acesses[NUM_STAT_IDX];
+    double *m_num_const_acesses[NUM_STAT_IDX];
+    double *m_num_tex_acesses[NUM_STAT_IDX];
+    double *m_num_mem_acesses[NUM_STAT_IDX];
     unsigned *m_num_sp_committed[NUM_STAT_IDX];
     unsigned *m_num_sfu_committed[NUM_STAT_IDX];
     unsigned *m_num_mem_committed[NUM_STAT_IDX];
     unsigned *m_active_sp_lanes[NUM_STAT_IDX];
     unsigned *m_active_sfu_lanes[NUM_STAT_IDX];
+    double *m_active_exu_threads[NUM_STAT_IDX];
+    double *m_active_exu_warps[NUM_STAT_IDX];    
     unsigned *m_read_regfile_acesses[NUM_STAT_IDX];
     unsigned *m_write_regfile_acesses[NUM_STAT_IDX];
     unsigned *m_non_rf_operands[NUM_STAT_IDX];
 };
 
 class power_core_stat_t : public shader_core_power_stats_pod {
-public:
-   power_core_stat_t(const struct shader_core_config *shader_config, shader_core_stats *core_stats);
-   void visualizer_print( gzFile visualizer_file );
-   void print (FILE *fout);
-   void init();
-   void save_stats();
-
-private:
-   shader_core_stats * m_core_stats;
-   const shader_core_config *m_config;
-   float average_duty_cycle;
-
-
+ public:
+  power_core_stat_t(const shader_core_config *shader_config,
+                    shader_core_stats *core_stats);
+  void visualizer_print(gzFile visualizer_file);
+  void print(FILE *fout);
+  void init();
+  void save_stats();
+ 
+
+ private:
+  shader_core_stats *m_core_stats;
+  const shader_core_config *m_config;
+  float average_duty_cycle;
 };
 
-struct mem_power_stats_pod{
-    // [CURRENT_STAT_IDX] = CURRENT_STAT_IDX stat, [PREV_STAT_IDX] = last reading
-    class cache_stats core_cache_stats[NUM_STAT_IDX]; // Total core stats
-    class cache_stats l2_cache_stats[NUM_STAT_IDX]; // Total L2 partition stats
-
-    unsigned *shmem_read_access[NUM_STAT_IDX];   // Shared memory access
-
-    // Low level DRAM stats
-    unsigned *n_cmd[NUM_STAT_IDX];
-    unsigned *n_activity[NUM_STAT_IDX];
-    unsigned *n_nop[NUM_STAT_IDX];
-    unsigned *n_act[NUM_STAT_IDX];
-    unsigned *n_pre[NUM_STAT_IDX];
-    unsigned *n_rd[NUM_STAT_IDX];
-    unsigned *n_wr[NUM_STAT_IDX];
-    unsigned *n_req[NUM_STAT_IDX];
-
-    // Interconnect stats
-    long *n_simt_to_mem[NUM_STAT_IDX];
-    long *n_mem_to_simt[NUM_STAT_IDX];
+struct mem_power_stats_pod {
+  // [CURRENT_STAT_IDX] = CURRENT_STAT_IDX stat, [PREV_STAT_IDX] = last reading
+  class cache_stats core_cache_stats[NUM_STAT_IDX];  // Total core stats
+  class cache_stats l2_cache_stats[NUM_STAT_IDX];    // Total L2 partition stats
+
+  unsigned *shmem_access[NUM_STAT_IDX];  // Shared memory access
+  // Low level DRAM stats
+  unsigned *n_cmd[NUM_STAT_IDX];
+  unsigned *n_activity[NUM_STAT_IDX];
+  unsigned *n_nop[NUM_STAT_IDX];
+  unsigned *n_act[NUM_STAT_IDX];
+  unsigned *n_pre[NUM_STAT_IDX];
+  unsigned *n_rd[NUM_STAT_IDX];
+  unsigned *n_wr[NUM_STAT_IDX];
+  unsigned *n_wr_WB[NUM_STAT_IDX];
+  unsigned *n_req[NUM_STAT_IDX];
+
+  // Interconnect stats
+  long *n_simt_to_mem[NUM_STAT_IDX];
+  long *n_mem_to_simt[NUM_STAT_IDX];
 };
 
-
-
-class power_mem_stat_t : public mem_power_stats_pod{
-public:
-   power_mem_stat_t(const struct memory_config *mem_config, const struct shader_core_config *shdr_config, memory_stats_t *mem_stats, shader_core_stats *shdr_stats);
-   void visualizer_print( gzFile visualizer_file );
-   void print (FILE *fout) const;
-   void init();
-   void save_stats();
-private:
-   memory_stats_t *m_mem_stats;
-   shader_core_stats * m_core_stats;
-   const memory_config *m_config;
-   const shader_core_config *m_core_config;
+class power_mem_stat_t : public mem_power_stats_pod {
+ public:
+  power_mem_stat_t(const memory_config *mem_config,
+                   const shader_core_config *shdr_config,
+                   memory_stats_t *mem_stats, shader_core_stats *shdr_stats);
+  void visualizer_print(gzFile visualizer_file);
+  void print(FILE *fout) const;
+  void init();
+  void save_stats();
+
+ private:
+  memory_stats_t *m_mem_stats;
+  shader_core_stats *m_core_stats;
+  const memory_config *m_config;
+  const shader_core_config *m_core_config;
 };
 
-
 class power_stat_t {
-public:
-   power_stat_t( const struct shader_core_config *shader_config,float * average_pipeline_duty_cycle,float * active_sms,shader_core_stats * shader_stats, const struct memory_config *mem_config,memory_stats_t * memory_stats);
-   void visualizer_print( gzFile visualizer_file );
-   void print (FILE *fout) const;
-   void save_stats(){
-	   pwr_core_stat->save_stats();
-	   pwr_mem_stat->save_stats();
-	   *m_average_pipeline_duty_cycle=0;
-	   *m_active_sms=0;
-   }
-
-    unsigned get_total_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_decoded_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_decoded_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_int_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_INTdecoded_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_fp_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_FPdecoded_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_FPdecoded_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_load_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_loadqueued_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_loadqueued_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_store_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_storequeued_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_storequeued_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_sp_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sp_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sp_committed[PREV_STAT_IDX][i]);
+ public:
+  power_stat_t(const shader_core_config *shader_config,
+               float *average_pipeline_duty_cycle, float *active_sms,
+               shader_core_stats *shader_stats, const memory_config *mem_config,
+               memory_stats_t *memory_stats);
+  void visualizer_print(gzFile visualizer_file);
+  void print(FILE *fout) const;
+  void save_stats() {
+    pwr_core_stat->save_stats();
+    pwr_mem_stat->save_stats();
+    *m_average_pipeline_duty_cycle = 0;
+    *m_active_sms = 0;
+  }
+  void clear();
+  unsigned l1i_misses_kernel;
+  unsigned l1i_hits_kernel;
+  unsigned long long l1r_hits_kernel;
+  unsigned long long l1r_misses_kernel;
+  unsigned long long l1w_hits_kernel;
+  unsigned long long l1w_misses_kernel;
+  unsigned long long shared_accesses_kernel;
+  unsigned long long cc_accesses_kernel;
+  unsigned long long dram_rd_kernel;
+  unsigned long long dram_wr_kernel;
+  unsigned long long dram_pre_kernel;
+  unsigned long long l2r_hits_kernel;
+  unsigned long long l2r_misses_kernel;
+  unsigned long long l2w_hits_kernel;
+  unsigned long long l2w_misses_kernel;
+  unsigned long long noc_tr_kernel;
+  unsigned long long noc_rc_kernel;
+  unsigned long long tot_inst_execution;
+  unsigned long long tot_int_inst_execution;
+  unsigned long long tot_fp_inst_execution;
+  unsigned long long commited_inst_execution;
+  unsigned long long ialu_acc_execution;
+  unsigned long long imul24_acc_execution;
+  unsigned long long imul32_acc_execution;
+  unsigned long long imul_acc_execution;
+  unsigned long long idiv_acc_execution;
+  unsigned long long dp_acc_execution;
+  unsigned long long dpmul_acc_execution;
+  unsigned long long dpdiv_acc_execution;
+  unsigned long long fp_acc_execution;
+  unsigned long long fpmul_acc_execution;
+  unsigned long long fpdiv_acc_execution;
+  unsigned long long sqrt_acc_execution;
+  unsigned long long log_acc_execution;
+  unsigned long long sin_acc_execution;
+  unsigned long long exp_acc_execution;
+  unsigned long long tensor_acc_execution;
+  unsigned long long tex_acc_execution;
+  unsigned long long tot_fpu_acc_execution;
+  unsigned long long tot_sfu_acc_execution;
+  unsigned long long tot_threads_acc_execution;
+  unsigned long long tot_warps_acc_execution;
+  unsigned long long sp_active_lanes_execution;
+  unsigned long long sfu_active_lanes_execution;
+  double get_total_inst(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_decoded_insn[CURRENT_STAT_IDX][i]);
+      else
+        total_inst += (pwr_core_stat->m_num_decoded_insn[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_decoded_insn[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_total_int_inst(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+          total_inst +=
+          (pwr_core_stat->m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]);
+      else 
+        total_inst +=
+          (pwr_core_stat->m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]) -
+          (pwr_core_stat->m_num_INTdecoded_insn[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_total_fp_inst(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_FPdecoded_insn[CURRENT_STAT_IDX][i]);
+      else 
+        total_inst += (pwr_core_stat->m_num_FPdecoded_insn[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_FPdecoded_insn[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_total_load_inst() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst +=
+          (pwr_core_stat->m_num_loadqueued_insn[CURRENT_STAT_IDX][i]) -
+          (pwr_core_stat->m_num_loadqueued_insn[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_total_store_inst() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst +=
+          (pwr_core_stat->m_num_storequeued_insn[CURRENT_STAT_IDX][i]) -
+          (pwr_core_stat->m_num_storequeued_insn[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_sp_committed_inst() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_num_sp_committed[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_sp_committed[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_sfu_committed_inst() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_num_sfu_committed[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_sfu_committed[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_mem_committed_inst() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_num_mem_committed[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_mem_committed[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_committed_inst(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_mem_committed[CURRENT_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_sfu_committed[CURRENT_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_sp_committed[CURRENT_STAT_IDX][i]);
+      else
+        total_inst += (pwr_core_stat->m_num_mem_committed[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_mem_committed[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_sfu_committed[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_sfu_committed[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_sp_committed[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_sp_committed[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_regfile_reads(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+         total_inst +=
+          (pwr_core_stat->m_read_regfile_acesses[CURRENT_STAT_IDX][i]);
+      else
+        total_inst +=
+          (pwr_core_stat->m_read_regfile_acesses[CURRENT_STAT_IDX][i]) -
+          (pwr_core_stat->m_read_regfile_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+  double get_regfile_writes(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst +=
+          (pwr_core_stat->m_write_regfile_acesses[CURRENT_STAT_IDX][i]);
+      else
+        total_inst +=
+          (pwr_core_stat->m_write_regfile_acesses[CURRENT_STAT_IDX][i]) -
+          (pwr_core_stat->m_write_regfile_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  float get_pipeline_duty() {
+    float total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst +=
+          (pwr_core_stat->m_pipeline_duty_cycle[CURRENT_STAT_IDX][i]) -
+          (pwr_core_stat->m_pipeline_duty_cycle[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_non_regfile_operands(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+         total_inst += (pwr_core_stat->m_non_rf_operands[CURRENT_STAT_IDX][i]);
+      else
+        total_inst += (pwr_core_stat->m_non_rf_operands[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_non_rf_operands[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_sp_accessess() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_num_sp_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_sp_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_sfu_accessess() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_num_sfu_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_sfu_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_sqrt_accessess(bool aggregate_stat){
+      double total_inst=0;
+      for(unsigned i=0; i<m_config->num_shader();i++){
+          if(aggregate_stat)
+            total_inst+=(pwr_core_stat->m_num_sqrt_acesses[CURRENT_STAT_IDX][i]);
+          else
+            total_inst+=(pwr_core_stat->m_num_sqrt_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sqrt_acesses[PREV_STAT_IDX][i]);
+      }
+      return total_inst;
+  }
+  double get_log_accessess(bool aggregate_stat){
+      double total_inst=0;
+      for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)
+          total_inst+=(pwr_core_stat->m_num_log_acesses[CURRENT_STAT_IDX][i]);
+        else 
+          total_inst+=(pwr_core_stat->m_num_log_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_log_acesses[PREV_STAT_IDX][i]);
+      }
+      return total_inst;
+  }
+  double get_sin_accessess(bool aggregate_stat){
+      double total_inst=0;
+      for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)  
+          total_inst+=(pwr_core_stat->m_num_sin_acesses[CURRENT_STAT_IDX][i]);
+        else 
+          total_inst+=(pwr_core_stat->m_num_sin_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sin_acesses[PREV_STAT_IDX][i]);
+      }
+      return total_inst;
+  }
+  double get_exp_accessess(bool aggregate_stat){
+      double total_inst=0;
+      for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)  
+          total_inst+=(pwr_core_stat->m_num_exp_acesses[CURRENT_STAT_IDX][i]);
+        else  
+          total_inst+=(pwr_core_stat->m_num_exp_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_exp_acesses[PREV_STAT_IDX][i]);
+      }
+      return total_inst;
+  }
+
+  double get_mem_accessess() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_num_mem_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_mem_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_intdiv_accessess(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i]);
+      else
+        total_inst += (pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_idiv_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_fpdiv_accessess(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_fpdiv_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_intmul32_accessess(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_intmul24_accessess(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_intmul_accessess(bool aggregate_stat){
+      double total_inst=0;
+      for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)
+          total_inst+= (pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i]); 
+        else  
+          total_inst+= (pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i]) - 
+                       (pwr_core_stat->m_num_imul_acesses[PREV_STAT_IDX][i]);
+      }
+      return total_inst;
+  }
+
+  double get_fpmul_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)
+          total_inst += (pwr_core_stat->m_num_fpmul_acesses[CURRENT_STAT_IDX][i]);
+        else
+          total_inst += (pwr_core_stat->m_num_fpmul_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_fpmul_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_fp_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)
+          total_inst += (pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i]);
+        else  
+          total_inst += (pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_fp_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_dp_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)
+          total_inst += (pwr_core_stat->m_num_dp_acesses[CURRENT_STAT_IDX][i]);
+        else  
+          total_inst += (pwr_core_stat->m_num_dp_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_dp_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_dpmul_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+      if(aggregate_stat)  
+        total_inst += (pwr_core_stat->m_num_dpmul_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_dpmul_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_dpmul_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_dpdiv_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+      if(aggregate_stat)  
+        total_inst += (pwr_core_stat->m_num_dpdiv_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_dpdiv_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_dpdiv_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_tensor_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+      if(aggregate_stat)  
+        total_inst += (pwr_core_stat->m_num_tensor_core_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_tensor_core_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_tensor_core_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_const_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+        if(aggregate_stat)
+          total_inst += pwr_core_stat->m_num_const_acesses[CURRENT_STAT_IDX][i];
+        else
+          total_inst += (pwr_core_stat->m_num_const_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_const_acesses[PREV_STAT_IDX][i]);
+    }
+    return (total_inst);
+  }
+
+  double get_tex_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+      if(aggregate_stat)  
+        total_inst += (pwr_core_stat->m_num_tex_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_tex_acesses[CURRENT_STAT_IDX][i]) - 
+                      (pwr_core_stat->m_num_tex_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_sp_active_lanes() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_active_sp_lanes[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_active_sp_lanes[PREV_STAT_IDX][i]);
+    }
+    return (total_inst / m_config->num_shader()) / m_config->gpgpu_num_sp_units;
+  }
+
+  float get_sfu_active_lanes() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_active_sfu_lanes[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_active_sfu_lanes[PREV_STAT_IDX][i]);
+    }
+
+    return (total_inst / m_config->num_shader()) /
+           m_config->gpgpu_num_sfu_units;
+  }
+
+
+  float get_active_threads(bool aggregate_stat) {
+    unsigned total_threads = 0;
+    unsigned total_warps = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat){
+        total_threads += (pwr_core_stat->m_active_exu_threads[CURRENT_STAT_IDX][i]) ;
+        total_warps += (pwr_core_stat->m_active_exu_warps[CURRENT_STAT_IDX][i]);
+      }
+      else{
+        total_threads += (pwr_core_stat->m_active_exu_threads[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_active_exu_threads[PREV_STAT_IDX][i]);
+        total_warps += (pwr_core_stat->m_active_exu_warps[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_active_exu_warps[PREV_STAT_IDX][i]);
         }
-        return total_inst;
     }
-    unsigned get_sfu_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sfu_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sfu_committed[PREV_STAT_IDX][i]);
+    if(total_warps != 0)
+      return (float)((float)total_threads / (float)total_warps);
+    else
+      return 0;
+  }
+
+  unsigned long long get_tot_threads_kernel(bool aggregate_stat) {
+    unsigned total_threads = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat){
+        total_threads += (pwr_core_stat->m_active_exu_threads[CURRENT_STAT_IDX][i]) ;
+      }
+      else{
+        total_threads += (pwr_core_stat->m_active_exu_threads[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_active_exu_threads[PREV_STAT_IDX][i]);
         }
-        return total_inst;
-    }
-    unsigned get_mem_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_mem_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_mem_committed[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_mem_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_mem_committed[PREV_STAT_IDX][i])
-                    +(pwr_core_stat->m_num_sfu_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sfu_committed[PREV_STAT_IDX][i])
-                    +(pwr_core_stat->m_num_sp_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sp_committed[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_regfile_reads(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_read_regfile_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_read_regfile_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_regfile_writes(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_write_regfile_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_write_regfile_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    float get_pipeline_duty(){
-        float total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_pipeline_duty_cycle[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_pipeline_duty_cycle[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
     }
 
-    unsigned get_non_regfile_operands(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_non_rf_operands[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_non_rf_operands[PREV_STAT_IDX][i]);
+      return total_threads;
+  }
+  unsigned long long get_tot_warps_kernel(bool aggregate_stat) {
+    unsigned long long total_warps = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat){
+        total_warps += (pwr_core_stat->m_active_exu_warps[CURRENT_STAT_IDX][i]);
+      }
+      else{
+        total_warps += (pwr_core_stat->m_active_exu_warps[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_active_exu_warps[PREV_STAT_IDX][i]);
         }
-        return total_inst;
     }
-
-    unsigned get_sp_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sp_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sp_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_sfu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sfu_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sfu_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_trans_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_trans_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_trans_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_mem_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_mem_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_mem_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intdiv_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_idiv_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_fpdiv_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fpdiv_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intmul32_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intmul24_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intmul_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_fpmul_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fp_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    float get_sp_active_lanes(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_active_sp_lanes[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_active_sp_lanes[PREV_STAT_IDX][i]);
-        }
-        return (total_inst/m_config->num_shader())/m_config->gpgpu_num_sp_units;
-    }
-
-    float get_sfu_active_lanes(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_active_sfu_lanes[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_active_sfu_lanes[PREV_STAT_IDX][i]);
-        }
-
-        return (total_inst/m_config->num_shader())/m_config->gpgpu_num_sfu_units;
-    }
-
-    unsigned get_tot_fpu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fp_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fpdiv_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_fpmul_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fpmul_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul_acesses[PREV_STAT_IDX][i]);
-        }
-        total_inst += get_total_load_inst()+get_total_store_inst()+get_tex_inst();
-        return total_inst;
-    }
-
-    unsigned get_tot_sfu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-                total_inst+= (pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_idiv_acesses[PREV_STAT_IDX][i])+
-                            (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i])+
-                            (pwr_core_stat->m_num_trans_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_trans_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_ialu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_ialu_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_ialu_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_tex_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_tex_inst[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_tex_inst[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_constant_c_accesses(){
-        enum mem_access_type access_type[] = {CONST_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_constant_c_misses(){
-        enum mem_access_type access_type[] = {CONST_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_constant_c_hits(){
-        return (get_constant_c_accesses()-get_constant_c_misses());
-    }
-    unsigned get_texture_c_accesses(){
-        enum mem_access_type access_type[] = {TEXTURE_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_texture_c_misses(){
-        enum mem_access_type access_type[] = {TEXTURE_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_texture_c_hits(){
-        return ( get_texture_c_accesses()- get_texture_c_misses());
-    }
-    unsigned get_inst_c_accesses(){
-        enum mem_access_type access_type[] = {INST_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_inst_c_misses(){
-        enum mem_access_type access_type[] = {INST_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_inst_c_hits(){
-        return (get_inst_c_accesses()-get_inst_c_misses());
-    }
-
-    unsigned get_l1d_read_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_read_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_read_hits(){
-        return (get_l1d_read_accesses()-get_l1d_read_misses());
-    }
-    unsigned get_l1d_write_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_write_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_write_hits(){
-        return (get_l1d_write_accesses()-get_l1d_write_misses());
-    }
-    unsigned get_cache_misses(){
-        return get_l1d_read_misses()+get_constant_c_misses()+get_l1d_write_misses()+get_texture_c_misses();
-    }
-	
-    unsigned get_cache_read_misses(){
-        return get_l1d_read_misses()+get_constant_c_misses()+get_texture_c_misses();
-    }
-
-    unsigned get_cache_write_misses(){
-        return get_l1d_write_misses();
-    }
-
-    unsigned get_shmem_read_access(){
-       unsigned total_inst=0;
-       for(unsigned i=0; i<m_config->num_shader();i++){
-           total_inst+=(pwr_mem_stat->shmem_read_access[CURRENT_STAT_IDX][i]) - (pwr_mem_stat->shmem_read_access[PREV_STAT_IDX][i]);
-       }
-       return total_inst;
-    }
-
-    unsigned get_l2_read_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R, CONST_ACC_R, TEXTURE_ACC_R, INST_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-
-    unsigned get_l2_read_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R, CONST_ACC_R, TEXTURE_ACC_R, INST_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-
-    unsigned get_l2_read_hits(){
-        return (get_l2_read_accesses()-get_l2_read_misses());
-    }
-
-    unsigned get_l2_write_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W, L1_WRBK_ACC};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-
-    unsigned get_l2_write_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W, L1_WRBK_ACC};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l2_write_hits(){
-        return (get_l2_write_accesses()-get_l2_write_misses());
-    }
-    unsigned get_dram_cmd(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_cmd[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_cmd[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_activity(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_activity[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_activity[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_nop(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_nop[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_nop[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_act(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_act[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_act[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_pre(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_pre[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_rd(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_rd[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_wr(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_wr[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_req(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_req[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_req[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-
-    long get_icnt_simt_to_mem(){
-        long total=0;
-        for(unsigned i=0; i<m_config->n_simt_clusters; ++i){
-            total += (pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_simt_to_mem[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-
-    long get_icnt_mem_to_simt(){
-        long total=0;
-        for(unsigned i=0; i<m_config->n_simt_clusters; ++i){
-            total += (pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_mem_to_simt[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-
-   power_core_stat_t * pwr_core_stat;
-   power_mem_stat_t * pwr_mem_stat;
-   float * m_average_pipeline_duty_cycle;
-   float * m_active_sms;
-   const shader_core_config *m_config;
-   const struct memory_config *m_mem_config;
+      return total_warps;
+  }
+
+
+  double get_tot_fpu_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_dp_acesses[CURRENT_STAT_IDX][i]);
+      else
+        total_inst += (pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_fp_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_dp_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_dp_acesses[PREV_STAT_IDX][i]);
+    }
+    //total_inst += get_total_load_inst()+get_total_store_inst()+get_tex_inst();
+    return total_inst;
+  }
+
+
+
+  double get_tot_sfu_accessess(bool aggregate_stat){
+    double total_inst=0;
+    for(unsigned i=0; i<m_config->num_shader();i++){
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_sqrt_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_log_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_sin_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_exp_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_fpmul_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_dpmul_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_dpdiv_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_tensor_core_acesses[CURRENT_STAT_IDX][i])+
+                    (pwr_core_stat->m_num_tex_acesses[CURRENT_STAT_IDX][i]);
+        else
+            total_inst += (pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_idiv_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_sqrt_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_sqrt_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_log_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_log_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_sin_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_sin_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_exp_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_exp_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_fpdiv_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_fpmul_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_fpmul_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_dpmul_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_dpmul_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_dpdiv_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_dpdiv_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_imul_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_tensor_core_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_tensor_core_acesses[PREV_STAT_IDX][i]) +
+                    (pwr_core_stat->m_num_tex_acesses[CURRENT_STAT_IDX][i]) - 
+                    (pwr_core_stat->m_num_tex_acesses[PREV_STAT_IDX][i]);
+
+    }
+    return total_inst;
+  }
+
+  double get_ialu_accessess(bool aggregate_stat) {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_core_stat->m_num_ialu_acesses[CURRENT_STAT_IDX][i]);
+      else  
+        total_inst += (pwr_core_stat->m_num_ialu_acesses[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_ialu_acesses[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_tex_inst() {
+    double total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      total_inst += (pwr_core_stat->m_num_tex_inst[CURRENT_STAT_IDX][i]) -
+                    (pwr_core_stat->m_num_tex_inst[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  double get_constant_c_accesses() {
+    enum mem_access_type access_type[] = {CONST_ACC_R};
+    enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+  }
+  double get_constant_c_misses() {
+    enum mem_access_type access_type[] = {CONST_ACC_R};
+    enum cache_request_status request_status[] = {MISS};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+  }
+  double get_constant_c_hits() {
+    return (get_constant_c_accesses() - get_constant_c_misses());
+  }
+  double get_texture_c_accesses() {
+    enum mem_access_type access_type[] = {TEXTURE_ACC_R};
+    enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+  }
+  double get_texture_c_misses() {
+    enum mem_access_type access_type[] = {TEXTURE_ACC_R};
+    enum cache_request_status request_status[] = {MISS};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+  }
+  double get_texture_c_hits() {
+    return (get_texture_c_accesses() - get_texture_c_misses());
+  }
+  double get_inst_c_accesses(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {INST_ACC_R};
+    enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+    if(aggregate_stat)
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    else
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+  }
+  double get_inst_c_misses(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {INST_ACC_R};
+    enum cache_request_status request_status[] = {MISS};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+    if(aggregate_stat)
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    else
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+  }
+  double get_inst_c_hits(bool aggregate_stat) {
+    return (get_inst_c_accesses(aggregate_stat) - get_inst_c_misses(aggregate_stat));
+  }
+
+  double get_l1d_read_accesses(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R};
+    enum cache_request_status request_status[] = {HIT, MISS, SECTOR_MISS}; 
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    if(aggregate_stat){
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+      }
+  }
+  double get_l1d_read_misses(bool aggregate_stat) {
+    return (get_l1d_read_accesses(aggregate_stat) - get_l1d_read_hits(aggregate_stat));
+  }
+  double get_l1d_read_hits(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R};
+    enum cache_request_status request_status[] = {HIT, MSHR_HIT};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    if(aggregate_stat){
+       return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+      }
+  }
+  double get_l1d_write_accesses(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W};
+    enum cache_request_status request_status[] = {HIT, MISS, SECTOR_MISS};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    if(aggregate_stat){
+       return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+      }
+  }
+  double get_l1d_write_misses(bool aggregate_stat) {
+    return (get_l1d_write_accesses(aggregate_stat) - get_l1d_write_hits(aggregate_stat));
+  }
+  double get_l1d_write_hits(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W};
+    enum cache_request_status request_status[] = {HIT, MSHR_HIT};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+
+    if(aggregate_stat){
+       return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+      }
+  }
+  double get_cache_misses() {
+    return get_l1d_read_misses(0) + get_constant_c_misses() +
+           get_l1d_write_misses(0) + get_texture_c_misses();
+  }
+
+  double get_cache_read_misses() {
+    return get_l1d_read_misses(0) + get_constant_c_misses() +
+           get_texture_c_misses();
+  }
+
+  double get_cache_write_misses() { return get_l1d_write_misses(0); }
+
+  double get_shmem_access(bool aggregate_stat) {
+    unsigned total_inst = 0;
+    for (unsigned i = 0; i < m_config->num_shader(); i++) {
+      if(aggregate_stat)
+        total_inst += (pwr_mem_stat->shmem_access[CURRENT_STAT_IDX][i]);
+      else
+        total_inst += (pwr_mem_stat->shmem_access[CURRENT_STAT_IDX][i]) -
+                    (pwr_mem_stat->shmem_access[PREV_STAT_IDX][i]);
+    }
+    return total_inst;
+  }
+
+  unsigned long long  get_l2_read_accesses(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {
+        GLOBAL_ACC_R, LOCAL_ACC_R, CONST_ACC_R, TEXTURE_ACC_R, INST_ACC_R};
+    enum cache_request_status request_status[] = {HIT, HIT_RESERVED, MISS, SECTOR_MISS}; 
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+    if(aggregate_stat){
+       return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+  }
+
+  unsigned long long get_l2_read_misses(bool aggregate_stat) {
+    return (get_l2_read_accesses(aggregate_stat) - get_l2_read_hits(aggregate_stat));
+  }
+
+  unsigned long long get_l2_read_hits(bool aggregate_stat) {
+       enum mem_access_type access_type[] = {
+        GLOBAL_ACC_R, LOCAL_ACC_R, CONST_ACC_R, TEXTURE_ACC_R, INST_ACC_R};
+    enum cache_request_status request_status[] =  {HIT, HIT_RESERVED};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+    if(aggregate_stat){
+       return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+  }
+
+  unsigned long long get_l2_write_accesses(bool aggregate_stat) {
+    enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W,
+                                          L1_WRBK_ACC};
+    enum cache_request_status request_status[] = {HIT, HIT_RESERVED, MISS, SECTOR_MISS}; 
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+    if(aggregate_stat){
+      return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+  }
+
+  unsigned long long get_l2_write_misses(bool aggregate_stat) {
+    return (get_l2_write_accesses(aggregate_stat) - get_l2_write_hits(aggregate_stat));
+  }
+  unsigned long long get_l2_write_hits(bool aggregate_stat) {
+        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W,
+                                          L1_WRBK_ACC};
+    enum cache_request_status request_status[] = {HIT, HIT_RESERVED};
+    unsigned num_access_type =
+        sizeof(access_type) / sizeof(enum mem_access_type);
+    unsigned num_request_status =
+        sizeof(request_status) / sizeof(enum cache_request_status);
+    if(aggregate_stat){
+      return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+    else{
+      return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status)) -
+           (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(
+               access_type, num_access_type, request_status,
+               num_request_status));
+    }
+  }
+  double get_dram_cmd() {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      total += (pwr_mem_stat->n_cmd[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_cmd[PREV_STAT_IDX][i]);
+    }
+    return total;
+  }
+  double get_dram_activity() {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      total += (pwr_mem_stat->n_activity[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_activity[PREV_STAT_IDX][i]);
+    }
+    return total;
+  }
+  double get_dram_nop() {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      total += (pwr_mem_stat->n_nop[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_nop[PREV_STAT_IDX][i]);
+    }
+    return total;
+  }
+  double get_dram_act() {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      total += (pwr_mem_stat->n_act[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_act[PREV_STAT_IDX][i]);
+    }
+    return total;
+  }
+  double get_dram_pre(bool aggregate_stat) {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      if(aggregate_stat){
+        total += pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i];
+      }
+      else{
+        total += (pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_pre[PREV_STAT_IDX][i]);
+      }
+    }
+    return total;
+  }
+  double get_dram_rd(bool aggregate_stat) {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      if(aggregate_stat){
+        total += pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i];
+      }
+      else{
+        total += (pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_rd[PREV_STAT_IDX][i]);
+      }
+    }
+    return total;
+  }
+  double get_dram_wr(bool aggregate_stat) {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      if(aggregate_stat){
+        total += pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i] + 
+                pwr_mem_stat->n_wr_WB[CURRENT_STAT_IDX][i];
+      }
+      else{
+        total += (pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i] - 
+                pwr_mem_stat->n_wr[PREV_STAT_IDX][i]) +
+                (pwr_mem_stat->n_wr_WB[CURRENT_STAT_IDX][i] - 
+                pwr_mem_stat->n_wr_WB[PREV_STAT_IDX][i]);
+      }
+    }
+    return total;
+  }
+  double get_dram_req() {
+    unsigned total = 0;
+    for (unsigned i = 0; i < m_mem_config->m_n_mem; ++i) {
+      total += (pwr_mem_stat->n_req[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_req[PREV_STAT_IDX][i]);
+    }
+    return total;
+  }
+
+  unsigned long long get_icnt_simt_to_mem(bool aggregate_stat) {
+    long total = 0;
+    for (unsigned i = 0; i < m_config->n_simt_clusters; ++i){
+      if(aggregate_stat){
+        total += pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i];
+      }
+      else{
+        total += (pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_simt_to_mem[PREV_STAT_IDX][i]);
+      }
+    }
+    return total;
+  }
+
+  unsigned long long get_icnt_mem_to_simt(bool aggregate_stat) {
+    long total = 0;
+    for (unsigned i = 0; i < m_config->n_simt_clusters; ++i) {
+      if(aggregate_stat){
+        total += pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i];
+      }
+      
+      else{
+        total += (pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i] -
+                pwr_mem_stat->n_mem_to_simt[PREV_STAT_IDX][i]);
+      }
+    }
+    return total;
+  }
+
+  power_core_stat_t *pwr_core_stat;
+  power_mem_stat_t *pwr_mem_stat;
+  float *m_average_pipeline_duty_cycle;
+  float *m_active_sms;
+  const shader_core_config *m_config;
+  const memory_config *m_mem_config;
 };
 
-
 #endif /*POWER_LATENCY_STAT_H*/
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.cc
index 6e04b9bb7c..3c2d4fa9a0 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.cc
@@ -26,139 +26,128 @@
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #include "scoreboard.h"
-#include "shader.h"
 #include "../cuda-sim/ptx_sim.h"
+#include "shader.h"
 #include "shader_trace.h"
 
+// Constructor
+Scoreboard::Scoreboard(unsigned sid, unsigned n_warps, class gpgpu_t* gpu)
+    : longopregs() {
+  m_sid = sid;
+  // Initialize size of table
+  reg_table.resize(n_warps);
+  longopregs.resize(n_warps);
 
-//Constructor
-Scoreboard::Scoreboard( unsigned sid, unsigned n_warps )
-: longopregs()
-{
-	m_sid = sid;
-	//Initialize size of table
-	reg_table.resize(n_warps);
-	longopregs.resize(n_warps);
+  m_gpu = gpu;
 }
 
 // Print scoreboard contents
-void Scoreboard::printContents() const
-{
-	printf("scoreboard contents (sid=%d): \n", m_sid);
-	for(unsigned i=0; i<reg_table.size(); i++) {
-		if(reg_table[i].size() == 0 ) continue;
-		printf("  wid = %2d: ", i);
-		std::set<unsigned>::const_iterator it;
-		for( it=reg_table[i].begin() ; it != reg_table[i].end(); it++ )
-			printf("%u ", *it);
-		printf("\n");
-	}
+void Scoreboard::printContents() const {
+  printf("scoreboard contents (sid=%d): \n", m_sid);
+  for (unsigned i = 0; i < reg_table.size(); i++) {
+    if (reg_table[i].size() == 0) continue;
+    printf("  wid = %2d: ", i);
+    std::set<unsigned>::const_iterator it;
+    for (it = reg_table[i].begin(); it != reg_table[i].end(); it++)
+      printf("%u ", *it);
+    printf("\n");
+  }
 }
 
-void Scoreboard::reserveRegister(unsigned wid, unsigned regnum) 
-{
-	if( !(reg_table[wid].find(regnum) == reg_table[wid].end()) ){
-		printf("Error: trying to reserve an already reserved register (sid=%d, wid=%d, regnum=%d).", m_sid, wid, regnum);
-        abort();
-	}
-    SHADER_DPRINTF( SCOREBOARD,
-                    "Reserved Register - warp:%d, reg: %d\n", wid, regnum );
-	reg_table[wid].insert(regnum);
+void Scoreboard::reserveRegister(unsigned wid, unsigned regnum) {
+  if (!(reg_table[wid].find(regnum) == reg_table[wid].end())) {
+    printf(
+        "Error: trying to reserve an already reserved register (sid=%d, "
+        "wid=%d, regnum=%d).",
+        m_sid, wid, regnum);
+    abort();
+  }
+  SHADER_DPRINTF(SCOREBOARD, "Reserved Register - warp:%d, reg: %d\n", wid,
+                 regnum);
+  reg_table[wid].insert(regnum);
 }
 
 // Unmark register as write-pending
-void Scoreboard::releaseRegister(unsigned wid, unsigned regnum) 
-{
-	if( !(reg_table[wid].find(regnum) != reg_table[wid].end()) ) 
-        return;
-    SHADER_DPRINTF( SCOREBOARD,
-                    "Release register - warp:%d, reg: %d\n", wid, regnum );
-	reg_table[wid].erase(regnum);
+void Scoreboard::releaseRegister(unsigned wid, unsigned regnum) {
+  if (!(reg_table[wid].find(regnum) != reg_table[wid].end())) return;
+  SHADER_DPRINTF(SCOREBOARD, "Release register - warp:%d, reg: %d\n", wid,
+                 regnum);
+  reg_table[wid].erase(regnum);
 }
 
-// const bool Scoreboard::islongop (unsigned warp_id,unsigned regnum) {
-bool Scoreboard::islongop (unsigned warp_id,unsigned regnum) {
-	return longopregs[warp_id].find(regnum) != longopregs[warp_id].end();
+bool Scoreboard::islongop(unsigned warp_id, unsigned regnum) {
+  return longopregs[warp_id].find(regnum) != longopregs[warp_id].end();
 }
 
-void Scoreboard::reserveRegisters(const class warp_inst_t* inst) 
-{
-    for( unsigned r=0; r < MAX_OUTPUT_VALUES; r++) {
-        if(inst->out[r] > 0) {
-            reserveRegister(inst->warp_id(), inst->out[r]);
-            SHADER_DPRINTF( SCOREBOARD,
-                            "Reserved register - warp:%d, reg: %d\n",
-                            inst->warp_id(),
-                            inst->out[r] );
-        }
+void Scoreboard::reserveRegisters(const class warp_inst_t* inst) {
+  for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++) {
+    if (inst->out[r] > 0) {
+      reserveRegister(inst->warp_id(), inst->out[r]);
+      SHADER_DPRINTF(SCOREBOARD, "Reserved register - warp:%d, reg: %d\n",
+                     inst->warp_id(), inst->out[r]);
     }
-
-    //Keep track of long operations
-    if (inst->is_load() &&
-    		(	inst->space.get_type() == global_space ||
-    			inst->space.get_type() == local_space ||
-                inst->space.get_type() == param_space_kernel ||
-                inst->space.get_type() == param_space_local ||
-                inst->space.get_type() == param_space_unclassified ||
-    			inst->space.get_type() == tex_space)){
-    	for ( unsigned r=0; r<MAX_OUTPUT_VALUES; r++) {
-    		if(inst->out[r] > 0) {
-                SHADER_DPRINTF( SCOREBOARD,
-                                "New longopreg marked - warp:%d, reg: %d\n",
-                                inst->warp_id(),
-                                inst->out[r] );
-                longopregs[inst->warp_id()].insert(inst->out[r]);
-            }
-    	}
+  }
+
+  // Keep track of long operations
+  if (inst->is_load() && (inst->space.get_type() == global_space ||
+                          inst->space.get_type() == local_space ||
+                          inst->space.get_type() == param_space_kernel ||
+                          inst->space.get_type() == param_space_local ||
+                          inst->space.get_type() == param_space_unclassified ||
+                          inst->space.get_type() == tex_space)) {
+    for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++) {
+      if (inst->out[r] > 0) {
+        SHADER_DPRINTF(SCOREBOARD, "New longopreg marked - warp:%d, reg: %d\n",
+                       inst->warp_id(), inst->out[r]);
+        longopregs[inst->warp_id()].insert(inst->out[r]);
+      }
     }
+  }
 }
 
 // Release registers for an instruction
-void Scoreboard::releaseRegisters(const class warp_inst_t *inst) 
-{
-    for( unsigned r=0; r < MAX_OUTPUT_VALUES; r++) {
-        if(inst->out[r] > 0) {
-            SHADER_DPRINTF( SCOREBOARD,
-                            "Register Released - warp:%d, reg: %d\n",
-                            inst->warp_id(),
-                            inst->out[r] );
-            releaseRegister(inst->warp_id(), inst->out[r]);
-            longopregs[inst->warp_id()].erase(inst->out[r]);
-        }
+void Scoreboard::releaseRegisters(const class warp_inst_t* inst) {
+  for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++) {
+    if (inst->out[r] > 0) {
+      SHADER_DPRINTF(SCOREBOARD, "Register Released - warp:%d, reg: %d\n",
+                     inst->warp_id(), inst->out[r]);
+      releaseRegister(inst->warp_id(), inst->out[r]);
+      longopregs[inst->warp_id()].erase(inst->out[r]);
     }
+  }
 }
 
-/** 
- * Checks to see if registers used by an instruction are reserved in the scoreboard
- *  
- * @return 
+/**
+ * Checks to see if registers used by an instruction are reserved in the
+ *scoreboard
+ *
+ * @return
  * true if WAW or RAW hazard (no WAR since in-order issue)
- **/ 
-bool Scoreboard::checkCollision( unsigned wid, const class inst_t *inst ) const
-{
-	// Get list of all input and output registers
-	std::set<int> inst_regs;
-
-	for(int iii=0;iii<inst->outcount;iii++)
-		inst_regs.insert(inst->out[iii]);
-
-	for(int jjj=0;jjj<inst->incount;jjj++)
-		inst_regs.insert(inst->in[jjj]);
-
-        if(inst->pred > 0) inst_regs.insert(inst->pred);
-	if(inst->ar1 > 0) inst_regs.insert(inst->ar1);
-	if(inst->ar2 > 0) inst_regs.insert(inst->ar2);
-
-	// Check for collision, get the intersection of reserved registers and instruction registers
-	std::set<int>::const_iterator it2;
-	for ( it2=inst_regs.begin() ; it2 != inst_regs.end(); it2++ )
-		if(reg_table[wid].find(*it2) != reg_table[wid].end()) {
-			return true;
-		}
-	return false;
+ **/
+bool Scoreboard::checkCollision(unsigned wid, const class inst_t* inst) const {
+  // Get list of all input and output registers
+  std::set<int> inst_regs;
+
+  for (unsigned iii = 0; iii < inst->outcount; iii++)
+    inst_regs.insert(inst->out[iii]);
+
+  for (unsigned jjj = 0; jjj < inst->incount; jjj++)
+    inst_regs.insert(inst->in[jjj]);
+
+  if (inst->pred > 0) inst_regs.insert(inst->pred);
+  if (inst->ar1 > 0) inst_regs.insert(inst->ar1);
+  if (inst->ar2 > 0) inst_regs.insert(inst->ar2);
+
+  // Check for collision, get the intersection of reserved registers and
+  // instruction registers
+  std::set<int>::const_iterator it2;
+  for (it2 = inst_regs.begin(); it2 != inst_regs.end(); it2++)
+    if (reg_table[wid].find(*it2) != reg_table[wid].end()) {
+      return true;
+    }
+  return false;
 }
 
-bool Scoreboard::pendingWrites(unsigned wid) const
-{
-	return !reg_table[wid].empty();
+bool Scoreboard::pendingWrites(unsigned wid) const {
+  return !reg_table[wid].empty();
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.h
index a64b9bcb9b..594822a391 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/scoreboard.h
@@ -38,7 +38,7 @@
 
 class Scoreboard {
 public:
-    Scoreboard( unsigned sid, unsigned n_warps );
+  Scoreboard(unsigned sid, unsigned n_warps, class gpgpu_t *gpu);
 
     void reserveRegisters(const warp_inst_t *inst);
     void releaseRegisters(const warp_inst_t *inst);
@@ -60,6 +60,7 @@ private:
     std::vector< std::set<unsigned> > reg_table;
     //Register that depend on a long operation (global, local or tex memory)
     std::vector< std::set<unsigned> > longopregs;
+  class gpgpu_t *m_gpu;
 };
 
 
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.cc
index 19e96f59f0..175306995d 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.cc
@@ -26,902 +26,1084 @@
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-#include <float.h>
 #include "shader.h"
+#include <float.h>
+#include <limits.h>
+#include <string.h>
 #include "gpu-sim.h"
+#include "../../libcuda_sim/gpgpu_context.h"
+#include "../cuda-sim/cuda-sim.h"
+#include "../cuda-sim/ptx-stats.h"
+#include "../cuda-sim/ptx_sim.h"
+#include "../statwrapper.h"
 #include "addrdec.h"
 #include "dram.h"
-#include "stat-tool.h"
 #include "gpu-misc.h"
-#include "../cuda-sim/ptx_sim.h"
-#include "../cuda-sim/ptx-stats.h"
-#include "../cuda-sim/cuda-sim.h"
 #include "gpu-cache_gem5.h"
 #include "gpu-sim.h"
+#include "icnt_wrapper.h"
 #include "gpu/gpgpu-sim/cuda_gpu.hh"
 #include "mem_fetch.h"
 #include "mem_latency_stat.h"
-#include "visualizer.h"
-#include "../statwrapper.h"
-#include "icnt_wrapper.h"
-#include <string.h>
-#include <limits.h>
-#include "traffic_breakdown.h"
 #include "shader_trace.h"
+#include "stat-tool.h"
+#include "traffic_breakdown.h"
+#include "visualizer.h"
 
 #define PRIORITIZE_MSHR_OVER_WB 1
-#define MAX(a,b) (((a)>(b))?(a):(b))
-#define MIN(a,b) (((a)<(b))?(a):(b))
-    
-extern gpgpu_sim *g_the_gpu;
-
+#define MAX(a, b) (((a) > (b)) ? (a) : (b))
+#define MIN(a, b) (((a) < (b)) ? (a) : (b))
+
+// extern gpgpu_sim *g_the_gpu;
+mem_fetch *shader_core_mem_fetch_allocator::alloc(
+    new_addr_type addr, mem_access_type type, unsigned size, bool wr,
+    unsigned long long cycle) const {
+  mem_access_t access(type, addr, size, wr, m_memory_config->gpgpu_ctx);
+  mem_fetch *mf =
+      new mem_fetch(access, NULL, wr ? WRITE_PACKET_SIZE : READ_PACKET_SIZE, -1,
+                    m_core_id, m_cluster_id, m_memory_config, cycle);
+  return mf;
+}
+
+mem_fetch *shader_core_mem_fetch_allocator::alloc(
+    new_addr_type addr, mem_access_type type, const active_mask_t &active_mask,
+    const mem_access_byte_mask_t &byte_mask,
+    const mem_access_sector_mask_t &sector_mask, unsigned size, bool wr,
+    unsigned long long cycle, unsigned wid, unsigned sid, unsigned tpc,
+    mem_fetch *original_mf) const {
+  mem_access_t access(type, addr, size, wr, active_mask, byte_mask, sector_mask,
+                      m_memory_config->gpgpu_ctx);
+  mem_fetch *mf = new mem_fetch(
+      access, NULL, wr ? WRITE_PACKET_SIZE : READ_PACKET_SIZE, wid, m_core_id,
+      m_cluster_id, m_memory_config, cycle, original_mf);
+  return mf;
+}
 /////////////////////////////////////////////////////////////////////////////
 
-std::list<unsigned> shader_core_ctx::get_regs_written( const inst_t &fvt ) const
-{
-   std::list<unsigned> result;
-   for( unsigned op=0; op < MAX_REG_OPERANDS; op++ ) {
-      int reg_num = fvt.arch_reg.dst[op]; // this math needs to match that used in function_info::ptx_decode_inst
-      if( reg_num >= 0 ) // valid register
-         result.push_back(reg_num);
-   }
-   return result;
-}
-
-shader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu, 
-                                  class simt_core_cluster *cluster,
-                                  unsigned shader_id,
-                                  unsigned tpc_id,
-                                  const struct shader_core_config *config,
-                                  const struct memory_config *mem_config,
-                                  shader_core_stats *stats )
-   : core_t( gpu, NULL, config->warp_size, config->n_thread_per_shader ),
-     m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),
-     m_dynamic_warp_id(0), m_active_warps(0)
-{
-    // TODO schi 
-    m_kernel_finishing = false;
+std::list<unsigned> shader_core_ctx::get_regs_written(const inst_t &fvt) const {
+  std::list<unsigned> result;
+  for (unsigned op = 0; op < MAX_REG_OPERANDS; op++) {
+    int reg_num = fvt.arch_reg.dst[op];  // this math needs to match that used
+                                         // in function_info::ptx_decode_inst
+    if (reg_num >= 0)                    // valid register
+      result.push_back(reg_num);
+  }
+  return result;
+}
 
-    m_cluster = cluster;
-    m_config = config;
-    m_memory_config = mem_config;
-    m_stats = stats;
-    unsigned warp_size=config->warp_size;
-    Issue_Prio = 0;
-    
-    m_sid = shader_id;
-    m_tpc = tpc_id;
-    
-    m_pipeline_reg.reserve(N_PIPELINE_STAGES);
-    for (int j = 0; j<N_PIPELINE_STAGES; j++) {
-        m_pipeline_reg.push_back(register_set(m_config->pipe_widths[j],pipeline_stage_name_decode[j]));
-    }
-    if(m_config->sub_core_model) {
-    	//in subcore model, each scheduler should has its own issue register, so num scheduler = reg width
-    	assert(m_config->gpgpu_num_sched_per_core == m_pipeline_reg[ID_OC_SP].get_size() );
-    	assert(m_config->gpgpu_num_sched_per_core == m_pipeline_reg[ID_OC_SFU].get_size() );
-    	assert(m_config->gpgpu_num_sched_per_core == m_pipeline_reg[ID_OC_MEM].get_size() );
-    	if(m_config->gpgpu_tensor_core_avail)
-    		 assert(m_config->gpgpu_num_sched_per_core == m_pipeline_reg[ID_OC_TENSOR_CORE].get_size() );
-    	if(m_config->gpgpu_num_dp_units > 0)
-    		assert(m_config->gpgpu_num_sched_per_core == m_pipeline_reg[ID_OC_DP].get_size() );
-    	if(m_config->gpgpu_num_int_units > 0)
-    	    assert(m_config->gpgpu_num_sched_per_core == m_pipeline_reg[ID_OC_INT].get_size() );
+void exec_shader_core_ctx::create_shd_warp() {
+  m_warp.resize(m_config->max_warps_per_shader);
+  for (unsigned k = 0; k < m_config->max_warps_per_shader; ++k) {
+    m_warp[k] = new shd_warp_t(this, m_config->warp_size);
+  }
+}
+
+void shader_core_ctx::create_front_pipeline() {
+  // pipeline_stages is the sum of normal pipeline stages and specialized_unit
+  // stages * 2 (for ID and EX)
+  unsigned total_pipeline_stages =
+      N_PIPELINE_STAGES + m_config->m_specialized_unit.size() * 2;
+  m_pipeline_reg.reserve(total_pipeline_stages);
+  for (int j = 0; j < N_PIPELINE_STAGES; j++) {
+    m_pipeline_reg.push_back(
+        register_set(m_config->pipe_widths[j], pipeline_stage_name_decode[j]));
+  }
+  for (int j = 0; j < m_config->m_specialized_unit.size(); j++) {
+    m_pipeline_reg.push_back(
+        register_set(m_config->m_specialized_unit[j].id_oc_spec_reg_width,
+                     m_config->m_specialized_unit[j].name));
+    m_config->m_specialized_unit[j].ID_OC_SPEC_ID = m_pipeline_reg.size() - 1;
+    m_specilized_dispatch_reg.push_back(
+        &m_pipeline_reg[m_pipeline_reg.size() - 1]);
+  }
+  for (int j = 0; j < m_config->m_specialized_unit.size(); j++) {
+    m_pipeline_reg.push_back(
+        register_set(m_config->m_specialized_unit[j].oc_ex_spec_reg_width,
+                     m_config->m_specialized_unit[j].name));
+    m_config->m_specialized_unit[j].OC_EX_SPEC_ID = m_pipeline_reg.size() - 1;
+  }
+
+  if (m_config->sub_core_model) {
+    // in subcore model, each scheduler should has its own issue register, so
+    // ensure num scheduler = reg width
+    assert(m_config->gpgpu_num_sched_per_core ==
+           m_pipeline_reg[ID_OC_SP].get_size());
+    assert(m_config->gpgpu_num_sched_per_core ==
+           m_pipeline_reg[ID_OC_SFU].get_size());
+    assert(m_config->gpgpu_num_sched_per_core ==
+           m_pipeline_reg[ID_OC_MEM].get_size());
+    if (m_config->gpgpu_tensor_core_avail)
+      assert(m_config->gpgpu_num_sched_per_core ==
+             m_pipeline_reg[ID_OC_TENSOR_CORE].get_size());
+    if (m_config->gpgpu_num_dp_units > 0)
+      assert(m_config->gpgpu_num_sched_per_core ==
+             m_pipeline_reg[ID_OC_DP].get_size());
+    if (m_config->gpgpu_num_int_units > 0)
+      assert(m_config->gpgpu_num_sched_per_core ==
+             m_pipeline_reg[ID_OC_INT].get_size());
+    for (int j = 0; j < m_config->m_specialized_unit.size(); j++) {
+      if (m_config->m_specialized_unit[j].num_units > 0)
+        assert(m_config->gpgpu_num_sched_per_core ==
+               m_config->m_specialized_unit[j].id_oc_spec_reg_width);
     }
-    
-    m_threadState = (thread_ctx_t*) calloc(sizeof(thread_ctx_t), config->n_thread_per_shader);
-    
-    m_not_completed = 0;
-    m_active_threads.reset();
-    m_n_active_cta = 0;
-    for ( unsigned i = 0; i<MAX_CTA_PER_SHADER; i++ ) 
-        m_cta_status[i]=0;
-    for (unsigned i = 0; i<config->n_thread_per_shader; i++) {
-        m_thread[i]= NULL;
-        m_threadState[i].m_cta_id = -1;
-        m_threadState[i].m_active = false;
+  }
+
+  m_threadState = (thread_ctx_t *)calloc(sizeof(thread_ctx_t),
+                                         m_config->n_thread_per_shader);
+
+  m_not_completed = 0;
+  m_active_threads.reset();
+  m_n_active_cta = 0;
+  for (unsigned i = 0; i < MAX_CTA_PER_SHADER; i++) m_cta_status[i] = 0;
+  for (unsigned i = 0; i < m_config->n_thread_per_shader; i++) {
+    m_thread[i] = NULL;
+    m_threadState[i].m_cta_id = -1;
+    m_threadState[i].m_active = false;
+  }
+
+  // m_icnt = new shader_memory_interface(this,cluster);
+  if (m_config->gpgpu_perfect_mem) {
+    m_icnt = new perfect_memory_interface(this, m_cluster);
+  } else {
+    m_icnt = new shader_memory_interface(this, m_cluster);
+  }
+  m_mem_fetch_allocator =
+      new shader_core_mem_fetch_allocator(m_sid, m_tpc, m_memory_config);
+
+  // fetch
+  m_last_warp_fetched = 0;
+
+#define STRSIZE 1024
+  char name[STRSIZE];
+  snprintf(name, STRSIZE, "L1I_%03d", m_sid);
+  m_L1I = new l1icache_gem5(m_gpu, name,m_config->m_L1I_config,m_sid,get_shader_instruction_cache_id(),m_icnt,IN_L1I_MISS_QUEUE);
+  /*
+  m_L1I = new read_only_cache(name, m_config->m_L1I_config, m_sid,
+                              get_shader_instruction_cache_id(), m_icnt,
+                              IN_L1I_MISS_QUEUE);
+                              */
+}
+
+void shader_core_ctx::create_schedulers() {
+  m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader, m_gpu);
+
+  // scedulers
+  // must currently occur after all inputs have been initialized.
+  std::string sched_config = m_config->gpgpu_scheduler_string;
+  const concrete_scheduler scheduler =
+      sched_config.find("lrr") != std::string::npos
+          ? CONCRETE_SCHEDULER_LRR
+          : sched_config.find("two_level_active") != std::string::npos
+                ? CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE
+                : sched_config.find("gto") != std::string::npos
+                      ? CONCRETE_SCHEDULER_GTO
+                      : sched_config.find("rrr") != std::string::npos
+                            ? CONCRETE_SCHEDULER_RRR
+                      : sched_config.find("old") != std::string::npos
+                            ? CONCRETE_SCHEDULER_OLDEST_FIRST
+                            : sched_config.find("warp_limiting") !=
+                                      std::string::npos
+                                  ? CONCRETE_SCHEDULER_WARP_LIMITING
+                                  : NUM_CONCRETE_SCHEDULERS;
+  assert(scheduler != NUM_CONCRETE_SCHEDULERS);
+
+  for (unsigned i = 0; i < m_config->gpgpu_num_sched_per_core; i++) {
+    switch (scheduler) {
+      case CONCRETE_SCHEDULER_LRR:
+        schedulers.push_back(new lrr_scheduler(
+            m_stats, this, m_scoreboard, m_simt_stack, &m_warp,
+            &m_pipeline_reg[ID_OC_SP], &m_pipeline_reg[ID_OC_DP],
+            &m_pipeline_reg[ID_OC_SFU], &m_pipeline_reg[ID_OC_INT],
+            &m_pipeline_reg[ID_OC_TENSOR_CORE], m_specilized_dispatch_reg,
+            &m_pipeline_reg[ID_OC_MEM], i));
+        break;
+      case CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE:
+        schedulers.push_back(new two_level_active_scheduler(
+            m_stats, this, m_scoreboard, m_simt_stack, &m_warp,
+            &m_pipeline_reg[ID_OC_SP], &m_pipeline_reg[ID_OC_DP],
+            &m_pipeline_reg[ID_OC_SFU], &m_pipeline_reg[ID_OC_INT],
+            &m_pipeline_reg[ID_OC_TENSOR_CORE], m_specilized_dispatch_reg,
+            &m_pipeline_reg[ID_OC_MEM], i, m_config->gpgpu_scheduler_string));
+        break;
+      case CONCRETE_SCHEDULER_GTO:
+        schedulers.push_back(new gto_scheduler(
+            m_stats, this, m_scoreboard, m_simt_stack, &m_warp,
+            &m_pipeline_reg[ID_OC_SP], &m_pipeline_reg[ID_OC_DP],
+            &m_pipeline_reg[ID_OC_SFU], &m_pipeline_reg[ID_OC_INT],
+            &m_pipeline_reg[ID_OC_TENSOR_CORE], m_specilized_dispatch_reg,
+            &m_pipeline_reg[ID_OC_MEM], i));
+        break;
+      case CONCRETE_SCHEDULER_RRR:
+        schedulers.push_back(new rrr_scheduler(
+            m_stats, this, m_scoreboard, m_simt_stack, &m_warp,
+            &m_pipeline_reg[ID_OC_SP], &m_pipeline_reg[ID_OC_DP],
+            &m_pipeline_reg[ID_OC_SFU], &m_pipeline_reg[ID_OC_INT],
+            &m_pipeline_reg[ID_OC_TENSOR_CORE], m_specilized_dispatch_reg,
+            &m_pipeline_reg[ID_OC_MEM], i));
+        break;
+      case CONCRETE_SCHEDULER_OLDEST_FIRST:
+        schedulers.push_back(new oldest_scheduler(
+            m_stats, this, m_scoreboard, m_simt_stack, &m_warp,
+            &m_pipeline_reg[ID_OC_SP], &m_pipeline_reg[ID_OC_DP],
+            &m_pipeline_reg[ID_OC_SFU], &m_pipeline_reg[ID_OC_INT],
+            &m_pipeline_reg[ID_OC_TENSOR_CORE], m_specilized_dispatch_reg,
+            &m_pipeline_reg[ID_OC_MEM], i));
+        break;
+      case CONCRETE_SCHEDULER_WARP_LIMITING:
+        schedulers.push_back(new swl_scheduler(
+            m_stats, this, m_scoreboard, m_simt_stack, &m_warp,
+            &m_pipeline_reg[ID_OC_SP], &m_pipeline_reg[ID_OC_DP],
+            &m_pipeline_reg[ID_OC_SFU], &m_pipeline_reg[ID_OC_INT],
+            &m_pipeline_reg[ID_OC_TENSOR_CORE], m_specilized_dispatch_reg,
+            &m_pipeline_reg[ID_OC_MEM], i, m_config->gpgpu_scheduler_string));
+        break;
+      default:
+        abort();
+    };
+  }
+
+  for (unsigned i = 0; i < m_warp.size(); i++) {
+    // distribute i's evenly though schedulers;
+    schedulers[i % m_config->gpgpu_num_sched_per_core]->add_supervised_warp_id(
+        i);
+  }
+  for (unsigned i = 0; i < m_config->gpgpu_num_sched_per_core; ++i) {
+    schedulers[i]->done_adding_supervised_warps();
+  }
+}
+
+void shader_core_ctx::create_exec_pipeline() {
+  // op collector configuration
+  enum { SP_CUS, DP_CUS, SFU_CUS, TENSOR_CORE_CUS, INT_CUS, MEM_CUS, GEN_CUS };
+
+  opndcoll_rfu_t::port_vector_t in_ports;
+  opndcoll_rfu_t::port_vector_t out_ports;
+  opndcoll_rfu_t::uint_vector_t cu_sets;
+
+  // configure generic collectors
+  m_operand_collector.add_cu_set(
+      GEN_CUS, m_config->gpgpu_operand_collector_num_units_gen,
+      m_config->gpgpu_operand_collector_num_out_ports_gen);
+
+  for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_gen;
+       i++) {
+    in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
+    in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
+    in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
+    out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
+    out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
+    out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
+    if (m_config->gpgpu_tensor_core_avail) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_TENSOR_CORE]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_TENSOR_CORE]);
     }
-    
-    // m_icnt = new shader_memory_interface(this,cluster);
-    if ( m_config->gpgpu_perfect_mem ) {
-        m_icnt = new perfect_memory_interface(this,cluster);
-    } else {
-        m_icnt = new shader_memory_interface(this,cluster);
+    if (m_config->gpgpu_num_dp_units > 0) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_DP]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_DP]);
     }
-    m_mem_fetch_allocator = new shader_core_mem_fetch_allocator(shader_id,tpc_id,mem_config);
-    
-    // fetch
-    m_last_warp_fetched = 0;
-    
-    #define STRSIZE 1024
-    char name[STRSIZE];
-    snprintf(name, STRSIZE, "L1I_%03d", m_sid);
-    m_L1I = new l1icache_gem5(m_gpu, name,m_config->m_L1I_config,m_sid,get_shader_instruction_cache_id(),m_icnt,IN_L1I_MISS_QUEUE);
-    
-    m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));
-    m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader);
-    
-    //scedulers
-    //must currently occur after all inputs have been initialized.
-    std::string sched_config = m_config->gpgpu_scheduler_string;
-    const concrete_scheduler scheduler = sched_config.find("lrr") != std::string::npos ?
-                                         CONCRETE_SCHEDULER_LRR :
-                                         sched_config.find("two_level_active") != std::string::npos ?
-                                         CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE :
-                                         sched_config.find("gto") != std::string::npos ?
-                                         CONCRETE_SCHEDULER_GTO :
-					 sched_config.find("old") != std::string::npos ?
-					 CONCRETE_SCHEDULER_OLDEST_FIRST :
-                                         sched_config.find("warp_limiting") != std::string::npos ?
-                                         CONCRETE_SCHEDULER_WARP_LIMITING:
-                                         NUM_CONCRETE_SCHEDULERS;
-    assert ( scheduler != NUM_CONCRETE_SCHEDULERS );
-    
-    for (int i = 0; i < m_config->gpgpu_num_sched_per_core; i++) {
-        switch( scheduler )
-        {
-            case CONCRETE_SCHEDULER_LRR:
-                schedulers.push_back(
-                    new lrr_scheduler( m_stats,
-                                       this,
-                                       m_scoreboard,
-                                       m_simt_stack,
-                                       &m_warp,
-                                       &m_pipeline_reg[ID_OC_SP],
-									   &m_pipeline_reg[ID_OC_DP],
-                                       &m_pipeline_reg[ID_OC_SFU],
-									   &m_pipeline_reg[ID_OC_INT],
-                                       &m_pipeline_reg[ID_OC_TENSOR_CORE],
-                                       &m_pipeline_reg[ID_OC_MEM],
-                                       i
-                                     )
-                );
-                break;
-            case CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE:
-                schedulers.push_back(
-                    new two_level_active_scheduler( m_stats,
-                                                    this,
-                                                    m_scoreboard,
-                                                    m_simt_stack,
-                                                    &m_warp,
-                                                    &m_pipeline_reg[ID_OC_SP],
-													&m_pipeline_reg[ID_OC_DP],
-                                                    &m_pipeline_reg[ID_OC_SFU],
-													&m_pipeline_reg[ID_OC_INT],
-                                                    &m_pipeline_reg[ID_OC_TENSOR_CORE],
-                                                    &m_pipeline_reg[ID_OC_MEM],
-                                                    i,
-                                                    config->gpgpu_scheduler_string
-                                                  )
-                );
-                break;
-            case CONCRETE_SCHEDULER_GTO:
-                schedulers.push_back(
-                    new gto_scheduler( m_stats,
-                                       this,
-                                       m_scoreboard,
-                                       m_simt_stack,
-                                       &m_warp,
-                                       &m_pipeline_reg[ID_OC_SP],
-									   &m_pipeline_reg[ID_OC_DP],
-                                       &m_pipeline_reg[ID_OC_SFU],
-									   &m_pipeline_reg[ID_OC_INT],
-                                       &m_pipeline_reg[ID_OC_TENSOR_CORE],
-                                       &m_pipeline_reg[ID_OC_MEM],
-                                       i
-                                     )
-                );
-                break;
-            case CONCRETE_SCHEDULER_OLDEST_FIRST:
-				schedulers.push_back(
-		    new oldest_scheduler( m_stats,
-					  this,
-					  m_scoreboard,
-					  m_simt_stack,
-					  &m_warp,
-					  &m_pipeline_reg[ID_OC_SP],
-					  &m_pipeline_reg[ID_OC_DP],
-					  &m_pipeline_reg[ID_OC_SFU],
-					  &m_pipeline_reg[ID_OC_INT],
-                      &m_pipeline_reg[ID_OC_TENSOR_CORE],
-					  &m_pipeline_reg[ID_OC_MEM],
-					  i
-					 )
-				);
-				break;
-            case CONCRETE_SCHEDULER_WARP_LIMITING:
-                schedulers.push_back(
-                    new swl_scheduler( m_stats,
-                                       this,
-                                       m_scoreboard,
-                                       m_simt_stack,
-                                       &m_warp,
-                                       &m_pipeline_reg[ID_OC_SP],
-									   &m_pipeline_reg[ID_OC_DP],
-                                       &m_pipeline_reg[ID_OC_SFU],
-									   &m_pipeline_reg[ID_OC_INT],
-                                       &m_pipeline_reg[ID_OC_TENSOR_CORE],
-                                       &m_pipeline_reg[ID_OC_MEM],
-                                       i,
-                                       config->gpgpu_scheduler_string
-                                     )
-                );
-                break;
-            default:
-                abort();
-        };
+    if (m_config->gpgpu_num_int_units > 0) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_INT]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_INT]);
     }
-    
-    for (unsigned i = 0; i < m_warp.size(); i++) {
-        //distribute i's evenly though schedulers;
-        schedulers[i%m_config->gpgpu_num_sched_per_core]->add_supervised_warp_id(i);
+    if (m_config->m_specialized_unit.size() > 0) {
+      for (unsigned j = 0; j < m_config->m_specialized_unit.size(); ++j) {
+        in_ports.push_back(
+            &m_pipeline_reg[m_config->m_specialized_unit[j].ID_OC_SPEC_ID]);
+        out_ports.push_back(
+            &m_pipeline_reg[m_config->m_specialized_unit[j].OC_EX_SPEC_ID]);
+      }
     }
-    for ( int i = 0; i < m_config->gpgpu_num_sched_per_core; ++i ) {
-        schedulers[i]->done_adding_supervised_warps();
+    cu_sets.push_back((unsigned)GEN_CUS);
+    m_operand_collector.add_port(in_ports, out_ports, cu_sets);
+    in_ports.clear(), out_ports.clear(), cu_sets.clear();
+  }
+
+  if (m_config->enable_specialized_operand_collector) {
+    m_operand_collector.add_cu_set(
+        SP_CUS, m_config->gpgpu_operand_collector_num_units_sp,
+        m_config->gpgpu_operand_collector_num_out_ports_sp);
+    m_operand_collector.add_cu_set(
+        DP_CUS, m_config->gpgpu_operand_collector_num_units_dp,
+        m_config->gpgpu_operand_collector_num_out_ports_dp);
+    m_operand_collector.add_cu_set(
+        TENSOR_CORE_CUS,
+        m_config->gpgpu_operand_collector_num_units_tensor_core,
+        m_config->gpgpu_operand_collector_num_out_ports_tensor_core);
+    m_operand_collector.add_cu_set(
+        SFU_CUS, m_config->gpgpu_operand_collector_num_units_sfu,
+        m_config->gpgpu_operand_collector_num_out_ports_sfu);
+    m_operand_collector.add_cu_set(
+        MEM_CUS, m_config->gpgpu_operand_collector_num_units_mem,
+        m_config->gpgpu_operand_collector_num_out_ports_mem);
+    m_operand_collector.add_cu_set(
+        INT_CUS, m_config->gpgpu_operand_collector_num_units_int,
+        m_config->gpgpu_operand_collector_num_out_ports_int);
+
+    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sp;
+         i++) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
+      cu_sets.push_back((unsigned)SP_CUS);
+      cu_sets.push_back((unsigned)GEN_CUS);
+      m_operand_collector.add_port(in_ports, out_ports, cu_sets);
+      in_ports.clear(), out_ports.clear(), cu_sets.clear();
     }
-    
-    //op collector configuration
-
-	enum { SP_CUS, DP_CUS, SFU_CUS, TENSOR_CORE_CUS, INT_CUS, MEM_CUS,  GEN_CUS };
-
-    opndcoll_rfu_t::port_vector_t in_ports;
-    opndcoll_rfu_t::port_vector_t out_ports;
-    opndcoll_rfu_t::uint_vector_t cu_sets;
-
-    //configure generic collectors
-    m_operand_collector.add_cu_set(GEN_CUS, m_config->gpgpu_operand_collector_num_units_gen, m_config->gpgpu_operand_collector_num_out_ports_gen);
-
-    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_gen; i++) {
-        in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
-        in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
-        in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
-        out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
-        out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
-        out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
-        if(m_config->gpgpu_tensor_core_avail) {
-        	in_ports.push_back(&m_pipeline_reg[ID_OC_TENSOR_CORE]);
-        	out_ports.push_back(&m_pipeline_reg[OC_EX_TENSOR_CORE]);
-        }
-        if(m_config->gpgpu_num_dp_units > 0) {
-			in_ports.push_back(&m_pipeline_reg[ID_OC_DP]);
-			out_ports.push_back(&m_pipeline_reg[OC_EX_DP]);
-        }
-        if(m_config->gpgpu_num_int_units > 0) {
-			in_ports.push_back(&m_pipeline_reg[ID_OC_INT]);
-			out_ports.push_back(&m_pipeline_reg[OC_EX_INT]);
-        }
-        cu_sets.push_back((unsigned)GEN_CUS);
-        m_operand_collector.add_port(in_ports,out_ports,cu_sets);
-        in_ports.clear(),out_ports.clear(),cu_sets.clear();
+
+    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_dp;
+         i++) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_DP]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_DP]);
+      cu_sets.push_back((unsigned)DP_CUS);
+      cu_sets.push_back((unsigned)GEN_CUS);
+      m_operand_collector.add_port(in_ports, out_ports, cu_sets);
+      in_ports.clear(), out_ports.clear(), cu_sets.clear();
     }
 
-    if(m_config->enable_specialized_operand_collector) {
-		m_operand_collector.add_cu_set(SP_CUS, m_config->gpgpu_operand_collector_num_units_sp, m_config->gpgpu_operand_collector_num_out_ports_sp);
-		m_operand_collector.add_cu_set(DP_CUS, m_config->gpgpu_operand_collector_num_units_dp, m_config->gpgpu_operand_collector_num_out_ports_dp);
-	    m_operand_collector.add_cu_set(TENSOR_CORE_CUS, config->gpgpu_operand_collector_num_units_tensor_core, config->gpgpu_operand_collector_num_out_ports_tensor_core);
-	    m_operand_collector.add_cu_set(SFU_CUS, m_config->gpgpu_operand_collector_num_units_sfu, m_config->gpgpu_operand_collector_num_out_ports_sfu);
-		m_operand_collector.add_cu_set(MEM_CUS, m_config->gpgpu_operand_collector_num_units_mem, m_config->gpgpu_operand_collector_num_out_ports_mem);
-		m_operand_collector.add_cu_set(INT_CUS, m_config->gpgpu_operand_collector_num_units_int, m_config->gpgpu_operand_collector_num_out_ports_int);
-
-		for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sp; i++) {
-			in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
-			out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
-			cu_sets.push_back((unsigned)SP_CUS);
-			cu_sets.push_back((unsigned)GEN_CUS);
-			m_operand_collector.add_port(in_ports,out_ports,cu_sets);
-			in_ports.clear(),out_ports.clear(),cu_sets.clear();
-		}
-
-		for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_dp; i++) {
-				in_ports.push_back(&m_pipeline_reg[ID_OC_DP]);
-				out_ports.push_back(&m_pipeline_reg[OC_EX_DP]);
-				cu_sets.push_back((unsigned)DP_CUS);
-				cu_sets.push_back((unsigned)GEN_CUS);
-				m_operand_collector.add_port(in_ports,out_ports,cu_sets);
-				in_ports.clear(),out_ports.clear(),cu_sets.clear();
-			}
-
-		for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sfu; i++) {
-			in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
-			out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
-			cu_sets.push_back((unsigned)SFU_CUS);
-			cu_sets.push_back((unsigned)GEN_CUS);
-			m_operand_collector.add_port(in_ports,out_ports,cu_sets);
-			in_ports.clear(),out_ports.clear(),cu_sets.clear();
-		}
-
-	    for (unsigned i = 0; i < config->gpgpu_operand_collector_num_in_ports_tensor_core; i++) {
-	        in_ports.push_back(&m_pipeline_reg[ID_OC_TENSOR_CORE]);
-	        out_ports.push_back(&m_pipeline_reg[OC_EX_TENSOR_CORE]);
-	        cu_sets.push_back((unsigned)TENSOR_CORE_CUS);
-	        cu_sets.push_back((unsigned)GEN_CUS);
-	        m_operand_collector.add_port(in_ports,out_ports,cu_sets);
-	        in_ports.clear(),out_ports.clear(),cu_sets.clear();
-	    }
-
-		for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_mem; i++) {
-			in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
-			out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
-			cu_sets.push_back((unsigned)MEM_CUS);
-			cu_sets.push_back((unsigned)GEN_CUS);
-			m_operand_collector.add_port(in_ports,out_ports,cu_sets);
-			in_ports.clear(),out_ports.clear(),cu_sets.clear();
-		}
-
-		for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_int; i++) {
-			in_ports.push_back(&m_pipeline_reg[ID_OC_INT]);
-			out_ports.push_back(&m_pipeline_reg[OC_EX_INT]);
-			cu_sets.push_back((unsigned)INT_CUS);
-			cu_sets.push_back((unsigned)GEN_CUS);
-			m_operand_collector.add_port(in_ports,out_ports,cu_sets);
-			in_ports.clear(),out_ports.clear(),cu_sets.clear();
-		}
+    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sfu;
+         i++) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
+      cu_sets.push_back((unsigned)SFU_CUS);
+      cu_sets.push_back((unsigned)GEN_CUS);
+      m_operand_collector.add_port(in_ports, out_ports, cu_sets);
+      in_ports.clear(), out_ports.clear(), cu_sets.clear();
     }
-    
-    m_operand_collector.init( m_config->gpgpu_num_reg_banks, this );
-    
-    m_num_function_units = m_config->gpgpu_num_sp_units + m_config->gpgpu_num_dp_units + m_config->gpgpu_num_sfu_units + m_config->gpgpu_num_tensor_core_units + m_config->gpgpu_num_int_units + 1; // sp_unit, sfu, dp, tensor, int, ldst_unit
-    //m_dispatch_port = new enum pipeline_stage_name_t[ m_num_function_units ];
-    //m_issue_port = new enum pipeline_stage_name_t[ m_num_function_units ];
-    
-    //m_fu = new simd_function_unit*[m_num_function_units];
-    
-    for (int k = 0; k < m_config->gpgpu_num_sp_units; k++) {
-        m_fu.push_back(new sp_unit( &m_pipeline_reg[EX_WB], m_config, this ));
-        m_dispatch_port.push_back(ID_OC_SP);
-        m_issue_port.push_back(OC_EX_SP);
+
+    for (unsigned i = 0;
+         i < m_config->gpgpu_operand_collector_num_in_ports_tensor_core; i++) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_TENSOR_CORE]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_TENSOR_CORE]);
+      cu_sets.push_back((unsigned)TENSOR_CORE_CUS);
+      cu_sets.push_back((unsigned)GEN_CUS);
+      m_operand_collector.add_port(in_ports, out_ports, cu_sets);
+      in_ports.clear(), out_ports.clear(), cu_sets.clear();
     }
-    
-    for (int k = 0; k < m_config->gpgpu_num_dp_units; k++) {
-            m_fu.push_back(new dp_unit( &m_pipeline_reg[EX_WB], m_config, this ));
-            m_dispatch_port.push_back(ID_OC_DP);
-            m_issue_port.push_back(OC_EX_DP);
-        }
-    for (int k = 0; k < m_config->gpgpu_num_int_units; k++) {
-            m_fu.push_back(new int_unit( &m_pipeline_reg[EX_WB], m_config, this ));
-            m_dispatch_port.push_back(ID_OC_INT);
-            m_issue_port.push_back(OC_EX_INT);
-        }
 
-    for (int k = 0; k < m_config->gpgpu_num_sfu_units; k++) {
-        m_fu.push_back(new sfu( &m_pipeline_reg[EX_WB], m_config, this ));
-        m_dispatch_port.push_back(ID_OC_SFU);
-        m_issue_port.push_back(OC_EX_SFU);
+    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_mem;
+         i++) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
+      cu_sets.push_back((unsigned)MEM_CUS);
+      cu_sets.push_back((unsigned)GEN_CUS);
+      m_operand_collector.add_port(in_ports, out_ports, cu_sets);
+      in_ports.clear(), out_ports.clear(), cu_sets.clear();
     }
-       
-    for (int k = 0; k < config->gpgpu_num_tensor_core_units; k++) {
-        m_fu.push_back(new tensor_core( &m_pipeline_reg[EX_WB], m_config, this ));
-        m_dispatch_port.push_back(ID_OC_TENSOR_CORE);
-        m_issue_port.push_back(OC_EX_TENSOR_CORE);
+
+    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_int;
+         i++) {
+      in_ports.push_back(&m_pipeline_reg[ID_OC_INT]);
+      out_ports.push_back(&m_pipeline_reg[OC_EX_INT]);
+      cu_sets.push_back((unsigned)INT_CUS);
+      cu_sets.push_back((unsigned)GEN_CUS);
+      m_operand_collector.add_port(in_ports, out_ports, cu_sets);
+      in_ports.clear(), out_ports.clear(), cu_sets.clear();
     }
+  }
+
+  m_operand_collector.init(m_config->gpgpu_num_reg_banks, this);
+
+  m_num_function_units =
+      m_config->gpgpu_num_sp_units + m_config->gpgpu_num_dp_units +
+      m_config->gpgpu_num_sfu_units + m_config->gpgpu_num_tensor_core_units +
+      m_config->gpgpu_num_int_units + m_config->m_specialized_unit_num +
+      1;  // sp_unit, sfu, dp, tensor, int, ldst_unit
+  // m_dispatch_port = new enum pipeline_stage_name_t[ m_num_function_units ];
+  // m_issue_port = new enum pipeline_stage_name_t[ m_num_function_units ];
+
+  // m_fu = new simd_function_unit*[m_num_function_units];
+
+  for (unsigned k = 0; k < m_config->gpgpu_num_sp_units; k++) {
+    m_fu.push_back(new sp_unit(&m_pipeline_reg[EX_WB], m_config, this, k));
+    m_dispatch_port.push_back(ID_OC_SP);
+    m_issue_port.push_back(OC_EX_SP);
+  }
+
+  for (unsigned k = 0; k < m_config->gpgpu_num_dp_units; k++) {
+    m_fu.push_back(new dp_unit(&m_pipeline_reg[EX_WB], m_config, this, k));
+    m_dispatch_port.push_back(ID_OC_DP);
+    m_issue_port.push_back(OC_EX_DP);
+  }
+  for (unsigned k = 0; k < m_config->gpgpu_num_int_units; k++) {
+    m_fu.push_back(new int_unit(&m_pipeline_reg[EX_WB], m_config, this, k));
+    m_dispatch_port.push_back(ID_OC_INT);
+    m_issue_port.push_back(OC_EX_INT);
+  }
+
+  for (unsigned k = 0; k < m_config->gpgpu_num_sfu_units; k++) {
+    m_fu.push_back(new sfu(&m_pipeline_reg[EX_WB], m_config, this, k));
+    m_dispatch_port.push_back(ID_OC_SFU);
+    m_issue_port.push_back(OC_EX_SFU);
+  }
 
-    m_ldst_unit = new ldst_unit( m_icnt, m_mem_fetch_allocator, this, &m_operand_collector, m_scoreboard, config, mem_config, stats, shader_id, tpc_id );
-    m_fu.push_back(m_ldst_unit);
-    m_dispatch_port.push_back(ID_OC_MEM);
-    m_issue_port.push_back(OC_EX_MEM);
-   
-    assert(m_num_function_units == m_fu.size() and m_fu.size() == m_dispatch_port.size() and m_fu.size() == m_issue_port.size());
-    
-    //there are as many result buses as the width of the EX_WB stage
-    num_result_bus = config->pipe_widths[EX_WB];
-    for(unsigned i=0; i<num_result_bus; i++){
-        this->m_result_bus.push_back(new std::bitset<MAX_ALU_LATENCY>());
+  for (unsigned k = 0; k < m_config->gpgpu_num_tensor_core_units; k++) {
+    m_fu.push_back(new tensor_core(&m_pipeline_reg[EX_WB], m_config, this, k));
+    m_dispatch_port.push_back(ID_OC_TENSOR_CORE);
+    m_issue_port.push_back(OC_EX_TENSOR_CORE);
+  }
+
+  for (unsigned j = 0; j < m_config->m_specialized_unit.size(); j++) {
+    for (unsigned k = 0; k < m_config->m_specialized_unit[j].num_units; k++) {
+      m_fu.push_back(new specialized_unit(
+          &m_pipeline_reg[EX_WB], m_config, this, SPEC_UNIT_START_ID + j,
+          m_config->m_specialized_unit[j].name,
+          m_config->m_specialized_unit[j].latency, k));
+      m_dispatch_port.push_back(m_config->m_specialized_unit[j].ID_OC_SPEC_ID);
+      m_issue_port.push_back(m_config->m_specialized_unit[j].OC_EX_SPEC_ID);
     }
-    
-    m_last_inst_gpu_sim_cycle = 0;
-    m_last_inst_gpu_tot_sim_cycle = 0;
+  }
+
+  m_ldst_unit = new ldst_unit(m_icnt, m_mem_fetch_allocator, this,
+                              &m_operand_collector, m_scoreboard, m_config,
+                              m_memory_config, m_stats, m_sid, m_tpc);
+  m_fu.push_back(m_ldst_unit);
+  m_dispatch_port.push_back(ID_OC_MEM);
+  m_issue_port.push_back(OC_EX_MEM);
+
+  assert(m_num_function_units == m_fu.size() and
+         m_fu.size() == m_dispatch_port.size() and
+         m_fu.size() == m_issue_port.size());
+
+  // there are as many result buses as the width of the EX_WB stage
+  num_result_bus = m_config->pipe_widths[EX_WB];
+  for (unsigned i = 0; i < num_result_bus; i++) {
+    this->m_result_bus.push_back(new std::bitset<MAX_ALU_LATENCY>());
+  }
+}
+
+shader_core_ctx::shader_core_ctx(class gpgpu_sim *gpu,
+                                 class simt_core_cluster *cluster,
+                                 unsigned shader_id, unsigned tpc_id,
+                                 const shader_core_config *config,
+                                 const memory_config *mem_config,
+                                 shader_core_stats *stats)
+    : core_t(gpu, NULL, config->warp_size, config->n_thread_per_shader),
+      m_barriers(this, config->max_warps_per_shader, config->max_cta_per_core,
+                 config->max_barriers_per_cta, config->warp_size),
+      m_active_warps(0),
+      m_dynamic_warp_id(0) {
+    // TODO schi 
+    m_kernel_finishing = false;
+
+  m_cluster = cluster;
+  m_config = config;
+  m_memory_config = mem_config;
+  m_stats = stats;
+  unsigned warp_size = config->warp_size;
+  Issue_Prio = 0;
+
+  m_sid = shader_id;
+  m_tpc = tpc_id;
+
+  if(get_gpu()->get_config().g_power_simulation_enabled){
+    scaling_coeffs =  get_gpu()->get_scaling_coeffs();
+  }
+
+  m_last_inst_gpu_sim_cycle = 0;
+  m_last_inst_gpu_tot_sim_cycle = 0;
+
+  // Jin: for concurrent kernels on a SM
+  m_occupied_n_threads = 0;
+  m_occupied_shmem = 0;
+  m_occupied_regs = 0;
+  m_occupied_ctas = 0;
+  m_occupied_hwtid.reset();
+  m_occupied_cta_to_hwtid.clear();
+}
 
-    //Jin: for concurrent kernels on a SM
+void shader_core_ctx::reinit(unsigned start_thread, unsigned end_thread,
+                             bool reset_not_completed) {
+  if (reset_not_completed) {
+    m_not_completed = 0;
+    m_active_threads.reset();
+
+    // Jin: for concurrent kernels on a SM
     m_occupied_n_threads = 0;
     m_occupied_shmem = 0;
     m_occupied_regs = 0;
     m_occupied_ctas = 0;
     m_occupied_hwtid.reset();
     m_occupied_cta_to_hwtid.clear();
+    m_active_warps = 0;
+  }
+  for (unsigned i = start_thread; i < end_thread; i++) {
+    m_threadState[i].n_insn = 0;
+    m_threadState[i].m_cta_id = -1;
+  }
+  for (unsigned i = start_thread / m_config->warp_size;
+       i < end_thread / m_config->warp_size; ++i) {
+    m_warp[i]->reset();
+    m_simt_stack[i]->reset();
+  }
 }
 
-void shader_core_ctx::reinit(unsigned start_thread, unsigned end_thread, bool reset_not_completed ) 
-{
-   if( reset_not_completed ) {
-       m_not_completed = 0;
-       m_active_threads.reset();
-
-       //Jin: for concurrent kernels on a SM
-       m_occupied_n_threads = 0;
-       m_occupied_shmem = 0;
-       m_occupied_regs = 0;
-       m_occupied_ctas = 0;
-       m_occupied_hwtid.reset();
-       m_occupied_cta_to_hwtid.clear();
-       m_active_warps = 0;
-
-   }
-   for (unsigned i = start_thread; i<end_thread; i++) {
-      m_threadState[i].n_insn = 0;
-      m_threadState[i].m_cta_id = -1;
-   }
-   for (unsigned i = start_thread / m_config->warp_size; i < end_thread / m_config->warp_size; ++i) {
-      m_warp[i].reset();
-      m_simt_stack[i]->reset();
-   }
-}
-
-void shader_core_ctx::init_warps( unsigned cta_id, unsigned start_thread, unsigned end_thread, unsigned ctaid, int cta_size, unsigned kernel_id )
-{
-    address_type start_pc = next_pc(start_thread);
-    if (m_config->model == POST_DOMINATOR) {
-        unsigned start_warp = start_thread / m_config->warp_size;
-        unsigned warp_per_cta =  cta_size / m_config->warp_size;
-        unsigned end_warp = end_thread / m_config->warp_size + ((end_thread % m_config->warp_size)? 1 : 0);
-        for (unsigned i = start_warp; i < end_warp; ++i) {
-            unsigned n_active=0;
-            simt_mask_t active_threads;
-            for (unsigned t = 0; t < m_config->warp_size; t++) {
-                unsigned hwtid = i * m_config->warp_size + t;
-                if ( hwtid < end_thread ) {
-                    n_active++;
-                    assert( !m_active_threads.test(hwtid) );
-                    m_active_threads.set( hwtid );
-                    active_threads.set(t);
-                }
-            }
-            m_simt_stack[i]->launch(start_pc,active_threads);
-
-              if(m_gpu->resume_option==1 && kernel_id==m_gpu->resume_kernel && ctaid>=m_gpu->resume_CTA && ctaid<m_gpu->checkpoint_CTA_t )
-               { 
-                char fname[2048];
-                snprintf(fname,2048,"checkpoint_files/warp_%d_%d_simt.txt",i%warp_per_cta,ctaid );
-                unsigned pc,rpc;
-                m_simt_stack[i]->resume(fname);
-                m_simt_stack[i]->get_pdom_stack_top_info(&pc,&rpc);
-                for (unsigned t = 0; t < m_config->warp_size; t++) {
-                  m_thread[i * m_config->warp_size + t]->set_npc(pc);
-                  m_thread[i * m_config->warp_size + t]->update_pc();
-                }   
-                start_pc=pc;
-              }
-               
-            m_warp[i].init(start_pc,cta_id,i,active_threads, m_dynamic_warp_id);
-            ++m_dynamic_warp_id;
-            m_not_completed += n_active;
-            ++m_active_warps;
+void shader_core_ctx::init_warps(unsigned cta_id, unsigned start_thread,
+                                 unsigned end_thread, unsigned ctaid,
+                                 int cta_size, kernel_info_t &kernel) {
+  //
+  address_type start_pc = next_pc(start_thread);
+  unsigned kernel_id = kernel.get_uid();
+  if (m_config->model == POST_DOMINATOR) {
+    unsigned start_warp = start_thread / m_config->warp_size;
+    unsigned warp_per_cta = cta_size / m_config->warp_size;
+    unsigned end_warp = end_thread / m_config->warp_size +
+                        ((end_thread % m_config->warp_size) ? 1 : 0);
+    for (unsigned i = start_warp; i < end_warp; ++i) {
+      unsigned n_active = 0;
+      simt_mask_t active_threads;
+      for (unsigned t = 0; t < m_config->warp_size; t++) {
+        unsigned hwtid = i * m_config->warp_size + t;
+        if (hwtid < end_thread) {
+          n_active++;
+          assert(!m_active_threads.test(hwtid));
+          m_active_threads.set(hwtid);
+          active_threads.set(t);
+        }
       }
-   }
+      m_simt_stack[i]->launch(start_pc, active_threads);
+
+      if (m_gpu->resume_option == 1 && kernel_id == m_gpu->resume_kernel &&
+          ctaid >= m_gpu->resume_CTA && ctaid < m_gpu->checkpoint_CTA_t) {
+        char fname[2048];
+        snprintf(fname, 2048, "checkpoint_files/warp_%d_%d_simt.txt",
+                 i % warp_per_cta, ctaid);
+        unsigned pc, rpc;
+        m_simt_stack[i]->resume(fname);
+        m_simt_stack[i]->get_pdom_stack_top_info(&pc, &rpc);
+        for (unsigned t = 0; t < m_config->warp_size; t++) {
+          if (m_thread != NULL) {
+            m_thread[i * m_config->warp_size + t]->set_npc(pc);
+            m_thread[i * m_config->warp_size + t]->update_pc();
+          }
+        }
+        start_pc = pc;
+      }
+
+      m_warp[i]->init(start_pc, cta_id, i, active_threads, m_dynamic_warp_id);
+      ++m_dynamic_warp_id;
+      m_not_completed += n_active;
+      ++m_active_warps;
+    }
+  }
 }
 
-// return the next pc of a thread 
-address_type shader_core_ctx::next_pc( int tid ) const
-{
-    if( tid == -1 ) 
-        return -1;
-    ptx_thread_info *the_thread = m_thread[tid];
-    if ( the_thread == NULL )
-        return -1;
-    return the_thread->get_pc(); // PC should already be updatd to next PC at this point (was set in shader_decode() last time thread ran)
+// return the next pc of a thread
+address_type shader_core_ctx::next_pc(int tid) const {
+  if (tid == -1) return -1;
+  ptx_thread_info *the_thread = m_thread[tid];
+  if (the_thread == NULL) return -1;
+  return the_thread
+      ->get_pc();  // PC should already be updatd to next PC at this point (was
+                   // set in shader_decode() last time thread ran)
 }
 
-void gpgpu_sim::get_pdom_stack_top_info( unsigned sid, unsigned tid, unsigned *pc, unsigned *rpc )
-{
-    unsigned cluster_id = m_shader_config->sid_to_cluster(sid);
-    m_cluster[cluster_id]->get_pdom_stack_top_info(sid,tid,pc,rpc);
+void gpgpu_sim::get_pdom_stack_top_info(unsigned sid, unsigned tid,
+                                        unsigned *pc, unsigned *rpc) {
+  unsigned cluster_id = m_shader_config->sid_to_cluster(sid);
+  m_cluster[cluster_id]->get_pdom_stack_top_info(sid, tid, pc, rpc);
 }
 
-void shader_core_ctx::get_pdom_stack_top_info( unsigned tid, unsigned *pc, unsigned *rpc ) const
-{
-    unsigned warp_id = tid/m_config->warp_size;
-    m_simt_stack[warp_id]->get_pdom_stack_top_info(pc,rpc);
+void shader_core_ctx::get_pdom_stack_top_info(unsigned tid, unsigned *pc,
+                                              unsigned *rpc) const {
+  unsigned warp_id = tid / m_config->warp_size;
+  m_simt_stack[warp_id]->get_pdom_stack_top_info(pc, rpc);
 }
 
-float shader_core_ctx::get_current_occupancy( unsigned long long & active, unsigned long long & total ) const
-{
-    // To match the achieved_occupancy in nvprof, only SMs that are active are counted toward the occupancy.
-    if ( m_active_warps > 0 ) {
-        total += m_warp.size();
-        active += m_active_warps;
-        return float(active) / float(total);
-    } else {
-        return 0;
-    }
+float shader_core_ctx::get_current_occupancy(unsigned long long &active,
+                                             unsigned long long &total) const {
+  // To match the achieved_occupancy in nvprof, only SMs that are active are
+  // counted toward the occupancy.
+  if (m_active_warps > 0) {
+    total += m_warp.size();
+    active += m_active_warps;
+    return float(active) / float(total);
+  } else {
+    return 0;
+  }
 }
 
-void shader_core_stats::print( FILE* fout ) const
-{
-	unsigned long long  thread_icount_uarch=0;
-	unsigned long long  warp_icount_uarch=0;
+void shader_core_stats::print(FILE *fout) const {
+  unsigned long long thread_icount_uarch = 0;
+  unsigned long long warp_icount_uarch = 0;
 
-    for(unsigned i=0; i < m_config->num_shader(); i++) {
-        thread_icount_uarch += m_num_sim_insn[i];
-        warp_icount_uarch += m_num_sim_winsn[i];
+  for (unsigned i = 0; i < m_config->num_shader(); i++) {
+    thread_icount_uarch += m_num_sim_insn[i];
+    warp_icount_uarch += m_num_sim_winsn[i];
+  }
+  fprintf(fout, "gpgpu_n_tot_thrd_icount = %lld\n", thread_icount_uarch);
+  fprintf(fout, "gpgpu_n_tot_w_icount = %lld\n", warp_icount_uarch);
+
+  fprintf(fout, "gpgpu_n_stall_shd_mem = %d\n", gpgpu_n_stall_shd_mem);
+  fprintf(fout, "gpgpu_n_mem_read_local = %d\n", gpgpu_n_mem_read_local);
+  fprintf(fout, "gpgpu_n_mem_write_local = %d\n", gpgpu_n_mem_write_local);
+  fprintf(fout, "gpgpu_n_mem_read_global = %d\n", gpgpu_n_mem_read_global);
+  fprintf(fout, "gpgpu_n_mem_write_global = %d\n", gpgpu_n_mem_write_global);
+  fprintf(fout, "gpgpu_n_mem_texture = %d\n", gpgpu_n_mem_texture);
+  fprintf(fout, "gpgpu_n_mem_const = %d\n", gpgpu_n_mem_const);
+
+  fprintf(fout, "gpgpu_n_load_insn  = %d\n", gpgpu_n_load_insn);
+  fprintf(fout, "gpgpu_n_store_insn = %d\n", gpgpu_n_store_insn);
+  fprintf(fout, "gpgpu_n_shmem_insn = %d\n", gpgpu_n_shmem_insn);
+  fprintf(fout, "gpgpu_n_sstarr_insn = %d\n", gpgpu_n_sstarr_insn);
+  fprintf(fout, "gpgpu_n_tex_insn = %d\n", gpgpu_n_tex_insn);
+  fprintf(fout, "gpgpu_n_const_mem_insn = %d\n", gpgpu_n_const_insn);
+  fprintf(fout, "gpgpu_n_param_mem_insn = %d\n", gpgpu_n_param_insn);
+
+  fprintf(fout, "gpgpu_n_shmem_bkconflict = %d\n", gpgpu_n_shmem_bkconflict);
+  fprintf(fout, "gpgpu_n_cache_bkconflict = %d\n", gpgpu_n_cache_bkconflict);
+
+  fprintf(fout, "gpgpu_n_intrawarp_mshr_merge = %d\n",
+          gpgpu_n_intrawarp_mshr_merge);
+  fprintf(fout, "gpgpu_n_cmem_portconflict = %d\n", gpgpu_n_cmem_portconflict);
+
+  fprintf(fout, "gpgpu_stall_shd_mem[c_mem][resource_stall] = %d\n",
+          gpu_stall_shd_mem_breakdown[C_MEM][BK_CONF]);
+  // fprintf(fout, "gpgpu_stall_shd_mem[c_mem][mshr_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[C_MEM][MSHR_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[c_mem][icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[C_MEM][ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[c_mem][data_port_stall] = %d\n",
+  // gpu_stall_shd_mem_breakdown[C_MEM][DATA_PORT_STALL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[t_mem][mshr_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[T_MEM][MSHR_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[t_mem][icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[T_MEM][ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[t_mem][data_port_stall] = %d\n",
+  // gpu_stall_shd_mem_breakdown[T_MEM][DATA_PORT_STALL]);
+  fprintf(fout, "gpgpu_stall_shd_mem[s_mem][bk_conf] = %d\n",
+          gpu_stall_shd_mem_breakdown[S_MEM][BK_CONF]);
+  fprintf(
+      fout, "gpgpu_stall_shd_mem[gl_mem][resource_stall] = %d\n",
+      gpu_stall_shd_mem_breakdown[G_MEM_LD][BK_CONF] +
+          gpu_stall_shd_mem_breakdown[G_MEM_ST][BK_CONF] +
+          gpu_stall_shd_mem_breakdown[L_MEM_LD][BK_CONF] +
+          gpu_stall_shd_mem_breakdown[L_MEM_ST][BK_CONF]);  // coalescing stall
+                                                            // at data cache
+  fprintf(
+      fout, "gpgpu_stall_shd_mem[gl_mem][coal_stall] = %d\n",
+      gpu_stall_shd_mem_breakdown[G_MEM_LD][COAL_STALL] +
+          gpu_stall_shd_mem_breakdown[G_MEM_ST][COAL_STALL] +
+          gpu_stall_shd_mem_breakdown[L_MEM_LD][COAL_STALL] +
+          gpu_stall_shd_mem_breakdown[L_MEM_ST]
+                                     [COAL_STALL]);  // coalescing stall + bank
+                                                     // conflict at data cache
+  fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][data_port_stall] = %d\n",
+          gpu_stall_shd_mem_breakdown[G_MEM_LD][DATA_PORT_STALL] +
+              gpu_stall_shd_mem_breakdown[G_MEM_ST][DATA_PORT_STALL] +
+              gpu_stall_shd_mem_breakdown[L_MEM_LD][DATA_PORT_STALL] +
+              gpu_stall_shd_mem_breakdown[L_MEM_ST]
+                                         [DATA_PORT_STALL]);  // data port stall
+                                                              // at data cache
+  // fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][mshr_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_LD][MSHR_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[g_mem_ld][icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_LD][ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[g_mem_ld][wb_icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[g_mem_ld][wb_rsrv_fail] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_CACHE_RSRV_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[g_mem_st][mshr_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_ST][MSHR_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[g_mem_st][icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_ST][ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[g_mem_st][wb_icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[g_mem_st][wb_rsrv_fail] = %d\n",
+  // gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_CACHE_RSRV_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_ld][mshr_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_LD][MSHR_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_ld][icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_LD][ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_CACHE_RSRV_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_st][mshr_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_ST][MSHR_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_st][icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_ST][ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_ICNT_RC_FAIL]); fprintf(fout,
+  // "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n",
+  // gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_CACHE_RSRV_FAIL]);
+
+  fprintf(fout, "gpu_reg_bank_conflict_stalls = %d\n",
+          gpu_reg_bank_conflict_stalls);
+
+  fprintf(fout, "Warp Occupancy Distribution:\n");
+  fprintf(fout, "Stall:%d\t", shader_cycle_distro[2]);
+  fprintf(fout, "W0_Idle:%d\t", shader_cycle_distro[0]);
+  fprintf(fout, "W0_Scoreboard:%d", shader_cycle_distro[1]);
+  for (unsigned i = 3; i < m_config->warp_size + 3; i++)
+    fprintf(fout, "\tW%d:%d", i - 2, shader_cycle_distro[i]);
+  fprintf(fout, "\n");
+  fprintf(fout, "single_issue_nums: ");
+  for (unsigned i = 0; i < m_config->gpgpu_num_sched_per_core; i++)
+    fprintf(fout, "WS%d:%d\t", i, single_issue_nums[i]);
+  fprintf(fout, "\n");
+  fprintf(fout, "dual_issue_nums: ");
+  for (unsigned i = 0; i < m_config->gpgpu_num_sched_per_core; i++)
+    fprintf(fout, "WS%d:%d\t", i, dual_issue_nums[i]);
+  fprintf(fout, "\n");
+
+  m_outgoing_traffic_stats->print(fout);
+  m_incoming_traffic_stats->print(fout);
+}
+
+void shader_core_stats::event_warp_issued(unsigned s_id, unsigned warp_id,
+                                          unsigned num_issued,
+                                          unsigned dynamic_warp_id) {
+  assert(warp_id <= m_config->max_warps_per_shader);
+  for (unsigned i = 0; i < num_issued; ++i) {
+    if (m_shader_dynamic_warp_issue_distro[s_id].size() <= dynamic_warp_id) {
+      m_shader_dynamic_warp_issue_distro[s_id].resize(dynamic_warp_id + 1);
     }
-    fprintf(fout,"gpgpu_n_tot_thrd_icount = %lld\n", thread_icount_uarch);
-    fprintf(fout,"gpgpu_n_tot_w_icount = %lld\n", warp_icount_uarch);
-
-    fprintf(fout,"gpgpu_n_stall_shd_mem = %d\n", gpgpu_n_stall_shd_mem );
-    fprintf(fout,"gpgpu_n_mem_read_local = %d\n", gpgpu_n_mem_read_local);
-    fprintf(fout,"gpgpu_n_mem_write_local = %d\n", gpgpu_n_mem_write_local);
-    fprintf(fout,"gpgpu_n_mem_read_global = %d\n", gpgpu_n_mem_read_global);
-    fprintf(fout,"gpgpu_n_mem_write_global = %d\n", gpgpu_n_mem_write_global);
-    fprintf(fout,"gpgpu_n_mem_texture = %d\n", gpgpu_n_mem_texture);
-    fprintf(fout,"gpgpu_n_mem_const = %d\n", gpgpu_n_mem_const);
-
-   fprintf(fout, "gpgpu_n_load_insn  = %d\n", gpgpu_n_load_insn);
-   fprintf(fout, "gpgpu_n_store_insn = %d\n", gpgpu_n_store_insn);
-   fprintf(fout, "gpgpu_n_shmem_insn = %d\n", gpgpu_n_shmem_insn);
-   fprintf(fout, "gpgpu_n_sstarr_insn = %d\n", gpgpu_n_sstarr_insn);
-   fprintf(fout, "gpgpu_n_tex_insn = %d\n", gpgpu_n_tex_insn);
-   fprintf(fout, "gpgpu_n_const_mem_insn = %d\n", gpgpu_n_const_insn);
-   fprintf(fout, "gpgpu_n_param_mem_insn = %d\n", gpgpu_n_param_insn);
-
-   fprintf(fout, "gpgpu_n_shmem_bkconflict = %d\n", gpgpu_n_shmem_bkconflict);
-   fprintf(fout, "gpgpu_n_cache_bkconflict = %d\n", gpgpu_n_cache_bkconflict);   
-
-   fprintf(fout, "gpgpu_n_intrawarp_mshr_merge = %d\n", gpgpu_n_intrawarp_mshr_merge);
-   fprintf(fout, "gpgpu_n_cmem_portconflict = %d\n", gpgpu_n_cmem_portconflict);
-
-   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][resource_stall] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][BK_CONF]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[c_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][MSHR_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[c_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[c_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][DATA_PORT_STALL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[t_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][MSHR_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[t_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[t_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][DATA_PORT_STALL]);
-   fprintf(fout, "gpgpu_stall_shd_mem[s_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[S_MEM][BK_CONF]);
-   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][resource_stall] = %d\n",
-           gpu_stall_shd_mem_breakdown[G_MEM_LD][BK_CONF] + 
-           gpu_stall_shd_mem_breakdown[G_MEM_ST][BK_CONF] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_LD][BK_CONF] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_ST][BK_CONF]   
-           ); // coalescing stall at data cache 
-   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][coal_stall] = %d\n", 
-           gpu_stall_shd_mem_breakdown[G_MEM_LD][COAL_STALL] + 
-           gpu_stall_shd_mem_breakdown[G_MEM_ST][COAL_STALL] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_LD][COAL_STALL] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_ST][COAL_STALL]    
-           ); // coalescing stall + bank conflict at data cache 
-   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][data_port_stall] = %d\n", 
-           gpu_stall_shd_mem_breakdown[G_MEM_LD][DATA_PORT_STALL] + 
-           gpu_stall_shd_mem_breakdown[G_MEM_ST][DATA_PORT_STALL] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_LD][DATA_PORT_STALL] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_ST][DATA_PORT_STALL]    
-           ); // data port stall at data cache 
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][MSHR_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_CACHE_RSRV_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][MSHR_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_CACHE_RSRV_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][MSHR_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_CACHE_RSRV_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][MSHR_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_ICNT_RC_FAIL]);
-   //fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_CACHE_RSRV_FAIL]);
-
-   fprintf(fout, "gpu_reg_bank_conflict_stalls = %d\n", gpu_reg_bank_conflict_stalls);
-
-   fprintf(fout, "Warp Occupancy Distribution:\n");
-   fprintf(fout, "Stall:%d\t", shader_cycle_distro[2]);
-   fprintf(fout, "W0_Idle:%d\t", shader_cycle_distro[0]);
-   fprintf(fout, "W0_Scoreboard:%d", shader_cycle_distro[1]);
-   for (unsigned i = 3; i < m_config->warp_size + 3; i++) 
-      fprintf(fout, "\tW%d:%d", i-2, shader_cycle_distro[i]);
-   fprintf(fout, "\n");
-   fprintf(fout, "single_issue_nums: ");
-   for (unsigned i = 0; i < m_config->gpgpu_num_sched_per_core; i++)
-        fprintf(fout, "WS%d:%d\t", i, single_issue_nums[i]);
-   fprintf(fout, "\n");
-   fprintf(fout, "dual_issue_nums: ");
-   for (unsigned i = 0; i < m_config->gpgpu_num_sched_per_core; i++)
-          fprintf(fout, "WS%d:%d\t", i, dual_issue_nums[i]);
-   fprintf(fout, "\n");
-
-   m_outgoing_traffic_stats->print(fout); 
-   m_incoming_traffic_stats->print(fout); 
-}
-
-void shader_core_stats::event_warp_issued( unsigned s_id, unsigned warp_id, unsigned num_issued, unsigned dynamic_warp_id ) {
-    assert( warp_id <= m_config->max_warps_per_shader );
-    for ( unsigned i = 0; i < num_issued; ++i ) {
-        if ( m_shader_dynamic_warp_issue_distro[ s_id ].size() <= dynamic_warp_id ) {
-            m_shader_dynamic_warp_issue_distro[ s_id ].resize(dynamic_warp_id + 1);
-        }
-        ++m_shader_dynamic_warp_issue_distro[ s_id ][ dynamic_warp_id ];
-        if ( m_shader_warp_slot_issue_distro[ s_id ].size() <= warp_id ) {
-            m_shader_warp_slot_issue_distro[ s_id ].resize(warp_id + 1);
-        }
-        ++m_shader_warp_slot_issue_distro[ s_id ][ warp_id ];
+    ++m_shader_dynamic_warp_issue_distro[s_id][dynamic_warp_id];
+    if (m_shader_warp_slot_issue_distro[s_id].size() <= warp_id) {
+      m_shader_warp_slot_issue_distro[s_id].resize(warp_id + 1);
     }
+    ++m_shader_warp_slot_issue_distro[s_id][warp_id];
+  }
 }
 
-void shader_core_stats::visualizer_print( gzFile visualizer_file )
-{
-    // warp divergence breakdown
-    gzprintf(visualizer_file, "WarpDivergenceBreakdown:");
-    unsigned int total=0;
-    unsigned int cf = (m_config->gpgpu_warpdistro_shader==-1)?m_config->num_shader():1;
-    gzprintf(visualizer_file, " %d", (shader_cycle_distro[0] - last_shader_cycle_distro[0]) / cf );
-    gzprintf(visualizer_file, " %d", (shader_cycle_distro[1] - last_shader_cycle_distro[1]) / cf );
-    gzprintf(visualizer_file, " %d", (shader_cycle_distro[2] - last_shader_cycle_distro[2]) / cf );
-    for (unsigned i=0; i<m_config->warp_size+3; i++) {
-       if ( i>=3 ) {
-          total += (shader_cycle_distro[i] - last_shader_cycle_distro[i]);
-          if ( ((i-3) % (m_config->warp_size/8)) == ((m_config->warp_size/8)-1) ) {
-             gzprintf(visualizer_file, " %d", total / cf );
-             total=0;
-          }
-       }
-       last_shader_cycle_distro[i] = shader_cycle_distro[i];
+void shader_core_stats::visualizer_print(gzFile visualizer_file) {
+  // warp divergence breakdown
+  gzprintf(visualizer_file, "WarpDivergenceBreakdown:");
+  unsigned int total = 0;
+  unsigned int cf =
+      (m_config->gpgpu_warpdistro_shader == -1) ? m_config->num_shader() : 1;
+  gzprintf(visualizer_file, " %d",
+           (shader_cycle_distro[0] - last_shader_cycle_distro[0]) / cf);
+  gzprintf(visualizer_file, " %d",
+           (shader_cycle_distro[1] - last_shader_cycle_distro[1]) / cf);
+  gzprintf(visualizer_file, " %d",
+           (shader_cycle_distro[2] - last_shader_cycle_distro[2]) / cf);
+  for (unsigned i = 0; i < m_config->warp_size + 3; i++) {
+    if (i >= 3) {
+      total += (shader_cycle_distro[i] - last_shader_cycle_distro[i]);
+      if (((i - 3) % (m_config->warp_size / 8)) ==
+          ((m_config->warp_size / 8) - 1)) {
+        gzprintf(visualizer_file, " %d", total / cf);
+        total = 0;
+      }
     }
-    gzprintf(visualizer_file,"\n");
-
-    // warp issue breakdown
-    unsigned sid = m_config->gpgpu_warp_issue_shader;
-    unsigned count = 0;
-    unsigned warp_id_issued_sum = 0;
-    gzprintf(visualizer_file, "WarpIssueSlotBreakdown:");
-    if(m_shader_warp_slot_issue_distro[sid].size() > 0){
-        for ( std::vector<unsigned>::const_iterator iter = m_shader_warp_slot_issue_distro[ sid ].begin();
-              iter != m_shader_warp_slot_issue_distro[ sid ].end(); iter++, count++ ) {
-            unsigned diff = count < m_last_shader_warp_slot_issue_distro.size() ?
-                            *iter - m_last_shader_warp_slot_issue_distro[ count ] :
-                            *iter;
-            gzprintf( visualizer_file, " %d", diff );
-            warp_id_issued_sum += diff;
-        }
-        m_last_shader_warp_slot_issue_distro = m_shader_warp_slot_issue_distro[ sid ];
-    }else{
-        gzprintf( visualizer_file, " 0");
+    last_shader_cycle_distro[i] = shader_cycle_distro[i];
+  }
+  gzprintf(visualizer_file, "\n");
+
+  gzprintf(visualizer_file, "ctas_completed: %d\n", ctas_completed);
+  ctas_completed = 0;
+  // warp issue breakdown
+  unsigned sid = m_config->gpgpu_warp_issue_shader;
+  unsigned count = 0;
+  unsigned warp_id_issued_sum = 0;
+  gzprintf(visualizer_file, "WarpIssueSlotBreakdown:");
+  if (m_shader_warp_slot_issue_distro[sid].size() > 0) {
+    for (std::vector<unsigned>::const_iterator iter =
+             m_shader_warp_slot_issue_distro[sid].begin();
+         iter != m_shader_warp_slot_issue_distro[sid].end(); iter++, count++) {
+      unsigned diff = count < m_last_shader_warp_slot_issue_distro.size()
+                          ? *iter - m_last_shader_warp_slot_issue_distro[count]
+                          : *iter;
+      gzprintf(visualizer_file, " %d", diff);
+      warp_id_issued_sum += diff;
     }
-    gzprintf(visualizer_file,"\n");
-
-    #define DYNAMIC_WARP_PRINT_RESOLUTION 32
-    unsigned total_issued_this_resolution = 0;
-    unsigned dynamic_id_issued_sum = 0;
-    count = 0;
-    gzprintf(visualizer_file, "WarpIssueDynamicIdBreakdown:");
-    if(m_shader_dynamic_warp_issue_distro[sid].size() > 0){
-        for ( std::vector<unsigned>::const_iterator iter = m_shader_dynamic_warp_issue_distro[ sid ].begin();
-              iter != m_shader_dynamic_warp_issue_distro[ sid ].end(); iter++, count++ ) {
-            unsigned diff = count < m_last_shader_dynamic_warp_issue_distro.size() ?
-                            *iter - m_last_shader_dynamic_warp_issue_distro[ count ] :
-                            *iter;
-            total_issued_this_resolution += diff;
-            if ( ( count + 1 ) % DYNAMIC_WARP_PRINT_RESOLUTION == 0 ) {
-                gzprintf( visualizer_file, " %d", total_issued_this_resolution );
-                dynamic_id_issued_sum += total_issued_this_resolution;
-                total_issued_this_resolution = 0;
-            }
-        }
-        if ( count % DYNAMIC_WARP_PRINT_RESOLUTION != 0 ) {
-            gzprintf( visualizer_file, " %d", total_issued_this_resolution );
-            dynamic_id_issued_sum += total_issued_this_resolution;
-        }
-        m_last_shader_dynamic_warp_issue_distro = m_shader_dynamic_warp_issue_distro[ sid ];
-        assert( warp_id_issued_sum == dynamic_id_issued_sum );
-    }else{
-        gzprintf( visualizer_file, " 0");
+    m_last_shader_warp_slot_issue_distro = m_shader_warp_slot_issue_distro[sid];
+  } else {
+    gzprintf(visualizer_file, " 0");
+  }
+  gzprintf(visualizer_file, "\n");
+
+#define DYNAMIC_WARP_PRINT_RESOLUTION 32
+  unsigned total_issued_this_resolution = 0;
+  unsigned dynamic_id_issued_sum = 0;
+  count = 0;
+  gzprintf(visualizer_file, "WarpIssueDynamicIdBreakdown:");
+  if (m_shader_dynamic_warp_issue_distro[sid].size() > 0) {
+    for (std::vector<unsigned>::const_iterator iter =
+             m_shader_dynamic_warp_issue_distro[sid].begin();
+         iter != m_shader_dynamic_warp_issue_distro[sid].end();
+         iter++, count++) {
+      unsigned diff =
+          count < m_last_shader_dynamic_warp_issue_distro.size()
+              ? *iter - m_last_shader_dynamic_warp_issue_distro[count]
+              : *iter;
+      total_issued_this_resolution += diff;
+      if ((count + 1) % DYNAMIC_WARP_PRINT_RESOLUTION == 0) {
+        gzprintf(visualizer_file, " %d", total_issued_this_resolution);
+        dynamic_id_issued_sum += total_issued_this_resolution;
+        total_issued_this_resolution = 0;
+      }
     }
-    gzprintf(visualizer_file,"\n");
-
-    // overall cache miss rates
-    gzprintf(visualizer_file, "gpgpu_n_cache_bkconflict: %d\n", gpgpu_n_cache_bkconflict);
-    gzprintf(visualizer_file, "gpgpu_n_shmem_bkconflict: %d\n", gpgpu_n_shmem_bkconflict);     
-
-
-   // instruction count per shader core
-   gzprintf(visualizer_file, "shaderinsncount:  ");
-   for (unsigned i=0;i<m_config->num_shader();i++) 
-      gzprintf(visualizer_file, "%u ", m_num_sim_insn[i] );
-   gzprintf(visualizer_file, "\n");
-   // warp instruction count per shader core
-   gzprintf(visualizer_file, "shaderwarpinsncount:  ");
-   for (unsigned i=0;i<m_config->num_shader();i++)
-      gzprintf(visualizer_file, "%u ", m_num_sim_winsn[i] );
-   gzprintf(visualizer_file, "\n");
-   // warp divergence per shader core
-   gzprintf(visualizer_file, "shaderwarpdiv: ");
-   for (unsigned i=0;i<m_config->num_shader();i++) 
-      gzprintf(visualizer_file, "%u ", m_n_diverge[i] );
-   gzprintf(visualizer_file, "\n");
-}
-
-
-extern long long g_program_memory_start;
-#define PROGRAM_MEM_START g_program_memory_start;
+    if (count % DYNAMIC_WARP_PRINT_RESOLUTION != 0) {
+      gzprintf(visualizer_file, " %d", total_issued_this_resolution);
+      dynamic_id_issued_sum += total_issued_this_resolution;
+    }
+    m_last_shader_dynamic_warp_issue_distro =
+        m_shader_dynamic_warp_issue_distro[sid];
+    assert(warp_id_issued_sum == dynamic_id_issued_sum);
+  } else {
+    gzprintf(visualizer_file, " 0");
+  }
+  gzprintf(visualizer_file, "\n");
+
+  // overall cache miss rates
+  gzprintf(visualizer_file, "gpgpu_n_cache_bkconflict: %d\n",
+           gpgpu_n_cache_bkconflict);
+  gzprintf(visualizer_file, "gpgpu_n_shmem_bkconflict: %d\n",
+           gpgpu_n_shmem_bkconflict);
+
+  // instruction count per shader core
+  gzprintf(visualizer_file, "shaderinsncount:  ");
+  for (unsigned i = 0; i < m_config->num_shader(); i++)
+    gzprintf(visualizer_file, "%u ", m_num_sim_insn[i]);
+  gzprintf(visualizer_file, "\n");
+  // warp instruction count per shader core
+  gzprintf(visualizer_file, "shaderwarpinsncount:  ");
+  for (unsigned i = 0; i < m_config->num_shader(); i++)
+    gzprintf(visualizer_file, "%u ", m_num_sim_winsn[i]);
+  gzprintf(visualizer_file, "\n");
+  // warp divergence per shader core
+  gzprintf(visualizer_file, "shaderwarpdiv: ");
+  for (unsigned i = 0; i < m_config->num_shader(); i++)
+    gzprintf(visualizer_file, "%u ", m_n_diverge[i]);
+  gzprintf(visualizer_file, "\n");
+}
+
+
+// extern long long g_program_memory_start;
+//#define PROGRAM_MEM_START g_program_memory_start;
 
 #if 0
-#define PROGRAM_MEM_START 0xF0000000 /* should be distinct from other memory spaces...
-                                        check ptx_ir.h to verify this does not overlap
-                                        other memory spaces */
+#define PROGRAM_MEM_START                                      \
+  0xF0000000 /* should be distinct from other memory spaces... \
+                check ptx_ir.h to verify this does not overlap \
+                other memory spaces */
+
 #endif
 
-void shader_core_ctx::decode()
-{
-    if( m_inst_fetch_buffer.m_valid ) {
-        // decode 1 or 2 instructions and place them into ibuffer
-        address_type pc = m_inst_fetch_buffer.m_pc;
-        const warp_inst_t* pI1 = ptx_fetch_inst(pc);
-        m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0,pI1);
-        m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
-        if( pI1 ) {
-            m_stats->m_num_decoded_insn[m_sid]++;
-            if(pI1->oprnd_type==INT_OP){
-                m_stats->m_num_INTdecoded_insn[m_sid]++;
-            }else if(pI1->oprnd_type==FP_OP) {
-            	m_stats->m_num_FPdecoded_insn[m_sid]++;
-            }
-           const warp_inst_t* pI2 = ptx_fetch_inst(pc+pI1->isize);
-           if( pI2 ) {
-               m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(1,pI2);
-               m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
-               m_stats->m_num_decoded_insn[m_sid]++;
-               if(pI2->oprnd_type==INT_OP){
-                   m_stats->m_num_INTdecoded_insn[m_sid]++;
-               }else if(pI2->oprnd_type==FP_OP) {
-            	   m_stats->m_num_FPdecoded_insn[m_sid]++;
-               }
-           }
+const warp_inst_t *exec_shader_core_ctx::get_next_inst(unsigned warp_id,
+                                                       address_type pc) {
+  // read the inst from the functional model
+  return m_gpu->gpgpu_ctx->ptx_fetch_inst(pc);
+}
+
+void exec_shader_core_ctx::get_pdom_stack_top_info(unsigned warp_id,
+                                                   const warp_inst_t *pI,
+                                                   unsigned *pc,
+                                                   unsigned *rpc) {
+  m_simt_stack[warp_id]->get_pdom_stack_top_info(pc, rpc);
+}
+
+const active_mask_t &exec_shader_core_ctx::get_active_mask(
+    unsigned warp_id, const warp_inst_t *pI) {
+  return m_simt_stack[warp_id]->get_active_mask();
+}
+
+void shader_core_ctx::decode() {
+  if (m_inst_fetch_buffer.m_valid) {
+    // decode 1 or 2 instructions and place them into ibuffer
+    address_type pc = m_inst_fetch_buffer.m_pc;
+    const warp_inst_t *pI1 = get_next_inst(m_inst_fetch_buffer.m_warp_id, pc);
+    m_warp[m_inst_fetch_buffer.m_warp_id]->ibuffer_fill(0, pI1);
+    m_warp[m_inst_fetch_buffer.m_warp_id]->inc_inst_in_pipeline();
+    if (pI1) {
+      m_stats->m_num_decoded_insn[m_sid]++;
+      if ((pI1->oprnd_type == INT_OP) || (pI1->oprnd_type == UN_OP))  { //these counters get added up in mcPat to compute scheduler power
+        m_stats->m_num_INTdecoded_insn[m_sid]++;
+      } else if (pI1->oprnd_type == FP_OP) {
+        m_stats->m_num_FPdecoded_insn[m_sid]++;
+      }
+      const warp_inst_t *pI2 =
+          get_next_inst(m_inst_fetch_buffer.m_warp_id, pc + pI1->isize);
+      if (pI2) {
+        m_warp[m_inst_fetch_buffer.m_warp_id]->ibuffer_fill(1, pI2);
+        m_warp[m_inst_fetch_buffer.m_warp_id]->inc_inst_in_pipeline();
+        m_stats->m_num_decoded_insn[m_sid]++;
+        if ((pI1->oprnd_type == INT_OP) || (pI1->oprnd_type == UN_OP))  { //these counters get added up in mcPat to compute scheduler power
+          m_stats->m_num_INTdecoded_insn[m_sid]++;
+        } else if (pI2->oprnd_type == FP_OP) {
+          m_stats->m_num_FPdecoded_insn[m_sid]++;
         }
-        m_inst_fetch_buffer.m_valid = false;
+      }
     }
+    m_inst_fetch_buffer.m_valid = false;
+  }
 }
 
-void shader_core_ctx::fetch()
-{
-
-    if( !m_inst_fetch_buffer.m_valid ) {
-        if( m_L1I->access_ready() ) {
-            mem_fetch *mf = m_L1I->next_access();
-            m_warp[mf->get_wid()].clear_imiss_pending();
-            m_inst_fetch_buffer = ifetch_buffer_t(m_warp[mf->get_wid()].get_pc(), mf->get_access_size(), mf->get_wid());
-
-
-            // TODO schi assert( m_warp[mf->get_wid()].get_pc() == (mf->get_addr()-PROGRAM_MEM_START)); // Verify that we got the instruction we were expecting.
-
-            m_inst_fetch_buffer.m_valid = true;
-            m_warp[mf->get_wid()].set_last_fetch(gpu_sim_cycle);
-            delete mf;
+void shader_core_ctx::fetch() {
+  if (!m_inst_fetch_buffer.m_valid) {
+    if (m_L1I->access_ready()) {
+      mem_fetch *mf = m_L1I->next_access();
+      m_warp[mf->get_wid()]->clear_imiss_pending();
+      m_inst_fetch_buffer =
+          ifetch_buffer_t(m_warp[mf->get_wid()]->get_pc(),
+                          mf->get_access_size(), mf->get_wid());
+      /*TODO schi
+          assert(m_warp[mf->get_wid()]->get_pc() ==
+             (mf->get_addr() -
+              PROGRAM_MEM_START));  // Verify that we got the instruction we
+                                    // were expecting.
+            */
+
+      m_inst_fetch_buffer.m_valid = true;
+      m_warp[mf->get_wid()]->set_last_fetch(m_gpu->gpu_sim_cycle);
+      delete mf;
+    } else {
+      // find an active warp with space in instruction buffer that is not
+      // already waiting on a cache miss and get next 1-2 instructions from
+      // i-cache...
+      for (unsigned i = 0; i < m_config->max_warps_per_shader; i++) {
+        unsigned warp_id =
+            (m_last_warp_fetched + 1 + i) % m_config->max_warps_per_shader;
+
+        // this code checks if this warp has finished executing and can be
+        // reclaimed
+        if (m_warp[warp_id]->hardware_done() &&
+            !m_scoreboard->pendingWrites(warp_id) &&
+            !m_warp[warp_id]->done_exit()) {
+          bool did_exit = false;
+          for (unsigned t = 0; t < m_config->warp_size; t++) {
+            unsigned tid = warp_id * m_config->warp_size + t;
+            if (m_threadState[tid].m_active == true) {
+              m_threadState[tid].m_active = false;
+              unsigned cta_id = m_warp[warp_id]->get_cta_id();
+              if (m_thread[tid] == NULL) {
+                register_cta_thread_exit(cta_id, m_kernel);
+              } else {
+                register_cta_thread_exit(cta_id,
+                                         &(m_thread[tid]->get_kernel()));
+              }
+              m_not_completed -= 1;
+              m_active_threads.reset(tid);
+              did_exit = true;
+            }
+          }
+          if (did_exit) m_warp[warp_id]->set_done_exit();
+          --m_active_warps;
+          assert(m_active_warps >= 0);
         }
-        else {
-            // find an active warp with space in instruction buffer that is not already waiting on a cache miss
-            // and get next 1-2 instructions from i-cache...
-            for( unsigned i=0; i < m_config->max_warps_per_shader; i++ ) {
-                unsigned warp_id = (m_last_warp_fetched+1+i) % m_config->max_warps_per_shader;
-
-                // this code checks if this warp has finished executing and can be reclaimed
-                if( m_warp[warp_id].hardware_done() && !m_scoreboard->pendingWrites(warp_id) && !m_warp[warp_id].done_exit() ) {
-                    bool did_exit=false;
-                    for( unsigned t=0; t<m_config->warp_size;t++) {
-                        unsigned tid=warp_id*m_config->warp_size+t;
-                        if( m_threadState[tid].m_active == true ) {
-                            m_threadState[tid].m_active = false; 
-                            unsigned cta_id = m_warp[warp_id].get_cta_id();
-                            register_cta_thread_exit(cta_id, &(m_thread[tid]->get_kernel()));
-                            m_not_completed -= 1;
-                            m_active_threads.reset(tid);
-                            assert( m_thread[tid]!= NULL );
-                            did_exit=true;
-                        }
-                    }
-                    if( did_exit ) 
-                        m_warp[warp_id].set_done_exit();
-                        --m_active_warps;
-                        assert(m_active_warps >= 0);
-                }
 
-                // this code fetches instructions from the i-cache or generates memory requests
-                if( !m_warp[warp_id].functional_done() && !m_warp[warp_id].imiss_pending() && m_warp[warp_id].ibuffer_empty() ) {
-                    address_type pc  = m_warp[warp_id].get_pc();
-                    // TODO schi address_type ppc = pc + PROGRAM_MEM_START;
-                    // FIXME FIXME
-                    address_type ppc = pc + PROGRAM_MEM_START;
-                    // address_type ppc = pc + m_kernel->get_inst_base_vaddr();
-                    unsigned nbytes=16;
-                    unsigned offset_in_block = pc & (m_config->m_L1I_config.get_line_sz()-1);
-                    if( (offset_in_block+nbytes) > m_config->m_L1I_config.get_line_sz() )
-                        nbytes = (m_config->m_L1I_config.get_line_sz()-offset_in_block);
-
-                    // TODO: replace with use of allocator
-                    // mem_fetch *mf = m_mem_fetch_allocator->alloc()
-                    // HACK: This access will get sent into gem5, so the control
-                // header size must be zero, since gem5 packets will assess
-                // control header sizing
-                    mem_access_t acc(INST_ACC_R,ppc,nbytes,false);
-                    mem_fetch *mf = new mem_fetch(acc,
-                            NULL/*we don't have an instruction yet*/,
-                            READ_PACKET_SIZE,
-                            warp_id,
-                            m_sid,
-                            m_tpc,
-                            m_memory_config );
-                    std::list<cache_event> events;
-                    enum cache_request_status status = m_L1I->access( (new_addr_type)ppc, mf, gpu_sim_cycle+gpu_tot_sim_cycle,events);
-                    if( status == MISS ) {
-                        m_last_warp_fetched=warp_id;
-                        m_warp[warp_id].set_imiss_pending();
-                        m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
-                    } else if( status == HIT ) {
-                        m_last_warp_fetched=warp_id;
-                        m_inst_fetch_buffer = ifetch_buffer_t(pc,nbytes,warp_id);
-                        m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
-                        delete mf;
-                    } else {
-                        m_last_warp_fetched=warp_id;
-                        assert( status == RESERVATION_FAIL );
-                        delete mf;
-                    }
-                    break;
-                }
-            }
+        // this code fetches instructions from the i-cache or generates memory
+        if (!m_warp[warp_id]->functional_done() &&
+            !m_warp[warp_id]->imiss_pending() &&
+            m_warp[warp_id]->ibuffer_empty()) {
+          address_type pc;
+          pc = m_warp[warp_id]->get_pc();
+          // FIXME FIXME
+          // address_type ppc = pc + PROGRAM_MEM_START;
+          address_type ppc = pc + m_kernel->get_inst_base_vaddr();
+          unsigned nbytes = 16;
+          unsigned offset_in_block =
+              pc & (m_config->m_L1I_config.get_line_sz() - 1);
+          if ((offset_in_block + nbytes) > m_config->m_L1I_config.get_line_sz())
+            nbytes = (m_config->m_L1I_config.get_line_sz() - offset_in_block);
+
+          // TODO: replace with use of allocator
+          // mem_fetch *mf = m_mem_fetch_allocator->alloc()
+          // HACK: This access will get sent into gem5, so the control
+          // header size must be zero, since gem5 packets will assess
+          // control header sizing
+          mem_access_t acc(INST_ACC_R, ppc, nbytes, false, m_gpu->gpgpu_ctx);
+          mem_fetch *mf = new mem_fetch(
+              acc, NULL /*we don't have an instruction yet*/, READ_PACKET_SIZE,
+              warp_id, m_sid, m_tpc, m_memory_config,
+              m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle);
+          std::list<cache_event> events;
+          enum cache_request_status status;
+          if (m_config->perfect_inst_const_cache){
+            status = HIT;
+            shader_cache_access_log(m_sid, INSTRUCTION, 0);
+          }
+          else
+            status = m_L1I->access(
+                (new_addr_type)ppc, mf,
+                m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle, events);
+
+          if (status == MISS) {
+            m_last_warp_fetched = warp_id;
+            m_warp[warp_id]->set_imiss_pending();
+            m_warp[warp_id]->set_last_fetch(m_gpu->gpu_sim_cycle);
+          } else if (status == HIT) {
+            m_last_warp_fetched = warp_id;
+            m_inst_fetch_buffer = ifetch_buffer_t(pc, nbytes, warp_id);
+            m_warp[warp_id]->set_last_fetch(m_gpu->gpu_sim_cycle);
+            delete mf;
+          } else {
+            m_last_warp_fetched = warp_id;
+            assert(status == RESERVATION_FAIL);
+            delete mf;
+          }
+          break;
         }
+      }
     }
+  }
 
-    m_L1I->cycle();
+  m_L1I->cycle();
 }
 
-void shader_core_ctx::func_exec_inst( warp_inst_t &inst )
-{
-    execute_warp_inst_t(inst);
-    if( inst.is_load() || inst.is_store() )
-    {
-	inst.generate_mem_accesses();
-        //inst.print_m_accessq();	
-    }	
+void exec_shader_core_ctx::func_exec_inst(warp_inst_t &inst) {
+  execute_warp_inst_t(inst);
+  if (inst.is_load() || inst.is_store()) {
+    inst.generate_mem_accesses();
+    // inst.print_m_accessq();
+  }
 }
 
 // TODO schi add
 void shader_core_ctx::warp_reaches_barrier(warp_inst_t &inst) {
-    m_barriers.warp_reaches_barrier(m_warp[inst.warp_id()].get_cta_id(), inst.warp_id(), &inst);
-}
+    m_barriers.warp_reaches_barrier(m_warp[inst.warp_id()]->get_cta_id(), inst.warp_id(), &inst);
+}
+
+void shader_core_ctx::issue_warp(register_set &pipe_reg_set,
+                                 const warp_inst_t *next_inst,
+                                 const active_mask_t &active_mask,
+                                 unsigned warp_id, unsigned sch_id) {
+  warp_inst_t **pipe_reg =
+      pipe_reg_set.get_free(m_config->sub_core_model, sch_id);
+  assert(pipe_reg);
+
+  m_warp[warp_id]->ibuffer_free();
+  assert(next_inst->valid());
+  **pipe_reg = *next_inst;  // static instruction information
+  (*pipe_reg)->issue(active_mask, warp_id,
+                     m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle,
+                     m_warp[warp_id]->get_dynamic_warp_id(),
+                     sch_id);  // dynamic instruction information
+  m_stats->shader_cycle_distro[2 + (*pipe_reg)->active_count()]++;
+  func_exec_inst(**pipe_reg);
+
+  if (next_inst->op == BARRIER_OP) {
+    m_warp[warp_id]->store_info_of_last_inst_at_barrier(*pipe_reg);
+    m_barriers.warp_reaches_barrier(m_warp[warp_id]->get_cta_id(), warp_id,
+                                    const_cast<warp_inst_t *>(next_inst));
+
+  } else if (next_inst->op == MEMORY_BARRIER_OP) {
+    m_warp[warp_id]->set_membar();
+  }
 
-void shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id, unsigned sch_id )
-{
-	warp_inst_t** pipe_reg = pipe_reg = pipe_reg_set.get_free(m_config->sub_core_model, sch_id);
-    assert(pipe_reg);
-
-    m_warp[warp_id].ibuffer_free();
-    assert(next_inst->valid());
-    **pipe_reg = *next_inst; // static instruction information
-    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id(), sch_id ); // dynamic instruction information
-    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;
-    func_exec_inst( **pipe_reg );
-    if( next_inst->op == BARRIER_OP ){
-        m_warp[warp_id].store_info_of_last_inst_at_barrier(*pipe_reg);
-        m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(),warp_id,const_cast<warp_inst_t*> (next_inst));
-
-    }else if( next_inst->op == MEMORY_BARRIER_OP ){
-        m_warp[warp_id].set_membar();
-    }
+  updateSIMTStack(warp_id, *pipe_reg);
 
-    updateSIMTStack(warp_id,*pipe_reg);
-    m_scoreboard->reserveRegisters(*pipe_reg);
-    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);
+  m_scoreboard->reserveRegisters(*pipe_reg);
+  m_warp[warp_id]->set_next_pc(next_inst->pc + next_inst->isize);
 }
 
-void shader_core_ctx::issue(){
-
-     //Ensure fair round robin issu between schedulers 
-     unsigned j;
-     for (unsigned i = 0; i < schedulers.size(); i++) {
-	j = (Issue_Prio + i) % schedulers.size();
-	  schedulers[j]->cycle();
-     }
-     Issue_Prio = (Issue_Prio+1)% schedulers.size();
-
-    //really is issue;
-    //for (unsigned i = 0; i < schedulers.size(); i++) {
-    //    schedulers[i]->cycle();
-    //}
-}
+void shader_core_ctx::issue() {
+  // Ensure fair round robin issu between schedulers
+  unsigned j;
+  for (unsigned i = 0; i < schedulers.size(); i++) {
+    j = (Issue_Prio + i) % schedulers.size();
+    schedulers[j]->cycle();
+  }
+  Issue_Prio = (Issue_Prio + 1) % schedulers.size();
 
-shd_warp_t& scheduler_unit::warp(int i){
-    return (*m_warp)[i];
+  // really is issue;
+  // for (unsigned i = 0; i < schedulers.size(); i++) {
+  //    schedulers[i]->cycle();
+  //}
 }
 
+shd_warp_t &scheduler_unit::warp(int i) { return *((*m_warp)[i]); }
 
 /**
  * A general function to order things in a Loose Round Robin way. The simplist use of this
@@ -942,28 +1124,52 @@ shd_warp_t& scheduler_unit::warp(int i){
  *                          limit this number. If the number if < m_supervised_warps.size(), then only
  *                          the warps with highest RR priority will be placed in the result_list.
  */
-    template < class T >
-void scheduler_unit::order_lrr( std::vector< T >& result_list,
-        const typename std::vector< T >& input_list,
-        const typename std::vector< T >::const_iterator& last_issued_from_input,
-        unsigned num_warps_to_add )
-{
-    assert( num_warps_to_add <= input_list.size() );
-    result_list.clear();
-    typename std::vector< T >::const_iterator iter
-        = ( last_issued_from_input ==  input_list.end() ) ? input_list.begin()
-        : last_issued_from_input + 1;
-
-    for ( unsigned count = 0;
-            count < num_warps_to_add;
-            ++iter, ++count) {
-        if ( iter ==  input_list.end() ) {
-            iter = input_list.begin();
-        }
-        result_list.push_back( *iter );
+template <class T>
+void scheduler_unit::order_lrr(
+    std::vector<T> &result_list, const typename std::vector<T> &input_list,
+    const typename std::vector<T>::const_iterator &last_issued_from_input,
+    unsigned num_warps_to_add) {
+  assert(num_warps_to_add <= input_list.size());
+  result_list.clear();
+  typename std::vector<T>::const_iterator iter =
+      (last_issued_from_input == input_list.end()) ? input_list.begin()
+                                                   : last_issued_from_input + 1;
+
+  for (unsigned count = 0; count < num_warps_to_add; ++iter, ++count) {
+    if (iter == input_list.end()) {
+      iter = input_list.begin();
     }
+    result_list.push_back(*iter);
+  }
 }
 
+template <class T>
+void scheduler_unit::order_rrr(
+    std::vector<T> &result_list, const typename std::vector<T> &input_list,
+    const typename std::vector<T>::const_iterator &last_issued_from_input,
+    unsigned num_warps_to_add) {
+  result_list.clear();
+
+  if (m_num_issued_last_cycle > 0 || warp(m_current_turn_warp).done_exit() ||
+      warp(m_current_turn_warp).waiting()) {
+    std::vector<shd_warp_t *>::const_iterator iter =
+      (last_issued_from_input == input_list.end()) ? 
+        input_list.begin() : last_issued_from_input + 1;
+    for (unsigned count = 0; count < num_warps_to_add; ++iter, ++count) {
+      if (iter == input_list.end()) {
+      iter = input_list.begin();
+      }
+      unsigned warp_id = (*iter)->get_warp_id();
+      if (!(*iter)->done_exit() && !(*iter)->waiting()) {
+        result_list.push_back(*iter);
+        m_current_turn_warp = warp_id;
+        break;
+      }
+    }
+  } else {
+    result_list.push_back(&warp(m_current_turn_warp));
+  }
+}
 /**
  * A general function to order things in an priority-based way.
  * The core usage of the function is similar to order_lrr.
@@ -975,897 +1181,1044 @@ void scheduler_unit::order_lrr( std::vector< T >& result_list,
  *                           with the oldest warps having the most priority, then the priority_function
  *                           would compare the age of the two warps.
  */
-    template < class T >
-void scheduler_unit::order_by_priority( std::vector< T >& result_list,
-        const typename std::vector< T >& input_list,
-        const typename std::vector< T >::const_iterator& last_issued_from_input,
-        unsigned num_warps_to_add,
-        OrderingType ordering,
-        bool (*priority_func)(T lhs, T rhs) )
-{
-    assert( num_warps_to_add <= input_list.size() );
-    result_list.clear();
-    typename std::vector< T > temp = input_list;
-
-    if ( ORDERING_GREEDY_THEN_PRIORITY_FUNC == ordering ) {
-        T greedy_value = *last_issued_from_input;
-        result_list.push_back( greedy_value );
-
-        std::sort( temp.begin(), temp.end(), priority_func );
-        typename std::vector< T >::iterator iter = temp.begin();
-        for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
-            if ( *iter != greedy_value ) {
-                result_list.push_back( *iter );
-            }
-        }
-    } else if ( ORDERED_PRIORITY_FUNC_ONLY == ordering ) {
-        std::sort( temp.begin(), temp.end(), priority_func );
-        typename std::vector< T >::iterator iter = temp.begin();
-        for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
-            result_list.push_back( *iter );
-        }
-    } else {
-        fprintf( stderr, "Unknown ordering - %d\n", ordering );
-        abort();
+template <class T>
+void scheduler_unit::order_by_priority(
+    std::vector<T> &result_list, const typename std::vector<T> &input_list,
+    const typename std::vector<T>::const_iterator &last_issued_from_input,
+    unsigned num_warps_to_add, OrderingType ordering,
+    bool (*priority_func)(T lhs, T rhs)) {
+  assert(num_warps_to_add <= input_list.size());
+  result_list.clear();
+  typename std::vector<T> temp = input_list;
+
+  if (ORDERING_GREEDY_THEN_PRIORITY_FUNC == ordering) {
+    T greedy_value = *last_issued_from_input;
+    result_list.push_back(greedy_value);
+
+    std::sort(temp.begin(), temp.end(), priority_func);
+    typename std::vector<T>::iterator iter = temp.begin();
+    for (unsigned count = 0; count < num_warps_to_add; ++count, ++iter) {
+      if (*iter != greedy_value) {
+        result_list.push_back(*iter);
+      }
     }
+  } else if (ORDERED_PRIORITY_FUNC_ONLY == ordering) {
+    std::sort(temp.begin(), temp.end(), priority_func);
+    typename std::vector<T>::iterator iter = temp.begin();
+    for (unsigned count = 0; count < num_warps_to_add; ++count, ++iter) {
+      result_list.push_back(*iter);
+    }
+  } else {
+    fprintf(stderr, "Unknown ordering - %d\n", ordering);
+    abort();
+  }
 }
 
-void scheduler_unit::cycle()
-{
-    SCHED_DPRINTF( "scheduler_unit::cycle()\n" );
-    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)
-    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes
-    bool issued_inst = false; // of these we issued one
-
-    order_warps();
-    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();
-          iter != m_next_cycle_prioritized_warps.end();
-          iter++ ) {
-        // Don't consider warps that are not yet valid
-        if ( (*iter) == NULL || (*iter)->done_exit() ) {
-            continue;
-        }
-        SCHED_DPRINTF( "Testing (warp_id %u, dynamic_warp_id %u)\n",
-                       (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
-        unsigned warp_id = (*iter)->get_warp_id();
-        unsigned checked=0;
-        unsigned issued=0;
-        exec_unit_type_t previous_issued_inst_exec_type = exec_unit_type_t::NONE;
-        unsigned max_issue = m_shader->m_config->gpgpu_max_insn_issue_per_warp;
-        bool diff_exec_units = m_shader->m_config->gpgpu_dual_issue_diff_exec_units;  //In tis mode, we only allow dual issue to diff execution units (as in Maxwell and Pascal)
-
-        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {
-            const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();
-            //Jin: handle cdp latency;
-            if(pI && pI->m_is_cdp && warp(warp_id).m_cdp_latency > 0) {
-                assert(warp(warp_id).m_cdp_dummy);
-                warp(warp_id).m_cdp_latency--;
-                break;
-            }
+void scheduler_unit::cycle() {
+  SCHED_DPRINTF("scheduler_unit::cycle()\n");
+  bool valid_inst =
+      false;  // there was one warp with a valid instruction to issue (didn't
+              // require flush due to control hazard)
+  bool ready_inst = false;   // of the valid instructions, there was one not
+                             // waiting for pending register writes
+  bool issued_inst = false;  // of these we issued one
+
+  order_warps();
+  for (std::vector<shd_warp_t *>::const_iterator iter =
+           m_next_cycle_prioritized_warps.begin();
+       iter != m_next_cycle_prioritized_warps.end(); iter++) {
+    // Don't consider warps that are not yet valid
+    if ((*iter) == NULL || (*iter)->done_exit()) {
+      continue;
+    }
+    SCHED_DPRINTF("Testing (warp_id %u, dynamic_warp_id %u)\n",
+                  (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id());
+    unsigned warp_id = (*iter)->get_warp_id();
+    unsigned checked = 0;
+    unsigned issued = 0;
+    exec_unit_type_t previous_issued_inst_exec_type = exec_unit_type_t::NONE;
+    unsigned max_issue = m_shader->m_config->gpgpu_max_insn_issue_per_warp;
+    bool diff_exec_units =
+        m_shader->m_config
+            ->gpgpu_dual_issue_diff_exec_units;  // In tis mode, we only allow
+                                                 // dual issue to diff execution
+                                                 // units (as in Maxwell and
+                                                 // Pascal)
+
+    if (warp(warp_id).ibuffer_empty())
+      SCHED_DPRINTF(
+          "Warp (warp_id %u, dynamic_warp_id %u) fails as ibuffer_empty\n",
+          (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id());
+
+    if (warp(warp_id).waiting())
+      SCHED_DPRINTF(
+          "Warp (warp_id %u, dynamic_warp_id %u) fails as waiting for "
+          "barrier\n",
+          (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id());
+
+    while (!warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() &&
+           (checked < max_issue) && (checked <= issued) &&
+           (issued < max_issue)) {
+      const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();
+      // Jin: handle cdp latency;
+      if (pI && pI->m_is_cdp && warp(warp_id).m_cdp_latency > 0) {
+        assert(warp(warp_id).m_cdp_dummy);
+        warp(warp_id).m_cdp_latency--;
+        break;
+      }
 
-            bool valid = warp(warp_id).ibuffer_next_valid();
-            bool warp_inst_issued = false;
-            unsigned pc,rpc;
-            m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc,&rpc);
-            SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) has valid instruction (%s)\n",
-                           (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id(),
-                           ptx_get_insn_str( pc).c_str() );
-            if( pI ) {
-                assert(valid);
-                if( pc != pI->pc ) {
-                    SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) control hazard instruction flush\n",
-                                   (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
-                    // control hazard
-                    warp(warp_id).set_next_pc(pc);
-                    warp(warp_id).ibuffer_flush();
-                } else {
-                    valid_inst = true;
-                    if ( !m_scoreboard->checkCollision(warp_id, pI) ) {
-                        SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) passes scoreboard\n",
-                                       (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
-                        ready_inst = true;
-                        const active_mask_t &active_mask = m_simt_stack[warp_id]->get_active_mask();
-                        assert( warp(warp_id).inst_in_pipeline() );
-
-                        if ( (pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP)||(pI->op==TENSOR_CORE_LOAD_OP)||(pI->op==TENSOR_CORE_STORE_OP) ) {
-                        	if( m_mem_out->has_free(m_shader->m_config->sub_core_model, m_id) && (!diff_exec_units || previous_issued_inst_exec_type != exec_unit_type_t::MEM)) {
-                                m_shader->issue_warp(*m_mem_out,pI,active_mask,warp_id,m_id);
-                                issued++;
-                                issued_inst=true;
-                                warp_inst_issued = true;
-                                previous_issued_inst_exec_type = exec_unit_type_t::MEM;
-                            }
-                        } else {
-
-                            bool sp_pipe_avail = m_sp_out->has_free(m_shader->m_config->sub_core_model, m_id);
-                            bool sfu_pipe_avail = m_sfu_out->has_free(m_shader->m_config->sub_core_model, m_id);
-                            bool tensor_core_pipe_avail = m_tensor_core_out->has_free(m_shader->m_config->sub_core_model, m_id);
-                            bool dp_pipe_avail = m_dp_out->has_free(m_shader->m_config->sub_core_model, m_id);
-                            bool int_pipe_avail = m_int_out->has_free(m_shader->m_config->sub_core_model, m_id);
-
-                            //This code need to be refactored
-                            if(pI->op != TENSOR_CORE_OP && pI->op != SFU_OP && pI->op != DP_OP) {
-                                
-									bool execute_on_SP = false;
-									bool execute_on_INT = false;
-
-									//if INT unit pipline exist, then execute ALU and INT operations on INT unit and SP-FPU on SP unit (like in Volta)
-									//if INT unit pipline does not exist, then execute all ALU, INT and SP operations on SP unit (as in Fermi, Pascal GPUs)
-									if(m_shader->m_config->gpgpu_num_int_units > 0 &&
-											int_pipe_avail &&
-											pI->op != SP_OP &&
-											!(diff_exec_units && previous_issued_inst_exec_type == exec_unit_type_t::INT))
-										execute_on_INT = true;
-									else if (sp_pipe_avail &&
-											(m_shader->m_config->gpgpu_num_int_units == 0 ||
-											(m_shader->m_config->gpgpu_num_int_units > 0 && pI->op == SP_OP)) &&
-											!(diff_exec_units && previous_issued_inst_exec_type == exec_unit_type_t::SP) )
-										execute_on_SP = true;
-
-
-									if(execute_on_INT || execute_on_SP) {
-										//Jin: special for CDP api
-										if(pI->m_is_cdp && !warp(warp_id).m_cdp_dummy) {
-											assert(warp(warp_id).m_cdp_latency == 0);
-
-											extern unsigned cdp_latency[5];
-											if(pI->m_is_cdp == 1)
-												warp(warp_id).m_cdp_latency = cdp_latency[pI->m_is_cdp - 1];
-											else //cudaLaunchDeviceV2 and cudaGetParameterBufferV2
-												warp(warp_id).m_cdp_latency = cdp_latency[pI->m_is_cdp - 1]
-													+ cdp_latency[pI->m_is_cdp] * active_mask.count();
-											warp(warp_id).m_cdp_dummy = true;
-											break;
-										}
-										else if(pI->m_is_cdp && warp(warp_id).m_cdp_dummy) {
-											assert(warp(warp_id).m_cdp_latency == 0);
-											warp(warp_id).m_cdp_dummy = false;
-										}
-									}
-
-									if(execute_on_SP) {
-										m_shader->issue_warp(*m_sp_out,pI,active_mask,warp_id,m_id);
-										issued++;
-										issued_inst=true;
-										warp_inst_issued = true;
-										previous_issued_inst_exec_type = exec_unit_type_t::SP;
-									} else if (execute_on_INT) {
-										m_shader->issue_warp(*m_int_out,pI,active_mask,warp_id,m_id);
-										issued++;
-										issued_inst=true;
-										warp_inst_issued = true;
-										previous_issued_inst_exec_type = exec_unit_type_t::INT;
-                                   }
-                            } else if ( (m_shader->m_config->gpgpu_num_dp_units > 0) && (pI->op == DP_OP) && !(diff_exec_units && previous_issued_inst_exec_type == exec_unit_type_t::DP)) {
-                                if( dp_pipe_avail ) {
-                                    m_shader->issue_warp(*m_dp_out,pI,active_mask,warp_id,m_id);
-                                    issued++;
-                                    issued_inst=true;
-                                    warp_inst_issued = true;
-                                    previous_issued_inst_exec_type = exec_unit_type_t::DP;
-                                }
-                            }  //If the DP units = 0 (like in Fermi archi), then execute DP inst on SFU unit
-                            else if ( ((m_shader->m_config->gpgpu_num_dp_units == 0 && pI->op == DP_OP) || (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP)) && !(diff_exec_units && previous_issued_inst_exec_type == exec_unit_type_t::SFU)) {
-                                if( sfu_pipe_avail ) {
-                                    m_shader->issue_warp(*m_sfu_out,pI,active_mask,warp_id,m_id);
-                                    issued++;
-                                    issued_inst=true;
-                                    warp_inst_issued = true;
-                                    previous_issued_inst_exec_type = exec_unit_type_t::SFU;
-                                }
-                            }                         
-                             else if ( (pI->op == TENSOR_CORE_OP) && !(diff_exec_units && previous_issued_inst_exec_type == exec_unit_type_t::SP) ) {
-                                if( tensor_core_pipe_avail ) {
-                                    m_shader->issue_warp(*m_tensor_core_out,pI,active_mask,warp_id,m_id);
-                                    issued++;
-                                    issued_inst=true;
-                                    warp_inst_issued = true;
-                                    previous_issued_inst_exec_type = exec_unit_type_t::TENSOR;
-                                }
-			    }
-                         }//end of else
-                   } else {
-
-                        SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) fails scoreboard\n",
-                                       (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
-                   }
-                }
-            } else if( valid ) {
-               // this case can happen after a return instruction in diverged warp
-               SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) return from diverged warp flush\n",
-                              (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
-               warp(warp_id).set_next_pc(pc);
-               warp(warp_id).ibuffer_flush();
-            }
-            if(warp_inst_issued) {
-                SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) issued %u instructions\n",
-                               (*iter)->get_warp_id(),
-                               (*iter)->get_dynamic_warp_id(),
-                               issued );
-                do_on_warp_issued( warp_id, issued, iter );
-            }
-            checked++;
-        }
-        if ( issued ) {
-            // This might be a bit inefficient, but we need to maintain
-            // two ordered list for proper scheduler execution.
-            // We could remove the need for this loop by associating a
-            // supervised_is index with each entry in the m_next_cycle_prioritized_warps
-            // vector. For now, just run through until you find the right warp_id
-            for ( std::vector< shd_warp_t* >::const_iterator supervised_iter = m_supervised_warps.begin();
-                  supervised_iter != m_supervised_warps.end();
-                  ++supervised_iter ) {
-                if ( *iter == *supervised_iter ) {
-                    m_last_supervised_issued = supervised_iter;
+      bool valid = warp(warp_id).ibuffer_next_valid();
+      bool warp_inst_issued = false;
+      unsigned pc, rpc;
+      m_shader->get_pdom_stack_top_info(warp_id, pI, &pc, &rpc);
+      SCHED_DPRINTF(
+          "Warp (warp_id %u, dynamic_warp_id %u) has valid instruction (%s)\n",
+          (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id(),
+          m_shader->m_config->gpgpu_ctx->func_sim->ptx_get_insn_str(pc)
+              .c_str());
+      if (pI) {
+        assert(valid);
+        if (pc != pI->pc) {
+          SCHED_DPRINTF(
+              "Warp (warp_id %u, dynamic_warp_id %u) control hazard (pc %u, pI->pc %u)"
+              "instruction flush\n",
+              (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id(), pc, pI->pc);
+          // control hazard
+          warp(warp_id).set_next_pc(pc);
+          warp(warp_id).ibuffer_flush();
+        } else {
+          valid_inst = true;
+          if (!m_scoreboard->checkCollision(warp_id, pI)) {
+            SCHED_DPRINTF(
+                "Warp (warp_id %u, dynamic_warp_id %u) passes scoreboard\n",
+                (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id());
+            ready_inst = true;
+
+            const active_mask_t &active_mask =
+                m_shader->get_active_mask(warp_id, pI);
+
+            assert(warp(warp_id).inst_in_pipeline());
+
+            if ((pI->op == LOAD_OP) || (pI->op == STORE_OP) ||
+                (pI->op == MEMORY_BARRIER_OP) ||
+                (pI->op == TENSOR_CORE_LOAD_OP) ||
+                (pI->op == TENSOR_CORE_STORE_OP)) {
+              if (m_mem_out->has_free(m_shader->m_config->sub_core_model,
+                                      m_id) &&
+                  (!diff_exec_units ||
+                   previous_issued_inst_exec_type != exec_unit_type_t::MEM)) {
+                m_shader->issue_warp(*m_mem_out, pI, active_mask, warp_id,
+                                     m_id);
+                issued++;
+                issued_inst = true;
+                warp_inst_issued = true;
+                previous_issued_inst_exec_type = exec_unit_type_t::MEM;
+              }
+            } else {
+              // This code need to be refactored
+              if (pI->op != TENSOR_CORE_OP && pI->op != SFU_OP &&
+                  pI->op != DP_OP && !(pI->op >= SPEC_UNIT_START_ID)) {
+                bool execute_on_SP = false;
+                bool execute_on_INT = false;
+
+                bool sp_pipe_avail =
+                    (m_shader->m_config->gpgpu_num_sp_units > 0) &&
+                    m_sp_out->has_free(m_shader->m_config->sub_core_model,
+                                       m_id);
+                bool int_pipe_avail =
+                    (m_shader->m_config->gpgpu_num_int_units > 0) &&
+                    m_int_out->has_free(m_shader->m_config->sub_core_model,
+                                        m_id);
+
+                // if INT unit pipline exist, then execute ALU and INT
+                // operations on INT unit and SP-FPU on SP unit (like in Volta)
+                // if INT unit pipline does not exist, then execute all ALU, INT
+                // and SP operations on SP unit (as in Fermi, Pascal GPUs)
+                if (m_shader->m_config->gpgpu_num_int_units > 0 &&
+                    int_pipe_avail && pI->op != SP_OP &&
+                    !(diff_exec_units &&
+                      previous_issued_inst_exec_type == exec_unit_type_t::INT))
+                  execute_on_INT = true;
+                else if (sp_pipe_avail &&
+                         (m_shader->m_config->gpgpu_num_int_units == 0 ||
+                          (m_shader->m_config->gpgpu_num_int_units > 0 &&
+                           pI->op == SP_OP)) &&
+                         !(diff_exec_units && previous_issued_inst_exec_type ==
+                                                  exec_unit_type_t::SP))
+                  execute_on_SP = true;
+
+                if (execute_on_INT || execute_on_SP) {
+                  // Jin: special for CDP api
+                  if (pI->m_is_cdp && !warp(warp_id).m_cdp_dummy) {
+                    assert(warp(warp_id).m_cdp_latency == 0);
+
+                    if (pI->m_is_cdp == 1)
+                      warp(warp_id).m_cdp_latency =
+                          m_shader->m_config->gpgpu_ctx->func_sim
+                              ->cdp_latency[pI->m_is_cdp - 1];
+                    else  // cudaLaunchDeviceV2 and cudaGetParameterBufferV2
+                      warp(warp_id).m_cdp_latency =
+                          m_shader->m_config->gpgpu_ctx->func_sim
+                              ->cdp_latency[pI->m_is_cdp - 1] +
+                          m_shader->m_config->gpgpu_ctx->func_sim
+                                  ->cdp_latency[pI->m_is_cdp] *
+                              active_mask.count();
+                    warp(warp_id).m_cdp_dummy = true;
+                    break;
+                  } else if (pI->m_is_cdp && warp(warp_id).m_cdp_dummy) {
+                    assert(warp(warp_id).m_cdp_latency == 0);
+                    warp(warp_id).m_cdp_dummy = false;
+                  }
                 }
-            }
 
-            if(issued == 1)
-            	m_stats->single_issue_nums[m_id]++;
-            else if(issued > 1)
-            	m_stats->dual_issue_nums[m_id]++;
-            else
-            	abort();   //issued should be > 0
+                if (execute_on_SP) {
+                  m_shader->issue_warp(*m_sp_out, pI, active_mask, warp_id,
+                                       m_id);
+                  issued++;
+                  issued_inst = true;
+                  warp_inst_issued = true;
+                  previous_issued_inst_exec_type = exec_unit_type_t::SP;
+                } else if (execute_on_INT) {
+                  m_shader->issue_warp(*m_int_out, pI, active_mask, warp_id,
+                                       m_id);
+                  issued++;
+                  issued_inst = true;
+                  warp_inst_issued = true;
+                  previous_issued_inst_exec_type = exec_unit_type_t::INT;
+                }
+              } else if ((m_shader->m_config->gpgpu_num_dp_units > 0) &&
+                         (pI->op == DP_OP) &&
+                         !(diff_exec_units && previous_issued_inst_exec_type ==
+                                                  exec_unit_type_t::DP)) {
+                bool dp_pipe_avail =
+                    (m_shader->m_config->gpgpu_num_dp_units > 0) &&
+                    m_dp_out->has_free(m_shader->m_config->sub_core_model,
+                                       m_id);
+
+                if (dp_pipe_avail) {
+                  m_shader->issue_warp(*m_dp_out, pI, active_mask, warp_id,
+                                       m_id);
+                  issued++;
+                  issued_inst = true;
+                  warp_inst_issued = true;
+                  previous_issued_inst_exec_type = exec_unit_type_t::DP;
+                }
+              }  // If the DP units = 0 (like in Fermi archi), then execute DP
+                 // inst on SFU unit
+              else if (((m_shader->m_config->gpgpu_num_dp_units == 0 &&
+                         pI->op == DP_OP) ||
+                        (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP)) &&
+                       !(diff_exec_units && previous_issued_inst_exec_type ==
+                                                exec_unit_type_t::SFU)) {
+                bool sfu_pipe_avail =
+                    (m_shader->m_config->gpgpu_num_sfu_units > 0) &&
+                    m_sfu_out->has_free(m_shader->m_config->sub_core_model,
+                                        m_id);
+
+                if (sfu_pipe_avail) {
+                  m_shader->issue_warp(*m_sfu_out, pI, active_mask, warp_id,
+                                       m_id);
+                  issued++;
+                  issued_inst = true;
+                  warp_inst_issued = true;
+                  previous_issued_inst_exec_type = exec_unit_type_t::SFU;
+                }
+              } else if ((pI->op == TENSOR_CORE_OP) &&
+                         !(diff_exec_units && previous_issued_inst_exec_type ==
+                                                  exec_unit_type_t::TENSOR)) {
+                bool tensor_core_pipe_avail =
+                    (m_shader->m_config->gpgpu_num_tensor_core_units > 0) &&
+                    m_tensor_core_out->has_free(
+                        m_shader->m_config->sub_core_model, m_id);
+
+                if (tensor_core_pipe_avail) {
+                  m_shader->issue_warp(*m_tensor_core_out, pI, active_mask,
+                                       warp_id, m_id);
+                  issued++;
+                  issued_inst = true;
+                  warp_inst_issued = true;
+                  previous_issued_inst_exec_type = exec_unit_type_t::TENSOR;
+                }
+              } else if ((pI->op >= SPEC_UNIT_START_ID) &&
+                         !(diff_exec_units &&
+                           previous_issued_inst_exec_type ==
+                               exec_unit_type_t::SPECIALIZED)) {
+                unsigned spec_id = pI->op - SPEC_UNIT_START_ID;
+                assert(spec_id < m_shader->m_config->m_specialized_unit.size());
+                register_set *spec_reg_set = m_spec_cores_out[spec_id];
+                bool spec_pipe_avail =
+                    (m_shader->m_config->m_specialized_unit[spec_id].num_units >
+                     0) &&
+                    spec_reg_set->has_free(m_shader->m_config->sub_core_model,
+                                           m_id);
+
+                if (spec_pipe_avail) {
+                  m_shader->issue_warp(*spec_reg_set, pI, active_mask, warp_id,
+                                       m_id);
+                  issued++;
+                  issued_inst = true;
+                  warp_inst_issued = true;
+                  previous_issued_inst_exec_type =
+                      exec_unit_type_t::SPECIALIZED;
+                }
+              }
 
-            break;
-        } 
+            }  // end of else
+          } else {
+            SCHED_DPRINTF(
+                "Warp (warp_id %u, dynamic_warp_id %u) fails scoreboard\n",
+                (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id());
+          }
+        }
+      } else if (valid) {
+        // this case can happen after a return instruction in diverged warp
+        SCHED_DPRINTF(
+            "Warp (warp_id %u, dynamic_warp_id %u) return from diverged warp "
+            "flush\n",
+            (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id());
+        warp(warp_id).set_next_pc(pc);
+        warp(warp_id).ibuffer_flush();
+      }
+      if (warp_inst_issued) {
+        SCHED_DPRINTF(
+            "Warp (warp_id %u, dynamic_warp_id %u) issued %u instructions\n",
+            (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id(), issued);
+        do_on_warp_issued(warp_id, issued, iter);
+      }
+      checked++;
+    }
+    if (issued) {
+      // This might be a bit inefficient, but we need to maintain
+      // two ordered list for proper scheduler execution.
+      // We could remove the need for this loop by associating a
+      // supervised_is index with each entry in the
+      // m_next_cycle_prioritized_warps vector. For now, just run through until
+      // you find the right warp_id
+      for (std::vector<shd_warp_t *>::const_iterator supervised_iter =
+               m_supervised_warps.begin();
+           supervised_iter != m_supervised_warps.end(); ++supervised_iter) {
+        if (*iter == *supervised_iter) {
+          m_last_supervised_issued = supervised_iter;
+        }
+      }
+      m_num_issued_last_cycle = issued;
+      if (issued == 1)
+        m_stats->single_issue_nums[m_id]++;
+      else if (issued > 1)
+        m_stats->dual_issue_nums[m_id]++;
+      else
+        abort();  // issued should be > 0
+
+      break;
     }
+  }
 
-    // issue stall statistics:
-    if( !valid_inst ) 
-        m_stats->shader_cycle_distro[0]++; // idle or control hazard
-    else if( !ready_inst ) 
-        m_stats->shader_cycle_distro[1]++; // waiting for RAW hazards (possibly due to memory) 
-    else if( !issued_inst ) 
-        m_stats->shader_cycle_distro[2]++; // pipeline stalled
+  // issue stall statistics:
+  if (!valid_inst)
+    m_stats->shader_cycle_distro[0]++;  // idle or control hazard
+  else if (!ready_inst)
+    m_stats->shader_cycle_distro[1]++;  // waiting for RAW hazards (possibly due
+                                        // to memory)
+  else if (!issued_inst)
+    m_stats->shader_cycle_distro[2]++;  // pipeline stalled
 }
 
-void scheduler_unit::do_on_warp_issued( unsigned warp_id,
-                                        unsigned num_issued,
-                                        const std::vector< shd_warp_t* >::const_iterator& prioritized_iter )
-{
-    m_stats->event_warp_issued( m_shader->get_sid(),
-                                warp_id,
-                                num_issued,
-                                warp(warp_id).get_dynamic_warp_id() );
-    warp(warp_id).ibuffer_step();
+void scheduler_unit::do_on_warp_issued(
+    unsigned warp_id, unsigned num_issued,
+    const std::vector<shd_warp_t *>::const_iterator &prioritized_iter) {
+  m_stats->event_warp_issued(m_shader->get_sid(), warp_id, num_issued,
+                             warp(warp_id).get_dynamic_warp_id());
+  warp(warp_id).ibuffer_step();
 }
 
-bool scheduler_unit::sort_warps_by_oldest_dynamic_id(shd_warp_t* lhs, shd_warp_t* rhs)
-{
-    if (rhs && lhs) {
-        if ( lhs->done_exit() || lhs->waiting() ) {
-            return false;
-        } else if ( rhs->done_exit() || rhs->waiting() ) {
-            return true;
-        } else {
-            return lhs->get_dynamic_warp_id() < rhs->get_dynamic_warp_id();
-        }
+bool scheduler_unit::sort_warps_by_oldest_dynamic_id(shd_warp_t *lhs,
+                                                     shd_warp_t *rhs) {
+  if (rhs && lhs) {
+    if (lhs->done_exit() || lhs->waiting()) {
+      return false;
+    } else if (rhs->done_exit() || rhs->waiting()) {
+      return true;
     } else {
-        return lhs < rhs;
+      return lhs->get_dynamic_warp_id() < rhs->get_dynamic_warp_id();
     }
+  } else {
+    return lhs < rhs;
+  }
 }
 
-void lrr_scheduler::order_warps()
-{
-    order_lrr( m_next_cycle_prioritized_warps,
-               m_supervised_warps,
-               m_last_supervised_issued,
-               m_supervised_warps.size() );
+void lrr_scheduler::order_warps() {
+  order_lrr(m_next_cycle_prioritized_warps, m_supervised_warps,
+            m_last_supervised_issued, m_supervised_warps.size());
 }
-
-void gto_scheduler::order_warps()
-{
-    order_by_priority( m_next_cycle_prioritized_warps,
-                       m_supervised_warps,
-                       m_last_supervised_issued,
-                       m_supervised_warps.size(),
-                       ORDERING_GREEDY_THEN_PRIORITY_FUNC,
-                       scheduler_unit::sort_warps_by_oldest_dynamic_id );
+void rrr_scheduler::order_warps() {
+  order_rrr(m_next_cycle_prioritized_warps, m_supervised_warps,
+            m_last_supervised_issued, m_supervised_warps.size());
 }
 
-void oldest_scheduler::order_warps()
-{
-    order_by_priority( m_next_cycle_prioritized_warps,
-                       m_supervised_warps,
-                       m_last_supervised_issued,
-                       m_supervised_warps.size(),
-		       ORDERED_PRIORITY_FUNC_ONLY,
-                       scheduler_unit::sort_warps_by_oldest_dynamic_id );
-}
-
-void
-two_level_active_scheduler::do_on_warp_issued( unsigned warp_id,
-                                               unsigned num_issued,
-                                               const std::vector< shd_warp_t* >::const_iterator& prioritized_iter )
-{
-    scheduler_unit::do_on_warp_issued( warp_id, num_issued, prioritized_iter );
-    if ( SCHEDULER_PRIORITIZATION_LRR == m_inner_level_prioritization ) {
-        std::vector< shd_warp_t* > new_active; 
-        order_lrr( new_active,
-                   m_next_cycle_prioritized_warps,
-                   prioritized_iter,
-                   m_next_cycle_prioritized_warps.size() );
-        m_next_cycle_prioritized_warps = new_active;
-    } else {
-        fprintf( stderr,
-                 "Unimplemented m_inner_level_prioritization: %d\n",
-                 m_inner_level_prioritization );
-        abort();
-    }
+void gto_scheduler::order_warps() {
+  order_by_priority(m_next_cycle_prioritized_warps, m_supervised_warps,
+                    m_last_supervised_issued, m_supervised_warps.size(),
+                    ORDERING_GREEDY_THEN_PRIORITY_FUNC,
+                    scheduler_unit::sort_warps_by_oldest_dynamic_id);
 }
 
-void two_level_active_scheduler::order_warps()
-{
-    //Move waiting warps to m_pending_warps
-    unsigned num_demoted = 0;
-    for (   std::vector< shd_warp_t* >::iterator iter = m_next_cycle_prioritized_warps.begin();
-            iter != m_next_cycle_prioritized_warps.end(); ) {
-        bool waiting = (*iter)->waiting();
-        for (int i=0; i<MAX_INPUT_VALUES; i++){
-            const warp_inst_t* inst = (*iter)->ibuffer_next_inst();
-            //Is the instruction waiting on a long operation?
-            if ( inst && inst->in[i] > 0 && this->m_scoreboard->islongop((*iter)->get_warp_id(), inst->in[i])){
-                waiting = true;
-            }
-        }
+void oldest_scheduler::order_warps() {
+  order_by_priority(m_next_cycle_prioritized_warps, m_supervised_warps,
+                    m_last_supervised_issued, m_supervised_warps.size(),
+                    ORDERED_PRIORITY_FUNC_ONLY,
+                    scheduler_unit::sort_warps_by_oldest_dynamic_id);
+}
 
-        if( waiting ) {
-            m_pending_warps.push_back(*iter);
-            iter = m_next_cycle_prioritized_warps.erase(iter);
-            SCHED_DPRINTF( "DEMOTED warp_id=%d, dynamic_warp_id=%d\n",
-                           (*iter)->get_warp_id(),
-                           (*iter)->get_dynamic_warp_id() );
-            ++num_demoted;
-        } else {
-            ++iter;
-        }
-    }
+void two_level_active_scheduler::do_on_warp_issued(
+    unsigned warp_id, unsigned num_issued,
+    const std::vector<shd_warp_t *>::const_iterator &prioritized_iter) {
+  scheduler_unit::do_on_warp_issued(warp_id, num_issued, prioritized_iter);
+  if (SCHEDULER_PRIORITIZATION_LRR == m_inner_level_prioritization) {
+    std::vector<shd_warp_t *> new_active;
+    order_lrr(new_active, m_next_cycle_prioritized_warps, prioritized_iter,
+              m_next_cycle_prioritized_warps.size());
+    m_next_cycle_prioritized_warps = new_active;
+  } else {
+    fprintf(stderr, "Unimplemented m_inner_level_prioritization: %d\n",
+            m_inner_level_prioritization);
+    abort();
+  }
+}
 
-    //If there is space in m_next_cycle_prioritized_warps, promote the next m_pending_warps
-    unsigned num_promoted = 0;
-    if ( SCHEDULER_PRIORITIZATION_SRR == m_outer_level_prioritization ) {
-        while ( m_next_cycle_prioritized_warps.size() < m_max_active_warps ) {
-            m_next_cycle_prioritized_warps.push_back(m_pending_warps.front());
-            m_pending_warps.pop_front();
-            SCHED_DPRINTF( "PROMOTED warp_id=%d, dynamic_warp_id=%d\n",
-                           (m_next_cycle_prioritized_warps.back())->get_warp_id(),
-                           (m_next_cycle_prioritized_warps.back())->get_dynamic_warp_id() );
-            ++num_promoted;
-        }
-    } else {
-        fprintf( stderr,
-                 "Unimplemented m_outer_level_prioritization: %d\n",
-                 m_outer_level_prioritization );
-        abort();
+void two_level_active_scheduler::order_warps() {
+  // Move waiting warps to m_pending_warps
+  unsigned num_demoted = 0;
+  for (std::vector<shd_warp_t *>::iterator iter =
+           m_next_cycle_prioritized_warps.begin();
+       iter != m_next_cycle_prioritized_warps.end();) {
+    bool waiting = (*iter)->waiting();
+    for (int i = 0; i < MAX_INPUT_VALUES; i++) {
+      const warp_inst_t *inst = (*iter)->ibuffer_next_inst();
+      // Is the instruction waiting on a long operation?
+      if (inst && inst->in[i] > 0 &&
+          this->m_scoreboard->islongop((*iter)->get_warp_id(), inst->in[i])) {
+        waiting = true;
+      }
     }
-    assert( num_promoted == num_demoted );
-}
-
-swl_scheduler::swl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                               Scoreboard* scoreboard, simt_stack** simt,
-                               std::vector<shd_warp_t>* warp,
-                               register_set* sp_out,
-							   register_set* dp_out,
-                               register_set* sfu_out,
-							   register_set* int_out,
-                               register_set* tensor_core_out,
-                               register_set* mem_out,
-                               int id,
-                               char* config_string )
-    : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, dp_out, sfu_out, int_out, tensor_core_out, mem_out, id )
-{
-    unsigned m_prioritization_readin;
-    int ret = sscanf( config_string,
-                      "warp_limiting:%d:%d",
-                      &m_prioritization_readin,
-                      &m_num_warps_to_limit
-                     );
-    assert( 2 == ret );
-    m_prioritization = (scheduler_prioritization_type)m_prioritization_readin;
-    // Currently only GTO is implemented
-    assert( m_prioritization == SCHEDULER_PRIORITIZATION_GTO );
-    assert( m_num_warps_to_limit <= shader->get_config()->max_warps_per_shader );
-}
-
-void swl_scheduler::order_warps()
-{
-    if ( SCHEDULER_PRIORITIZATION_GTO == m_prioritization ) {
-        order_by_priority( m_next_cycle_prioritized_warps,
-                           m_supervised_warps,
-                           m_last_supervised_issued,
-                           MIN( m_num_warps_to_limit, m_supervised_warps.size() ),
-                           ORDERING_GREEDY_THEN_PRIORITY_FUNC,
-                           scheduler_unit::sort_warps_by_oldest_dynamic_id );
+
+    if (waiting) {
+      m_pending_warps.push_back(*iter);
+      iter = m_next_cycle_prioritized_warps.erase(iter);
+      SCHED_DPRINTF("DEMOTED warp_id=%d, dynamic_warp_id=%d\n",
+                    (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id());
+      ++num_demoted;
     } else {
-        fprintf(stderr, "swl_scheduler m_prioritization = %d\n", m_prioritization);
-        abort();
+      ++iter;
     }
-}
-
-void shader_core_ctx::read_operands()
-{
-}
+  }
 
-address_type coalesced_segment(address_type addr, unsigned segment_size_lg2bytes)
-{
-   return  (addr >> segment_size_lg2bytes);
+  // If there is space in m_next_cycle_prioritized_warps, promote the next
+  // m_pending_warps
+  unsigned num_promoted = 0;
+  if (SCHEDULER_PRIORITIZATION_SRR == m_outer_level_prioritization) {
+    while (m_next_cycle_prioritized_warps.size() < m_max_active_warps) {
+      m_next_cycle_prioritized_warps.push_back(m_pending_warps.front());
+      m_pending_warps.pop_front();
+      SCHED_DPRINTF(
+          "PROMOTED warp_id=%d, dynamic_warp_id=%d\n",
+          (m_next_cycle_prioritized_warps.back())->get_warp_id(),
+          (m_next_cycle_prioritized_warps.back())->get_dynamic_warp_id());
+      ++num_promoted;
+    }
+  } else {
+    fprintf(stderr, "Unimplemented m_outer_level_prioritization: %d\n",
+            m_outer_level_prioritization);
+    abort();
+  }
+  assert(num_promoted == num_demoted);
+}
+
+swl_scheduler::swl_scheduler(shader_core_stats *stats, shader_core_ctx *shader,
+                             Scoreboard *scoreboard, simt_stack **simt,
+                             std::vector<shd_warp_t *> *warp,
+                             register_set *sp_out, register_set *dp_out,
+                             register_set *sfu_out, register_set *int_out,
+                             register_set *tensor_core_out,
+                             std::vector<register_set *> &spec_cores_out,
+                             register_set *mem_out, int id, char *config_string)
+    : scheduler_unit(stats, shader, scoreboard, simt, warp, sp_out, dp_out,
+                     sfu_out, int_out, tensor_core_out, spec_cores_out, mem_out,
+                     id) {
+  unsigned m_prioritization_readin;
+  int ret = sscanf(config_string, "warp_limiting:%d:%d",
+                   &m_prioritization_readin, &m_num_warps_to_limit);
+  assert(2 == ret);
+  m_prioritization = (scheduler_prioritization_type)m_prioritization_readin;
+  // Currently only GTO is implemented
+  assert(m_prioritization == SCHEDULER_PRIORITIZATION_GTO);
+  assert(m_num_warps_to_limit <= shader->get_config()->max_warps_per_shader);
+}
+
+void swl_scheduler::order_warps() {
+  if (SCHEDULER_PRIORITIZATION_GTO == m_prioritization) {
+    order_by_priority(m_next_cycle_prioritized_warps, m_supervised_warps,
+                      m_last_supervised_issued,
+                      MIN(m_num_warps_to_limit, m_supervised_warps.size()),
+                      ORDERING_GREEDY_THEN_PRIORITY_FUNC,
+                      scheduler_unit::sort_warps_by_oldest_dynamic_id);
+  } else {
+    fprintf(stderr, "swl_scheduler m_prioritization = %d\n", m_prioritization);
+    abort();
+  }
 }
 
-// Returns numbers of addresses in translated_addrs, each addr points to a 4B (32-bit) word
-unsigned shader_core_ctx::translate_local_memaddr( address_type localaddr, unsigned tid, unsigned num_shader, unsigned datasize, new_addr_type* translated_addrs )
-{
-   // During functional execution, each thread sees its own memory space for local memory, but these
-   // need to be mapped to a shared address space for timing simulation.  We do that mapping here.
-
-   address_type thread_base = 0;
-   unsigned max_concurrent_threads=0;
-   if (m_config->gpgpu_local_mem_map) {
-      // Dnew = D*N + T%nTpC + nTpC*C
-      // N = nTpC*nCpS*nS (max concurent threads)
-      // C = nS*K + S (hw cta number per gpu)
-      // K = T/nTpC   (hw cta number per core)
-      // D = data index
-      // T = thread
-      // nTpC = number of threads per CTA
-      // nCpS = number of CTA per shader
-      // 
-      // for a given local memory address threads in a CTA map to contiguous addresses,
-      // then distribute across memory space by CTAs from successive shader cores first, 
-      // then by successive CTA in same shader core
-      thread_base = 4*(kernel_padded_threads_per_cta * (m_sid + num_shader * (tid / kernel_padded_threads_per_cta))
-                       + tid % kernel_padded_threads_per_cta); 
-      max_concurrent_threads = kernel_padded_threads_per_cta * kernel_max_cta_per_shader * num_shader;
-   } else {
+void shader_core_ctx::read_operands() {
+  for (int i = 0; i < m_config->reg_file_port_throughput; ++i)
+    m_operand_collector.step();
+}
+
+address_type coalesced_segment(address_type addr,
+                               unsigned segment_size_lg2bytes) {
+  return (addr >> segment_size_lg2bytes);
+}
+
+// Returns numbers of addresses in translated_addrs, each addr points to a 4B
+// (32-bit) word
+unsigned shader_core_ctx::translate_local_memaddr(
+    address_type localaddr, unsigned tid, unsigned num_shader,
+    unsigned datasize, new_addr_type *translated_addrs) {
+  // During functional execution, each thread sees its own memory space for
+  // local memory, but these need to be mapped to a shared address space for
+  // timing simulation.  We do that mapping here.
+
+  address_type thread_base = 0;
+  unsigned max_concurrent_threads = 0;
+  if (m_config->gpgpu_local_mem_map) {
+    // Dnew = D*N + T%nTpC + nTpC*C
+    // N = nTpC*nCpS*nS (max concurent threads)
+    // C = nS*K + S (hw cta number per gpu)
+    // K = T/nTpC   (hw cta number per core)
+    // D = data index
+    // T = thread
+    // nTpC = number of threads per CTA
+    // nCpS = number of CTA per shader
+    //
+    // for a given local memory address threads in a CTA map to contiguous
+    // addresses, then distribute across memory space by CTAs from successive
+    // shader cores first, then by successive CTA in same shader core
+    thread_base =
+        4 * (kernel_padded_threads_per_cta *
+                 (m_sid + num_shader * (tid / kernel_padded_threads_per_cta)) +
+             tid % kernel_padded_threads_per_cta);
+    max_concurrent_threads =
+        kernel_padded_threads_per_cta * kernel_max_cta_per_shader * num_shader;
+  } else {
       panic("gem5-gpu does not work with legacy local mapping!\n");
-      // legacy mapping that maps the same address in the local memory space of all threads 
-      // to a single contiguous address region 
-      thread_base = 4*(m_config->n_thread_per_shader * m_sid + tid);
-      max_concurrent_threads = num_shader * m_config->n_thread_per_shader;
-   }
-   assert( thread_base < 4/*word size*/*max_concurrent_threads );
-
-   // If requested datasize > 4B, split into multiple 4B accesses
-   // otherwise do one sub-4 byte memory access
-   unsigned num_accesses = 0;
-
-   if(datasize >= 4) {
-      // >4B access, split into 4B chunks
-      assert(datasize%4 == 0);   // Must be a multiple of 4B
-      num_accesses = datasize/4;
-      assert(num_accesses <= MAX_ACCESSES_PER_INSN_PER_THREAD); // max 32B
-      assert(localaddr%4 == 0); // Address must be 4B aligned - required if accessing 4B per request, otherwise access will overflow into next thread's space
-      for(unsigned i=0; i<num_accesses; i++) {
-          address_type local_word = localaddr/4 + i;
-          // TODO schi 
-          // address_type linear_address = local_word*max_concurrent_threads*4 + thread_base + LOCAL_GENERIC_START;
-          address_type linear_address = local_word*max_concurrent_threads*4 + thread_base + get_gpu()->gem5CudaGPU->getLocalBaseVaddr();
-          assert(linear_address < get_gpu()->gem5CudaGPU->getLocalBaseVaddr() + (LOCAL_MEM_SIZE_MAX * max_concurrent_threads));
-          translated_addrs[i] = linear_address;
-      }
-   } else {
-      // Sub-4B access, do only one access
-      assert(datasize > 0);
-      num_accesses = 1;
-      address_type local_word = localaddr/4;
-      address_type local_word_offset = localaddr%4;
-      assert( (localaddr+datasize-1)/4  == local_word ); // Make sure access doesn't overflow into next 4B chunk
-      // address_type linear_address = local_word*max_concurrent_threads*4 + local_word_offset + thread_base + LOCAL_GENERIC_START;
-      address_type linear_address = local_word*max_concurrent_threads*4 + local_word_offset + thread_base + get_gpu()->gem5CudaGPU->getLocalBaseVaddr();
+    // legacy mapping that maps the same address in the local memory space of
+    // all threads to a single contiguous address region
+    thread_base = 4 * (m_config->n_thread_per_shader * m_sid + tid);
+    max_concurrent_threads = num_shader * m_config->n_thread_per_shader;
+  }
+  assert(thread_base < 4 /*word size*/ * max_concurrent_threads);
+
+  // If requested datasize > 4B, split into multiple 4B accesses
+  // otherwise do one sub-4 byte memory access
+  unsigned num_accesses = 0;
+
+  if (datasize >= 4) {
+    // >4B access, split into 4B chunks
+    assert(datasize % 4 == 0);  // Must be a multiple of 4B
+    num_accesses = datasize / 4;
+    assert(num_accesses <= MAX_ACCESSES_PER_INSN_PER_THREAD);  // max 32B
+    assert(
+        localaddr % 4 ==
+        0);  // Address must be 4B aligned - required if accessing 4B per
+             // request, otherwise access will overflow into next thread's space
+    for (unsigned i = 0; i < num_accesses; i++) {
+      address_type local_word = localaddr / 4 + i;
+      // TODO schi 
+      // address_type linear_address = local_word * max_concurrent_threads * 4 +
+                                    thread_base + LOCAL_GENERIC_START;
+      address_type linear_address = local_word*max_concurrent_threads*4 +
+                                    thread_base + get_gpu()->gem5CudaGPU->getLocalBaseVaddr();
       assert(linear_address < get_gpu()->gem5CudaGPU->getLocalBaseVaddr() + (LOCAL_MEM_SIZE_MAX * max_concurrent_threads));
-      translated_addrs[0] = linear_address;
-   }
-   return num_accesses;
+      translated_addrs[i] = linear_address;
+    }
+  } else {
+    // Sub-4B access, do only one access
+    assert(datasize > 0);
+    num_accesses = 1;
+    address_type local_word = localaddr / 4;
+    address_type local_word_offset = localaddr % 4;
+    assert((localaddr + datasize - 1) / 4 ==
+           local_word);  // Make sure access doesn't overflow into next 4B chunk
+    // TODO schi
+    // address_type linear_address = local_word * max_concurrent_threads * 4 +
+    //                              local_word_offset + thread_base +
+    //                              LOCAL_GENERIC_START;
+    address_type linear_address = local_word * max_concurrent_threads * 4 +
+                                    local_word_offset + thread_base +
+                                    get_gpu()->gem5CudaGPU->getLocalBaseVaddr();
+    assert(linear_address < get_gpu()->gem5CudaGPU->getLocalBaseVaddr() + (LOCAL_MEM_SIZE_MAX * max_concurrent_threads));
+    translated_addrs[0] = linear_address;
+  }
+  return num_accesses;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////
-int shader_core_ctx::test_res_bus(int latency){
-	for(unsigned i=0; i<num_result_bus; i++){
-		if(!m_result_bus[i]->test(latency)){return i;}
-	}
-	return -1;
+int shader_core_ctx::test_res_bus(int latency) {
+  for (unsigned i = 0; i < num_result_bus; i++) {
+    if (!m_result_bus[i]->test(latency)) {
+      return i;
+    }
+  }
+  return -1;
 }
 
-void shader_core_ctx::execute()
-{
-	for(unsigned i=0; i<num_result_bus; i++){
-		*(m_result_bus[i]) >>=1;
-	}
-    for( unsigned n=0; n < m_num_function_units; n++ ) {
-        unsigned multiplier = m_fu[n]->clock_multiplier();
-        for( unsigned c=0; c < multiplier; c++ ) 
-            m_fu[n]->cycle();
-        m_fu[n]->active_lanes_in_pipeline();
-        enum pipeline_stage_name_t issue_port = m_issue_port[n];
-        register_set& issue_inst = m_pipeline_reg[ issue_port ];
-        warp_inst_t** ready_reg = issue_inst.get_ready();
-        if( issue_inst.has_ready() && m_fu[n]->can_issue( **ready_reg ) ) {
-            bool schedule_wb_now = !m_fu[n]->stallable();
-            int resbus = -1;
-            if( schedule_wb_now && (resbus=test_res_bus( (*ready_reg)->latency ))!=-1 ) {
-                assert( (*ready_reg)->latency < MAX_ALU_LATENCY );
-                m_result_bus[resbus]->set( (*ready_reg)->latency );
-                m_fu[n]->issue( issue_inst );
-            } else if( !schedule_wb_now ) {
-                m_fu[n]->issue( issue_inst );
-            } else {
-                // stall issue (cannot reserve result bus)
-            }
-        }
+void shader_core_ctx::execute() {
+  for (unsigned i = 0; i < num_result_bus; i++) {
+    *(m_result_bus[i]) >>= 1;
+  }
+  for (unsigned n = 0; n < m_num_function_units; n++) {
+    unsigned multiplier = m_fu[n]->clock_multiplier();
+    for (unsigned c = 0; c < multiplier; c++) m_fu[n]->cycle();
+    m_fu[n]->active_lanes_in_pipeline();
+    unsigned issue_port = m_issue_port[n];
+    register_set &issue_inst = m_pipeline_reg[issue_port];
+    unsigned reg_id;
+    bool partition_issue =
+        m_config->sub_core_model && m_fu[n]->is_issue_partitioned();
+    if (partition_issue) {
+      reg_id = m_fu[n]->get_issue_reg_id();
     }
+    warp_inst_t **ready_reg = issue_inst.get_ready(partition_issue, reg_id);
+    if (issue_inst.has_ready(partition_issue, reg_id) &&
+        m_fu[n]->can_issue(**ready_reg)) {
+      bool schedule_wb_now = !m_fu[n]->stallable();
+      int resbus = -1;
+      if (schedule_wb_now &&
+          (resbus = test_res_bus((*ready_reg)->latency)) != -1) {
+        assert((*ready_reg)->latency < MAX_ALU_LATENCY);
+        m_result_bus[resbus]->set((*ready_reg)->latency);
+        m_fu[n]->issue(issue_inst);
+      } else if (!schedule_wb_now) {
+        m_fu[n]->issue(issue_inst);
+      } else {
+        // stall issue (cannot reserve result bus)
+      }
+    }
+  }
 }
 
-void ldst_unit::print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) {
-   if( m_L1D ) {
-       m_L1D->print( fp, dl1_accesses, dl1_misses );
-   }
+void ldst_unit::print_cache_stats(FILE *fp, unsigned &dl1_accesses,
+                                  unsigned &dl1_misses) {
+  if (m_L1D) {
+    m_L1D->print(fp, dl1_accesses, dl1_misses);
+  }
 }
 
 void ldst_unit::get_cache_stats(cache_stats &cs) {
-    // Adds stats to 'cs' from each cache
-    if(m_L1D)
-        cs += m_L1D->get_stats();
-    if(m_L1C)
-        cs += m_L1C->get_stats();
-    if(m_L1T)
-        cs += m_L1T->get_stats();
-
+  // Adds stats to 'cs' from each cache
+  if (m_L1D) cs += m_L1D->get_stats();
+  if (m_L1C) cs += m_L1C->get_stats();
+  if (m_L1T) cs += m_L1T->get_stats();
 }
 
-void ldst_unit::get_L1D_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1D)
-        m_L1D->get_sub_stats(css);
+void ldst_unit::get_L1D_sub_stats(struct cache_sub_stats &css) const {
+  if (m_L1D) m_L1D->get_sub_stats(css);
 }
-void ldst_unit::get_L1C_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1C)
-        m_L1C->get_sub_stats(css);
+void ldst_unit::get_L1C_sub_stats(struct cache_sub_stats &css) const {
+  if (m_L1C) m_L1C->get_sub_stats(css);
 }
-void ldst_unit::get_L1T_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1T)
-        m_L1T->get_sub_stats(css);
+void ldst_unit::get_L1T_sub_stats(struct cache_sub_stats &css) const {
+  if (m_L1T) m_L1T->get_sub_stats(css);
 }
 
-void shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)
-{
-
-  #if 0
-      printf("[warp_inst_complete] uid=%u core=%u warp=%u pc=%#x @ time=%llu issued@%llu\n",
-             inst.get_uid(), m_sid, inst.warp_id(), inst.pc, gpu_tot_sim_cycle + gpu_sim_cycle, inst.get_issue_cycle());
-  #endif
+void shader_core_ctx::warp_inst_complete(const warp_inst_t &inst) {
+#if 0
+      printf("[warp_inst_complete] uid=%u core=%u warp=%u pc=%#x @ time=%llu \n",
+             inst.get_uid(), m_sid, inst.warp_id(), inst.pc,  m_gpu->gpu_tot_sim_cycle +  m_gpu->gpu_sim_cycle);
+#endif
 
-  if(inst.op_pipe==SP__OP)
-	  m_stats->m_num_sp_committed[m_sid]++;
-  else if(inst.op_pipe==SFU__OP)
-	  m_stats->m_num_sfu_committed[m_sid]++;
-  else if(inst.op_pipe==MEM__OP)
-	  m_stats->m_num_mem_committed[m_sid]++;
+  if (inst.op_pipe == SP__OP)
+    m_stats->m_num_sp_committed[m_sid]++;
+  else if (inst.op_pipe == SFU__OP)
+    m_stats->m_num_sfu_committed[m_sid]++;
+  else if (inst.op_pipe == MEM__OP)
+    m_stats->m_num_mem_committed[m_sid]++;
 
-  if(m_config->gpgpu_clock_gated_lanes==false)
-	  m_stats->m_num_sim_insn[m_sid] += m_config->warp_size;
+  if (m_config->gpgpu_clock_gated_lanes == false)
+    m_stats->m_num_sim_insn[m_sid] += m_config->warp_size;
   else
-	  m_stats->m_num_sim_insn[m_sid] += inst.active_count();
+    m_stats->m_num_sim_insn[m_sid] += inst.active_count();
 
   m_stats->m_num_sim_winsn[m_sid]++;
   m_gpu->gpu_sim_insn += inst.active_count();
-  inst.completed(gpu_tot_sim_cycle + gpu_sim_cycle);
+  inst.completed(m_gpu->gpu_tot_sim_cycle + m_gpu->gpu_sim_cycle);
 }
 
-void shader_core_ctx::writeback()
-{
+void shader_core_ctx::writeback() {
+  unsigned max_committed_thread_instructions =
+      m_config->warp_size *
+      (m_config->pipe_widths[EX_WB]);  // from the functional units
+  m_stats->m_pipeline_duty_cycle[m_sid] =
+      ((float)(m_stats->m_num_sim_insn[m_sid] -
+               m_stats->m_last_num_sim_insn[m_sid])) /
+      max_committed_thread_instructions;
 
-	unsigned max_committed_thread_instructions=m_config->warp_size * (m_config->pipe_widths[EX_WB]); //from the functional units
-	m_stats->m_pipeline_duty_cycle[m_sid]=((float)(m_stats->m_num_sim_insn[m_sid]-m_stats->m_last_num_sim_insn[m_sid]))/max_committed_thread_instructions;
-
-    m_stats->m_last_num_sim_insn[m_sid]=m_stats->m_num_sim_insn[m_sid];
-    m_stats->m_last_num_sim_winsn[m_sid]=m_stats->m_num_sim_winsn[m_sid];
-
-    warp_inst_t** preg = m_pipeline_reg[EX_WB].get_ready();
-    warp_inst_t* pipe_reg = (preg==NULL)? NULL:*preg;
-    while( preg and !pipe_reg->empty()) {
-    	/*
-    	 * Right now, the writeback stage drains all waiting instructions
-    	 * assuming there are enough ports in the register file or the
-    	 * conflicts are resolved at issue.
-    	 */
-    	/*
-    	 * The operand collector writeback can generally generate a stall
-    	 * However, here, the pipelines should be un-stallable. This is
-    	 * guaranteed because this is the first time the writeback function
-    	 * is called after the operand collector's step function, which
-    	 * resets the allocations. There is one case which could result in
-    	 * the writeback function returning false (stall), which is when
-    	 * an instruction tries to modify two registers (GPR and predicate)
-    	 * To handle this case, we ignore the return value (thus allowing
-    	 * no stalling).
-    	 */
-
-        m_operand_collector.writeback(*pipe_reg);
-        unsigned warp_id = pipe_reg->warp_id();
-        m_scoreboard->releaseRegisters( pipe_reg );
-        m_warp[warp_id].dec_inst_in_pipeline();
-        warp_inst_complete(*pipe_reg);
-        m_gpu->gpu_sim_insn_last_update_sid = m_sid;
-        m_gpu->gpu_sim_insn_last_update = gpu_sim_cycle;
-        m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
-        m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
-        pipe_reg->clear();
-        preg = m_pipeline_reg[EX_WB].get_ready();
-        pipe_reg = (preg==NULL)? NULL:*preg;
-    }
-}
+  m_stats->m_last_num_sim_insn[m_sid] = m_stats->m_num_sim_insn[m_sid];
+  m_stats->m_last_num_sim_winsn[m_sid] = m_stats->m_num_sim_winsn[m_sid];
 
-bool ldst_unit::shared_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type)
-{
-   if( inst.space.get_type() != shared_space )
-       return true;
+  warp_inst_t **preg = m_pipeline_reg[EX_WB].get_ready();
+  warp_inst_t *pipe_reg = (preg == NULL) ? NULL : *preg;
+  while (preg and !pipe_reg->empty()) {
+    /*
+     * Right now, the writeback stage drains all waiting instructions
+     * assuming there are enough ports in the register file or the
+     * conflicts are resolved at issue.
+     */
+    /*
+     * The operand collector writeback can generally generate a stall
+     * However, here, the pipelines should be un-stallable. This is
+     * guaranteed because this is the first time the writeback function
+     * is called after the operand collector's step function, which
+     * resets the allocations. There is one case which could result in
+     * the writeback function returning false (stall), which is when
+     * an instruction tries to modify two registers (GPR and predicate)
+     * To handle this case, we ignore the return value (thus allowing
+     * no stalling).
+     */
+
+    m_operand_collector.writeback(*pipe_reg);
+    unsigned warp_id = pipe_reg->warp_id();
+    m_scoreboard->releaseRegisters(pipe_reg);
+    m_warp[warp_id]->dec_inst_in_pipeline();
+    warp_inst_complete(*pipe_reg);
+    m_gpu->gpu_sim_insn_last_update_sid = m_sid;
+    m_gpu->gpu_sim_insn_last_update = m_gpu->gpu_sim_cycle;
+    m_last_inst_gpu_sim_cycle = m_gpu->gpu_sim_cycle;
+    m_last_inst_gpu_tot_sim_cycle = m_gpu->gpu_tot_sim_cycle;
+    pipe_reg->clear();
+    preg = m_pipeline_reg[EX_WB].get_ready();
+    pipe_reg = (preg == NULL) ? NULL : *preg;
+  }
+}
 
-   if(inst.has_dispatch_delay()){
-	   m_stats->gpgpu_n_shmem_bank_access[m_sid]++;
-   }
+bool ldst_unit::shared_cycle(warp_inst_t &inst, mem_stage_stall_type &rc_fail,
+                             mem_stage_access_type &fail_type) {
+  if (inst.space.get_type() != shared_space) return true;
 
-   bool stall = inst.dispatch_delay();
-   if( stall ) {
-       fail_type = S_MEM;
-       rc_fail = BK_CONF;
-   } else 
-       rc_fail = NO_RC_FAIL;
-   return !stall; 
-}
-
-mem_stage_stall_type
-ldst_unit::process_cache_access( cache_t* cache,
-                                 new_addr_type address,
-                                 warp_inst_t &inst,
-                                 std::list<cache_event>& events,
-                                 mem_fetch *mf,
-                                 enum cache_request_status status )
-{
-    mem_stage_stall_type result = NO_RC_FAIL;
-    bool write_sent = was_write_sent(events);
-    bool read_sent = was_read_sent(events);
-    if( write_sent ) {
-    	unsigned inc_ack = (m_config->m_L1D_config.get_mshr_type() == SECTOR_ASSOC)?
-    			(mf->get_data_size()/SECTOR_SIZE) : 1;
+  if (inst.active_count() == 0) return true;
 
-		for(unsigned i=0; i< inc_ack; ++i)
-			m_core->inc_store_req( inst.warp_id() );
+  if (inst.has_dispatch_delay()) {
+    m_stats->gpgpu_n_shmem_bank_access[m_sid]++;
+  }
 
-    }
-    if ( status == HIT ) {
+  bool stall = inst.dispatch_delay();
+  if (stall) {
+    fail_type = S_MEM;
+    rc_fail = BK_CONF;
+  } else
+    rc_fail = NO_RC_FAIL;
+  return !stall;
+}
+
+mem_stage_stall_type ldst_unit::process_cache_access(
+    cache_t *cache, new_addr_type address, warp_inst_t &inst,
+    std::list<cache_event> &events, mem_fetch *mf,
+    enum cache_request_status status) {
+  mem_stage_stall_type result = NO_RC_FAIL;
+  bool write_sent = was_write_sent(events);
+  bool read_sent = was_read_sent(events);
+  if (write_sent) {
+    unsigned inc_ack = (m_config->m_L1D_config.get_mshr_type() == SECTOR_ASSOC)
+                           ? (mf->get_data_size() / SECTOR_SIZE)
+                           : 1;
+
+    for (unsigned i = 0; i < inc_ack; ++i)
+      m_core->inc_store_req(inst.warp_id());
+  }
+  if (status == HIT) {
         // HACK for gem5-gpu: Reads should not be sent with hit status
-        assert( !read_sent );
-        inst.accessq_pop_back();
-        if ( inst.is_load() ) {
-            for ( unsigned r=0; r < MAX_OUTPUT_VALUES; r++)
-                if (inst.out[r] > 0)
-                    m_pending_writes[inst.warp_id()][inst.out[r]]--; 
+    assert(!read_sent);
+    inst.accessq_pop_back();
+    if (inst.is_load()) {
+      for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++)
+        if (inst.out[r] > 0) m_pending_writes[inst.warp_id()][inst.out[r]]--;
+    }
+    if (!write_sent) delete mf;
+  } else if (status == RESERVATION_FAIL) {
+    result = BK_CONF;
+    // schi HACK for gem5-gpu: Reads should not be sent with reservation_fail status
+    if (read_sent) assert( !read_sent );
+    // assert( !read_sent );
+    assert( !write_sent );
+    delete mf;
+  } else {
+    assert( status == MISS || status == HIT_RESERVED );
+    //inst.clear_active( access.get_warp_mask() ); // threads in mf writeback when mf returns
+    inst.accessq_pop_back();
+  }
+  if (!inst.accessq_empty() && result == NO_RC_FAIL) result = COAL_STALL;
+  return result;
+}
+
+mem_stage_stall_type ldst_unit::process_memory_access_queue(cache_t *cache,
+                                                            warp_inst_t &inst) {
+  mem_stage_stall_type result = NO_RC_FAIL;
+  if (inst.accessq_empty()) return result;
+
+  if (!cache->data_port_free()) return DATA_PORT_STALL;
+
+  // const mem_access_t &access = inst.accessq_back();
+  mem_fetch *mf = m_mf_allocator->alloc(
+      inst, inst.accessq_back(),
+      m_core->get_gpu()->gpu_sim_cycle + m_core->get_gpu()->gpu_tot_sim_cycle);
+  std::list<cache_event> events;
+  enum cache_request_status status = cache->access(
+      mf->get_addr(), mf,
+      m_core->get_gpu()->gpu_sim_cycle + m_core->get_gpu()->gpu_tot_sim_cycle,
+      events);
+  return process_cache_access(cache, mf->get_addr(), inst, events, mf, status);
+}
+
+mem_stage_stall_type ldst_unit::process_memory_access_queue_l1cache(
+    l1_cache *cache, warp_inst_t &inst) {
+  mem_stage_stall_type result = NO_RC_FAIL;
+  if (inst.accessq_empty()) return result;
+
+  if (m_config->m_L1D_config.l1_latency > 0) {
+    for (int j = 0; j < m_config->m_L1D_config.l1_banks;
+         j++) {  // We can handle at max l1_banks reqs per cycle
+
+      if (inst.accessq_empty()) return result;
+
+      mem_fetch *mf =
+          m_mf_allocator->alloc(inst, inst.accessq_back(),
+                                m_core->get_gpu()->gpu_sim_cycle +
+                                    m_core->get_gpu()->gpu_tot_sim_cycle);
+      unsigned bank_id = m_config->m_L1D_config.set_bank(mf->get_addr());
+      assert(bank_id < m_config->m_L1D_config.l1_banks);
+
+      if ((l1_latency_queue[bank_id][m_config->m_L1D_config.l1_latency - 1]) ==
+          NULL) {
+        l1_latency_queue[bank_id][m_config->m_L1D_config.l1_latency - 1] = mf;
+
+        if (mf->get_inst().is_store()) {
+          unsigned inc_ack =
+              (m_config->m_L1D_config.get_mshr_type() == SECTOR_ASSOC)
+                  ? (mf->get_data_size() / SECTOR_SIZE)
+                  : 1;
+
+          for (unsigned i = 0; i < inc_ack; ++i)
+            m_core->inc_store_req(inst.warp_id());
         }
-        if( !write_sent ) 
-            delete mf;
-    } else if ( status == RESERVATION_FAIL ) {
+
+        inst.accessq_pop_back();
+      } else {
         result = BK_CONF;
-        // HACK for gem5-gpu: Reads should not be sent with reservation_fail status
-        if (read_sent) assert( !read_sent );
-        // assert( !read_sent );
-        assert( !write_sent );
         delete mf;
-    } else {
-        assert( status == MISS || status == HIT_RESERVED );
-        //inst.clear_active( access.get_warp_mask() ); // threads in mf writeback when mf returns
-        inst.accessq_pop_back();
+        break;  // do not try again, just break from the loop and try the next
+                // cycle
+      }
     }
-    if( !inst.accessq_empty() && result == NO_RC_FAIL)
-        result = COAL_STALL;
+    if (!inst.accessq_empty() && result != BK_CONF) result = COAL_STALL;
+
     return result;
+  } else {
+    mem_fetch *mf =
+        m_mf_allocator->alloc(inst, inst.accessq_back(),
+                              m_core->get_gpu()->gpu_sim_cycle +
+                                  m_core->get_gpu()->gpu_tot_sim_cycle);
+    std::list<cache_event> events;
+    enum cache_request_status status = cache->access(
+        mf->get_addr(), mf,
+        m_core->get_gpu()->gpu_sim_cycle + m_core->get_gpu()->gpu_tot_sim_cycle,
+        events);
+    return process_cache_access(cache, mf->get_addr(), inst, events, mf,
+                                status);
+  }
 }
 
-mem_stage_stall_type ldst_unit::process_memory_access_queue( cache_t *cache, warp_inst_t &inst )
-{
-    mem_stage_stall_type result = NO_RC_FAIL;
-    if( inst.accessq_empty() )
-        return result;
+void ldst_unit::L1_latency_queue_cycle() {
+  for (int j = 0; j < m_config->m_L1D_config.l1_banks; j++) {
+    if ((l1_latency_queue[j][0]) != NULL) {
+      mem_fetch *mf_next = l1_latency_queue[j][0];
+      std::list<cache_event> events;
+      enum cache_request_status status =
+          m_L1D->access(mf_next->get_addr(), mf_next,
+                        m_core->get_gpu()->gpu_sim_cycle +
+                            m_core->get_gpu()->gpu_tot_sim_cycle,
+                        events);
+
+      bool write_sent = was_write_sent(events);
+      bool read_sent = was_read_sent(events);
+
+      if (status == HIT) {
+        assert(!read_sent);
+        l1_latency_queue[j][0] = NULL;
+        if (mf_next->get_inst().is_load()) {
+          for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++)
+            if (mf_next->get_inst().out[r] > 0) {
+              assert(m_pending_writes[mf_next->get_inst().warp_id()]
+                                     [mf_next->get_inst().out[r]] > 0);
+              unsigned still_pending =
+                  --m_pending_writes[mf_next->get_inst().warp_id()]
+                                    [mf_next->get_inst().out[r]];
+              if (!still_pending) {
+                m_pending_writes[mf_next->get_inst().warp_id()].erase(
+                    mf_next->get_inst().out[r]);
+                m_scoreboard->releaseRegister(mf_next->get_inst().warp_id(),
+                                              mf_next->get_inst().out[r]);
+                m_core->warp_inst_complete(mf_next->get_inst());
+              }
+            }
+        }
 
-    if( !cache->data_port_free() ) 
-        return DATA_PORT_STALL; 
+        // For write hit in WB policy
+        if (mf_next->get_inst().is_store() && !write_sent) {
+          unsigned dec_ack =
+              (m_config->m_L1D_config.get_mshr_type() == SECTOR_ASSOC)
+                  ? (mf_next->get_data_size() / SECTOR_SIZE)
+                  : 1;
 
-    //const mem_access_t &access = inst.accessq_back();
-    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_back());
-    std::list<cache_event> events;
-    enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
-    return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );
-}
+          mf_next->set_reply();
 
-mem_stage_stall_type ldst_unit::process_memory_access_queue_l1cache( l1_cache *cache, warp_inst_t &inst )
-{
-    mem_stage_stall_type result = NO_RC_FAIL;
-    if( inst.accessq_empty() )
-        return result;
-
-    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_back());
-
-    if(m_config->m_L1D_config.l1_latency > 0)
-	{
-    	if((l1_latency_queue[m_config->m_L1D_config.l1_latency-1]) == NULL)
-    	{
-    		l1_latency_queue[m_config->m_L1D_config.l1_latency-1] = mf;
-
-    		if( mf->get_inst().is_store() ) {
-				unsigned inc_ack = (m_config->m_L1D_config.get_mshr_type() == SECTOR_ASSOC)?
-						(mf->get_data_size()/SECTOR_SIZE) : 1;
-
-				for(unsigned i=0; i< inc_ack; ++i)
-					m_core->inc_store_req( inst.warp_id() );
-			}
-
-    		inst.accessq_pop_back();
-    	}
-    	else
-        {
-        	result = BK_CONF;
-        	delete mf;
+          for (unsigned i = 0; i < dec_ack; ++i) m_core->store_ack(mf_next);
         }
-        if( !inst.accessq_empty() &&  result !=BK_CONF)
-		   result = COAL_STALL;
-	   return result;
-	}
-    else
-    {
-		std::list<cache_event> events;
-		enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
-		return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );
-    }
-}
 
-void ldst_unit::L1_latency_queue_cycle()
-{
-	//std::deque< std::pair<mem_fetch*,bool> >::iterator it = m_latency_queue.begin();
-	if((l1_latency_queue[0]) != NULL)
-    {
-		    mem_fetch* mf_next = l1_latency_queue[0];
-			std::list<cache_event> events;
-			enum cache_request_status status = m_L1D->access(mf_next->get_addr(),mf_next,gpu_sim_cycle+gpu_tot_sim_cycle,events);
-
-		   bool write_sent = was_write_sent(events);
-		   bool read_sent = was_read_sent(events);
-
-		   if ( status == HIT ) {
-			   assert( !read_sent );
-			   l1_latency_queue[0] = NULL;
-			   if ( mf_next->get_inst().is_load() ) {
-				   for ( unsigned r=0; r < MAX_OUTPUT_VALUES; r++)
-					   if (mf_next->get_inst().out[r] > 0)
-					   {
-						   assert(m_pending_writes[mf_next->get_inst().warp_id()][mf_next->get_inst().out[r]]>0);
-						   unsigned still_pending = --m_pending_writes[mf_next->get_inst().warp_id()][mf_next->get_inst().out[r]];
-						   if(!still_pending)
-						   {
-							m_pending_writes[mf_next->get_inst().warp_id()].erase(mf_next->get_inst().out[r]);
-							m_scoreboard->releaseRegister(mf_next->get_inst().warp_id(),mf_next->get_inst().out[r]);
-							m_core->warp_inst_complete(mf_next->get_inst());
-						   }
-					   }
-			   }
-
-			   //For write hit in WB policy
-			   if(mf_next->get_inst().is_store() && !write_sent)
-			   {
-				   unsigned dec_ack = (m_config->m_L1D_config.get_mshr_type() == SECTOR_ASSOC)?
-				   						(mf_next->get_data_size()/SECTOR_SIZE) : 1;
-
-				   mf_next->set_reply();
-
-				   for(unsigned i=0; i< dec_ack; ++i)
-				      m_core->store_ack(mf_next);
-			   }
-
-			   if( !write_sent )
-				   delete mf_next;
-
-		   } else if ( status == RESERVATION_FAIL ) {
-			   assert( !read_sent );
-			   assert( !write_sent );
-		   } else {
-			   assert( status == MISS || status == HIT_RESERVED );
-			   l1_latency_queue[0] = NULL;
-	   }
+        if (!write_sent) delete mf_next;
+
+      } else if (status == RESERVATION_FAIL) {
+        assert(!read_sent);
+        assert(!write_sent);
+      } else {
+        assert(status == MISS || status == HIT_RESERVED);
+        l1_latency_queue[j][0] = NULL;
+        if (m_config->m_L1D_config.get_write_policy() != WRITE_THROUGH &&
+            mf_next->get_inst().is_store() &&
+            (m_config->m_L1D_config.get_write_allocate_policy() ==
+                 FETCH_ON_WRITE ||
+             m_config->m_L1D_config.get_write_allocate_policy() ==
+                 LAZY_FETCH_ON_READ) &&
+            !was_writeallocate_sent(events)) {
+          unsigned dec_ack =
+              (m_config->m_L1D_config.get_mshr_type() == SECTOR_ASSOC)
+                  ? (mf_next->get_data_size() / SECTOR_SIZE)
+                  : 1;
+          mf_next->set_reply();
+          for (unsigned i = 0; i < dec_ack; ++i) m_core->store_ack(mf_next);
+          if (!write_sent && !read_sent) delete mf_next;
+        }
+      }
     }
 
-	 for( unsigned stage = 0; stage<m_config->m_L1D_config.l1_latency-1; ++stage)
-	  if( l1_latency_queue[stage] == NULL) {
-		   l1_latency_queue[stage] = l1_latency_queue[stage+1] ;
-		   l1_latency_queue[stage+1] = NULL;
-	   }
-
+    for (unsigned stage = 0; stage < m_config->m_L1D_config.l1_latency - 1;
+         ++stage)
+      if (l1_latency_queue[j][stage] == NULL) {
+        l1_latency_queue[j][stage] = l1_latency_queue[j][stage + 1];
+        l1_latency_queue[j][stage + 1] = NULL;
+      }
+  }
 }
 
+bool ldst_unit::constant_cycle(warp_inst_t &inst, mem_stage_stall_type &rc_fail,
+                               mem_stage_access_type &fail_type) {
+  if (inst.empty() || ((inst.space.get_type() != const_space) &&
+                       (inst.space.get_type() != param_space_kernel)))
+    return true;
+  if (inst.active_count() == 0) return true;
+
+  mem_stage_stall_type fail;
+  if (m_config->perfect_inst_const_cache) {
+    fail = NO_RC_FAIL;
+    while (inst.accessq_count() > 0) inst.accessq_pop_back();
+    if (inst.is_load()) {
+      for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++)
+        if (inst.out[r] > 0) m_pending_writes[inst.warp_id()][inst.out[r]]--;
+    }
+  } else {
+    fail = process_memory_access_queue(m_L1C, inst);
+  }
 
-
-bool ldst_unit::constant_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type)
-{
-   if( inst.empty() || ((inst.space.get_type() != const_space) && (inst.space.get_type() != param_space_kernel)) )
-       return true;
-   if( inst.active_count() == 0 ) 
-       return true;
-   mem_stage_stall_type fail = process_memory_access_queue(m_L1C,inst);
-   if (fail != NO_RC_FAIL){ 
-      rc_fail = fail; //keep other fails if this didn't fail.
-      fail_type = C_MEM;
-      if (rc_fail == BK_CONF or rc_fail == COAL_STALL) {
-         m_stats->gpgpu_n_cmem_portconflict++; //coal stalls aren't really a bank conflict, but this maintains previous behavior.
-      }
-   }
-   return inst.accessq_empty(); //done if empty.
+  if (fail != NO_RC_FAIL) {
+    rc_fail = fail;  // keep other fails if this didn't fail.
+    fail_type = C_MEM;
+    if (rc_fail == BK_CONF or rc_fail == COAL_STALL) {
+      m_stats->gpgpu_n_cmem_portconflict++;  // coal stalls aren't really a bank
+                                             // conflict, but this maintains
+                                             // previous behavior.
+    }
+  }
+  return inst.accessq_empty();  // done if empty.
 }
 
-bool ldst_unit::texture_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type)
-{
-   if( inst.empty() || inst.space.get_type() != tex_space )
-       return true;
-   if( inst.active_count() == 0 ) 
-       return true;
-   mem_stage_stall_type fail = process_memory_access_queue(m_L1T,inst);
-   if (fail != NO_RC_FAIL){ 
-      rc_fail = fail; //keep other fails if this didn't fail.
-      fail_type = T_MEM;
-   }
-   return inst.accessq_empty(); //done if empty.
+bool ldst_unit::texture_cycle(warp_inst_t &inst, mem_stage_stall_type &rc_fail,
+                              mem_stage_access_type &fail_type) {
+  if (inst.empty() || inst.space.get_type() != tex_space) return true;
+  if (inst.active_count() == 0) return true;
+  mem_stage_stall_type fail = process_memory_access_queue(m_L1T, inst);
+  if (fail != NO_RC_FAIL) {
+    rc_fail = fail;  // keep other fails if this didn't fail.
+    fail_type = T_MEM;
+  }
+  return inst.accessq_empty();  // done if empty.
 }
 
-bool ldst_unit::memory_cycle( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )
-{
-   if( inst.empty() || 
-       ((inst.space.get_type() != global_space) &&
-        (inst.space.get_type() != local_space) &&
-        (inst.space.get_type() != param_space_local)) ) 
-       return true;
-   if( inst.active_count() == 0 ) 
-       return true;
-   assert( !inst.accessq_empty() );
-   mem_stage_stall_type stall_cond = NO_RC_FAIL;
-   const mem_access_t &access = inst.accessq_back();
-
-   bool bypassL1D = false; 
-   if ( CACHE_GLOBAL == inst.cache_op || (m_L1D == NULL) ) {
-       bypassL1D = true; 
-   } else if (inst.space.is_global()) { // global memory access 
-       // skip L1 cache if the option is enabled
-       if (m_core->get_config()->gmem_skip_L1D && (CACHE_L1 != inst.cache_op))
-           bypassL1D = true; 
-   }
-   if( bypassL1D ) {
-       // bypass L1 cache
-       unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;
-       unsigned size = access.get_size() + control_size;
-       //printf("Interconnect:Addr: %x, size=%d\n",access.get_addr(),size);
-       if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {
-           stall_cond = ICNT_RC_FAIL;
-       } else {
-           mem_fetch *mf = m_mf_allocator->alloc(inst,access);
-           m_icnt->push(mf);
-           inst.accessq_pop_back();
-           //inst.clear_active( access.get_warp_mask() );
-           if( inst.is_load() ) { 
-              for( unsigned r=0; r < MAX_OUTPUT_VALUES; r++) 
-                  if(inst.out[r] > 0) 
-                      assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );
-           } else if( inst.is_store() ) 
-              m_core->inc_store_req( inst.warp_id() );
-       }
-   } else {
-       assert( CACHE_UNDEFINED != inst.cache_op );
-       stall_cond = process_memory_access_queue_l1cache(m_L1D,inst);
-   }
-   if( !inst.accessq_empty() && stall_cond == NO_RC_FAIL)
-       stall_cond = COAL_STALL;
-   if (stall_cond != NO_RC_FAIL) {
-      stall_reason = stall_cond;
-      bool iswrite = inst.is_store();
-      if (inst.space.is_local()) 
-         access_type = (iswrite)?L_MEM_ST:L_MEM_LD;
-      else 
-         access_type = (iswrite)?G_MEM_ST:G_MEM_LD;
-   }
-   return inst.accessq_empty(); 
+bool ldst_unit::memory_cycle(warp_inst_t &inst,
+                             mem_stage_stall_type &stall_reason,
+                             mem_stage_access_type &access_type) {
+  if (inst.empty() || ((inst.space.get_type() != global_space) &&
+                       (inst.space.get_type() != local_space) &&
+                       (inst.space.get_type() != param_space_local)))
+    return true;
+  if (inst.active_count() == 0) return true;
+  if (inst.accessq_empty()) return true;
+
+  mem_stage_stall_type stall_cond = NO_RC_FAIL;
+  const mem_access_t &access = inst.accessq_back();
+
+  bool bypassL1D = false;
+  if (CACHE_GLOBAL == inst.cache_op || (m_L1D == NULL)) {
+    bypassL1D = true;
+  } else if (inst.space.is_global()) {  // global memory access
+    // skip L1 cache if the option is enabled
+    if (m_core->get_config()->gmem_skip_L1D && (CACHE_L1 != inst.cache_op))
+      bypassL1D = true;
+  }
+  if (bypassL1D) {
+    // bypass L1 cache
+    unsigned control_size =
+        inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;
+    unsigned size = access.get_size() + control_size;
+    // printf("Interconnect:Addr: %x, size=%d\n",access.get_addr(),size);
+    if (m_icnt->full(size, inst.is_store() || inst.isatomic())) {
+      stall_cond = ICNT_RC_FAIL;
+    } else {
+      mem_fetch *mf =
+          m_mf_allocator->alloc(inst, access,
+                                m_core->get_gpu()->gpu_sim_cycle +
+                                    m_core->get_gpu()->gpu_tot_sim_cycle);
+      m_icnt->push(mf);
+      inst.accessq_pop_back();
+      // inst.clear_active( access.get_warp_mask() );
+      if (inst.is_load()) {
+        for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++)
+          if (inst.out[r] > 0)
+            assert(m_pending_writes[inst.warp_id()][inst.out[r]] > 0);
+      } else if (inst.is_store())
+        m_core->inc_store_req(inst.warp_id());
+    }
+  } else {
+    assert(CACHE_UNDEFINED != inst.cache_op);
+    stall_cond = process_memory_access_queue_l1cache(m_L1D, inst);
+  }
+  if (!inst.accessq_empty() && stall_cond == NO_RC_FAIL)
+    stall_cond = COAL_STALL;
+  if (stall_cond != NO_RC_FAIL) {
+    stall_reason = stall_cond;
+    bool iswrite = inst.is_store();
+    if (inst.space.is_local())
+      access_type = (iswrite) ? L_MEM_ST : L_MEM_LD;
+    else
+      access_type = (iswrite) ? G_MEM_ST : G_MEM_LD;
+  }
+  return inst.accessq_empty();
 }
 
 bool ldst_unit::memory_cycle_gem5( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )
@@ -1917,209 +2270,251 @@ bool ldst_unit::memory_cycle_gem5( warp_inst_t &inst, mem_stage_stall_type &stal
     return true;
 }
 
-bool ldst_unit::response_buffer_full() const
-{
-    return m_response_fifo.size() >= m_config->ldst_unit_response_queue_size;
-}
-
-void ldst_unit::fill( mem_fetch *mf )
-{
-    mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO,gpu_sim_cycle+gpu_tot_sim_cycle);
-    m_response_fifo.push_back(mf);
-}
-
-void ldst_unit::flush(){
-	// Flush L1D cache
-	m_L1D->flush();
-}
-
-void ldst_unit::invalidate(){
-	// Flush L1D cache
-	m_L1D->invalidate();
-}
-
-simd_function_unit::simd_function_unit( const shader_core_config *config )
-{ 
-    m_config=config;
-    m_dispatch_reg = new warp_inst_t(config); 
-}
-
-
-sfu:: sfu(  register_set* result_port, const shader_core_config *config,shader_core_ctx *core  )
-    : pipelined_simd_unit(result_port,config,config->max_sfu_latency,core)
-{ 
-    m_name = "SFU"; 
-}
-
-tensor_core:: tensor_core(  register_set* result_port, const shader_core_config *config,shader_core_ctx *core  )
-    : pipelined_simd_unit(result_port,config,config->max_tensor_core_latency,core)
-{ 
-    m_name = "TENSOR_CORE"; 
-}
-
-void sfu::issue( register_set& source_reg )
-{
-    warp_inst_t** ready_reg = source_reg.get_ready();
-	//m_core->incexecstat((*ready_reg));
-
-	(*ready_reg)->op_pipe=SFU__OP;
-	m_core->incsfu_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
-	pipelined_simd_unit::issue(source_reg);
-}
-
-void tensor_core::issue( register_set& source_reg )
-{
-    warp_inst_t** ready_reg = source_reg.get_ready();
-	//m_core->incexecstat((*ready_reg));
-
-	(*ready_reg)->op_pipe= TENSOR_CORE__OP;
-	m_core->incsfu_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
-	pipelined_simd_unit::issue(source_reg);
-}
-
-unsigned pipelined_simd_unit::get_active_lanes_in_pipeline(){
-	active_mask_t active_lanes;
-	active_lanes.reset();
-	 if(m_core->get_gpu()->get_config().g_power_simulation_enabled){
-		for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ ){
-			if( !m_pipeline_reg[stage]->empty() )
-				active_lanes|=m_pipeline_reg[stage]->get_active_mask();
-		}
-	 }
-	return active_lanes.count();
+bool ldst_unit::response_buffer_full() const {
+  return m_response_fifo.size() >= m_config->ldst_unit_response_queue_size;
 }
 
-void ldst_unit::active_lanes_in_pipeline(){
-	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
-	assert(active_count<=m_core->get_config()->warp_size);
-	m_core->incfumemactivelanes_stat(active_count);
+void ldst_unit::fill(mem_fetch *mf) {
+  mf->set_status(
+      IN_SHADER_LDST_RESPONSE_FIFO,
+      m_core->get_gpu()->gpu_sim_cycle + m_core->get_gpu()->gpu_tot_sim_cycle);
+  m_response_fifo.push_back(mf);
 }
 
-void sp_unit::active_lanes_in_pipeline(){
-	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
-	assert(active_count<=m_core->get_config()->warp_size);
-	m_core->incspactivelanes_stat(active_count);
-	m_core->incfuactivelanes_stat(active_count);
-	m_core->incfumemactivelanes_stat(active_count);
-}
-void dp_unit::active_lanes_in_pipeline(){
-	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
-	assert(active_count<=m_core->get_config()->warp_size);
-	m_core->incspactivelanes_stat(active_count);
-	m_core->incfuactivelanes_stat(active_count);
-	m_core->incfumemactivelanes_stat(active_count);
+void ldst_unit::flush() {
+  // Flush L1D cache
+  m_L1D->flush();
 }
 
-void int_unit::active_lanes_in_pipeline(){
-	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
-	assert(active_count<=m_core->get_config()->warp_size);
-	m_core->incspactivelanes_stat(active_count);
-	m_core->incfuactivelanes_stat(active_count);
-	m_core->incfumemactivelanes_stat(active_count);
-}
-void sfu::active_lanes_in_pipeline(){
-	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
-	assert(active_count<=m_core->get_config()->warp_size);
-	m_core->incsfuactivelanes_stat(active_count);
-	m_core->incfuactivelanes_stat(active_count);
-	m_core->incfumemactivelanes_stat(active_count);
+void ldst_unit::invalidate() {
+  // Flush L1D cache
+  m_L1D->invalidate();
 }
 
-void tensor_core::active_lanes_in_pipeline(){
-	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
-	assert(active_count<=m_core->get_config()->warp_size);
-	m_core->incsfuactivelanes_stat(active_count);
-	m_core->incfuactivelanes_stat(active_count);
-	m_core->incfumemactivelanes_stat(active_count);
+simd_function_unit::simd_function_unit(const shader_core_config *config) {
+  m_config = config;
+  m_dispatch_reg = new warp_inst_t(config);
 }
 
-
-sp_unit::sp_unit( register_set* result_port, const shader_core_config *config,shader_core_ctx *core)
-    : pipelined_simd_unit(result_port,config,config->max_sp_latency,core)
-{ 
-    m_name = "SP "; 
+void simd_function_unit::issue(register_set &source_reg) {
+  bool partition_issue =
+      m_config->sub_core_model && this->is_issue_partitioned();
+  source_reg.move_out_to(partition_issue, this->get_issue_reg_id(),
+                         m_dispatch_reg);
+  occupied.set(m_dispatch_reg->latency);
 }
 
-dp_unit::dp_unit( register_set* result_port, const shader_core_config *config,shader_core_ctx *core)
-    : pipelined_simd_unit(result_port,config,config->max_dp_latency,core)
-{
-    m_name = "DP ";
+sfu::sfu(register_set *result_port, const shader_core_config *config,
+         shader_core_ctx *core, unsigned issue_reg_id)
+    : pipelined_simd_unit(result_port, config, config->max_sfu_latency, core,
+                          issue_reg_id) {
+  m_name = "SFU";
 }
 
-int_unit::int_unit( register_set* result_port, const shader_core_config *config,shader_core_ctx *core)
-    : pipelined_simd_unit(result_port,config,config->max_int_latency,core)
-{
-    m_name = "INT ";
+tensor_core::tensor_core(register_set *result_port,
+                         const shader_core_config *config,
+                         shader_core_ctx *core, unsigned issue_reg_id)
+    : pipelined_simd_unit(result_port, config, config->max_tensor_core_latency,
+                          core, issue_reg_id) {
+  m_name = "TENSOR_CORE";
 }
 
-void sp_unit :: issue(register_set& source_reg)
-{
-    warp_inst_t** ready_reg = source_reg.get_ready();
-	//m_core->incexecstat((*ready_reg));
-	(*ready_reg)->op_pipe=SP__OP;
-	m_core->incsp_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
-	pipelined_simd_unit::issue(source_reg);
-}
+void sfu::issue(register_set &source_reg) {
+  warp_inst_t **ready_reg =
+      source_reg.get_ready(m_config->sub_core_model, m_issue_reg_id);
+  // m_core->incexecstat((*ready_reg));
 
-void dp_unit :: issue(register_set& source_reg)
-{
-    warp_inst_t** ready_reg = source_reg.get_ready();
-	//m_core->incexecstat((*ready_reg));
-	(*ready_reg)->op_pipe=DP__OP;
-	m_core->incsp_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
-	pipelined_simd_unit::issue(source_reg);
+  (*ready_reg)->op_pipe = SFU__OP;
+  m_core->incsfu_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
+  pipelined_simd_unit::issue(source_reg);
 }
 
-void int_unit :: issue(register_set& source_reg)
-{
-    warp_inst_t** ready_reg = source_reg.get_ready();
-	//m_core->incexecstat((*ready_reg));
-	(*ready_reg)->op_pipe=INTP__OP;
-	m_core->incsp_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
-	pipelined_simd_unit::issue(source_reg);
-}
+void tensor_core::issue(register_set &source_reg) {
+  warp_inst_t **ready_reg =
+      source_reg.get_ready(m_config->sub_core_model, m_issue_reg_id);
+  // m_core->incexecstat((*ready_reg));
 
-pipelined_simd_unit::pipelined_simd_unit( register_set* result_port, const shader_core_config *config, unsigned max_latency,shader_core_ctx *core )
-    : simd_function_unit(config) 
-{
-    m_result_port = result_port;
-    m_pipeline_depth = max_latency;
-    m_pipeline_reg = new warp_inst_t*[m_pipeline_depth];
-    for( unsigned i=0; i < m_pipeline_depth; i++ ) 
-	m_pipeline_reg[i] = new warp_inst_t( config );
-    m_core=core;
-    active_insts_in_pipeline=0;
+  (*ready_reg)->op_pipe = TENSOR_CORE__OP;
+  m_core->incsfu_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
+  pipelined_simd_unit::issue(source_reg);
 }
 
-void pipelined_simd_unit::cycle()
-{
-    if( !m_pipeline_reg[0]->empty() ){
-        m_result_port->move_in(m_pipeline_reg[0]);
-        assert(active_insts_in_pipeline > 0);
-        active_insts_in_pipeline--;
-    }
-    if(active_insts_in_pipeline){
-		for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ )
-			move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage+1]);
+unsigned pipelined_simd_unit::get_active_lanes_in_pipeline() {
+  active_mask_t active_lanes;
+  active_lanes.reset();
+  if (m_core->get_gpu()->get_config().g_power_simulation_enabled) {
+    for (unsigned stage = 0; (stage + 1) < m_pipeline_depth; stage++) {
+      if (!m_pipeline_reg[stage]->empty())
+        active_lanes |= m_pipeline_reg[stage]->get_active_mask();
     }
-    if( !m_dispatch_reg->empty() ) {
-        if( !m_dispatch_reg->dispatch_delay()){
-            int start_stage = m_dispatch_reg->latency - m_dispatch_reg->initiation_interval;
-            move_warp(m_pipeline_reg[start_stage],m_dispatch_reg);
-            active_insts_in_pipeline++;
-        }
+  }
+  return active_lanes.count();
+}
+
+void ldst_unit::active_lanes_in_pipeline() {
+  unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
+  assert(active_count <= m_core->get_config()->warp_size);
+  m_core->incfumemactivelanes_stat(active_count);
+}
+
+void sp_unit::active_lanes_in_pipeline() {
+  unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
+  assert(active_count <= m_core->get_config()->warp_size);
+  m_core->incspactivelanes_stat(active_count);
+  m_core->incfuactivelanes_stat(active_count);
+  m_core->incfumemactivelanes_stat(active_count);
+}
+void dp_unit::active_lanes_in_pipeline() {
+  unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
+  assert(active_count <= m_core->get_config()->warp_size);
+  //m_core->incspactivelanes_stat(active_count);
+  m_core->incfuactivelanes_stat(active_count);
+  m_core->incfumemactivelanes_stat(active_count);
+}
+void specialized_unit::active_lanes_in_pipeline() {
+  unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
+  assert(active_count <= m_core->get_config()->warp_size);
+  m_core->incspactivelanes_stat(active_count);
+  m_core->incfuactivelanes_stat(active_count);
+  m_core->incfumemactivelanes_stat(active_count);
+}
+
+void int_unit::active_lanes_in_pipeline() {
+  unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
+  assert(active_count <= m_core->get_config()->warp_size);
+  m_core->incspactivelanes_stat(active_count);
+  m_core->incfuactivelanes_stat(active_count);
+  m_core->incfumemactivelanes_stat(active_count);
+}
+void sfu::active_lanes_in_pipeline() {
+  unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
+  assert(active_count <= m_core->get_config()->warp_size);
+  m_core->incsfuactivelanes_stat(active_count);
+  m_core->incfuactivelanes_stat(active_count);
+  m_core->incfumemactivelanes_stat(active_count);
+}
+
+void tensor_core::active_lanes_in_pipeline() {
+  unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
+  assert(active_count <= m_core->get_config()->warp_size);
+  m_core->incsfuactivelanes_stat(active_count);
+  m_core->incfuactivelanes_stat(active_count);
+  m_core->incfumemactivelanes_stat(active_count);
+}
+
+sp_unit::sp_unit(register_set *result_port, const shader_core_config *config,
+                 shader_core_ctx *core, unsigned issue_reg_id)
+    : pipelined_simd_unit(result_port, config, config->max_sp_latency, core,
+                          issue_reg_id) {
+  m_name = "SP ";
+}
+
+specialized_unit::specialized_unit(register_set *result_port,
+                                   const shader_core_config *config,
+                                   shader_core_ctx *core, unsigned supported_op,
+                                   char *unit_name, unsigned latency,
+                                   unsigned issue_reg_id)
+    : pipelined_simd_unit(result_port, config, latency, core, issue_reg_id) {
+  m_name = unit_name;
+  m_supported_op = supported_op;
+}
+
+dp_unit::dp_unit(register_set *result_port, const shader_core_config *config,
+                 shader_core_ctx *core, unsigned issue_reg_id)
+    : pipelined_simd_unit(result_port, config, config->max_dp_latency, core,
+                          issue_reg_id) {
+  m_name = "DP ";
+}
+
+int_unit::int_unit(register_set *result_port, const shader_core_config *config,
+                   shader_core_ctx *core, unsigned issue_reg_id)
+    : pipelined_simd_unit(result_port, config, config->max_int_latency, core,
+                          issue_reg_id) {
+  m_name = "INT ";
+}
+
+void sp_unit ::issue(register_set &source_reg) {
+  warp_inst_t **ready_reg =
+      source_reg.get_ready(m_config->sub_core_model, m_issue_reg_id);
+  // m_core->incexecstat((*ready_reg));
+  (*ready_reg)->op_pipe = SP__OP;
+  m_core->incsp_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
+  pipelined_simd_unit::issue(source_reg);
+}
+
+void dp_unit ::issue(register_set &source_reg) {
+  warp_inst_t **ready_reg =
+      source_reg.get_ready(m_config->sub_core_model, m_issue_reg_id);
+  // m_core->incexecstat((*ready_reg));
+  (*ready_reg)->op_pipe = DP__OP;
+  m_core->incsp_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
+  pipelined_simd_unit::issue(source_reg);
+}
+
+void specialized_unit ::issue(register_set &source_reg) {
+  warp_inst_t **ready_reg =
+      source_reg.get_ready(m_config->sub_core_model, m_issue_reg_id);
+  // m_core->incexecstat((*ready_reg));
+  (*ready_reg)->op_pipe = SPECIALIZED__OP;
+  m_core->incsp_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
+  pipelined_simd_unit::issue(source_reg);
+}
+
+void int_unit ::issue(register_set &source_reg) {
+  warp_inst_t **ready_reg =
+      source_reg.get_ready(m_config->sub_core_model, m_issue_reg_id);
+  // m_core->incexecstat((*ready_reg));
+  (*ready_reg)->op_pipe = INTP__OP;
+  m_core->incsp_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
+  pipelined_simd_unit::issue(source_reg);
+}
+
+pipelined_simd_unit::pipelined_simd_unit(register_set *result_port,
+                                         const shader_core_config *config,
+                                         unsigned max_latency,
+                                         shader_core_ctx *core,
+                                         unsigned issue_reg_id)
+    : simd_function_unit(config) {
+  m_result_port = result_port;
+  m_pipeline_depth = max_latency;
+  m_pipeline_reg = new warp_inst_t *[m_pipeline_depth];
+  for (unsigned i = 0; i < m_pipeline_depth; i++)
+    m_pipeline_reg[i] = new warp_inst_t(config);
+  m_core = core;
+  m_issue_reg_id = issue_reg_id;
+  active_insts_in_pipeline = 0;
+}
+
+void pipelined_simd_unit::cycle() {
+  if (!m_pipeline_reg[0]->empty()) {
+    m_result_port->move_in(m_pipeline_reg[0]);
+    assert(active_insts_in_pipeline > 0);
+    active_insts_in_pipeline--;
+  }
+  if (active_insts_in_pipeline) {
+    for (unsigned stage = 0; (stage + 1) < m_pipeline_depth; stage++)
+      move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage + 1]);
+  }
+  if (!m_dispatch_reg->empty()) {
+    if (!m_dispatch_reg->dispatch_delay()) {
+      int start_stage =
+          m_dispatch_reg->latency - m_dispatch_reg->initiation_interval;
+      move_warp(m_pipeline_reg[start_stage], m_dispatch_reg);
+      active_insts_in_pipeline++;
     }
-    occupied >>=1;
+  }
+  occupied >>= 1;
 }
 
-
-void pipelined_simd_unit::issue( register_set& source_reg )
-{
-    //move_warp(m_dispatch_reg,source_reg);
-    warp_inst_t** ready_reg = source_reg.get_ready();
-	m_core->incexecstat((*ready_reg));
-	//source_reg.move_out_to(m_dispatch_reg);
-	simd_function_unit::issue(source_reg);
+void pipelined_simd_unit::issue(register_set &source_reg) {
+  // move_warp(m_dispatch_reg,source_reg);
+  bool partition_issue =
+      m_config->sub_core_model && this->is_issue_partitioned();
+  warp_inst_t **ready_reg =
+      source_reg.get_ready(partition_issue, m_issue_reg_id);
+  m_core->incexecstat((*ready_reg));
+  // source_reg.move_out_to(m_dispatch_reg);
+  simd_function_unit::issue(source_reg);
 }
 
 /*
@@ -2131,230 +2526,210 @@ void pipelined_simd_unit::issue( register_set& source_reg )
     }
 */
 
-void ldst_unit::init( mem_fetch_interface *icnt,
-                      shader_core_mem_fetch_allocator *mf_allocator,
-                      shader_core_ctx *core, 
-                      opndcoll_rfu_t *operand_collector,
-                      Scoreboard *scoreboard,
-                      const shader_core_config *config,
-                      const memory_config *mem_config,  
-                      shader_core_stats *stats,
-                      unsigned sid,
-                      unsigned tpc )
-{
-    m_memory_config = mem_config;
-    m_icnt = icnt;
-    m_mf_allocator=mf_allocator;
-    m_core = core;
-    m_operand_collector = operand_collector;
-    m_scoreboard = scoreboard;
-    m_stats = stats;
-    m_sid = sid;
-    m_tpc = tpc;
-    #define STRSIZE 1024
-    char L1T_name[STRSIZE];
-    char L1C_name[STRSIZE];
-    snprintf(L1T_name, STRSIZE, "L1T_%03d", m_sid);
-    snprintf(L1C_name, STRSIZE, "L1C_%03d", m_sid);
-    m_L1T = new tex_cache(L1T_name,m_config->m_L1T_config,m_sid,get_shader_texture_cache_id(),icnt,IN_L1T_MISS_QUEUE,IN_SHADER_L1T_ROB);
-    m_L1C = new read_only_cache(L1C_name,m_config->m_L1C_config,m_sid,get_shader_constant_cache_id(),icnt,IN_L1C_MISS_QUEUE);
-    m_L1D = NULL;
-    m_mem_rc = NO_RC_FAIL;
-    m_num_writeback_clients=5; // = shared memory, global/local (uncached), L1D, L1T, L1C
-    m_writeback_arb = 0;
-    m_next_global=NULL;
-    m_last_inst_gpu_sim_cycle=0;
-    m_last_inst_gpu_tot_sim_cycle=0;
-}
-
-
-ldst_unit::ldst_unit( mem_fetch_interface *icnt,
-                      shader_core_mem_fetch_allocator *mf_allocator,
-                      shader_core_ctx *core, 
-                      opndcoll_rfu_t *operand_collector,
-                      Scoreboard *scoreboard,
-                      const shader_core_config *config,
-                      const memory_config *mem_config,  
-                      shader_core_stats *stats,
-                      unsigned sid,
-                      unsigned tpc ) : pipelined_simd_unit(NULL,config,config->smem_latency,core), m_next_wb(config)
-{
-	assert(config->smem_latency > 1);
-    init( icnt,
-          mf_allocator,
-          core, 
-          operand_collector,
-          scoreboard,
-          config, 
-          mem_config,  
-          stats, 
-          sid,
-          tpc );
-    if( !m_config->m_L1D_config.disabled() ) {
-        char L1D_name[STRSIZE];
-        snprintf(L1D_name, STRSIZE, "L1D_%03d", m_sid);
-        m_L1D = new l1_cache( L1D_name,
-                              m_config->m_L1D_config,
-                              m_sid,
-                              get_shader_normal_cache_id(),
-                              m_icnt,
-                              m_mf_allocator,
-                              IN_L1D_MISS_QUEUE );
-
-        if(m_config->m_L1D_config.l1_latency > 0)
-	    {
-        	for(int i=0; i<m_config->m_L1D_config.l1_latency; i++ )
-        		l1_latency_queue.push_back((mem_fetch*)NULL);
-	    }
-    }
-    m_name = "MEM ";
-}
-
-ldst_unit::ldst_unit( mem_fetch_interface *icnt,
-                      shader_core_mem_fetch_allocator *mf_allocator,
-                      shader_core_ctx *core, 
-                      opndcoll_rfu_t *operand_collector,
-                      Scoreboard *scoreboard,
-                      const shader_core_config *config,
-                      const memory_config *mem_config,  
-                      shader_core_stats *stats,
-                      unsigned sid,
-                      unsigned tpc,
-                      l1_cache* new_l1d_cache )
-    : pipelined_simd_unit(NULL,config,3,core), m_L1D(new_l1d_cache), m_next_wb(config)
-{
-    init( icnt,
-          mf_allocator,
-          core, 
-          operand_collector,
-          scoreboard,
-          config, 
-          mem_config,  
-          stats, 
-          sid,
-          tpc );
-}
-
-void ldst_unit:: issue( register_set &reg_set )
-{
-	warp_inst_t* inst = *(reg_set.get_ready());
-
-   // record how many pending register writes/memory accesses there are for this instruction
-   assert(inst->empty() == false);
-   if (inst->is_load() and inst->space.get_type() != shared_space) {
-      unsigned warp_id = inst->warp_id();
-      unsigned n_accesses = inst->accessq_count();
-      for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++) {
-         unsigned reg_id = inst->out[r];
-         if (reg_id > 0) {
-            m_pending_writes[warp_id][reg_id] += n_accesses;
-         }
+void ldst_unit::init(mem_fetch_interface *icnt,
+                     shader_core_mem_fetch_allocator *mf_allocator,
+                     shader_core_ctx *core, opndcoll_rfu_t *operand_collector,
+                     Scoreboard *scoreboard, const shader_core_config *config,
+                     const memory_config *mem_config, shader_core_stats *stats,
+                     unsigned sid, unsigned tpc) {
+  m_memory_config = mem_config;
+  m_icnt = icnt;
+  m_mf_allocator = mf_allocator;
+  m_core = core;
+  m_operand_collector = operand_collector;
+  m_scoreboard = scoreboard;
+  m_stats = stats;
+  m_sid = sid;
+  m_tpc = tpc;
+#define STRSIZE 1024
+  char L1T_name[STRSIZE];
+  char L1C_name[STRSIZE];
+  snprintf(L1T_name, STRSIZE, "L1T_%03d", m_sid);
+  snprintf(L1C_name, STRSIZE, "L1C_%03d", m_sid);
+  m_L1T = new tex_cache(L1T_name, m_config->m_L1T_config, m_sid,
+                        get_shader_texture_cache_id(), icnt, IN_L1T_MISS_QUEUE,
+                        IN_SHADER_L1T_ROB);
+  m_L1C = new read_only_cache(L1C_name, m_config->m_L1C_config, m_sid,
+                              get_shader_constant_cache_id(), icnt,
+                              IN_L1C_MISS_QUEUE);
+  m_L1D = NULL;
+  m_mem_rc = NO_RC_FAIL;
+  m_num_writeback_clients =
+      5;  // = shared memory, global/local (uncached), L1D, L1T, L1C
+  m_writeback_arb = 0;
+  m_next_global = NULL;
+  m_last_inst_gpu_sim_cycle = 0;
+  m_last_inst_gpu_tot_sim_cycle = 0;
+}
+
+ldst_unit::ldst_unit(mem_fetch_interface *icnt,
+                     shader_core_mem_fetch_allocator *mf_allocator,
+                     shader_core_ctx *core, opndcoll_rfu_t *operand_collector,
+                     Scoreboard *scoreboard, const shader_core_config *config,
+                     const memory_config *mem_config, shader_core_stats *stats,
+                     unsigned sid, unsigned tpc)
+    : pipelined_simd_unit(NULL, config, config->smem_latency, core, 0),
+      m_next_wb(config) {
+  assert(config->smem_latency > 1);
+  init(icnt, mf_allocator, core, operand_collector, scoreboard, config,
+       mem_config, stats, sid, tpc);
+  if (!m_config->m_L1D_config.disabled()) {
+    char L1D_name[STRSIZE];
+    snprintf(L1D_name, STRSIZE, "L1D_%03d", m_sid);
+    m_L1D = new l1_cache(L1D_name, m_config->m_L1D_config, m_sid,
+                         get_shader_normal_cache_id(), m_icnt, m_mf_allocator,
+                         IN_L1D_MISS_QUEUE, core->get_gpu());
+
+    l1_latency_queue.resize(m_config->m_L1D_config.l1_banks);
+    assert(m_config->m_L1D_config.l1_latency > 0);
+
+    for (unsigned j = 0; j < m_config->m_L1D_config.l1_banks; j++)
+      l1_latency_queue[j].resize(m_config->m_L1D_config.l1_latency,
+                                 (mem_fetch *)NULL);
+  }
+  m_name = "MEM ";
+}
+
+ldst_unit::ldst_unit(mem_fetch_interface *icnt,
+                     shader_core_mem_fetch_allocator *mf_allocator,
+                     shader_core_ctx *core, opndcoll_rfu_t *operand_collector,
+                     Scoreboard *scoreboard, const shader_core_config *config,
+                     const memory_config *mem_config, shader_core_stats *stats,
+                     unsigned sid, unsigned tpc, l1_cache *new_l1d_cache)
+    : pipelined_simd_unit(NULL, config, 3, core, 0),
+      m_L1D(new_l1d_cache),
+      m_next_wb(config) {
+  init(icnt, mf_allocator, core, operand_collector, scoreboard, config,
+       mem_config, stats, sid, tpc);
+}
+
+void ldst_unit::issue(register_set &reg_set) {
+  warp_inst_t *inst = *(reg_set.get_ready());
+
+  // record how many pending register writes/memory accesses there are for this
+  // instruction
+  assert(inst->empty() == false);
+  if (inst->is_load() and inst->space.get_type() != shared_space) {
+    unsigned warp_id = inst->warp_id();
+    unsigned n_accesses = inst->accessq_count();
+    for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++) {
+      unsigned reg_id = inst->out[r];
+      if (reg_id > 0) {
+        m_pending_writes[warp_id][reg_id] += n_accesses;
       }
-   }
-
+    }
+  }
 
-	inst->op_pipe=MEM__OP;
-	// stat collection
-	m_core->mem_instruction_stats(*inst);
-	m_core->incmem_stat(m_core->get_config()->warp_size,1);
-	pipelined_simd_unit::issue(reg_set);
+  inst->op_pipe = MEM__OP;
+  // stat collection
+  m_core->mem_instruction_stats(*inst);
+  m_core->incmem_stat(m_core->get_config()->warp_size, 1);
+  pipelined_simd_unit::issue(reg_set);
 }
 
-void ldst_unit::writeback()
-{
-    // process next instruction that is going to writeback
-    if( !m_next_wb.empty() ) {
-        if( m_operand_collector->writeback(m_next_wb) ) {
-            bool insn_completed = false; 
-            for( unsigned r=0; r < MAX_OUTPUT_VALUES; r++ ) {
-                if( m_next_wb.out[r] > 0 ) {
-                    if( m_next_wb.space.get_type() != shared_space ) {
-                        assert( m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]] > 0 );
-                        unsigned still_pending = --m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]];
-                        if( !still_pending ) {
-                            m_pending_writes[m_next_wb.warp_id()].erase(m_next_wb.out[r]);
-                            m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
-                            insn_completed = true; 
-                        }
-                    } else { // shared 
-                        m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
-                        insn_completed = true; 
-                    }
-                }
-            }
-            if( insn_completed ) {
-                m_core->warp_inst_complete(m_next_wb);
+void ldst_unit::writeback() {
+  // process next instruction that is going to writeback
+  if (!m_next_wb.empty()) {
+    if (m_operand_collector->writeback(m_next_wb)) {
+      bool insn_completed = false;
+      for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++) {
+        if (m_next_wb.out[r] > 0) {
+          if (m_next_wb.space.get_type() != shared_space) {
+            assert(m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]] > 0);
+            unsigned still_pending =
+                --m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]];
+            if (!still_pending) {
+              m_pending_writes[m_next_wb.warp_id()].erase(m_next_wb.out[r]);
+              m_scoreboard->releaseRegister(m_next_wb.warp_id(),
+                                            m_next_wb.out[r]);
+              insn_completed = true;
             }
-            m_next_wb.clear();
-
-            // signal gem5 that the wb hazard has cleared
-            // m_core->get_gpu()->gem5CudaGPU->getCudaCore(m_core->m_sid)->writebackClear();
-            m_core->get_gpu()->gem5CudaGPU->getCudaCore(m_sid)->writebackClear();
-            m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
-            m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
+          } else {  // shared
+            m_scoreboard->releaseRegister(m_next_wb.warp_id(),
+                                          m_next_wb.out[r]);
+            insn_completed = true;
+          }
         }
+      }
+      if (insn_completed) {
+        m_core->warp_inst_complete(m_next_wb);
+      }
+      m_next_wb.clear();
+
+      // signal gem5 that the wb hazard has cleared
+      // m_core->get_gpu()->gem5CudaGPU->getCudaCore(m_core->m_sid)->writebackClear();
+      m_core->get_gpu()->gem5CudaGPU->getCudaCore(m_sid)->writebackClear();
+      m_last_inst_gpu_sim_cycle = m_core->get_gpu()->gpu_sim_cycle;
+      m_last_inst_gpu_tot_sim_cycle = m_core->get_gpu()->gpu_tot_sim_cycle;
     }
+  }
 
-    unsigned serviced_client = -1; 
-    for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {
-        unsigned next_client = (c+m_writeback_arb)%m_num_writeback_clients;
-        switch( next_client ) {
-        case 0: // shared memory 
-            if( !m_pipeline_reg[0]->empty() ) {
-                m_next_wb = *m_pipeline_reg[0];
-                if(m_next_wb.isatomic()) {
-                    m_next_wb.do_atomic();
-                    m_core->decrement_atomic_count(m_next_wb.warp_id(), m_next_wb.active_count());
-                }
-                m_core->dec_inst_in_pipeline(m_pipeline_reg[0]->warp_id());
-                m_pipeline_reg[0]->clear();
-                serviced_client = next_client; 
-            }
-            break;
-        case 1: // texture response
-            if( m_L1T->access_ready() ) {
-                mem_fetch *mf = m_L1T->next_access();
-                m_next_wb = mf->get_inst();
-                delete mf;
-                serviced_client = next_client; 
-            }
-            break;
-        case 2: // const cache response
-            if( m_L1C->access_ready() ) {
-                mem_fetch *mf = m_L1C->next_access();
-                m_next_wb = mf->get_inst();
-                delete mf;
-                serviced_client = next_client; 
-            }
-            break;
-        case 3: // global/local
-            if( m_next_global ) {
+  unsigned serviced_client = -1;
+  for (unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients);
+       c++) {
+    unsigned next_client = (c + m_writeback_arb) % m_num_writeback_clients;
+    switch (next_client) {
+      case 0:  // shared memory
+        if (!m_pipeline_reg[0]->empty()) {
+          m_next_wb = *m_pipeline_reg[0];
+          if (m_next_wb.isatomic()) {
+            m_next_wb.do_atomic();
+            m_core->decrement_atomic_count(m_next_wb.warp_id(),
+                                           m_next_wb.active_count());
+          }
+          m_core->dec_inst_in_pipeline(m_pipeline_reg[0]->warp_id());
+          m_pipeline_reg[0]->clear();
+          serviced_client = next_client;
+        }
+        break;
+      case 1:  // texture response
+        if (m_L1T->access_ready()) {
+          mem_fetch *mf = m_L1T->next_access();
+          m_next_wb = mf->get_inst();
+          delete mf;
+          serviced_client = next_client;
+        }
+        break;
+      case 2:  // const cache response
+        if (m_L1C->access_ready()) {
+          mem_fetch *mf = m_L1C->next_access();
+          m_next_wb = mf->get_inst();
+          delete mf;
+          serviced_client = next_client;
+        }
+        break;
+      case 3:  // global/local
+        if (m_next_global) {
                 panic("This should never execute in gem5-gpu! Writebacks from CudaCore must occur with writebackInst()!");
-                m_next_wb = m_next_global->get_inst();
-                if( m_next_global->isatomic() ) 
-                    m_core->decrement_atomic_count(m_next_global->get_wid(),m_next_global->get_access_warp_mask().count());
-                delete m_next_global;
-                m_next_global = NULL;
-                serviced_client = next_client; 
-            }
-            break;
-        case 4: 
-            if( m_L1D && m_L1D->access_ready() ) {
-                mem_fetch *mf = m_L1D->next_access();
-                m_next_wb = mf->get_inst();
-                delete mf;
-                serviced_client = next_client; 
-            }
-            break;
-        default: abort();
+          m_next_wb = m_next_global->get_inst();
+          if (m_next_global->isatomic()) {
+            m_core->decrement_atomic_count(
+                m_next_global->get_wid(),
+                m_next_global->get_access_warp_mask().count());
+          }
+          delete m_next_global;
+          m_next_global = NULL;
+          serviced_client = next_client;
         }
+        break;
+      case 4:
+        if (m_L1D && m_L1D->access_ready()) {
+          mem_fetch *mf = m_L1D->next_access();
+          m_next_wb = mf->get_inst();
+          delete mf;
+          serviced_client = next_client;
+        }
+        break;
+      default:
+        abort();
     }
-    // update arbitration priority only if: 
-    // 1. the writeback buffer was available 
-    // 2. a client was serviced 
-    if (serviced_client != (unsigned)-1) {
-        m_writeback_arb = (serviced_client + 1) % m_num_writeback_clients; 
-    }
+  }
+  // update arbitration priority only if:
+  // 1. the writeback buffer was available
+  // 2. a client was serviced
+  if (serviced_client != (unsigned)-1) {
+    m_writeback_arb = (serviced_client + 1) % m_num_writeback_clients;
+  }
 }
 
 
@@ -2372,13 +2747,12 @@ bool ldst_unit::writebackInst(warp_inst_t &inst)
     return true;
 }
 
-unsigned ldst_unit::clock_multiplier() const
-{ 
-	//to model multiple read port, we give multiple cycles for the memory units
-	if(m_config->mem_unit_ports)
-		return m_config->mem_unit_ports;
-	else
-		return m_config->mem_warp_parts;
+unsigned ldst_unit::clock_multiplier() const {
+  // to model multiple read port, we give multiple cycles for the memory units
+  if (m_config->mem_unit_ports)
+    return m_config->mem_unit_ports;
+  else
+    return m_config->mem_warp_parts;
 }
 /*
 void ldst_unit::issue( register_set &reg_set )
@@ -2403,176 +2777,194 @@ void ldst_unit::issue( register_set &reg_set )
    pipelined_simd_unit::issue(reg_set);
 }
 */
-void ldst_unit::cycle()
-{
-   writeback();
-   m_operand_collector->step();
-   for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ ) 
-       if( m_pipeline_reg[stage]->empty() && !m_pipeline_reg[stage+1]->empty() )
-            move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage+1]);
-
-   if( !m_response_fifo.empty() ) {
-       mem_fetch *mf = m_response_fifo.front();
-       if (mf->get_access_type() == TEXTURE_ACC_R) {
-           if (m_L1T->fill_port_free()) {
-               m_L1T->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
-               m_response_fifo.pop_front(); 
-           }
-       } else if (mf->get_access_type() == CONST_ACC_R)  {
-           if (m_L1C->fill_port_free()) {
-               mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
-               m_L1C->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
-               m_response_fifo.pop_front(); 
-           }
-       } else {
-    	   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {
-               m_core->store_ack(mf);
-               m_response_fifo.pop_front();
-               delete mf;
-           } else {
-               assert( !mf->get_is_write() ); // L1 cache is write evict, allocate line on load miss only
-
-               bool bypassL1D = false; 
-               if ( CACHE_GLOBAL == mf->get_inst().cache_op || (m_L1D == NULL) ) {
-                   bypassL1D = true; 
-               } else if (mf->get_access_type() == GLOBAL_ACC_R || mf->get_access_type() == GLOBAL_ACC_W) { // global memory access 
-                   if (m_core->get_config()->gmem_skip_L1D)
-                       bypassL1D = true; 
-               }
-               if( bypassL1D ) {
-                   if ( m_next_global == NULL ) {
-                       mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
-                       m_response_fifo.pop_front();
-                       m_next_global = mf;
-                   }
-               } else {
-                   if (m_L1D->fill_port_free()) {
-                       m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
-                       m_response_fifo.pop_front();
-                   }
-               }
-           }
-       }
-   }
+void ldst_unit::cycle() {
+  writeback();
 
-   m_L1T->cycle();
-   m_L1C->cycle();
-   if( m_L1D ) {
-	   m_L1D->cycle();
-	   if(m_config->m_L1D_config.l1_latency > 0)
-	   		L1_latency_queue_cycle();
-   }
+  for (unsigned stage = 0; (stage + 1) < m_pipeline_depth; stage++)
+    if (m_pipeline_reg[stage]->empty() && !m_pipeline_reg[stage + 1]->empty())
+      move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage + 1]);
 
-   warp_inst_t &pipe_reg = *m_dispatch_reg;
-   enum mem_stage_stall_type rc_fail = NO_RC_FAIL;
-   mem_stage_access_type type;
-   bool done = true;
-   done &= shared_cycle(pipe_reg, rc_fail, type);
-   done &= constant_cycle(pipe_reg, rc_fail, type);
-   done &= texture_cycle(pipe_reg, rc_fail, type);
-   // TODO schi done &= memory_cycle(pipe_reg, rc_fail, type);
-   done &= memory_cycle_gem5(pipe_reg, rc_fail, type);
-   m_mem_rc = rc_fail;
-
-   if (!done) { // log stall types and return
-      assert(rc_fail != NO_RC_FAIL);
-      m_stats->gpgpu_n_stall_shd_mem++;
-      m_stats->gpu_stall_shd_mem_breakdown[type][rc_fail]++;
-      return;
-   }
 
-   if( !pipe_reg.empty() ) {
-       unsigned warp_id = pipe_reg.warp_id();
-       if( pipe_reg.is_load() ) {
-           if( pipe_reg.space.get_type() == shared_space ) {
-               if( m_pipeline_reg[m_config->smem_latency-1]->empty() ) {
-                   // new shared memory request
-                   move_warp(m_pipeline_reg[m_config->smem_latency-1],m_dispatch_reg);
-                   m_dispatch_reg->clear();
-               }
-           } else {
-               //if( pipe_reg.active_count() > 0 ) {
-               //    if( !m_operand_collector->writeback(pipe_reg) ) 
-               //        return;
-               //} 
-
-               bool pending_requests=false;
-               for( unsigned r=0; r<MAX_OUTPUT_VALUES; r++ ) {
-                   unsigned reg_id = pipe_reg.out[r];
-                   if( reg_id > 0 ) {
-                       if( m_pending_writes[warp_id].find(reg_id) != m_pending_writes[warp_id].end() ) {
-                           if ( m_pending_writes[warp_id][reg_id] > 0 ) {
-                               pending_requests=true;
-                               break;
-                           } else {
-                               // this instruction is done already
-                               m_pending_writes[warp_id].erase(reg_id); 
-                           }
-                       }
-                   }
-               }
-               if( !pending_requests ) {
-                   m_core->warp_inst_complete(*m_dispatch_reg);
-                   m_scoreboard->releaseRegisters(m_dispatch_reg);
-               }
-               m_core->dec_inst_in_pipeline(warp_id);
-               m_dispatch_reg->clear();
-           }
-       } else {
-           // stores exit pipeline here
-           m_core->dec_inst_in_pipeline(warp_id);
-           m_core->warp_inst_complete(*m_dispatch_reg);
-           m_dispatch_reg->clear();
-       }
-   }
+
+  if (!m_response_fifo.empty()) {
+    mem_fetch *mf = m_response_fifo.front();
+    if (mf->get_access_type() == TEXTURE_ACC_R) {
+      if (m_L1T->fill_port_free()) {
+        m_L1T->fill(mf, m_core->get_gpu()->gpu_sim_cycle +
+                            m_core->get_gpu()->gpu_tot_sim_cycle);
+        m_response_fifo.pop_front();
+      }
+    } else if (mf->get_access_type() == CONST_ACC_R) {
+      if (m_L1C->fill_port_free()) {
+        mf->set_status(IN_SHADER_FETCHED,
+                       m_core->get_gpu()->gpu_sim_cycle +
+                           m_core->get_gpu()->gpu_tot_sim_cycle);
+        m_L1C->fill(mf, m_core->get_gpu()->gpu_sim_cycle +
+                            m_core->get_gpu()->gpu_tot_sim_cycle);
+        m_response_fifo.pop_front();
+      }
+    } else {
+      if (mf->get_type() == WRITE_ACK ||
+          (m_config->gpgpu_perfect_mem && mf->get_is_write())) {
+        m_core->store_ack(mf);
+        m_response_fifo.pop_front();
+        delete mf;
+      } else {
+        assert(!mf->get_is_write());  // L1 cache is write evict, allocate line
+                                      // on load miss only
+
+        bool bypassL1D = false;
+        if (CACHE_GLOBAL == mf->get_inst().cache_op || (m_L1D == NULL)) {
+          bypassL1D = true;
+        } else if (mf->get_access_type() == GLOBAL_ACC_R ||
+                   mf->get_access_type() ==
+                       GLOBAL_ACC_W) {  // global memory access
+          if (m_core->get_config()->gmem_skip_L1D) bypassL1D = true;
+        }
+        if (bypassL1D) {
+          if (m_next_global == NULL) {
+            mf->set_status(IN_SHADER_FETCHED,
+                           m_core->get_gpu()->gpu_sim_cycle +
+                               m_core->get_gpu()->gpu_tot_sim_cycle);
+            m_response_fifo.pop_front();
+            m_next_global = mf;
+          }
+        } else {
+          if (m_L1D->fill_port_free()) {
+            m_L1D->fill(mf, m_core->get_gpu()->gpu_sim_cycle +
+                                m_core->get_gpu()->gpu_tot_sim_cycle);
+            m_response_fifo.pop_front();
+          }
+        }
+      }
+    }
+  }
+
+  m_L1T->cycle();
+  m_L1C->cycle();
+  if (m_L1D) {
+    m_L1D->cycle();
+    if (m_config->m_L1D_config.l1_latency > 0) L1_latency_queue_cycle();
+  }
+
+  warp_inst_t &pipe_reg = *m_dispatch_reg;
+  enum mem_stage_stall_type rc_fail = NO_RC_FAIL;
+  mem_stage_access_type type;
+  bool done = true;
+  done &= shared_cycle(pipe_reg, rc_fail, type);
+  done &= constant_cycle(pipe_reg, rc_fail, type);
+  done &= texture_cycle(pipe_reg, rc_fail, type);
+  // TODO schi done &= memory_cycle(pipe_reg, rc_fail, type);
+  done &= memory_cycle_gem5(pipe_reg, rc_fail, type);
+  m_mem_rc = rc_fail;
+
+  if (!done) {  // log stall types and return
+    assert(rc_fail != NO_RC_FAIL);
+    m_stats->gpgpu_n_stall_shd_mem++;
+    m_stats->gpu_stall_shd_mem_breakdown[type][rc_fail]++;
+    return;
+  }
+
+  if (!pipe_reg.empty()) {
+    unsigned warp_id = pipe_reg.warp_id();
+    if (pipe_reg.is_load()) {
+      if (pipe_reg.space.get_type() == shared_space) {
+        if (m_pipeline_reg[m_config->smem_latency - 1]->empty()) {
+          // new shared memory request
+          move_warp(m_pipeline_reg[m_config->smem_latency - 1], m_dispatch_reg);
+          m_dispatch_reg->clear();
+        }
+      } else {
+        // if( pipe_reg.active_count() > 0 ) {
+        //    if( !m_operand_collector->writeback(pipe_reg) )
+        //        return;
+        //}
+
+        bool pending_requests = false;
+        for (unsigned r = 0; r < MAX_OUTPUT_VALUES; r++) {
+          unsigned reg_id = pipe_reg.out[r];
+          if (reg_id > 0) {
+            if (m_pending_writes[warp_id].find(reg_id) !=
+                m_pending_writes[warp_id].end()) {
+              if (m_pending_writes[warp_id][reg_id] > 0) {
+                pending_requests = true;
+                break;
+              } else {
+                // this instruction is done already
+                m_pending_writes[warp_id].erase(reg_id);
+              }
+            }
+          }
+        }
+        if (!pending_requests) {
+          m_core->warp_inst_complete(*m_dispatch_reg);
+          m_scoreboard->releaseRegisters(m_dispatch_reg);
+        }
+        m_core->dec_inst_in_pipeline(warp_id);
+        m_dispatch_reg->clear();
+      }
+    } else {
+      // stores exit pipeline here
+      m_core->dec_inst_in_pipeline(warp_id);
+      m_core->warp_inst_complete(*m_dispatch_reg);
+      m_dispatch_reg->clear();
+    }
+  }
 }
 
-void shader_core_ctx::register_cta_thread_exit( unsigned cta_num, kernel_info_t * kernel)
-{
-   assert( m_cta_status[cta_num] > 0 );
-   m_cta_status[cta_num]--;
-   if (!m_cta_status[cta_num]) {
-      m_n_active_cta--;
-      m_barriers.deallocate_barrier(cta_num);
-      shader_CTA_count_unlog(m_sid, 1);
+void shader_core_ctx::register_cta_thread_exit(unsigned cta_num,
+                                               kernel_info_t *kernel) {
+  assert(m_cta_status[cta_num] > 0);
+  m_cta_status[cta_num]--;
+  if (!m_cta_status[cta_num]) {
+    // Increment the completed CTAs
+    m_stats->ctas_completed++;
+    m_gpu->inc_completed_cta();
+    m_n_active_cta--;
+    m_barriers.deallocate_barrier(cta_num);
+    shader_CTA_count_unlog(m_sid, 1);
 //      printf("GPGPU-Sim uArch: Shader %d finished CTA #%d (%lld,%lld), %u CTAs running\n", m_sid, cta_num, gpu_sim_cycle, gpu_tot_sim_cycle,
 //             m_n_active_cta );
-      m_gpu->gem5CudaGPU->getCudaCore(m_sid)->record_block_commit(cta_num);
-
+    m_gpu->gem5CudaGPU->getCudaCore(m_sid)->record_block_commit(cta_num);
 
 
-     SHADER_DPRINTF(LIVENESS, "GPGPU-Sim uArch: Finished CTA #%d (%lld,%lld), %u CTAs running\n",
-        cta_num, gpu_sim_cycle, gpu_tot_sim_cycle, m_n_active_cta);
+    SHADER_DPRINTF(
+        LIVENESS,
+        "GPGPU-Sim uArch: Finished CTA #%u (%lld,%lld), %u CTAs running\n",
+        cta_num, m_gpu->gpu_sim_cycle, m_gpu->gpu_tot_sim_cycle,
+        m_n_active_cta);
 
-      if( m_n_active_cta == 0 ) {
-        SHADER_DPRINTF(LIVENESS, "GPGPU-Sim uArch: Empty (last released kernel %u \'%s\').\n",
-            kernel->get_uid(), kernel->name().c_str());
-          fflush(stdout);
+    if (m_n_active_cta == 0) {
+      SHADER_DPRINTF(
+          LIVENESS,
+          "GPGPU-Sim uArch: Empty (last released kernel %u \'%s\').\n",
+          kernel->get_uid(), kernel->name().c_str());
+      fflush(stdout);
 
-          //Shader can only be empty when no more cta are dispatched
-          if(kernel != m_kernel) {
-              assert(m_kernel == NULL || !m_gpu->kernel_more_cta_left(m_kernel));
-          }
-          m_kernel = NULL;
+      // Shader can only be empty when no more cta are dispatched
+      if (kernel != m_kernel) {
+        assert(m_kernel == NULL || !m_gpu->kernel_more_cta_left(m_kernel));
       }
+      m_kernel = NULL;
+    }
 
-      //Jin: for concurrent kernels on sm
-      release_shader_resource_1block(cta_num, *kernel);
-      kernel->dec_running();
-      if( !m_gpu->kernel_more_cta_left(kernel) ) {
-          if( !kernel->running() ) {
-              SHADER_DPRINTF(LIVENESS,
-                "GPGPU-Sim uArch: GPU detected kernel %u \'%s\' finished on shader %u.\n", kernel->get_uid(),
-                kernel->name().c_str(), m_sid);
-
-              if(m_kernel == kernel)
-                m_kernel = NULL;
-              m_gpu->set_kernel_done( kernel );
-          }
+    // Jin: for concurrent kernels on sm
+    release_shader_resource_1block(cta_num, *kernel);
+    kernel->dec_running();
+    if (!m_gpu->kernel_more_cta_left(kernel)) {
+      if (!kernel->running()) {
+        SHADER_DPRINTF(LIVENESS,
+                       "GPGPU-Sim uArch: GPU detected kernel %u \'%s\' "
+                       "finished on shader %u.\n",
+                       kernel->get_uid(), kernel->name().c_str(), m_sid);
+
+        if (m_kernel == kernel) m_kernel = NULL;
+        m_gpu->set_kernel_done(kernel);
       }
-
-   }
+    }
+  }
 }
+
 void shader_core_ctx::start_kernel_finish()
 {
     assert(!m_kernel_finishing);
@@ -2597,230 +2989,240 @@ void shader_core_ctx::finish_kernel()
   fflush(stdout);
 }
 
-void gpgpu_sim::shader_print_runtime_stat( FILE *fout ) 
-{
-    /*
-   fprintf(fout, "SHD_INSN: ");
-   for (unsigned i=0;i<m_n_shader;i++) 
-      fprintf(fout, "%u ",m_sc[i]->get_num_sim_insn());
-   fprintf(fout, "\n");
-   fprintf(fout, "SHD_THDS: ");
-   for (unsigned i=0;i<m_n_shader;i++) 
-      fprintf(fout, "%u ",m_sc[i]->get_not_completed());
-   fprintf(fout, "\n");
-   fprintf(fout, "SHD_DIVG: ");
-   for (unsigned i=0;i<m_n_shader;i++) 
-      fprintf(fout, "%u ",m_sc[i]->get_n_diverge());
-   fprintf(fout, "\n");
-
-   fprintf(fout, "THD_INSN: ");
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
-      fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn(i) );
-   fprintf(fout, "\n");
-   */
+void gpgpu_sim::shader_print_runtime_stat(FILE *fout) {
+  /*
+ fprintf(fout, "SHD_INSN: ");
+ for (unsigned i=0;i<m_n_shader;i++)
+    fprintf(fout, "%u ",m_sc[i]->get_num_sim_insn());
+ fprintf(fout, "\n");
+ fprintf(fout, "SHD_THDS: ");
+ for (unsigned i=0;i<m_n_shader;i++)
+    fprintf(fout, "%u ",m_sc[i]->get_not_completed());
+ fprintf(fout, "\n");
+ fprintf(fout, "SHD_DIVG: ");
+ for (unsigned i=0;i<m_n_shader;i++)
+    fprintf(fout, "%u ",m_sc[i]->get_n_diverge());
+ fprintf(fout, "\n");
+
+ fprintf(fout, "THD_INSN: ");
+ for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
+    fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn(i) );
+ fprintf(fout, "\n");
+ */
 }
 
 
-void gpgpu_sim::shader_print_scheduler_stat( FILE* fout, bool print_dynamic_info ) const
-{
-    // Print out the stats from the sampling shader core
-    const unsigned scheduler_sampling_core = m_shader_config->gpgpu_warp_issue_shader;
-    #define STR_SIZE 55
-    char name_buff[ STR_SIZE ];
-    name_buff[ STR_SIZE - 1 ] = '\0';
-    const std::vector< unsigned >& distro
-        = print_dynamic_info ?
-          m_shader_stats->get_dynamic_warp_issue()[ scheduler_sampling_core ] :
-          m_shader_stats->get_warp_slot_issue()[ scheduler_sampling_core ];
-    if ( print_dynamic_info ) {
-        snprintf( name_buff, STR_SIZE - 1, "dynamic_warp_id" );
-    } else {
-        snprintf( name_buff, STR_SIZE - 1, "warp_id" );
-    }
-    fprintf( fout,
-             "Shader %d %s issue ditsribution:\n",
-             scheduler_sampling_core,
-             name_buff );
-    const unsigned num_warp_ids = distro.size();
-    // First print out the warp ids
-    fprintf( fout, "%s:\n", name_buff );
-    for ( unsigned warp_id = 0;
-          warp_id < num_warp_ids;
-          ++warp_id  ) {
-        fprintf( fout, "%d, ", warp_id );
-    }
+void gpgpu_sim::shader_print_scheduler_stat(FILE *fout,
+                                            bool print_dynamic_info) const {
+  fprintf(fout, "ctas_completed %d, ", m_shader_stats->ctas_completed);
+  // Print out the stats from the sampling shader core
+  const unsigned scheduler_sampling_core =
+      m_shader_config->gpgpu_warp_issue_shader;
+#define STR_SIZE 55
+  char name_buff[STR_SIZE];
+  name_buff[STR_SIZE - 1] = '\0';
+  const std::vector<unsigned> &distro =
+      print_dynamic_info
+          ? m_shader_stats->get_dynamic_warp_issue()[scheduler_sampling_core]
+          : m_shader_stats->get_warp_slot_issue()[scheduler_sampling_core];
+  if (print_dynamic_info) {
+    snprintf(name_buff, STR_SIZE - 1, "dynamic_warp_id");
+  } else {
+    snprintf(name_buff, STR_SIZE - 1, "warp_id");
+  }
+  fprintf(fout, "Shader %d %s issue ditsribution:\n", scheduler_sampling_core,
+          name_buff);
+  const unsigned num_warp_ids = distro.size();
+  // First print out the warp ids
+  fprintf(fout, "%s:\n", name_buff);
+  for (unsigned warp_id = 0; warp_id < num_warp_ids; ++warp_id) {
+    fprintf(fout, "%d, ", warp_id);
+  }
 
-    fprintf( fout, "\ndistro:\n" );
-    // Then print out the distribution of instuctions issued
-    for ( std::vector< unsigned >::const_iterator iter = distro.begin();
-          iter != distro.end();
-          iter++ ) {
-        fprintf( fout, "%d, ", *iter );
-    }
-    fprintf( fout, "\n" );
+  fprintf(fout, "\ndistro:\n");
+  // Then print out the distribution of instuctions issued
+  for (std::vector<unsigned>::const_iterator iter = distro.begin();
+       iter != distro.end(); iter++) {
+    fprintf(fout, "%d, ", *iter);
+  }
+  fprintf(fout, "\n");
 }
 
-void gpgpu_sim::shader_print_cache_stats( FILE *fout ) const{
-
-    // L1I
-    struct cache_sub_stats total_css;
-    struct cache_sub_stats css;
+void gpgpu_sim::shader_print_cache_stats(FILE *fout) const {
+  // L1I
+  struct cache_sub_stats total_css;
+  struct cache_sub_stats css;
 
-    if(!m_shader_config->m_L1I_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "\n========= Core cache stats =========\n");
-        fprintf(fout, "L1I_cache:\n");
-        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
-            m_cluster[i]->get_L1I_sub_stats(css);
-            total_css += css;
-        }
-        fprintf(fout, "\tL1I_total_cache_accesses = %llu\n", total_css.accesses);
-        fprintf(fout, "\tL1I_total_cache_misses = %llu\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1I_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1I_total_cache_pending_hits = %llu\n", total_css.pending_hits);
-        fprintf(fout, "\tL1I_total_cache_reservation_fails = %llu\n", total_css.res_fails);
+  if (!m_shader_config->m_L1I_config.disabled()) {
+    total_css.clear();
+    css.clear();
+    fprintf(fout, "\n========= Core cache stats =========\n");
+    fprintf(fout, "L1I_cache:\n");
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i) {
+      m_cluster[i]->get_L1I_sub_stats(css);
+      total_css += css;
     }
-
-    // L1D
-    if(!m_shader_config->m_L1D_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "L1D_cache:\n");
-        for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++){
-            m_cluster[i]->get_L1D_sub_stats(css);
-
-            fprintf( stdout, "\tL1D_cache_core[%d]: Access = %llu, Miss = %llu, Miss_rate = %.3lf, Pending_hits = %llu, Reservation_fails = %llu\n",
-                     i, css.accesses, css.misses, (double)css.misses / (double)css.accesses, css.pending_hits, css.res_fails);
-
-            total_css += css;
-        }
-        fprintf(fout, "\tL1D_total_cache_accesses = %llu\n", total_css.accesses);
-        fprintf(fout, "\tL1D_total_cache_misses = %llu\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1D_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1D_total_cache_pending_hits = %llu\n", total_css.pending_hits);
-        fprintf(fout, "\tL1D_total_cache_reservation_fails = %llu\n", total_css.res_fails);
-        total_css.print_port_stats(fout, "\tL1D_cache"); 
+    fprintf(fout, "\tL1I_total_cache_accesses = %llu\n", total_css.accesses);
+    fprintf(fout, "\tL1I_total_cache_misses = %llu\n", total_css.misses);
+    if (total_css.accesses > 0) {
+      fprintf(fout, "\tL1I_total_cache_miss_rate = %.4lf\n",
+              (double)total_css.misses / (double)total_css.accesses);
     }
+    fprintf(fout, "\tL1I_total_cache_pending_hits = %llu\n",
+            total_css.pending_hits);
+    fprintf(fout, "\tL1I_total_cache_reservation_fails = %llu\n",
+            total_css.res_fails);
+  }
 
-    // L1C
-    if(!m_shader_config->m_L1C_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "L1C_cache:\n");
-        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
-            m_cluster[i]->get_L1C_sub_stats(css);
-            total_css += css;
-        }
-        fprintf(fout, "\tL1C_total_cache_accesses = %llu\n", total_css.accesses);
-        fprintf(fout, "\tL1C_total_cache_misses = %llu\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1C_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1C_total_cache_pending_hits = %llu\n", total_css.pending_hits);
-        fprintf(fout, "\tL1C_total_cache_reservation_fails = %llu\n", total_css.res_fails);
+  // L1D
+  if (!m_shader_config->m_L1D_config.disabled()) {
+    total_css.clear();
+    css.clear();
+    fprintf(fout, "L1D_cache:\n");
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
+      m_cluster[i]->get_L1D_sub_stats(css);
+
+      fprintf(stdout,
+              "\tL1D_cache_core[%d]: Access = %llu, Miss = %llu, Miss_rate = "
+              "%.3lf, Pending_hits = %llu, Reservation_fails = %llu\n",
+              i, css.accesses, css.misses,
+              (double)css.misses / (double)css.accesses, css.pending_hits,
+              css.res_fails);
+
+      total_css += css;
+    }
+    fprintf(fout, "\tL1D_total_cache_accesses = %llu\n", total_css.accesses);
+    fprintf(fout, "\tL1D_total_cache_misses = %llu\n", total_css.misses);
+    if (total_css.accesses > 0) {
+      fprintf(fout, "\tL1D_total_cache_miss_rate = %.4lf\n",
+              (double)total_css.misses / (double)total_css.accesses);
     }
+    fprintf(fout, "\tL1D_total_cache_pending_hits = %llu\n",
+            total_css.pending_hits);
+    fprintf(fout, "\tL1D_total_cache_reservation_fails = %llu\n",
+            total_css.res_fails);
+    total_css.print_port_stats(fout, "\tL1D_cache");
+  }
 
-    // L1T
-    if(!m_shader_config->m_L1T_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "L1T_cache:\n");
-        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
-            m_cluster[i]->get_L1T_sub_stats(css);
-            total_css += css;
-        }
-        fprintf(fout, "\tL1T_total_cache_accesses = %llu\n", total_css.accesses);
-        fprintf(fout, "\tL1T_total_cache_misses = %llu\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1T_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1T_total_cache_pending_hits = %llu\n", total_css.pending_hits);
-        fprintf(fout, "\tL1T_total_cache_reservation_fails = %llu\n", total_css.res_fails);
+  // L1C
+  if (!m_shader_config->m_L1C_config.disabled()) {
+    total_css.clear();
+    css.clear();
+    fprintf(fout, "L1C_cache:\n");
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i) {
+      m_cluster[i]->get_L1C_sub_stats(css);
+      total_css += css;
     }
-}
+    fprintf(fout, "\tL1C_total_cache_accesses = %llu\n", total_css.accesses);
+    fprintf(fout, "\tL1C_total_cache_misses = %llu\n", total_css.misses);
+    if (total_css.accesses > 0) {
+      fprintf(fout, "\tL1C_total_cache_miss_rate = %.4lf\n",
+              (double)total_css.misses / (double)total_css.accesses);
+    }
+    fprintf(fout, "\tL1C_total_cache_pending_hits = %llu\n",
+            total_css.pending_hits);
+    fprintf(fout, "\tL1C_total_cache_reservation_fails = %llu\n",
+            total_css.res_fails);
+  }
 
-void gpgpu_sim::shader_print_l1_miss_stat( FILE *fout ) const
-{
-   unsigned total_d1_misses = 0, total_d1_accesses = 0;
-   for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
-         unsigned custer_d1_misses = 0, cluster_d1_accesses = 0;
-         m_cluster[ i ]->print_cache_stats( fout, cluster_d1_accesses, custer_d1_misses );
-         total_d1_misses += custer_d1_misses;
-         total_d1_accesses += cluster_d1_accesses;
-   }
-   fprintf( fout, "total_dl1_misses=%d\n", total_d1_misses );
-   fprintf( fout, "total_dl1_accesses=%d\n", total_d1_accesses );
-   fprintf( fout, "total_dl1_miss_rate= %f\n", (float)total_d1_misses / (float)total_d1_accesses );
-   /*
-   fprintf(fout, "THD_INSN_AC: ");
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
-      fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn_ac(i));
-   fprintf(fout, "\n");
-   fprintf(fout, "T_L1_Mss: "); //l1 miss rate per thread
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
-      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i));
-   fprintf(fout, "\n");
-   fprintf(fout, "T_L1_Mgs: "); //l1 merged miss rate per thread
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
-      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i));
-   fprintf(fout, "\n");
-   fprintf(fout, "T_L1_Acc: "); //l1 access per thread
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
-      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_access_ac(i));
-   fprintf(fout, "\n");
-
-   //per warp
-   int temp =0; 
-   fprintf(fout, "W_L1_Mss: "); //l1 miss rate per warp
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
-      temp += m_sc[0]->get_thread_n_l1_mis_ac(i);
-      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
-         fprintf(fout, "%d ", temp);
-         temp = 0;
-      }
-   }
-   fprintf(fout, "\n");
-   temp=0;
-   fprintf(fout, "W_L1_Mgs: "); //l1 merged miss rate per warp
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
-      temp += (m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i) );
-      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
-         fprintf(fout, "%d ", temp);
-         temp = 0;
-      }
-   }
-   fprintf(fout, "\n");
-   temp =0;
-   fprintf(fout, "W_L1_Acc: "); //l1 access per warp
-   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
-      temp += m_sc[0]->get_thread_n_l1_access_ac(i);
-      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
-         fprintf(fout, "%d ", temp);
-         temp = 0;
-      }
-   }
-   fprintf(fout, "\n");
-   */
+  // L1T
+  if (!m_shader_config->m_L1T_config.disabled()) {
+    total_css.clear();
+    css.clear();
+    fprintf(fout, "L1T_cache:\n");
+    for (unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i) {
+      m_cluster[i]->get_L1T_sub_stats(css);
+      total_css += css;
+    }
+    fprintf(fout, "\tL1T_total_cache_accesses = %llu\n", total_css.accesses);
+    fprintf(fout, "\tL1T_total_cache_misses = %llu\n", total_css.misses);
+    if (total_css.accesses > 0) {
+      fprintf(fout, "\tL1T_total_cache_miss_rate = %.4lf\n",
+              (double)total_css.misses / (double)total_css.accesses);
+    }
+    fprintf(fout, "\tL1T_total_cache_pending_hits = %llu\n",
+            total_css.pending_hits);
+    fprintf(fout, "\tL1T_total_cache_reservation_fails = %llu\n",
+            total_css.res_fails);
+  }
 }
 
-void warp_inst_t::print( FILE *fout ) const
-{
-    if (empty() ) {
-        fprintf(fout,"bubble\n" );
-        return;
-    } else 
-        fprintf(fout,"0x%04x ", pc );
-    fprintf(fout, "w%02d[", m_warp_id);
-    for (unsigned j=0; j<m_config->warp_size; j++)
-        fprintf(fout, "%c", (active(j)?'1':'0') );
-    fprintf(fout, "]: ");
-    ptx_print_insn( pc, fout );
-    fprintf(fout, "\n");
+void gpgpu_sim::shader_print_l1_miss_stat(FILE *fout) const {
+  unsigned total_d1_misses = 0, total_d1_accesses = 0;
+  for (unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i) {
+    unsigned custer_d1_misses = 0, cluster_d1_accesses = 0;
+    m_cluster[i]->print_cache_stats(fout, cluster_d1_accesses,
+                                    custer_d1_misses);
+    total_d1_misses += custer_d1_misses;
+    total_d1_accesses += cluster_d1_accesses;
+  }
+  fprintf(fout, "total_dl1_misses=%d\n", total_d1_misses);
+  fprintf(fout, "total_dl1_accesses=%d\n", total_d1_accesses);
+  fprintf(fout, "total_dl1_miss_rate= %f\n",
+          (float)total_d1_misses / (float)total_d1_accesses);
+  /*
+  fprintf(fout, "THD_INSN_AC: ");
+  for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
+     fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn_ac(i));
+  fprintf(fout, "\n");
+  fprintf(fout, "T_L1_Mss: "); //l1 miss rate per thread
+  for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
+     fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i));
+  fprintf(fout, "\n");
+  fprintf(fout, "T_L1_Mgs: "); //l1 merged miss rate per thread
+  for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
+     fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i) -
+  m_sc[0]->get_thread_n_l1_mrghit_ac(i)); fprintf(fout, "\n"); fprintf(fout,
+  "T_L1_Acc: "); //l1 access per thread for (unsigned i=0;
+  i<m_shader_config->n_thread_per_shader; i++) fprintf(fout, "%d ",
+  m_sc[0]->get_thread_n_l1_access_ac(i)); fprintf(fout, "\n");
+
+  //per warp
+  int temp =0;
+  fprintf(fout, "W_L1_Mss: "); //l1 miss rate per warp
+  for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
+     temp += m_sc[0]->get_thread_n_l1_mis_ac(i);
+     if (i%m_shader_config->warp_size ==
+  (unsigned)(m_shader_config->warp_size-1)) { fprintf(fout, "%d ", temp); temp =
+  0;
+     }
+  }
+  fprintf(fout, "\n");
+  temp=0;
+  fprintf(fout, "W_L1_Mgs: "); //l1 merged miss rate per warp
+  for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
+     temp += (m_sc[0]->get_thread_n_l1_mis_ac(i) -
+  m_sc[0]->get_thread_n_l1_mrghit_ac(i) ); if (i%m_shader_config->warp_size ==
+  (unsigned)(m_shader_config->warp_size-1)) { fprintf(fout, "%d ", temp); temp =
+  0;
+     }
+  }
+  fprintf(fout, "\n");
+  temp =0;
+  fprintf(fout, "W_L1_Acc: "); //l1 access per warp
+  for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
+     temp += m_sc[0]->get_thread_n_l1_access_ac(i);
+     if (i%m_shader_config->warp_size ==
+  (unsigned)(m_shader_config->warp_size-1)) { fprintf(fout, "%d ", temp); temp =
+  0;
+     }
+  }
+  fprintf(fout, "\n");
+  */
+}
+
+void warp_inst_t::print(FILE *fout) const {
+  if (empty()) {
+    fprintf(fout, "bubble\n");
+    return;
+  } else
+    fprintf(fout, "0x%04x ", pc);
+  fprintf(fout, "w%02d[", m_warp_id);
+  for (unsigned j = 0; j < m_config->warp_size; j++)
+    fprintf(fout, "%c", (active(j) ? '1' : '0'));
+  fprintf(fout, "]: ");
+  m_config->gpgpu_ctx->func_sim->ptx_print_insn(pc, fout);
+  fprintf(fout, "\n");
 }
 void shader_core_ctx::incexecstat(warp_inst_t *&inst)
 {
@@ -2830,1423 +3232,1540 @@ void shader_core_ctx::incexecstat(warp_inst_t *&inst)
     // Latency numbers for next operations are used to scale the power values
     // for special operations, according observations from microbenchmarking
     // TODO: put these numbers in the xml configuration
-
-	switch(inst->sp_op){
-	case INT__OP:
-		incialu_stat(inst->active_count(),32);
-		break;
-	case INT_MUL_OP:
-		incimul_stat(inst->active_count(),7.2);
-		break;
-	case INT_MUL24_OP:
-		incimul24_stat(inst->active_count(),4.2);
-		break;
-	case INT_MUL32_OP:
-		incimul32_stat(inst->active_count(),4);
-		break;
-	case INT_DIV_OP:
-		incidiv_stat(inst->active_count(),40);
-		break;
-	case FP__OP:
-		incfpalu_stat(inst->active_count(),1);
-		break;
-	case FP_MUL_OP:
-		incfpmul_stat(inst->active_count(),1.8);
-		break;
-	case FP_DIV_OP:
-		incfpdiv_stat(inst->active_count(),48);
-		break;
-	case FP_SQRT_OP:
-		inctrans_stat(inst->active_count(),25);
-		break;
-	case FP_LG_OP:
-		inctrans_stat(inst->active_count(),35);
-		break;
-	case FP_SIN_OP:
-		inctrans_stat(inst->active_count(),12);
-		break;
-	case FP_EXP_OP:
-		inctrans_stat(inst->active_count(),35);
-		break;
-	default:
-		break;
-	}
-}
-void shader_core_ctx::print_stage(unsigned int stage, FILE *fout ) const
-{
-   m_pipeline_reg[stage].print(fout);
-   //m_pipeline_reg[stage].print(fout);
+  if(get_gpu()->get_config().g_power_simulation_enabled){
+    switch(inst->sp_op){
+    case INT__OP:
+      incialu_stat(inst->active_count(), scaling_coeffs->int_coeff);
+      break;
+    case INT_MUL_OP:
+      incimul_stat(inst->active_count(), scaling_coeffs->int_mul_coeff);
+      break;
+    case INT_MUL24_OP:
+      incimul24_stat(inst->active_count(), scaling_coeffs->int_mul24_coeff);
+      break;
+    case INT_MUL32_OP:
+      incimul32_stat(inst->active_count(), scaling_coeffs->int_mul32_coeff);
+      break;
+    case INT_DIV_OP:
+      incidiv_stat(inst->active_count(), scaling_coeffs->int_div_coeff);
+      break;
+    case FP__OP:
+      incfpalu_stat(inst->active_count(),scaling_coeffs->fp_coeff);
+      break;
+    case FP_MUL_OP:
+      incfpmul_stat(inst->active_count(), scaling_coeffs->fp_mul_coeff);
+      break;
+    case FP_DIV_OP:
+      incfpdiv_stat(inst->active_count(), scaling_coeffs->fp_div_coeff);
+      break;
+    case DP___OP:
+      incdpalu_stat(inst->active_count(), scaling_coeffs->dp_coeff);
+      break;
+    case DP_MUL_OP:
+      incdpmul_stat(inst->active_count(), scaling_coeffs->dp_mul_coeff);
+      break;
+    case DP_DIV_OP:
+      incdpdiv_stat(inst->active_count(), scaling_coeffs->dp_div_coeff);
+      break;
+    case FP_SQRT_OP:
+      incsqrt_stat(inst->active_count(), scaling_coeffs->sqrt_coeff);
+      break;
+    case FP_LG_OP:
+      inclog_stat(inst->active_count(), scaling_coeffs->log_coeff);
+      break;
+    case FP_SIN_OP:
+      incsin_stat(inst->active_count(), scaling_coeffs->sin_coeff);
+      break;
+    case FP_EXP_OP:
+      incexp_stat(inst->active_count(), scaling_coeffs->exp_coeff);
+      break;
+    case TENSOR__OP:
+      inctensor_stat(inst->active_count(), scaling_coeffs->tensor_coeff);
+      break;
+    case TEX__OP:
+      inctex_stat(inst->active_count(), scaling_coeffs->tex_coeff);
+      break;
+    default:
+      break;
+    }
+    if(inst->const_cache_operand) //warp has const address space load as one operand
+      inc_const_accesses(1);
+  }
 }
-
-void shader_core_ctx::display_simt_state(FILE *fout, int mask ) const
-{
-    if ( (mask & 4) && m_config->model == POST_DOMINATOR ) {
-       fprintf(fout,"per warp SIMT control-flow state:\n");
-       unsigned n = m_config->n_thread_per_shader / m_config->warp_size;
-       for (unsigned i=0; i < n; i++) {
-          unsigned nactive = 0;
-          for (unsigned j=0; j<m_config->warp_size; j++ ) {
-             unsigned tid = i*m_config->warp_size + j;
-             int done = ptx_thread_done(tid);
-             nactive += (ptx_thread_done(tid)?0:1);
-             if ( done && (mask & 8) ) {
-                unsigned done_cycle = m_thread[tid]->donecycle();
-                if ( done_cycle ) {
-                   printf("\n w%02u:t%03u: done @ cycle %u", i, tid, done_cycle );
-                }
-             }
-          }
-          if ( nactive == 0 ) {
-             continue;
+void shader_core_ctx::print_stage(unsigned int stage, FILE *fout) const {
+  m_pipeline_reg[stage].print(fout);
+  // m_pipeline_reg[stage].print(fout);
+}
+
+void shader_core_ctx::display_simt_state(FILE *fout, int mask) const {
+  if ((mask & 4) && m_config->model == POST_DOMINATOR) {
+    fprintf(fout, "per warp SIMT control-flow state:\n");
+    unsigned n = m_config->n_thread_per_shader / m_config->warp_size;
+    for (unsigned i = 0; i < n; i++) {
+      unsigned nactive = 0;
+      for (unsigned j = 0; j < m_config->warp_size; j++) {
+        unsigned tid = i * m_config->warp_size + j;
+        int done = ptx_thread_done(tid);
+        nactive += (ptx_thread_done(tid) ? 0 : 1);
+        if (done && (mask & 8)) {
+          unsigned done_cycle = m_thread[tid]->donecycle();
+          if (done_cycle) {
+            printf("\n w%02u:t%03u: done @ cycle %u", i, tid, done_cycle);
           }
-          m_simt_stack[i]->print(fout);
-       }
-       fprintf(fout,"\n");
+        }
+      }
+      if (nactive == 0) {
+        continue;
+      }
+      m_simt_stack[i]->print(fout);
     }
+    fprintf(fout, "\n");
+  }
 }
 
-void ldst_unit::print(FILE *fout) const
-{
-    fprintf(fout,"LD/ST unit  = ");
-    m_dispatch_reg->print(fout);
-    if ( m_mem_rc != NO_RC_FAIL ) {
-        fprintf(fout,"              LD/ST stall condition: ");
-        switch ( m_mem_rc ) {
-        case BK_CONF:        fprintf(fout,"BK_CONF"); break;
-        case MSHR_RC_FAIL:   fprintf(fout,"MSHR_RC_FAIL"); break;
-        case ICNT_RC_FAIL:   fprintf(fout,"ICNT_RC_FAIL"); break;
-        case COAL_STALL:     fprintf(fout,"COAL_STALL"); break;
-        case WB_ICNT_RC_FAIL: fprintf(fout,"WB_ICNT_RC_FAIL"); break;
-        case WB_CACHE_RSRV_FAIL: fprintf(fout,"WB_CACHE_RSRV_FAIL"); break;
-        case N_MEM_STAGE_STALL_TYPE: fprintf(fout,"N_MEM_STAGE_STALL_TYPE"); break;
-        default: abort();
-        }
-        fprintf(fout,"\n");
-    }
-    fprintf(fout,"LD/ST wb    = ");
-    m_next_wb.print(fout);
-    fprintf(fout, "Last LD/ST writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
-                  m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );
-    fprintf(fout,"Pending register writes:\n");
-    std::map<unsigned/*warp_id*/, std::map<unsigned/*regnum*/,unsigned/*count*/> >::const_iterator w;
-    for( w=m_pending_writes.begin(); w!=m_pending_writes.end(); w++ ) {
-        unsigned warp_id = w->first;
-        const std::map<unsigned/*regnum*/,unsigned/*count*/> &warp_info = w->second;
-        if( warp_info.empty() ) 
-            continue;
-        fprintf(fout,"  w%2u : ", warp_id );
-        std::map<unsigned/*regnum*/,unsigned/*count*/>::const_iterator r;
-        for( r=warp_info.begin(); r!=warp_info.end(); ++r ) {
-            fprintf(fout,"  %u(%u)", r->first, r->second );
-        }
-        fprintf(fout,"\n");
+void ldst_unit::print(FILE *fout) const {
+  fprintf(fout, "LD/ST unit  = ");
+  m_dispatch_reg->print(fout);
+  if (m_mem_rc != NO_RC_FAIL) {
+    fprintf(fout, "              LD/ST stall condition: ");
+    switch (m_mem_rc) {
+      case BK_CONF:
+        fprintf(fout, "BK_CONF");
+        break;
+      case MSHR_RC_FAIL:
+        fprintf(fout, "MSHR_RC_FAIL");
+        break;
+      case ICNT_RC_FAIL:
+        fprintf(fout, "ICNT_RC_FAIL");
+        break;
+      case COAL_STALL:
+        fprintf(fout, "COAL_STALL");
+        break;
+      case WB_ICNT_RC_FAIL:
+        fprintf(fout, "WB_ICNT_RC_FAIL");
+        break;
+      case WB_CACHE_RSRV_FAIL:
+        fprintf(fout, "WB_CACHE_RSRV_FAIL");
+        break;
+      case N_MEM_STAGE_STALL_TYPE:
+        fprintf(fout, "N_MEM_STAGE_STALL_TYPE");
+        break;
+      default:
+        abort();
     }
-    m_L1C->display_state(fout);
-    m_L1T->display_state(fout);
-    if( !m_config->m_L1D_config.disabled() )
-    	m_L1D->display_state(fout);
-    fprintf(fout,"LD/ST response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
-    for( std::list<mem_fetch*>::const_iterator i=m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
-        const mem_fetch *mf = *i;
-        mf->print(fout);
+    fprintf(fout, "\n");
+  }
+  fprintf(fout, "LD/ST wb    = ");
+  m_next_wb.print(fout);
+  fprintf(
+      fout,
+      "Last LD/ST writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
+      m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle);
+  fprintf(fout, "Pending register writes:\n");
+  std::map<unsigned /*warp_id*/,
+           std::map<unsigned /*regnum*/, unsigned /*count*/> >::const_iterator
+      w;
+  for (w = m_pending_writes.begin(); w != m_pending_writes.end(); w++) {
+    unsigned warp_id = w->first;
+    const std::map<unsigned /*regnum*/, unsigned /*count*/> &warp_info =
+        w->second;
+    if (warp_info.empty()) continue;
+    fprintf(fout, "  w%2u : ", warp_id);
+    std::map<unsigned /*regnum*/, unsigned /*count*/>::const_iterator r;
+    for (r = warp_info.begin(); r != warp_info.end(); ++r) {
+      fprintf(fout, "  %u(%u)", r->first, r->second);
     }
+    fprintf(fout, "\n");
+  }
+  m_L1C->display_state(fout);
+  m_L1T->display_state(fout);
+  if (!m_config->m_L1D_config.disabled()) m_L1D->display_state(fout);
+  fprintf(fout, "LD/ST response FIFO (occupancy = %zu):\n",
+          m_response_fifo.size());
+  for (std::list<mem_fetch *>::const_iterator i = m_response_fifo.begin();
+       i != m_response_fifo.end(); i++) {
+    const mem_fetch *mf = *i;
+    mf->print(fout);
+  }
 }
 
-void shader_core_ctx::display_pipeline(FILE *fout, int print_mem, int mask ) const
-{
-   fprintf(fout, "=================================================\n");
-   fprintf(fout, "shader %u at cycle %Lu+%Lu (%u threads running)\n", m_sid, 
-           gpu_tot_sim_cycle, gpu_sim_cycle, m_not_completed);
-   fprintf(fout, "=================================================\n");
-
-   dump_warp_state(fout);
-   fprintf(fout,"\n");
-
-   m_L1I->display_state(fout);
-
-   fprintf(fout, "IF/ID       = ");
-   if( !m_inst_fetch_buffer.m_valid )
-       fprintf(fout,"bubble\n");
-   else {
-       fprintf(fout,"w%2u : pc = 0x%x, nbytes = %u\n", 
-               m_inst_fetch_buffer.m_warp_id,
-               m_inst_fetch_buffer.m_pc, 
-               m_inst_fetch_buffer.m_nbytes );
-   }
-   fprintf(fout,"\nibuffer status:\n");
-   for( unsigned i=0; i<m_config->max_warps_per_shader; i++) {
-       if( !m_warp[i].ibuffer_empty() ) 
-           m_warp[i].print_ibuffer(fout);
-   }
-   fprintf(fout,"\n");
-   display_simt_state(fout,mask);
-   fprintf(fout, "-------------------------- Scoreboard\n");
-   m_scoreboard->printContents();
-/*
-   fprintf(fout,"ID/OC (SP)  = ");
-   print_stage(ID_OC_SP, fout);
-   fprintf(fout,"ID/OC (SFU) = ");
-   print_stage(ID_OC_SFU, fout);
-   fprintf(fout,"ID/OC (MEM) = ");
-   print_stage(ID_OC_MEM, fout);
-*/
-   fprintf(fout, "-------------------------- OP COL\n");
-   m_operand_collector.dump(fout);
-/* fprintf(fout, "OC/EX (SP)  = ");
-   print_stage(OC_EX_SP, fout);
-   fprintf(fout, "OC/EX (SFU) = ");
-   print_stage(OC_EX_SFU, fout);
-   fprintf(fout, "OC/EX (MEM) = ");
-   print_stage(OC_EX_MEM, fout);
-*/
-   fprintf(fout, "-------------------------- Pipe Regs\n");
+void shader_core_ctx::display_pipeline(FILE *fout, int print_mem,
+                                       int mask) const {
+  fprintf(fout, "=================================================\n");
+  fprintf(fout, "shader %u at cycle %Lu+%Lu (%u threads running)\n", m_sid,
+          m_gpu->gpu_tot_sim_cycle, m_gpu->gpu_sim_cycle, m_not_completed);
+  fprintf(fout, "=================================================\n");
 
-   for (unsigned i = 0; i < N_PIPELINE_STAGES; i++) {
-       fprintf(fout,"--- %s ---\n",pipeline_stage_name_decode[i]);
-       print_stage(i,fout);fprintf(fout,"\n");
-   }
+  dump_warp_state(fout);
+  fprintf(fout, "\n");
 
-   fprintf(fout, "-------------------------- Fu\n");
-   for( unsigned n=0; n < m_num_function_units; n++ ){
-       m_fu[n]->print(fout);
-       fprintf(fout, "---------------\n");
-   }
-   fprintf(fout, "-------------------------- other:\n");
+  m_L1I->display_state(fout);
 
-   for(unsigned i=0; i<num_result_bus; i++){
-	   std::string bits = m_result_bus[i]->to_string();
-	   fprintf(fout, "EX/WB sched[%d]= %s\n", i, bits.c_str() );
-   }
-   fprintf(fout, "EX/WB      = ");
-   print_stage(EX_WB, fout);
-   fprintf(fout, "\n");
-   fprintf(fout, "Last EX/WB writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
-                 m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );
-
-   if( m_active_threads.count() <= 2*m_config->warp_size ) {
-       fprintf(fout,"Active Threads : ");
-       unsigned last_warp_id = -1;
-       for(unsigned tid=0; tid < m_active_threads.size(); tid++ ) {
-           unsigned warp_id = tid/m_config->warp_size;
-           if( m_active_threads.test(tid) ) {
-               if( warp_id != last_warp_id ) {
-                   fprintf(fout,"\n  warp %u : ", warp_id );
-                   last_warp_id=warp_id;
-               }
-               fprintf(fout,"%u ", tid );
-           }
-       }
-   }
+  fprintf(fout, "IF/ID       = ");
+  if (!m_inst_fetch_buffer.m_valid)
+    fprintf(fout, "bubble\n");
+  else {
+    fprintf(fout, "w%2u : pc = 0x%x, nbytes = %u\n",
+            m_inst_fetch_buffer.m_warp_id, m_inst_fetch_buffer.m_pc,
+            m_inst_fetch_buffer.m_nbytes);
+  }
+  fprintf(fout, "\nibuffer status:\n");
+  for (unsigned i = 0; i < m_config->max_warps_per_shader; i++) {
+    if (!m_warp[i]->ibuffer_empty()) m_warp[i]->print_ibuffer(fout);
+  }
+  fprintf(fout, "\n");
+  display_simt_state(fout, mask);
+  fprintf(fout, "-------------------------- Scoreboard\n");
+  m_scoreboard->printContents();
+  /*
+     fprintf(fout,"ID/OC (SP)  = ");
+     print_stage(ID_OC_SP, fout);
+     fprintf(fout,"ID/OC (SFU) = ");
+     print_stage(ID_OC_SFU, fout);
+     fprintf(fout,"ID/OC (MEM) = ");
+     print_stage(ID_OC_MEM, fout);
+  */
+  fprintf(fout, "-------------------------- OP COL\n");
+  m_operand_collector.dump(fout);
+  /* fprintf(fout, "OC/EX (SP)  = ");
+     print_stage(OC_EX_SP, fout);
+     fprintf(fout, "OC/EX (SFU) = ");
+     print_stage(OC_EX_SFU, fout);
+     fprintf(fout, "OC/EX (MEM) = ");
+     print_stage(OC_EX_MEM, fout);
+  */
+  fprintf(fout, "-------------------------- Pipe Regs\n");
+
+  for (unsigned i = 0; i < N_PIPELINE_STAGES; i++) {
+    fprintf(fout, "--- %s ---\n", pipeline_stage_name_decode[i]);
+    print_stage(i, fout);
+    fprintf(fout, "\n");
+  }
+
+  fprintf(fout, "-------------------------- Fu\n");
+  for (unsigned n = 0; n < m_num_function_units; n++) {
+    m_fu[n]->print(fout);
+    fprintf(fout, "---------------\n");
+  }
+  fprintf(fout, "-------------------------- other:\n");
 
+  for (unsigned i = 0; i < num_result_bus; i++) {
+    std::string bits = m_result_bus[i]->to_string();
+    fprintf(fout, "EX/WB sched[%d]= %s\n", i, bits.c_str());
+  }
+  fprintf(fout, "EX/WB      = ");
+  print_stage(EX_WB, fout);
+  fprintf(fout, "\n");
+  fprintf(
+      fout,
+      "Last EX/WB writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
+      m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle);
+
+  if (m_active_threads.count() <= 2 * m_config->warp_size) {
+    fprintf(fout, "Active Threads : ");
+    unsigned last_warp_id = -1;
+    for (unsigned tid = 0; tid < m_active_threads.size(); tid++) {
+      unsigned warp_id = tid / m_config->warp_size;
+      if (m_active_threads.test(tid)) {
+        if (warp_id != last_warp_id) {
+          fprintf(fout, "\n  warp %u : ", warp_id);
+          last_warp_id = warp_id;
+        }
+        fprintf(fout, "%u ", tid);
+      }
+    }
+  }
 }
 
-unsigned int shader_core_config::max_cta( const kernel_info_t &k ) const
-{
-   unsigned threads_per_cta  = k.threads_per_cta();
-   const class function_info *kernel = k.entry();
-   unsigned int padded_cta_size = threads_per_cta;
-   if (padded_cta_size%warp_size) 
-      padded_cta_size = ((padded_cta_size/warp_size)+1)*(warp_size);
-
-   //Limit by n_threads/shader
-   unsigned int result_thread = n_thread_per_shader / padded_cta_size;
-
-   const struct gpgpu_ptx_sim_info *kernel_info = ptx_sim_kernel_info(kernel);
-
-   //Limit by shmem/shader
-   unsigned int result_shmem = (unsigned)-1;
-   if (kernel_info->smem > 0)
-      result_shmem = gpgpu_shmem_size / kernel_info->smem;
-
-   //Limit by register count, rounded up to multiple of 4.
-   unsigned int result_regs = (unsigned)-1;
-   if (kernel_info->regs > 0)
-      result_regs = gpgpu_shader_registers / (padded_cta_size * ((kernel_info->regs+3)&~3));
-
-   //Limit by CTA
-   unsigned int result_cta = max_cta_per_core;
-
-   unsigned result = result_thread;
-   result = gs_min2(result, result_shmem);
-   result = gs_min2(result, result_regs);
-   result = gs_min2(result, result_cta);
-
-   static const struct gpgpu_ptx_sim_info* last_kinfo = NULL;
-   if (last_kinfo != kernel_info) {   //Only print out stats if kernel_info struct changes
-      last_kinfo = kernel_info;
-      printf ("GPGPU-Sim uArch: CTA/core = %u, limited by:", result);
-      if (result == result_thread) printf (" threads");
-      if (result == result_shmem) printf (" shmem");
-      if (result == result_regs) printf (" regs");
-      if (result == result_cta) printf (" cta_limit");
-      printf ("\n");
-   }
+unsigned int shader_core_config::max_cta(const kernel_info_t &k) const {
+  unsigned threads_per_cta = k.threads_per_cta();
+  const class function_info *kernel = k.entry();
+  unsigned int padded_cta_size = threads_per_cta;
+  if (padded_cta_size % warp_size)
+    padded_cta_size = ((padded_cta_size / warp_size) + 1) * (warp_size);
+
+  // Limit by n_threads/shader
+  unsigned int result_thread = n_thread_per_shader / padded_cta_size;
+
+  const struct gpgpu_ptx_sim_info *kernel_info = ptx_sim_kernel_info(kernel);
+
+  // Limit by shmem/shader
+  unsigned int result_shmem = (unsigned)-1;
+  if (kernel_info->smem > 0)
+    result_shmem = gpgpu_shmem_size / kernel_info->smem;
+
+  // Limit by register count, rounded up to multiple of 4.
+  unsigned int result_regs = (unsigned)-1;
+  if (kernel_info->regs > 0)
+    result_regs = gpgpu_shader_registers /
+                  (padded_cta_size * ((kernel_info->regs + 3) & ~3));
+
+  // Limit by CTA
+  unsigned int result_cta = max_cta_per_core;
+
+  unsigned result = result_thread;
+  result = gs_min2(result, result_shmem);
+  result = gs_min2(result, result_regs);
+  result = gs_min2(result, result_cta);
+
+  static const struct gpgpu_ptx_sim_info *last_kinfo = NULL;
+  if (last_kinfo !=
+      kernel_info) {  // Only print out stats if kernel_info struct changes
+    last_kinfo = kernel_info;
+    printf("GPGPU-Sim uArch: CTA/core = %u, limited by:", result);
+    if (result == result_thread) printf(" threads");
+    if (result == result_shmem) printf(" shmem");
+    if (result == result_regs) printf(" regs");
+    if (result == result_cta) printf(" cta_limit");
+    printf("\n");
+  }
 
-    //gpu_max_cta_per_shader is limited by number of CTAs if not enough to keep all cores busy    
-    if( k.num_blocks() < result*num_shader() ) { 
-       result = k.num_blocks() / num_shader();
-       if (k.num_blocks() % num_shader())
-          result++;
+  // gpu_max_cta_per_shader is limited by number of CTAs if not enough to keep
+  // all cores busy
+  if (k.num_blocks() < result * num_shader()) {
+    result = k.num_blocks() / num_shader();
+    if (k.num_blocks() % num_shader()) result++;
+  }
+
+  assert(result <= MAX_CTA_PER_SHADER);
+  if (result < 1) {
+    printf(
+        "GPGPU-Sim uArch: ERROR ** Kernel requires more resources than shader "
+        "has.\n");
+    if (gpgpu_ignore_resources_limitation) {
+      printf(
+          "GPGPU-Sim uArch: gpgpu_ignore_resources_limitation is set, ignore "
+          "the ERROR!\n");
+      return 1;
     }
+    abort();
+  }
 
-    assert( result <= MAX_CTA_PER_SHADER );
-    if (result < 1) {
-       printf ("GPGPU-Sim uArch: ERROR ** Kernel requires more resources than shader has.\n");
-       if(gpgpu_ignore_resources_limitation) {
-    	   printf ("GPGPU-Sim uArch: gpgpu_ignore_resources_limitation is set, ignore the ERROR!\n");
-    	   return 1;
-       }
-       abort();
+  if (adaptive_cache_config && !k.cache_config_set) {
+    // For more info about adaptive cache, see
+    // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-7-x
+    unsigned total_shmem = kernel_info->smem * result;
+    assert(total_shmem >= 0 && total_shmem <= shmem_opt_list.back());
+
+    // Unified cache config is in KB. Converting to B
+    unsigned total_unified = m_L1D_config.m_unified_cache_size * 1024;
+
+    bool l1d_configured = false;
+    unsigned max_assoc = m_L1D_config.get_max_assoc();
+
+    for (std::vector<unsigned>::const_iterator it = shmem_opt_list.begin();
+         it < shmem_opt_list.end(); it++) {
+      if (total_shmem <= *it) {
+        float l1_ratio = 1 - ((float)*(it) / total_unified);
+        // make sure the ratio is between 0 and 1
+        assert(0 <= l1_ratio && l1_ratio <= 1);
+        // round to nearest instead of round down
+        m_L1D_config.set_assoc(max_assoc * l1_ratio + 0.5f);
+        l1d_configured = true;
+        break;
+      }
     }
 
-    if(adaptive_volta_cache_config && !k.volta_cache_config_set) {
-    	//For Volta, we assign the remaining shared memory to L1 cache
-    	//For more info, see https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-7-x
-    	unsigned total_shmed = kernel_info->smem * result;
-    	assert(total_shmed >=0 && total_shmed <= gpgpu_shmem_size);
-    	assert(gpgpu_shmem_size == 98304); //Volta has 96 KB shared
-    	assert(m_L1D_config.get_nset() == 4);  //Volta L1 has four sets
-    	if(total_shmed < gpgpu_shmem_size){
-    		if(total_shmed == 0)
-    			m_L1D_config.set_assoc(256);  //L1 is 128KB ans shd=0
-    		else if(total_shmed > 0 && total_shmed <= 8192)
-    			m_L1D_config.set_assoc(240);  //L1 is 120KB ans shd=8KB
-    		else if(total_shmed > 8192 && total_shmed <= 16384)
-    		    m_L1D_config.set_assoc(224);  //L1 is 112KB ans shd=16KB
-    		else if(total_shmed > 16384 && total_shmed <= 32768)
-    		    m_L1D_config.set_assoc(192);  //L1 is 96KB ans shd=32KB
-    		else if(total_shmed > 32768 && total_shmed <= 65536)
-    		    m_L1D_config.set_assoc(128);	//L1 is 64KB ans shd=64KB
-    		else if(total_shmed > 65536 && total_shmed <= gpgpu_shmem_size)
-    		     m_L1D_config.set_assoc(64); //L1 is 32KB and shd=96KB
-    		else
-    			assert(0);
-
-    		 printf ("GPGPU-Sim: Reconfigure L1 cache in Volta Archi to %uKB\n", m_L1D_config.get_total_size_inKB());
-    	}
-
-    	k.volta_cache_config_set = true;
+    assert(l1d_configured && "no shared memory option found");
+
+    if (m_L1D_config.is_streaming()) {
+      // for streaming cache, if the whole memory is allocated
+      // to the L1 cache, then make the allocation to be on_MISS
+      // otherwise, make it ON_FILL to eliminate line allocation fails
+      // i.e. MSHR throughput is the same, independent on the L1 cache
+      // size/associativity
+      if (total_shmem == 0) {
+        m_L1D_config.set_allocation_policy(ON_MISS);
+        printf("GPGPU-Sim: Reconfigure L1 allocation to ON_MISS\n");
+      } else {
+        m_L1D_config.set_allocation_policy(ON_FILL);
+        printf("GPGPU-Sim: Reconfigure L1 allocation to ON_FILL\n");
+      }
     }
+    printf("GPGPU-Sim: Reconfigure L1 cache to %uKB\n",
+           m_L1D_config.get_total_size_inKB());
 
-    return result;
+    k.cache_config_set = true;
+  }
+
+  return result;
 }
 
 void shader_core_config::set_pipeline_latency() {
-
-		//calculate the max latency  based on the input
-
-		unsigned int_latency[6];
-		unsigned fp_latency[5];
-		unsigned dp_latency[5];
-		unsigned sfu_latency;
-		unsigned tensor_latency;
-
-			/*
-			 * [0] ADD,SUB
-			 * [1] MAX,Min
-			 * [2] MUL
-			 * [3] MAD
-			 * [4] DIV
-			 * [5] SHFL
-			 */
-			sscanf(opcode_latency_int, "%u,%u,%u,%u,%u,%u",
-					&int_latency[0],&int_latency[1],&int_latency[2],
-					&int_latency[3],&int_latency[4],&int_latency[5]);
-			sscanf(opcode_latency_fp, "%u,%u,%u,%u,%u",
-					&fp_latency[0],&fp_latency[1],&fp_latency[2],
-					&fp_latency[3],&fp_latency[4]);
-			sscanf(opcode_latency_dp, "%u,%u,%u,%u,%u",
-					&dp_latency[0],&dp_latency[1],&dp_latency[2],
-					&dp_latency[3],&dp_latency[4]);
-			sscanf(opcode_latency_sfu, "%u",
-					&sfu_latency);
-			sscanf(opcode_latency_tensor, "%u",
-					&tensor_latency);
-
-		//all div operation are executed on sfu
-		//assume that the max latency are dp div or normal sfu_latency
-		max_sfu_latency = std::max(dp_latency[4],sfu_latency);
-		//assume that the max operation has the max latency
-		max_sp_latency = fp_latency[1];
-		max_int_latency = std::max(int_latency[1],int_latency[5]);
-		max_dp_latency = dp_latency[1];
-		max_tensor_core_latency = tensor_latency;
-
-}
-
-void shader_core_ctx::cycle()
-{
-	if(!isactive() && get_not_completed() == 0)
-		return;
-
-	m_stats->shader_cycles[m_sid]++;
-    writeback();
-    execute();
-    read_operands();
-    issue();
+  // calculate the max latency  based on the input
+
+  unsigned int_latency[6];
+  unsigned fp_latency[5];
+  unsigned dp_latency[5];
+  unsigned sfu_latency;
+  unsigned tensor_latency;
+
+  /*
+   * [0] ADD,SUB
+   * [1] MAX,Min
+   * [2] MUL
+   * [3] MAD
+   * [4] DIV
+   * [5] SHFL
+   */
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_int, "%u,%u,%u,%u,%u,%u",
+         &int_latency[0], &int_latency[1], &int_latency[2], &int_latency[3],
+         &int_latency[4], &int_latency[5]);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_fp, "%u,%u,%u,%u,%u",
+         &fp_latency[0], &fp_latency[1], &fp_latency[2], &fp_latency[3],
+         &fp_latency[4]);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_dp, "%u,%u,%u,%u,%u",
+         &dp_latency[0], &dp_latency[1], &dp_latency[2], &dp_latency[3],
+         &dp_latency[4]);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_sfu, "%u", &sfu_latency);
+  sscanf(gpgpu_ctx->func_sim->opcode_latency_tensor, "%u", &tensor_latency);
+
+  // all div operation are executed on sfu
+  // assume that the max latency are dp div or normal sfu_latency
+  max_sfu_latency = std::max(dp_latency[4], sfu_latency);
+  // assume that the max operation has the max latency
+  max_sp_latency = fp_latency[1];
+  max_int_latency = std::max(int_latency[1], int_latency[5]);
+  max_dp_latency = dp_latency[1];
+  max_tensor_core_latency = tensor_latency;
+}
+
+void shader_core_ctx::cycle() {
+  if (!isactive() && get_not_completed() == 0) return;
+
+  m_stats->shader_cycles[m_sid]++;
+  writeback();
+  execute();
+  read_operands();
+  issue();
+  for (int i = 0; i < m_config->inst_fetch_throughput; ++i) {
     decode();
     fetch();
+  }
 }
 
 // Flushes all content of the cache to memory
 
-void shader_core_ctx::cache_flush()
-{
-   m_ldst_unit->flush();
-}
+void shader_core_ctx::cache_flush() { m_ldst_unit->flush(); }
 
-void shader_core_ctx::cache_invalidate()
-{
-   m_ldst_unit->invalidate();
-}
+void shader_core_ctx::cache_invalidate() { m_ldst_unit->invalidate(); }
 
 // modifiers
-std::list<opndcoll_rfu_t::op_t> opndcoll_rfu_t::arbiter_t::allocate_reads() 
-{
-   std::list<op_t> result;  // a list of registers that (a) are in different register banks, (b) do not go to the same operand collector
-
-   int input;
-   int output;
-   int _inputs = m_num_banks;
-   int _outputs = m_num_collectors;
-   int _square = ( _inputs > _outputs ) ? _inputs : _outputs;
-   assert(_square > 0);
-   int _pri = (int)m_last_cu;
-
-   // Clear matching
-   for ( int i = 0; i < _inputs; ++i ) 
-      _inmatch[i] = -1;
-   for ( int j = 0; j < _outputs; ++j ) 
-      _outmatch[j] = -1;
-
-   for( unsigned i=0; i<m_num_banks; i++) {
-      for( unsigned j=0; j<m_num_collectors; j++) {
-         assert( i < (unsigned)_inputs );
-         assert( j < (unsigned)_outputs );
-         _request[i][j] = 0;
-      }
-      if( !m_queue[i].empty() ) {
-         const op_t &op = m_queue[i].front();
-         int oc_id = op.get_oc_id();
-         assert( i < (unsigned)_inputs );
-         assert( oc_id < _outputs );
-         _request[i][oc_id] = 1;
-      }
-      if( m_allocated_bank[i].is_write() ) {
-         assert( i < (unsigned)_inputs );
-         _inmatch[i] = 0; // write gets priority
-      }
-   }
-
-   ///// wavefront allocator from booksim... --->
-   
-   // Loop through diagonals of request matrix
-
-   for ( int p = 0; p < _square; ++p ) {
-      output = ( _pri + p ) % _square;
-
-      // Step through the current diagonal
-      for ( input = 0; input < _inputs; ++input ) {
-          assert( input < _inputs );
-          assert( output < _outputs );
-         if ( ( output < _outputs ) && 
-              ( _inmatch[input] == -1 ) && 
-              ( _outmatch[output] == -1 ) &&
-              ( _request[input][output]/*.label != -1*/ ) ) {
-            // Grant!
-            _inmatch[input] = output;
-            _outmatch[output] = input;
-         }
+std::list<opndcoll_rfu_t::op_t> opndcoll_rfu_t::arbiter_t::allocate_reads() {
+  std::list<op_t>
+      result;  // a list of registers that (a) are in different register banks,
+               // (b) do not go to the same operand collector
+
+  int input;
+  int output;
+  int _inputs = m_num_banks;
+  int _outputs = m_num_collectors;
+  int _square = (_inputs > _outputs) ? _inputs : _outputs;
+  assert(_square > 0);
+  int _pri = (int)m_last_cu;
+
+  // Clear matching
+  for (int i = 0; i < _inputs; ++i) _inmatch[i] = -1;
+  for (int j = 0; j < _outputs; ++j) _outmatch[j] = -1;
+
+  for (unsigned i = 0; i < m_num_banks; i++) {
+    for (unsigned j = 0; j < m_num_collectors; j++) {
+      assert(i < (unsigned)_inputs);
+      assert(j < (unsigned)_outputs);
+      _request[i][j] = 0;
+    }
+    if (!m_queue[i].empty()) {
+      const op_t &op = m_queue[i].front();
+      int oc_id = op.get_oc_id();
+      assert(i < (unsigned)_inputs);
+      assert(oc_id < _outputs);
+      _request[i][oc_id] = 1;
+    }
+    if (m_allocated_bank[i].is_write()) {
+      assert(i < (unsigned)_inputs);
+      _inmatch[i] = 0;  // write gets priority
+    }
+  }
 
-         output = ( output + 1 ) % _square;
+  ///// wavefront allocator from booksim... --->
+
+  // Loop through diagonals of request matrix
+  // printf("####\n");
+
+  for (int p = 0; p < _square; ++p) {
+    output = (_pri + p) % _outputs;
+
+    // Step through the current diagonal
+    for (input = 0; input < _inputs; ++input) {
+      assert(input < _inputs);
+      assert(output < _outputs);
+      if ((output < _outputs) && (_inmatch[input] == -1) &&
+          //( _outmatch[output] == -1 ) &&   //allow OC to read multiple reg
+          // banks at the same cycle
+          (_request[input][output] /*.label != -1*/)) {
+        // Grant!
+        _inmatch[input] = output;
+        _outmatch[output] = input;
+        // printf("Register File: granting bank %d to OC %d, schedid %d, warpid
+        // %d, Regid %d\n", input, output, (m_queue[input].front()).get_sid(),
+        // (m_queue[input].front()).get_wid(),
+        // (m_queue[input].front()).get_reg());
       }
-   }
 
-   // Round-robin the priority diagonal
-   _pri = ( _pri + 1 ) % _square;
+      output = (output + 1) % _outputs;
+    }
+  }
 
-   /// <--- end code from booksim
+  // Round-robin the priority diagonal
+  _pri = (_pri + 1) % _outputs;
 
-   m_last_cu = _pri;
-   for( unsigned i=0; i < m_num_banks; i++ ) {
-      if( _inmatch[i] != -1 ) {
-         if( !m_allocated_bank[i].is_write() ) {
-            unsigned bank = (unsigned)i;
-            op_t &op = m_queue[bank].front();
-            result.push_back(op);
-            m_queue[bank].pop_front();
-         }
-      }
-   }
+  /// <--- end code from booksim
 
-   return result;
-}
+  m_last_cu = _pri;
+  for (unsigned i = 0; i < m_num_banks; i++) {
+    if (_inmatch[i] != -1) {
+      if (!m_allocated_bank[i].is_write()) {
+        unsigned bank = (unsigned)i;
+        op_t &op = m_queue[bank].front();
+        result.push_back(op);
+        m_queue[bank].pop_front();
+      }
+    }
+  }
 
-barrier_set_t::barrier_set_t(shader_core_ctx *shader,unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size)
-{
-   m_max_warps_per_core = max_warps_per_core;
-   m_max_cta_per_core = max_cta_per_core;
-   m_max_barriers_per_cta = max_barriers_per_cta;
-   m_warp_size = warp_size;
-   m_shader = shader;
-   if( max_warps_per_core > WARP_PER_CTA_MAX ) {
-      printf("ERROR ** increase WARP_PER_CTA_MAX in shader.h from %u to >= %u or warps per cta in gpgpusim.config\n",
-             WARP_PER_CTA_MAX, max_warps_per_core );
-      exit(1);
-   }
-   if(max_barriers_per_cta > MAX_BARRIERS_PER_CTA){
-	   printf("ERROR ** increase MAX_BARRIERS_PER_CTA in abstract_hardware_model.h from %u to >= %u or barriers per cta in gpgpusim.config\n",
-			   MAX_BARRIERS_PER_CTA, max_barriers_per_cta );
-	   exit(1);
-   }
-   m_warp_active.reset();
-   m_warp_at_barrier.reset();
-   for(unsigned i=0; i<max_barriers_per_cta; i++){
-	   m_bar_id_to_warps[i].reset();
-   }
+  return result;
+}
+
+barrier_set_t::barrier_set_t(shader_core_ctx *shader,
+                             unsigned max_warps_per_core,
+                             unsigned max_cta_per_core,
+                             unsigned max_barriers_per_cta,
+                             unsigned warp_size) {
+  m_max_warps_per_core = max_warps_per_core;
+  m_max_cta_per_core = max_cta_per_core;
+  m_max_barriers_per_cta = max_barriers_per_cta;
+  m_warp_size = warp_size;
+  m_shader = shader;
+  if (max_warps_per_core > WARP_PER_CTA_MAX) {
+    printf(
+        "ERROR ** increase WARP_PER_CTA_MAX in shader.h from %u to >= %u or "
+        "warps per cta in gpgpusim.config\n",
+        WARP_PER_CTA_MAX, max_warps_per_core);
+    exit(1);
+  }
+  if (max_barriers_per_cta > MAX_BARRIERS_PER_CTA) {
+    printf(
+        "ERROR ** increase MAX_BARRIERS_PER_CTA in abstract_hardware_model.h "
+        "from %u to >= %u or barriers per cta in gpgpusim.config\n",
+        MAX_BARRIERS_PER_CTA, max_barriers_per_cta);
+    exit(1);
+  }
+  m_warp_active.reset();
+  m_warp_at_barrier.reset();
+  for (unsigned i = 0; i < max_barriers_per_cta; i++) {
+    m_bar_id_to_warps[i].reset();
+  }
 }
 
 // during cta allocation
-void barrier_set_t::allocate_barrier( unsigned cta_id, warp_set_t warps )
-{
-   assert( cta_id < m_max_cta_per_core );
-   cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
-   assert( w == m_cta_to_warps.end() ); // cta should not already be active or allocated barrier resources
-   m_cta_to_warps[cta_id] = warps;
-   assert( m_cta_to_warps.size() <= m_max_cta_per_core ); // catch cta's that were not properly deallocated
-  
-   m_warp_active |= warps;
-   m_warp_at_barrier &= ~warps;
-   for(unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   m_bar_id_to_warps[i] &=~warps;
-   }
-
+void barrier_set_t::allocate_barrier(unsigned cta_id, warp_set_t warps) {
+  assert(cta_id < m_max_cta_per_core);
+  cta_to_warp_t::iterator w = m_cta_to_warps.find(cta_id);
+  assert(w == m_cta_to_warps.end());  // cta should not already be active or
+                                      // allocated barrier resources
+  m_cta_to_warps[cta_id] = warps;
+  assert(m_cta_to_warps.size() <=
+         m_max_cta_per_core);  // catch cta's that were not properly deallocated
+
+  m_warp_active |= warps;
+  m_warp_at_barrier &= ~warps;
+  for (unsigned i = 0; i < m_max_barriers_per_cta; i++) {
+    m_bar_id_to_warps[i] &= ~warps;
+  }
 }
 
 // during cta deallocation
-void barrier_set_t::deallocate_barrier( unsigned cta_id )
-{
-   cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
-   if( w == m_cta_to_warps.end() )
-      return;
-   warp_set_t warps = w->second;
-   warp_set_t at_barrier = warps & m_warp_at_barrier;
-   assert( at_barrier.any() == false ); // no warps stuck at barrier
-   warp_set_t active = warps & m_warp_active;
-   assert( active.any() == false ); // no warps in CTA still running
-   m_warp_active &= ~warps;
-   m_warp_at_barrier &= ~warps;
-
-   for(unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   warp_set_t at_a_specific_barrier = warps & m_bar_id_to_warps[i];
-	   assert( at_a_specific_barrier.any() == false ); // no warps stuck at barrier
-	   m_bar_id_to_warps[i] &=~warps;
-   }
-   m_cta_to_warps.erase(w);
+void barrier_set_t::deallocate_barrier(unsigned cta_id) {
+  cta_to_warp_t::iterator w = m_cta_to_warps.find(cta_id);
+  if (w == m_cta_to_warps.end()) return;
+  warp_set_t warps = w->second;
+  warp_set_t at_barrier = warps & m_warp_at_barrier;
+  assert(at_barrier.any() == false);  // no warps stuck at barrier
+  warp_set_t active = warps & m_warp_active;
+  assert(active.any() == false);  // no warps in CTA still running
+  m_warp_active &= ~warps;
+  m_warp_at_barrier &= ~warps;
+
+  for (unsigned i = 0; i < m_max_barriers_per_cta; i++) {
+    warp_set_t at_a_specific_barrier = warps & m_bar_id_to_warps[i];
+    assert(at_a_specific_barrier.any() == false);  // no warps stuck at barrier
+    m_bar_id_to_warps[i] &= ~warps;
+  }
+  m_cta_to_warps.erase(w);
 }
 
 // individual warp hits barrier
-void barrier_set_t::warp_reaches_barrier(unsigned cta_id,unsigned warp_id,warp_inst_t* inst)
-{
-	barrier_type bar_type = inst->bar_type;
-	unsigned bar_id = inst->bar_id;
-	unsigned bar_count = inst->bar_count;
-	assert(bar_id!=(unsigned)-1);
-   cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
-
-   if( w == m_cta_to_warps.end() ) { // cta is active
-      printf("ERROR ** cta_id %u not found in barrier set on cycle %llu+%llu...\n", cta_id, gpu_tot_sim_cycle, gpu_sim_cycle );
-      dump();
-      abort();
-   }
-   assert( w->second.test(warp_id) == true ); // warp is in cta
+void barrier_set_t::warp_reaches_barrier(unsigned cta_id, unsigned warp_id,
+                                         warp_inst_t *inst) {
+  barrier_type bar_type = inst->bar_type;
+  unsigned bar_id = inst->bar_id;
+  unsigned bar_count = inst->bar_count;
+  assert(bar_id != (unsigned)-1);
+  cta_to_warp_t::iterator w = m_cta_to_warps.find(cta_id);
+
+  if (w == m_cta_to_warps.end()) {  // cta is active
+    printf(
+        "ERROR ** cta_id %u not found in barrier set on cycle %llu+%llu...\n",
+        cta_id, m_shader->get_gpu()->gpu_tot_sim_cycle,
+        m_shader->get_gpu()->gpu_sim_cycle);
+    dump();
+    abort();
+  }
+  assert(w->second.test(warp_id) == true);  // warp is in cta
 
-   m_bar_id_to_warps[bar_id].set(warp_id);
-   if(bar_type==SYNC || bar_type==RED){
-	   m_warp_at_barrier.set(warp_id);
-   }
-   warp_set_t warps_in_cta = w->second;
-   warp_set_t at_barrier = warps_in_cta & m_bar_id_to_warps[bar_id];
-   warp_set_t active = warps_in_cta & m_warp_active;
-   if(bar_count==(unsigned)-1){
-	   if( at_barrier == active ) {
-		   // all warps have reached barrier, so release waiting warps...
-		   m_bar_id_to_warps[bar_id] &= ~at_barrier;
-		   m_warp_at_barrier &= ~at_barrier;
-		   if(bar_type==RED){
-			   m_shader->broadcast_barrier_reduction(cta_id, bar_id,at_barrier);
-		   }
-	   }
-  }else{
-	  // TODO: check on the hardware if the count should include warp that exited
-	  if ((at_barrier.count() * m_warp_size) == bar_count){
-		   // required number of warps have reached barrier, so release waiting warps...
-		   m_bar_id_to_warps[bar_id] &= ~at_barrier;
-		   m_warp_at_barrier &= ~at_barrier;
-		   if(bar_type==RED){
-			   m_shader->broadcast_barrier_reduction(cta_id, bar_id,at_barrier);
-		   }
-	  }
-  }
-
-
-}
-
-
-// warp reaches exit 
-void barrier_set_t::warp_exit( unsigned warp_id )
-{
-   // caller needs to verify all threads in warp are done, e.g., by checking PDOM stack to 
-   // see it has only one entry during exit_impl()
-   m_warp_active.reset(warp_id);
-
-   // test for barrier release 
-   cta_to_warp_t::iterator w=m_cta_to_warps.begin(); 
-   for (; w != m_cta_to_warps.end(); ++w) {
-      if (w->second.test(warp_id) == true) break; 
-   }
-   warp_set_t warps_in_cta = w->second;
-   warp_set_t active = warps_in_cta & m_warp_active;
-
-   for(unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   warp_set_t at_a_specific_barrier = warps_in_cta & m_bar_id_to_warps[i];
-	   if( at_a_specific_barrier == active ) {
-	      // all warps have reached barrier, so release waiting warps...
-		   m_bar_id_to_warps[i] &= ~at_a_specific_barrier;
-		   m_warp_at_barrier &= ~at_a_specific_barrier;
-	   }
-   }
+  m_bar_id_to_warps[bar_id].set(warp_id);
+  if (bar_type == SYNC || bar_type == RED) {
+    m_warp_at_barrier.set(warp_id);
+  }
+  warp_set_t warps_in_cta = w->second;
+  warp_set_t at_barrier = warps_in_cta & m_bar_id_to_warps[bar_id];
+  warp_set_t active = warps_in_cta & m_warp_active;
+  if (bar_count == (unsigned)-1) {
+    if (at_barrier == active) {
+      // all warps have reached barrier, so release waiting warps...
+      m_bar_id_to_warps[bar_id] &= ~at_barrier;
+      m_warp_at_barrier &= ~at_barrier;
+      if (bar_type == RED) {
+        m_shader->broadcast_barrier_reduction(cta_id, bar_id, at_barrier);
+      }
+    }
+  } else {
+    // TODO: check on the hardware if the count should include warp that exited
+    if ((at_barrier.count() * m_warp_size) == bar_count) {
+      // required number of warps have reached barrier, so release waiting
+      // warps...
+      m_bar_id_to_warps[bar_id] &= ~at_barrier;
+      m_warp_at_barrier &= ~at_barrier;
+      if (bar_type == RED) {
+        m_shader->broadcast_barrier_reduction(cta_id, bar_id, at_barrier);
+      }
+    }
+  }
 }
 
-// assertions
-bool barrier_set_t::warp_waiting_at_barrier( unsigned warp_id ) const
-{ 
-   return m_warp_at_barrier.test(warp_id);
-}
+// warp reaches exit
+void barrier_set_t::warp_exit(unsigned warp_id) {
+  // caller needs to verify all threads in warp are done, e.g., by checking PDOM
+  // stack to see it has only one entry during exit_impl()
+  m_warp_active.reset(warp_id);
 
-void barrier_set_t::dump()
-{
-   printf( "barrier set information\n");
-   printf( "  m_max_cta_per_core = %u\n",  m_max_cta_per_core );
-   printf( "  m_max_warps_per_core = %u\n", m_max_warps_per_core );
-   printf( " m_max_barriers_per_cta =%u\n", m_max_barriers_per_cta);
-   printf( "  cta_to_warps:\n");
-   
-   cta_to_warp_t::const_iterator i;
-   for( i=m_cta_to_warps.begin(); i!=m_cta_to_warps.end(); i++ ) {
-      unsigned cta_id = i->first;
-      warp_set_t warps = i->second;
-      printf("    cta_id %u : %s\n", cta_id, warps.to_string().c_str() );
-   }
-   printf("  warp_active: %s\n", m_warp_active.to_string().c_str() );
-   printf("  warp_at_barrier: %s\n", m_warp_at_barrier.to_string().c_str() );
-   for( unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   warp_set_t warps_reached_barrier = m_bar_id_to_warps[i];
-	   printf("  warp_at_barrier %u: %s\n", i, warps_reached_barrier.to_string().c_str() );
-   }
-   fflush(stdout); 
+  // test for barrier release
+  cta_to_warp_t::iterator w = m_cta_to_warps.begin();
+  for (; w != m_cta_to_warps.end(); ++w) {
+    if (w->second.test(warp_id) == true) break;
+  }
+  warp_set_t warps_in_cta = w->second;
+  warp_set_t active = warps_in_cta & m_warp_active;
+
+  for (unsigned i = 0; i < m_max_barriers_per_cta; i++) {
+    warp_set_t at_a_specific_barrier = warps_in_cta & m_bar_id_to_warps[i];
+    if (at_a_specific_barrier == active) {
+      // all warps have reached barrier, so release waiting warps...
+      m_bar_id_to_warps[i] &= ~at_a_specific_barrier;
+      m_warp_at_barrier &= ~at_a_specific_barrier;
+    }
+  }
 }
 
-void shader_core_ctx::warp_exit( unsigned warp_id )
-{
-	bool done = true;
-	for (	unsigned i = warp_id*get_config()->warp_size;
-			i < (warp_id+1)*get_config()->warp_size;
-			i++ ) {
-
-//		if(this->m_thread[i]->m_functional_model_thread_state && this->m_thread[i].m_functional_model_thread_state->donecycle()==0) {
-//			done = false;
-//		}
-
-
-		if (m_thread[i] && !m_thread[i]->is_done()) done = false;
-	}
-	//if (m_warp[warp_id].get_n_completed() == get_config()->warp_size)
-	//if (this->m_simt_stack[warp_id]->get_num_entries() == 0)
-	if (done)
-		m_barriers.warp_exit( warp_id );
+// assertions
+bool barrier_set_t::warp_waiting_at_barrier(unsigned warp_id) const {
+  return m_warp_at_barrier.test(warp_id);
+}
+
+void barrier_set_t::dump() {
+  printf("barrier set information\n");
+  printf("  m_max_cta_per_core = %u\n", m_max_cta_per_core);
+  printf("  m_max_warps_per_core = %u\n", m_max_warps_per_core);
+  printf(" m_max_barriers_per_cta =%u\n", m_max_barriers_per_cta);
+  printf("  cta_to_warps:\n");
+
+  cta_to_warp_t::const_iterator i;
+  for (i = m_cta_to_warps.begin(); i != m_cta_to_warps.end(); i++) {
+    unsigned cta_id = i->first;
+    warp_set_t warps = i->second;
+    printf("    cta_id %u : %s\n", cta_id, warps.to_string().c_str());
+  }
+  printf("  warp_active: %s\n", m_warp_active.to_string().c_str());
+  printf("  warp_at_barrier: %s\n", m_warp_at_barrier.to_string().c_str());
+  for (unsigned i = 0; i < m_max_barriers_per_cta; i++) {
+    warp_set_t warps_reached_barrier = m_bar_id_to_warps[i];
+    printf("  warp_at_barrier %u: %s\n", i,
+           warps_reached_barrier.to_string().c_str());
+  }
+  fflush(stdout);
 }
 
-bool shader_core_ctx::check_if_non_released_reduction_barrier(warp_inst_t &inst)
-{
-	unsigned warp_id = inst.warp_id();
-	bool bar_red_op = (inst.op == BARRIER_OP) && (inst.bar_type == RED);
-    bool non_released_barrier_reduction = false;
-    bool warp_stucked_at_barrier = warp_waiting_at_barrier(warp_id);
-    bool single_inst_in_pipeline = (m_warp[warp_id].num_issued_inst_in_pipeline()==1);
-    non_released_barrier_reduction = single_inst_in_pipeline and warp_stucked_at_barrier and bar_red_op;
-    printf("non_released_barrier_reduction=%u\n",non_released_barrier_reduction);
-    return non_released_barrier_reduction;
-}
+void shader_core_ctx::warp_exit(unsigned warp_id) {
+  bool done = true;
+  for (unsigned i = warp_id * get_config()->warp_size;
+       i < (warp_id + 1) * get_config()->warp_size; i++) {
+    //		if(this->m_thread[i]->m_functional_model_thread_state &&
+    // this->m_thread[i].m_functional_model_thread_state->donecycle()==0) {
+    // done = false;
+    //		}
 
-bool shader_core_ctx::warp_waiting_at_barrier( unsigned warp_id ) const
-{
-   return m_barriers.warp_waiting_at_barrier(warp_id);
+    if (m_thread[i] && !m_thread[i]->is_done()) done = false;
+  }
+  // if (m_warp[warp_id].get_n_completed() == get_config()->warp_size)
+  // if (this->m_simt_stack[warp_id]->get_num_entries() == 0)
+  if (done) m_barriers.warp_exit(warp_id);
+}
+
+bool shader_core_ctx::check_if_non_released_reduction_barrier(
+    warp_inst_t &inst) {
+  unsigned warp_id = inst.warp_id();
+  bool bar_red_op = (inst.op == BARRIER_OP) && (inst.bar_type == RED);
+  bool non_released_barrier_reduction = false;
+  bool warp_stucked_at_barrier = warp_waiting_at_barrier(warp_id);
+  bool single_inst_in_pipeline =
+      (m_warp[warp_id]->num_issued_inst_in_pipeline() == 1);
+  non_released_barrier_reduction =
+      single_inst_in_pipeline and warp_stucked_at_barrier and bar_red_op;
+  printf("non_released_barrier_reduction=%u\n", non_released_barrier_reduction);
+  return non_released_barrier_reduction;
+}
+
+bool shader_core_ctx::warp_waiting_at_barrier(unsigned warp_id) const {
+  return m_barriers.warp_waiting_at_barrier(warp_id);
+}
+
+bool shader_core_ctx::warp_waiting_at_mem_barrier(unsigned warp_id) {
+  if (!m_warp[warp_id]->get_membar()) return false;
+  if (!m_scoreboard->pendingWrites(warp_id)) {
+    m_warp[warp_id]->clear_membar();
+    if (m_gpu->get_config().flush_l1()) {
+      // Mahmoud fixed this on Nov 2019
+      // Invalidate L1 cache
+      // Based on Nvidia Doc, at MEM barrier, we have to
+      //(1) wait for all pending writes till they are acked
+      //(2) invalidate L1 cache to ensure coherence and avoid reading stall data
+      cache_invalidate();
+      // TO DO: you need to stall the SM for 5k cycles.
+    }
+    return false;
+  }
+  return true;
 }
 
-bool shader_core_ctx::warp_waiting_at_mem_barrier( unsigned warp_id ) 
-{
-   if( !m_warp[warp_id].get_membar() ) 
-      return false;
-   if( !m_scoreboard->pendingWrites(warp_id) ) {
-      m_warp[warp_id].clear_membar();
-      return false;
-   }
-   return true;
+void shader_core_ctx::set_max_cta(const kernel_info_t &kernel) {
+  // calculate the max cta count and cta size for local memory address mapping
+  kernel_max_cta_per_shader = m_config->max_cta(kernel);
+  unsigned int gpu_cta_size = kernel.threads_per_cta();
+  kernel_padded_threads_per_cta =
+      (gpu_cta_size % m_config->warp_size)
+          ? m_config->warp_size * ((gpu_cta_size / m_config->warp_size) + 1)
+          : gpu_cta_size;
 }
 
-void shader_core_ctx::set_max_cta( const kernel_info_t &kernel ) 
-{
-    // calculate the max cta count and cta size for local memory address mapping
-    kernel_max_cta_per_shader = m_config->max_cta(kernel);
-    unsigned int gpu_cta_size = kernel.threads_per_cta();
-    kernel_padded_threads_per_cta = (gpu_cta_size%m_config->warp_size) ? 
-        m_config->warp_size*((gpu_cta_size/m_config->warp_size)+1) : 
-        gpu_cta_size;
+void shader_core_ctx::decrement_atomic_count(unsigned wid, unsigned n) {
+  assert(m_warp[wid]->get_n_atomic() >= n);
+  m_warp[wid]->dec_n_atomic(n);
 }
 
-void shader_core_ctx::decrement_atomic_count( unsigned wid, unsigned n )
-{
-   assert( m_warp[wid].get_n_atomic() >= n );
-   m_warp[wid].dec_n_atomic(n);
+void shader_core_ctx::broadcast_barrier_reduction(unsigned cta_id,
+                                                  unsigned bar_id,
+                                                  warp_set_t warps) {
+  for (unsigned i = 0; i < m_config->max_warps_per_shader; i++) {
+    if (warps.test(i)) {
+      const warp_inst_t *inst =
+          m_warp[i]->restore_info_of_last_inst_at_barrier();
+      const_cast<warp_inst_t *>(inst)->broadcast_barrier_reduction(
+          inst->get_active_mask());
+    }
+  }
 }
 
-void shader_core_ctx::broadcast_barrier_reduction(unsigned cta_id,unsigned bar_id,warp_set_t warps)
-{
-	for(unsigned i=0; i<m_config->max_warps_per_shader;i++){
-		if(warps.test(i)){
-			const warp_inst_t * inst = m_warp[i].restore_info_of_last_inst_at_barrier();
-			const_cast<warp_inst_t *> (inst)->broadcast_barrier_reduction(inst->get_active_mask());
-		}
-	}
-}
+bool shader_core_ctx::fetch_unit_response_buffer_full() const { return false; }
 
-bool shader_core_ctx::fetch_unit_response_buffer_full() const
-{
-    return false;
+void shader_core_ctx::accept_fetch_response(mem_fetch *mf) {
+  mf->set_status(IN_SHADER_FETCHED,
+                 m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+  m_L1I->fill(mf, m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
 }
 
-void shader_core_ctx::accept_fetch_response( mem_fetch *mf )
-{
-    mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
-    m_L1I->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
+bool shader_core_ctx::ldst_unit_response_buffer_full() const {
+  return m_ldst_unit->response_buffer_full();
 }
 
-bool shader_core_ctx::ldst_unit_response_buffer_full() const
-{
-    return m_ldst_unit->response_buffer_full();
+void shader_core_ctx::accept_ldst_unit_response(mem_fetch *mf) {
+  m_ldst_unit->fill(mf);
 }
 
-void shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) 
-{
-   m_ldst_unit->fill(mf);
-}
-
-void shader_core_ctx::store_ack( class mem_fetch *mf )
-{
-	assert( mf->get_type() == WRITE_ACK  || ( m_config->gpgpu_perfect_mem && mf->get_is_write() ) );
-    unsigned warp_id = mf->get_wid();
-    m_warp[warp_id].dec_store_req();
+void shader_core_ctx::store_ack(class mem_fetch *mf) {
+  assert(mf->get_type() == WRITE_ACK ||
+         (m_config->gpgpu_perfect_mem && mf->get_is_write()));
+  unsigned warp_id = mf->get_wid();
+  m_warp[warp_id]->dec_store_req();
 }
 
-void shader_core_ctx::print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) {
-   m_ldst_unit->print_cache_stats( fp, dl1_accesses, dl1_misses );
+void shader_core_ctx::print_cache_stats(FILE *fp, unsigned &dl1_accesses,
+                                        unsigned &dl1_misses) {
+  m_ldst_unit->print_cache_stats(fp, dl1_accesses, dl1_misses);
 }
 
-void shader_core_ctx::get_cache_stats(cache_stats &cs){
-    // Adds stats from each cache to 'cs'
-    cs += m_L1I->get_stats(); // Get L1I stats
-    m_ldst_unit->get_cache_stats(cs); // Get L1D, L1C, L1T stats
+void shader_core_ctx::get_cache_stats(cache_stats &cs) {
+  // Adds stats from each cache to 'cs'
+  cs += m_L1I->get_stats();          // Get L1I stats
+  m_ldst_unit->get_cache_stats(cs);  // Get L1D, L1C, L1T stats
 }
 
-void shader_core_ctx::get_L1I_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1I)
-        m_L1I->get_sub_stats(css);
+void shader_core_ctx::get_L1I_sub_stats(struct cache_sub_stats &css) const {
+  if (m_L1I) m_L1I->get_sub_stats(css);
 }
-void shader_core_ctx::get_L1D_sub_stats(struct cache_sub_stats &css) const{
-    m_ldst_unit->get_L1D_sub_stats(css);
+void shader_core_ctx::get_L1D_sub_stats(struct cache_sub_stats &css) const {
+  m_ldst_unit->get_L1D_sub_stats(css);
 }
-void shader_core_ctx::get_L1C_sub_stats(struct cache_sub_stats &css) const{
-    m_ldst_unit->get_L1C_sub_stats(css);
+void shader_core_ctx::get_L1C_sub_stats(struct cache_sub_stats &css) const {
+  m_ldst_unit->get_L1C_sub_stats(css);
 }
-void shader_core_ctx::get_L1T_sub_stats(struct cache_sub_stats &css) const{
-    m_ldst_unit->get_L1T_sub_stats(css);
+void shader_core_ctx::get_L1T_sub_stats(struct cache_sub_stats &css) const {
+  m_ldst_unit->get_L1T_sub_stats(css);
 }
 
-void shader_core_ctx::get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const{
-	n_simt_to_mem += m_stats->n_simt_to_mem[m_sid];
-	n_mem_to_simt += m_stats->n_mem_to_simt[m_sid];
+void shader_core_ctx::get_icnt_power_stats(long &n_simt_to_mem,
+                                           long &n_mem_to_simt) const {
+  n_simt_to_mem += m_stats->n_simt_to_mem[m_sid];
+  n_mem_to_simt += m_stats->n_mem_to_simt[m_sid];
 }
 
-bool shd_warp_t::functional_done() const
-{
-    return get_n_completed() == m_warp_size;
+bool shd_warp_t::functional_done() const {
+  return get_n_completed() == m_warp_size;
 }
 
-bool shd_warp_t::hardware_done() const
-{
-    return functional_done() && stores_done() && !inst_in_pipeline(); 
+bool shd_warp_t::hardware_done() const {
+  return functional_done() && stores_done() && !inst_in_pipeline();
 }
 
-bool shd_warp_t::waiting() 
-{
-    if ( functional_done() ) {
-        // waiting to be initialized with a kernel
-        return true;
-    } else if ( m_shader->warp_waiting_at_barrier(m_warp_id) ) {
-        // waiting for other warps in CTA to reach barrier
-        return true;
-    } else if ( m_shader->warp_waiting_at_mem_barrier(m_warp_id) ) {
-        // waiting for memory barrier
-        return true;
-    } else if ( m_n_atomic >0 ) {
-        // waiting for atomic operation to complete at memory:
-        // this stall is not required for accurate timing model, but rather we
-        // stall here since if a call/return instruction occurs in the meantime
-        // the functional execution of the atomic when it hits DRAM can cause
-        // the wrong register to be read.
-        return true;
+bool shd_warp_t::waiting() {
+  if (functional_done()) {
+    // waiting to be initialized with a kernel
+    return true;
+  } else if (m_shader->warp_waiting_at_barrier(m_warp_id)) {
+    // waiting for other warps in CTA to reach barrier
+    return true;
+  } else if (m_shader->warp_waiting_at_mem_barrier(m_warp_id)) {
+    // waiting for memory barrier
+    return true;
+  } else if (m_n_atomic > 0) {
+    // waiting for atomic operation to complete at memory:
+    // this stall is not required for accurate timing model, but rather we
+    // stall here since if a call/return instruction occurs in the meantime
+    // the functional execution of the atomic when it hits DRAM can cause
+    // the wrong register to be read.
+    return true;
+  }
+  return false;
+}
+
+void shd_warp_t::print(FILE *fout) const {
+  if (!done_exit()) {
+    fprintf(fout, "w%02u npc: 0x%04x, done:%c%c%c%c:%2u i:%u s:%u a:%u (done: ",
+            m_warp_id, m_next_pc, (functional_done() ? 'f' : ' '),
+            (stores_done() ? 's' : ' '), (inst_in_pipeline() ? ' ' : 'i'),
+            (done_exit() ? 'e' : ' '), n_completed, m_inst_in_pipeline,
+            m_stores_outstanding, m_n_atomic);
+    for (unsigned i = m_warp_id * m_warp_size;
+         i < (m_warp_id + 1) * m_warp_size; i++) {
+      if (m_shader->ptx_thread_done(i))
+        fprintf(fout, "1");
+      else
+        fprintf(fout, "0");
+      if ((((i + 1) % 4) == 0) && (i + 1) < (m_warp_id + 1) * m_warp_size)
+        fprintf(fout, ",");
     }
-    return false;
+    fprintf(fout, ") ");
+    fprintf(fout, " active=%s", m_active_threads.to_string().c_str());
+    fprintf(fout, " last fetched @ %5llu", m_last_fetch);
+    if (m_imiss_pending) fprintf(fout, " i-miss pending");
+    fprintf(fout, "\n");
+  }
 }
 
-void shd_warp_t::print( FILE *fout ) const
-{
-    if( !done_exit() ) {
-        fprintf( fout, "w%02u npc: 0x%04x, done:%c%c%c%c:%2u i:%u s:%u a:%u (done: ", 
-                m_warp_id,
-                m_next_pc,
-                (functional_done()?'f':' '),
-                (stores_done()?'s':' '),
-                (inst_in_pipeline()?' ':'i'),
-                (done_exit()?'e':' '),
-                n_completed,
-                m_inst_in_pipeline, 
-                m_stores_outstanding,
-                m_n_atomic );
-        for (unsigned i = m_warp_id*m_warp_size; i < (m_warp_id+1)*m_warp_size; i++ ) {
-          if ( m_shader->ptx_thread_done(i) ) fprintf(fout,"1");
-          else fprintf(fout,"0");
-          if ( (((i+1)%4) == 0) && (i+1) < (m_warp_id+1)*m_warp_size ) 
-             fprintf(fout,",");
-        }
-        fprintf(fout,") ");
-        fprintf(fout," active=%s", m_active_threads.to_string().c_str() );
-        fprintf(fout," last fetched @ %5llu", m_last_fetch);
-        if( m_imiss_pending ) 
-            fprintf(fout," i-miss pending");
-        fprintf(fout,"\n");
-    }
+void shd_warp_t::print_ibuffer(FILE *fout) const {
+  fprintf(fout, "  ibuffer[%2u] : ", m_warp_id);
+  for (unsigned i = 0; i < IBUFFER_SIZE; i++) {
+    const inst_t *inst = m_ibuffer[i].m_inst;
+    if (inst)
+      inst->print_insn(fout);
+    else if (m_ibuffer[i].m_valid)
+      fprintf(fout, " <invalid instruction> ");
+    else
+      fprintf(fout, " <empty> ");
+  }
+  fprintf(fout, "\n");
 }
 
-void shd_warp_t::print_ibuffer( FILE *fout ) const
-{
-    fprintf(fout,"  ibuffer[%2u] : ", m_warp_id );
-    for( unsigned i=0; i < IBUFFER_SIZE; i++) {
-        const inst_t *inst = m_ibuffer[i].m_inst;
-        if( inst ) inst->print_insn(fout);
-        else if( m_ibuffer[i].m_valid ) 
-           fprintf(fout," <invalid instruction> ");
-        else fprintf(fout," <empty> ");
-    }
-    fprintf(fout,"\n");
+void opndcoll_rfu_t::add_cu_set(unsigned set_id, unsigned num_cu,
+                                unsigned num_dispatch) {
+  m_cus[set_id].reserve(num_cu);  // this is necessary to stop pointers in m_cu
+                                  // from being invalid do to a resize;
+  for (unsigned i = 0; i < num_cu; i++) {
+    m_cus[set_id].push_back(collector_unit_t());
+    m_cu.push_back(&m_cus[set_id].back());
+  }
+  // for now each collector set gets dedicated dispatch units.
+  for (unsigned i = 0; i < num_dispatch; i++) {
+    m_dispatch_units.push_back(dispatch_unit_t(&m_cus[set_id]));
+  }
 }
 
-void opndcoll_rfu_t::add_cu_set(unsigned set_id, unsigned num_cu, unsigned num_dispatch){
-    m_cus[set_id].reserve(num_cu); //this is necessary to stop pointers in m_cu from being invalid do to a resize;
-    for (unsigned i = 0; i < num_cu; i++) {
-        m_cus[set_id].push_back(collector_unit_t());
-        m_cu.push_back(&m_cus[set_id].back());
+void opndcoll_rfu_t::add_port(port_vector_t &input, port_vector_t &output,
+                              uint_vector_t cu_sets) {
+  // m_num_ports++;
+  // m_num_collectors += num_collector_units;
+  // m_input.resize(m_num_ports);
+  // m_output.resize(m_num_ports);
+  // m_num_collector_units.resize(m_num_ports);
+  // m_input[m_num_ports-1]=input_port;
+  // m_output[m_num_ports-1]=output_port;
+  // m_num_collector_units[m_num_ports-1]=num_collector_units;
+  m_in_ports.push_back(input_port_t(input, output, cu_sets));
+}
+
+void opndcoll_rfu_t::init(unsigned num_banks, shader_core_ctx *shader) {
+  m_shader = shader;
+  m_arbiter.init(m_cu.size(), num_banks);
+  // for( unsigned n=0; n<m_num_ports;n++ )
+  //    m_dispatch_units[m_output[n]].init( m_num_collector_units[n] );
+  m_num_banks = num_banks;
+  m_bank_warp_shift = 0;
+  m_warp_size = shader->get_config()->warp_size;
+  m_bank_warp_shift = (unsigned)(int)(log(m_warp_size + 0.5) / log(2.0));
+  assert((m_bank_warp_shift == 5) || (m_warp_size != 32));
+
+  sub_core_model = shader->get_config()->sub_core_model;
+  m_num_warp_scheds = shader->get_config()->gpgpu_num_sched_per_core;
+  unsigned reg_id;
+  if (sub_core_model) {
+    assert(num_banks % shader->get_config()->gpgpu_num_sched_per_core == 0);
+    assert(m_num_warp_scheds <= m_cu.size() &&
+           m_cu.size() % m_num_warp_scheds == 0);
+  }
+  m_num_banks_per_sched =
+      num_banks / shader->get_config()->gpgpu_num_sched_per_core;
+
+  for (unsigned j = 0; j < m_cu.size(); j++) {
+    if (sub_core_model) {
+      unsigned cusPerSched = m_cu.size() / m_num_warp_scheds;
+      reg_id = j / cusPerSched;
     }
-    // for now each collector set gets dedicated dispatch units.
-    for (unsigned i = 0; i < num_dispatch; i++) {
-        m_dispatch_units.push_back(dispatch_unit_t(&m_cus[set_id]));
+    m_cu[j]->init(j, num_banks, m_bank_warp_shift, shader->get_config(), this,
+                  sub_core_model, reg_id, m_num_banks_per_sched);
+  }
+  for (unsigned j = 0; j < m_dispatch_units.size(); j++) {
+    m_dispatch_units[j].init(sub_core_model,m_num_warp_scheds);
+  }
+  m_initialized = true;
+}
+
+int register_bank(int regnum, int wid, unsigned num_banks,
+                  unsigned bank_warp_shift, bool sub_core_model,
+                  unsigned banks_per_sched, unsigned sched_id) {
+  int bank = regnum;
+  if (bank_warp_shift) bank += wid;
+  if (sub_core_model) {
+    unsigned bank_num = (bank % banks_per_sched) + (sched_id * banks_per_sched);
+    assert(bank_num < num_banks);
+    return bank_num;
+  } else
+    return bank % num_banks;
+}
+
+bool opndcoll_rfu_t::writeback(warp_inst_t &inst) {
+  assert(!inst.empty());
+  std::list<unsigned> regs = m_shader->get_regs_written(inst);
+  for (unsigned op = 0; op < MAX_REG_OPERANDS; op++) {
+    int reg_num = inst.arch_reg.dst[op];  // this math needs to match that used
+                                          // in function_info::ptx_decode_inst
+    if (reg_num >= 0) {                   // valid register
+      unsigned bank = register_bank(reg_num, inst.warp_id(), m_num_banks,
+                                    m_bank_warp_shift, sub_core_model,
+                                    m_num_banks_per_sched, inst.get_schd_id());
+      if (m_arbiter.bank_idle(bank)) {
+        m_arbiter.allocate_bank_for_write(
+            bank,
+            op_t(&inst, reg_num, m_num_banks, m_bank_warp_shift, sub_core_model,
+                 m_num_banks_per_sched, inst.get_schd_id()));
+        inst.arch_reg.dst[op] = -1;
+      } else {
+        return false;
+      }
     }
-}
-
-
-void opndcoll_rfu_t::add_port(port_vector_t & input, port_vector_t & output, uint_vector_t cu_sets)
-{
-    //m_num_ports++;
-    //m_num_collectors += num_collector_units;
-    //m_input.resize(m_num_ports);
-    //m_output.resize(m_num_ports);
-    //m_num_collector_units.resize(m_num_ports);
-    //m_input[m_num_ports-1]=input_port;
-    //m_output[m_num_ports-1]=output_port;
-    //m_num_collector_units[m_num_ports-1]=num_collector_units;
-    m_in_ports.push_back(input_port_t(input,output,cu_sets));
-}
-
-void opndcoll_rfu_t::init( unsigned num_banks, shader_core_ctx *shader )
-{
-   m_shader=shader;
-   m_arbiter.init(m_cu.size(),num_banks);
-   //for( unsigned n=0; n<m_num_ports;n++ ) 
-   //    m_dispatch_units[m_output[n]].init( m_num_collector_units[n] );
-   m_num_banks = num_banks;
-   m_bank_warp_shift = 0; 
-   m_warp_size = shader->get_config()->warp_size;
-   m_bank_warp_shift = (unsigned)(int) (log(m_warp_size+0.5) / log(2.0));
-   assert( (m_bank_warp_shift == 5) || (m_warp_size != 32) );
-
-   sub_core_model = shader->get_config()->sub_core_model;
-   m_num_warp_sceds = shader->get_config()->gpgpu_num_sched_per_core;
-   if(sub_core_model)
-	   assert(num_banks % shader->get_config()->gpgpu_num_sched_per_core == 0);
-   m_num_banks_per_sched = num_banks / shader->get_config()->gpgpu_num_sched_per_core;
-
-   for( unsigned j=0; j<m_cu.size(); j++) {
-       m_cu[j]->init(j,num_banks,m_bank_warp_shift,shader->get_config(),this, sub_core_model, m_num_banks_per_sched );
-   }
-   m_initialized=true;
-
-
-
-
-}
-
-int register_bank(int regnum, int wid, unsigned num_banks, unsigned bank_warp_shift, bool sub_core_model, unsigned banks_per_sched, unsigned sched_id)
-{
-   int bank = regnum;
-   if (bank_warp_shift)
-      bank += wid;
-   if(sub_core_model) {
-	   unsigned bank_num  = (bank % banks_per_sched) + (sched_id * banks_per_sched);
-	   assert(bank_num < num_banks);
-	   return bank_num;
-   }
-   else
-	   return bank % num_banks;
-}
-
-bool opndcoll_rfu_t::writeback( warp_inst_t &inst )
-{
-   assert( !inst.empty() );
-   std::list<unsigned> regs = m_shader->get_regs_written(inst);
-   for( unsigned op=0; op < MAX_REG_OPERANDS; op++ ) {
-      int reg_num = inst.arch_reg.dst[op]; // this math needs to match that used in function_info::ptx_decode_inst
-      if( reg_num >= 0 ){ // valid register
-         unsigned bank = register_bank(reg_num,inst.warp_id(),m_num_banks,m_bank_warp_shift, sub_core_model, m_num_banks_per_sched, inst.get_schd_id());
-         if( m_arbiter.bank_idle(bank) ) {
-             m_arbiter.allocate_bank_for_write(bank,op_t(&inst,reg_num,m_num_banks,m_bank_warp_shift, sub_core_model, m_num_banks_per_sched, inst.get_schd_id()));
-             inst.arch_reg.dst[op] = -1;
-         } else {
-             return false;
-         }
+  }
+  for (unsigned i = 0; i < (unsigned)regs.size(); i++) {
+    if (m_shader->get_config()->gpgpu_clock_gated_reg_file) {
+      unsigned active_count = 0;
+      for (unsigned i = 0; i < m_shader->get_config()->warp_size;
+           i = i + m_shader->get_config()->n_regfile_gating_group) {
+        for (unsigned j = 0; j < m_shader->get_config()->n_regfile_gating_group;
+             j++) {
+          if (inst.get_active_mask().test(i + j)) {
+            active_count += m_shader->get_config()->n_regfile_gating_group;
+            break;
+          }
+        }
       }
-   }
-   for(unsigned i=0;i<(unsigned)regs.size();i++){
-	      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
-	    	  unsigned active_count=0;
-	    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
-	    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
-	    			  if(inst.get_active_mask().test(i+j)){
-	    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
-	    				  break;
-	    			  }
-	    		  }
-	    	  }
-	    	  m_shader->incregfile_writes(active_count);
-	      }else{
-	    	  m_shader->incregfile_writes(m_shader->get_config()->warp_size);//inst.active_count());
-	      }
-   }
-   return true;
-}
-
-void opndcoll_rfu_t::dispatch_ready_cu()
-{
-   for( unsigned p=0; p < m_dispatch_units.size(); ++p ) {
-      dispatch_unit_t &du = m_dispatch_units[p];
-      collector_unit_t *cu = du.find_ready();
-      if( cu ) {
-    	 for(unsigned i=0;i<(cu->get_num_operands()-cu->get_num_regs());i++){
-   	      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
-   	    	  unsigned active_count=0;
-   	    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
-   	    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
-   	    			  if(cu->get_active_mask().test(i+j)){
-   	    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
-   	    				  break;
-   	    			  }
-   	    		  }
-   	    	  }
-   	    	  m_shader->incnon_rf_operands(active_count);
-   	      }else{
-    		 m_shader->incnon_rf_operands(m_shader->get_config()->warp_size);//cu->get_active_count());
-   	      }
-    	}
-         cu->dispatch();
+      m_shader->incregfile_writes(active_count);
+    } else {
+      m_shader->incregfile_writes(
+          m_shader->get_config()->warp_size);  // inst.active_count());
+    }
+  }
+  return true;
+}
+
+void opndcoll_rfu_t::dispatch_ready_cu() {
+  for (unsigned p = 0; p < m_dispatch_units.size(); ++p) {
+    dispatch_unit_t &du = m_dispatch_units[p];
+    collector_unit_t *cu = du.find_ready();
+    if (cu) {
+      for (unsigned i = 0; i < (cu->get_num_operands() - cu->get_num_regs());
+           i++) {
+        if (m_shader->get_config()->gpgpu_clock_gated_reg_file) {
+          unsigned active_count = 0;
+          for (unsigned i = 0; i < m_shader->get_config()->warp_size;
+               i = i + m_shader->get_config()->n_regfile_gating_group) {
+            for (unsigned j = 0;
+                 j < m_shader->get_config()->n_regfile_gating_group; j++) {
+              if (cu->get_active_mask().test(i + j)) {
+                active_count += m_shader->get_config()->n_regfile_gating_group;
+                break;
+              }
+            }
+          }
+          m_shader->incnon_rf_operands(active_count);
+        } else {
+          m_shader->incnon_rf_operands(
+              m_shader->get_config()->warp_size);  // cu->get_active_count());
+        }
       }
-   }
+      cu->dispatch();
+    }
+  }
 }
 
-void opndcoll_rfu_t::allocate_cu( unsigned port_num )
-{
-   input_port_t& inp = m_in_ports[port_num];
-   for (unsigned i = 0; i < inp.m_in.size(); i++) {
-       if( (*inp.m_in[i]).has_ready() ) {
-          //find a free cu 
-          for (unsigned j = 0; j < inp.m_cu_sets.size(); j++) {
-              std::vector<collector_unit_t> & cu_set = m_cus[inp.m_cu_sets[j]];
-	      bool allocated = false;
-              for (unsigned k = 0; k < cu_set.size(); k++) {
-                  if(cu_set[k].is_free()) {
-                     collector_unit_t *cu = &cu_set[k];
-                     allocated = cu->allocate(inp.m_in[i],inp.m_out[i]);
-                     m_arbiter.add_read_requests(cu);
-                     break;
-                  }
-              }
-              if (allocated) break; //cu has been allocated, no need to search more.
+void opndcoll_rfu_t::allocate_cu(unsigned port_num) {
+  input_port_t &inp = m_in_ports[port_num];
+  for (unsigned i = 0; i < inp.m_in.size(); i++) {
+    if ((*inp.m_in[i]).has_ready()) {
+      // find a free cu
+      for (unsigned j = 0; j < inp.m_cu_sets.size(); j++) {
+        std::vector<collector_unit_t> &cu_set = m_cus[inp.m_cu_sets[j]];
+        bool allocated = false;
+        unsigned cuLowerBound = 0;
+        unsigned cuUpperBound = cu_set.size();
+        unsigned schd_id;
+        if (sub_core_model) {
+          // Sub core model only allocates on the subset of CUs assigned to the
+          // scheduler that issued
+          unsigned reg_id = (*inp.m_in[i]).get_ready_reg_id();
+          schd_id = (*inp.m_in[i]).get_schd_id(reg_id);
+          assert(cu_set.size() % m_num_warp_scheds == 0 &&
+                 cu_set.size() >= m_num_warp_scheds);
+          unsigned cusPerSched = cu_set.size() / m_num_warp_scheds;
+          cuLowerBound = schd_id * cusPerSched;
+          cuUpperBound = cuLowerBound + cusPerSched;
+          assert(0 <= cuLowerBound && cuUpperBound <= cu_set.size());
+        }
+        for (unsigned k = cuLowerBound; k < cuUpperBound; k++) {
+          if (cu_set[k].is_free()) {
+            collector_unit_t *cu = &cu_set[k];
+            allocated = cu->allocate(inp.m_in[i], inp.m_out[i]);
+            m_arbiter.add_read_requests(cu);
+            break;
           }
-          break; // can only service a single input, if it failed it will fail for others.
-       }
-   }
+        }
+        if (allocated) break;  // cu has been allocated, no need to search more.
+      }
+      // break;  // can only service a single input, if it failed it will fail
+      // for
+      // others.
+    }
+  }
 }
 
-void opndcoll_rfu_t::allocate_reads()
-{
-   // process read requests that do not have conflicts
-   std::list<op_t> allocated = m_arbiter.allocate_reads();
-   std::map<unsigned,op_t> read_ops;
-   for( std::list<op_t>::iterator r=allocated.begin(); r!=allocated.end(); r++ ) {
-      const op_t &rr = *r;
-      unsigned reg = rr.get_reg();
-      unsigned wid = rr.get_wid();
-      unsigned bank = register_bank(reg,wid,m_num_banks,m_bank_warp_shift,sub_core_model, m_num_banks_per_sched, rr.get_sid());
-      m_arbiter.allocate_for_read(bank,rr);
-      read_ops[bank] = rr;
-   }
-   std::map<unsigned,op_t>::iterator r;
-   for(r=read_ops.begin();r!=read_ops.end();++r ) {
-      op_t &op = r->second;
-      unsigned cu = op.get_oc_id();
-      unsigned operand = op.get_operand();
-      m_cu[cu]->collect_operand(operand);
-      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
-    	  unsigned active_count=0;
-    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
-    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
-    			  if(op.get_active_mask().test(i+j)){
-    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
-    				  break;
-    			  }
-    		  }
-    	  }
-    	  m_shader->incregfile_reads(active_count);
-      }else{
-    	  m_shader->incregfile_reads(m_shader->get_config()->warp_size);//op.get_active_count());
+void opndcoll_rfu_t::allocate_reads() {
+  // process read requests that do not have conflicts
+  std::list<op_t> allocated = m_arbiter.allocate_reads();
+  std::map<unsigned, op_t> read_ops;
+  for (std::list<op_t>::iterator r = allocated.begin(); r != allocated.end();
+       r++) {
+    const op_t &rr = *r;
+    unsigned reg = rr.get_reg();
+    unsigned wid = rr.get_wid();
+    unsigned bank =
+        register_bank(reg, wid, m_num_banks, m_bank_warp_shift, sub_core_model,
+                      m_num_banks_per_sched, rr.get_sid());
+    m_arbiter.allocate_for_read(bank, rr);
+    read_ops[bank] = rr;
+  }
+  std::map<unsigned, op_t>::iterator r;
+  for (r = read_ops.begin(); r != read_ops.end(); ++r) {
+    op_t &op = r->second;
+    unsigned cu = op.get_oc_id();
+    unsigned operand = op.get_operand();
+    m_cu[cu]->collect_operand(operand);
+    if (m_shader->get_config()->gpgpu_clock_gated_reg_file) {
+      unsigned active_count = 0;
+      for (unsigned i = 0; i < m_shader->get_config()->warp_size;
+           i = i + m_shader->get_config()->n_regfile_gating_group) {
+        for (unsigned j = 0; j < m_shader->get_config()->n_regfile_gating_group;
+             j++) {
+          if (op.get_active_mask().test(i + j)) {
+            active_count += m_shader->get_config()->n_regfile_gating_group;
+            break;
+          }
+        }
       }
+      m_shader->incregfile_reads(active_count);
+    } else {
+      m_shader->incregfile_reads(
+          m_shader->get_config()->warp_size);  // op.get_active_count());
+    }
   }
-} 
-
-bool opndcoll_rfu_t::collector_unit_t::ready() const 
-{ 
-   return (!m_free) && m_not_ready.none() && (*m_output_register).has_free(); 
 }
 
-void opndcoll_rfu_t::collector_unit_t::dump(FILE *fp, const shader_core_ctx *shader ) const
-{
-   if( m_free ) {
-      fprintf(fp,"    <free>\n");
-   } else {
-      m_warp->print(fp);
-      for( unsigned i=0; i < MAX_REG_OPERANDS*2; i++ ) {
-         if( m_not_ready.test(i) ) {
-            std::string r = m_src_op[i].get_reg_string();
-            fprintf(fp,"    '%s' not ready\n", r.c_str() );
-         }
-      }
-   }
+bool opndcoll_rfu_t::collector_unit_t::ready() const {
+  return (!m_free) && m_not_ready.none() &&
+         (*m_output_register).has_free(m_sub_core_model, m_reg_id);
 }
 
-void opndcoll_rfu_t::collector_unit_t::init( unsigned n, 
-                                             unsigned num_banks, 
-                                             unsigned log2_warp_size,
-                                             const core_config *config,
-                                             opndcoll_rfu_t *rfu,
-											 bool sub_core_model,
-											 unsigned banks_per_sched)
-{ 
-   m_rfu=rfu;
-   m_cuid=n; 
-   m_num_banks=num_banks;
-   assert(m_warp==NULL); 
-   m_warp = new warp_inst_t(config);
-   m_bank_warp_shift=log2_warp_size;
-   m_sub_core_model = sub_core_model;
-   m_num_banks_per_sched = banks_per_sched;
-}
-
-bool opndcoll_rfu_t::collector_unit_t::allocate( register_set* pipeline_reg_set, register_set* output_reg_set ) 
-{
-   assert(m_free);
-   assert(m_not_ready.none());
-   m_free = false;
-   m_output_register = output_reg_set;
-   warp_inst_t **pipeline_reg = pipeline_reg_set->get_ready();
-   if( (pipeline_reg) and !((*pipeline_reg)->empty()) ) {
-      m_warp_id = (*pipeline_reg)->warp_id();
-      for( unsigned op=0; op < MAX_REG_OPERANDS; op++ ) {
-         int reg_num = (*pipeline_reg)->arch_reg.src[op]; // this math needs to match that used in function_info::ptx_decode_inst
-         if( reg_num >= 0 ) { // valid register
-            m_src_op[op] = op_t( this, op, reg_num, m_num_banks, m_bank_warp_shift, m_sub_core_model, m_num_banks_per_sched, (*pipeline_reg)->get_schd_id() );
-            m_not_ready.set(op);
-         } else 
-            m_src_op[op] = op_t();
+void opndcoll_rfu_t::collector_unit_t::dump(
+    FILE *fp, const shader_core_ctx *shader) const {
+  if (m_free) {
+    fprintf(fp, "    <free>\n");
+  } else {
+    m_warp->print(fp);
+    for (unsigned i = 0; i < MAX_REG_OPERANDS * 2; i++) {
+      if (m_not_ready.test(i)) {
+        std::string r = m_src_op[i].get_reg_string();
+        fprintf(fp, "    '%s' not ready\n", r.c_str());
       }
-      //move_warp(m_warp,*pipeline_reg);
-      pipeline_reg_set->move_out_to(m_warp);
-      return true;
-   }
-   return false;
+    }
+  }
 }
 
-void opndcoll_rfu_t::collector_unit_t::dispatch()
-{
-   assert( m_not_ready.none() );
-   //move_warp(*m_output_register,m_warp);
-   m_output_register->move_in(m_warp);
-   m_free=true;
-   m_output_register = NULL;
-   for( unsigned i=0; i<MAX_REG_OPERANDS*2;i++)
-      m_src_op[i].reset();
-}
-
-simt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu, 
-                                      unsigned cluster_id, 
-                                      const struct shader_core_config *config, 
-                                      const struct memory_config *mem_config,
-                                      shader_core_stats *stats, 
-                                      class memory_stats_t *mstats )
-{
-    m_config = config;
-    m_cta_issue_next_core=m_config->n_simt_cores_per_cluster-1; // this causes first launch to use hw cta 0
-    m_cluster_id=cluster_id;
-    m_gpu = gpu;
-    m_stats = stats;
-    m_memory_stats = mstats;
-    m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];
-    for( unsigned i=0; i < config->n_simt_cores_per_cluster; i++ ) {
-        unsigned sid = m_config->cid_to_sid(i,m_cluster_id);
-        m_core[i] = new shader_core_ctx(gpu,this,sid,m_cluster_id,config,mem_config,stats);
-        m_core_sim_order.push_back(i); 
+void opndcoll_rfu_t::collector_unit_t::init(
+    unsigned n, unsigned num_banks, unsigned log2_warp_size,
+    const core_config *config, opndcoll_rfu_t *rfu, bool sub_core_model,
+    unsigned reg_id, unsigned banks_per_sched) {
+  m_rfu = rfu;
+  m_cuid = n;
+  m_num_banks = num_banks;
+  assert(m_warp == NULL);
+  m_warp = new warp_inst_t(config);
+  m_bank_warp_shift = log2_warp_size;
+  m_sub_core_model = sub_core_model;
+  m_reg_id = reg_id;
+  m_num_banks_per_sched = banks_per_sched;
+}
+
+bool opndcoll_rfu_t::collector_unit_t::allocate(register_set *pipeline_reg_set,
+                                                register_set *output_reg_set) {
+  assert(m_free);
+  assert(m_not_ready.none());
+  m_free = false;
+  m_output_register = output_reg_set;
+  warp_inst_t **pipeline_reg = pipeline_reg_set->get_ready();
+  if ((pipeline_reg) and !((*pipeline_reg)->empty())) {
+    m_warp_id = (*pipeline_reg)->warp_id();
+    for (unsigned op = 0; op < MAX_REG_OPERANDS; op++) {
+      int reg_num =
+          (*pipeline_reg)
+              ->arch_reg.src[op];  // this math needs to match that used in
+                                   // function_info::ptx_decode_inst
+      if (reg_num >= 0) {          // valid register
+        m_src_op[op] = op_t(this, op, reg_num, m_num_banks, m_bank_warp_shift,
+                            m_sub_core_model, m_num_banks_per_sched,
+                            (*pipeline_reg)->get_schd_id());
+        m_not_ready.set(op);
+      } else
+        m_src_op[op] = op_t();
     }
+    // move_warp(m_warp,*pipeline_reg);
+    pipeline_reg_set->move_out_to(m_warp);
+    return true;
+  }
+  return false;
 }
 
-void simt_core_cluster::core_cycle()
-{
-    for( std::list<unsigned>::iterator it = m_core_sim_order.begin(); it != m_core_sim_order.end(); ++it ) {
-        m_core[*it]->cycle();
-    }
+void opndcoll_rfu_t::collector_unit_t::dispatch() {
+  assert(m_not_ready.none());
+  m_output_register->move_in(m_sub_core_model, m_reg_id, m_warp);
+  m_free = true;
+  m_output_register = NULL;
+  for (unsigned i = 0; i < MAX_REG_OPERANDS * 2; i++) m_src_op[i].reset();
+}
 
-    if (m_config->simt_core_sim_order == 1) {
-        m_core_sim_order.splice(m_core_sim_order.end(), m_core_sim_order, m_core_sim_order.begin()); 
-    }
+void exec_simt_core_cluster::create_shader_core_ctx() {
+  m_core = new shader_core_ctx *[m_config->n_simt_cores_per_cluster];
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++) {
+    unsigned sid = m_config->cid_to_sid(i, m_cluster_id);
+    m_core[i] = new exec_shader_core_ctx(m_gpu, this, sid, m_cluster_id,
+                                         m_config, m_mem_config, m_stats);
+    m_core_sim_order.push_back(i);
+  }
 }
 
-void simt_core_cluster::reinit()
-{
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
-        m_core[i]->reinit(0,m_config->n_thread_per_shader,true);
+simt_core_cluster::simt_core_cluster(class gpgpu_sim *gpu, unsigned cluster_id,
+                                     const shader_core_config *config,
+                                     const memory_config *mem_config,
+                                     shader_core_stats *stats,
+                                     class memory_stats_t *mstats) {
+  m_config = config;
+  m_cta_issue_next_core = m_config->n_simt_cores_per_cluster -
+                          1;  // this causes first launch to use hw cta 0
+  m_cluster_id = cluster_id;
+  m_gpu = gpu;
+  m_stats = stats;
+  m_memory_stats = mstats;
+  m_mem_config = mem_config;
+}
+
+void simt_core_cluster::core_cycle() {
+  for (std::list<unsigned>::iterator it = m_core_sim_order.begin();
+       it != m_core_sim_order.end(); ++it) {
+    m_core[*it]->cycle();
+  }
+
+  if (m_config->simt_core_sim_order == 1) {
+    m_core_sim_order.splice(m_core_sim_order.end(), m_core_sim_order,
+                            m_core_sim_order.begin());
+  }
 }
 
-unsigned simt_core_cluster::max_cta( const kernel_info_t &kernel )
-{
-    return m_config->n_simt_cores_per_cluster * m_config->max_cta(kernel);
+void simt_core_cluster::reinit() {
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++)
+    m_core[i]->reinit(0, m_config->n_thread_per_shader, true);
 }
 
-unsigned simt_core_cluster::get_not_completed() const
-{
-    unsigned not_completed=0;
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
-        not_completed += m_core[i]->get_not_completed();
-    return not_completed;
+unsigned simt_core_cluster::max_cta(const kernel_info_t &kernel) {
+  return m_config->n_simt_cores_per_cluster * m_config->max_cta(kernel);
 }
 
-void simt_core_cluster::print_not_completed( FILE *fp ) const
-{
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {
-        unsigned not_completed=m_core[i]->get_not_completed();
-        unsigned sid=m_config->cid_to_sid(i,m_cluster_id);
-        fprintf(fp,"%u(%u) ", sid, not_completed );
-    }
+unsigned simt_core_cluster::get_not_completed() const {
+  unsigned not_completed = 0;
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++)
+    not_completed += m_core[i]->get_not_completed();
+  return not_completed;
 }
 
+void simt_core_cluster::print_not_completed(FILE *fp) const {
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++) {
+    unsigned not_completed = m_core[i]->get_not_completed();
+    unsigned sid = m_config->cid_to_sid(i, m_cluster_id);
+    fprintf(fp, "%u(%u) ", sid, not_completed);
+  }
+}
 
-float simt_core_cluster::get_current_occupancy( unsigned long long& active, unsigned long long& total ) const {
-    float aggregate = 0.f;
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {
-        aggregate+=m_core[i]->get_current_occupancy( active, total );
-    }
-    return aggregate / m_config->n_simt_cores_per_cluster;
+float simt_core_cluster::get_current_occupancy(
+    unsigned long long &active, unsigned long long &total) const {
+  float aggregate = 0.f;
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++) {
+    aggregate += m_core[i]->get_current_occupancy(active, total);
+  }
+  return aggregate / m_config->n_simt_cores_per_cluster;
 }
 
-unsigned simt_core_cluster::get_n_active_cta() const
-{
-    unsigned n=0;
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
-        n += m_core[i]->get_n_active_cta();
-    return n;
+unsigned simt_core_cluster::get_n_active_cta() const {
+  unsigned n = 0;
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++)
+    n += m_core[i]->get_n_active_cta();
+  return n;
 }
 
-unsigned simt_core_cluster::get_n_active_sms() const
-{
-    unsigned n=0;
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ )
-        n += m_core[i]->isactive();
-    return n;
+unsigned simt_core_cluster::get_n_active_sms() const {
+  unsigned n = 0;
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++)
+    n += m_core[i]->isactive();
+  return n;
 }
 
-unsigned simt_core_cluster::issue_block2core()
-{
-    unsigned num_blocks_issued=0;
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {
-        unsigned core = (i+m_cta_issue_next_core+1)%m_config->n_simt_cores_per_cluster;
-
-        kernel_info_t * kernel;
-         //Jin: fetch kernel according to concurrent kernel setting
-        if(m_config->gpgpu_concurrent_kernel_sm) {//concurrent kernel on sm 
-            //always select latest issued kernel
-            kernel_info_t *k = m_gpu->select_kernel();
-            kernel = k;
-        }
-        else {
-            //first select core kernel, if no more cta, get a new kernel
-            //only when core completes
-            kernel = m_core[core]->get_kernel();
-            if( !m_gpu->kernel_more_cta_left(kernel) ) {
-              //wait till current kernel finishes
-              if(m_core[core]->get_not_completed() == 0)
-              {
-                  kernel_info_t *k = m_gpu->select_kernel();
-                  if( k ) 
-                      m_core[core]->set_kernel(k);
-                  kernel = k;
-              }
-            }
-        }
+unsigned simt_core_cluster::issue_block2core() {
+  unsigned num_blocks_issued = 0;
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++) {
+    unsigned core =
+        (i + m_cta_issue_next_core + 1) % m_config->n_simt_cores_per_cluster;
 
-        if( m_gpu->kernel_more_cta_left(kernel) && 
-//            (m_core[core]->get_n_active_cta() < m_config->max_cta(*kernel)) ) {
-            m_core[core]->can_issue_1block(*kernel)) {
-            m_core[core]->issue_block2core(*kernel);
-            num_blocks_issued++;
-            m_cta_issue_next_core=core; 
-            break;
+    kernel_info_t *kernel;
+    // Jin: fetch kernel according to concurrent kernel setting
+    if (m_config->gpgpu_concurrent_kernel_sm) {  // concurrent kernel on sm
+      // always select latest issued kernel
+      kernel_info_t *k = m_gpu->select_kernel();
+      kernel = k;
+    } else {
+      // first select core kernel, if no more cta, get a new kernel
+      // only when core completes
+      kernel = m_core[core]->get_kernel();
+      if (!m_gpu->kernel_more_cta_left(kernel)) {
+        // wait till current kernel finishes
+        if (m_core[core]->get_not_completed() == 0) {
+          kernel_info_t *k = m_gpu->select_kernel();
+          if (k) m_core[core]->set_kernel(k);
+          kernel = k;
         }
+      }
     }
-    return num_blocks_issued;
-}
 
-void simt_core_cluster::cache_flush()
-{
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
-        m_core[i]->cache_flush();
+    if (m_gpu->kernel_more_cta_left(kernel) &&
+        //            (m_core[core]->get_n_active_cta() <
+        //            m_config->max_cta(*kernel)) ) {
+        m_core[core]->can_issue_1block(*kernel)) {
+      m_core[core]->issue_block2core(*kernel);
+      num_blocks_issued++;
+      m_cta_issue_next_core = core;
+      break;
+    }
+  }
+  return num_blocks_issued;
 }
 
-void simt_core_cluster::cache_invalidate()
-{
-    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ )
-        m_core[i]->cache_invalidate();
+void simt_core_cluster::cache_flush() {
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++)
+    m_core[i]->cache_flush();
 }
 
-bool simt_core_cluster::icnt_injection_buffer_full(unsigned size, bool write)
-{
-    unsigned request_size = size;
-    if (!write) 
-        request_size = READ_PACKET_SIZE;
-    return ! ::icnt_has_buffer(m_cluster_id, request_size);
+void simt_core_cluster::cache_invalidate() {
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++)
+    m_core[i]->cache_invalidate();
 }
 
-void simt_core_cluster::icnt_inject_request_packet(class mem_fetch *mf)
-{
-    // stats
-    if (mf->get_is_write()) m_stats->made_write_mfs++;
-    else m_stats->made_read_mfs++;
-    switch (mf->get_access_type()) {
-    case CONST_ACC_R: m_stats->gpgpu_n_mem_const++; break;
-    case TEXTURE_ACC_R: m_stats->gpgpu_n_mem_texture++; break;
-    case GLOBAL_ACC_R: m_stats->gpgpu_n_mem_read_global++; break;
-    //case GLOBAL_ACC_R: m_stats->gpgpu_n_mem_read_global++; printf("read_global%d\n",m_stats->gpgpu_n_mem_read_global); break;
-    case GLOBAL_ACC_W: m_stats->gpgpu_n_mem_write_global++; break;
-    case LOCAL_ACC_R: m_stats->gpgpu_n_mem_read_local++; break;
-    case LOCAL_ACC_W: m_stats->gpgpu_n_mem_write_local++; break;
-    case INST_ACC_R: m_stats->gpgpu_n_mem_read_inst++; break;
-    case L1_WRBK_ACC: m_stats->gpgpu_n_mem_write_global++; break;
-    case L2_WRBK_ACC: m_stats->gpgpu_n_mem_l2_writeback++; break;
-    case L1_WR_ALLOC_R: m_stats->gpgpu_n_mem_l1_write_allocate++; break;
-    case L2_WR_ALLOC_R: m_stats->gpgpu_n_mem_l2_write_allocate++; break;
-    default: assert(0);
-    }
-
-   // The packet size varies depending on the type of request: 
-   // - For write request and atomic request, the packet contains the data 
-   // - For read request (i.e. not write nor atomic), the packet only has control metadata
-   unsigned int packet_size = mf->size(); 
-   if (!mf->get_is_write() && !mf->isatomic()) {
-      packet_size = mf->get_ctrl_size(); 
-   }
-   m_stats->m_outgoing_traffic_stats->record_traffic(mf, packet_size); 
-   unsigned destination = mf->get_sub_partition_id();
-   mf->set_status(IN_ICNT_TO_MEM,gpu_sim_cycle+gpu_tot_sim_cycle);
-   if (!mf->get_is_write() && !mf->isatomic())
-      ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->get_ctrl_size() );
-   else 
-      ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->size());
+bool simt_core_cluster::icnt_injection_buffer_full(unsigned size, bool write) {
+  unsigned request_size = size;
+  if (!write) request_size = READ_PACKET_SIZE;
+  return !::icnt_has_buffer(m_cluster_id, request_size);
 }
 
-void simt_core_cluster::icnt_cycle()
-{
-    if( !m_response_fifo.empty() ) {
-        mem_fetch *mf = m_response_fifo.front();
-        unsigned cid = m_config->sid_to_cid(mf->get_sid());
-        if( mf->get_access_type() == INST_ACC_R ) {
-            // instruction fetch response
-            if( !m_core[cid]->fetch_unit_response_buffer_full() ) {
-                m_response_fifo.pop_front();
-                m_core[cid]->accept_fetch_response(mf);
-            }
-        } else {
-            // data response
-            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {
-                m_response_fifo.pop_front();
-                m_memory_stats->memlatstat_read_done(mf);
-                m_core[cid]->accept_ldst_unit_response(mf);
-            }
-        }
-    }
-    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {
-        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);
-        if (!mf) 
-            return;
-        assert(mf->get_tpc() == m_cluster_id);
-        assert(mf->get_type() == READ_REPLY || mf->get_type() == WRITE_ACK );
-
-        // The packet size varies depending on the type of request: 
-        // - For read request and atomic request, the packet contains the data 
-        // - For write-ack, the packet only has control metadata
-        unsigned int packet_size = (mf->get_is_write())? mf->get_ctrl_size() : mf->size(); 
-        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); 
-        mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-        //m_memory_stats->memlatstat_read_done(mf,m_shader_config->max_warps_per_shader);
-        m_response_fifo.push_back(mf);
-        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
+void simt_core_cluster::icnt_inject_request_packet(class mem_fetch *mf) {
+  // stats
+  if (mf->get_is_write())
+    m_stats->made_write_mfs++;
+  else
+    m_stats->made_read_mfs++;
+  switch (mf->get_access_type()) {
+    case CONST_ACC_R:
+      m_stats->gpgpu_n_mem_const++;
+      break;
+    case TEXTURE_ACC_R:
+      m_stats->gpgpu_n_mem_texture++;
+      break;
+    case GLOBAL_ACC_R:
+      m_stats->gpgpu_n_mem_read_global++;
+      break;
+    // case GLOBAL_ACC_R: m_stats->gpgpu_n_mem_read_global++;
+    // printf("read_global%d\n",m_stats->gpgpu_n_mem_read_global); break;
+    case GLOBAL_ACC_W:
+      m_stats->gpgpu_n_mem_write_global++;
+      break;
+    case LOCAL_ACC_R:
+      m_stats->gpgpu_n_mem_read_local++;
+      break;
+    case LOCAL_ACC_W:
+      m_stats->gpgpu_n_mem_write_local++;
+      break;
+    case INST_ACC_R:
+      m_stats->gpgpu_n_mem_read_inst++;
+      break;
+    case L1_WRBK_ACC:
+      m_stats->gpgpu_n_mem_write_global++;
+      break;
+    case L2_WRBK_ACC:
+      m_stats->gpgpu_n_mem_l2_writeback++;
+      break;
+    case L1_WR_ALLOC_R:
+      m_stats->gpgpu_n_mem_l1_write_allocate++;
+      break;
+    case L2_WR_ALLOC_R:
+      m_stats->gpgpu_n_mem_l2_write_allocate++;
+      break;
+    default:
+      assert(0);
+  }
+
+  // The packet size varies depending on the type of request:
+  // - For write request and atomic request, the packet contains the data
+  // - For read request (i.e. not write nor atomic), the packet only has control
+  // metadata
+  unsigned int packet_size = mf->size();
+  if (!mf->get_is_write() && !mf->isatomic()) {
+    packet_size = mf->get_ctrl_size();
+  }
+  m_stats->m_outgoing_traffic_stats->record_traffic(mf, packet_size);
+  unsigned destination = mf->get_sub_partition_id();
+  mf->set_status(IN_ICNT_TO_MEM,
+                 m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+  if (!mf->get_is_write() && !mf->isatomic())
+    ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void *)mf,
+                mf->get_ctrl_size());
+  else
+    ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void *)mf,
+                mf->size());
+}
+
+void simt_core_cluster::icnt_cycle() {
+  if (!m_response_fifo.empty()) {
+    mem_fetch *mf = m_response_fifo.front();
+    unsigned cid = m_config->sid_to_cid(mf->get_sid());
+    if (mf->get_access_type() == INST_ACC_R) {
+      // instruction fetch response
+      if (!m_core[cid]->fetch_unit_response_buffer_full()) {
+        m_response_fifo.pop_front();
+        m_core[cid]->accept_fetch_response(mf);
+      }
+    } else {
+      // data response
+      if (!m_core[cid]->ldst_unit_response_buffer_full()) {
+        m_response_fifo.pop_front();
+        m_memory_stats->memlatstat_read_done(mf);
+        m_core[cid]->accept_ldst_unit_response(mf);
+      }
     }
+  }
+  if (m_response_fifo.size() < m_config->n_simt_ejection_buffer_size) {
+    mem_fetch *mf = (mem_fetch *)::icnt_pop(m_cluster_id);
+    if (!mf) return;
+    assert(mf->get_tpc() == m_cluster_id);
+    assert(mf->get_type() == READ_REPLY || mf->get_type() == WRITE_ACK);
+
+    // The packet size varies depending on the type of request:
+    // - For read request and atomic request, the packet contains the data
+    // - For write-ack, the packet only has control metadata
+    unsigned int packet_size =
+        (mf->get_is_write()) ? mf->get_ctrl_size() : mf->size();
+    m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size);
+    mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,
+                   m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle);
+    // m_memory_stats->memlatstat_read_done(mf,m_shader_config->max_warps_per_shader);
+    m_response_fifo.push_back(mf);
+    m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
+  }
 }
 
-void simt_core_cluster::get_pdom_stack_top_info( unsigned sid, unsigned tid, unsigned *pc, unsigned *rpc ) const
-{
-    unsigned cid = m_config->sid_to_cid(sid);
-    m_core[cid]->get_pdom_stack_top_info(tid,pc,rpc);
+void simt_core_cluster::get_pdom_stack_top_info(unsigned sid, unsigned tid,
+                                                unsigned *pc,
+                                                unsigned *rpc) const {
+  unsigned cid = m_config->sid_to_cid(sid);
+  m_core[cid]->get_pdom_stack_top_info(tid, pc, rpc);
 }
 
-void simt_core_cluster::display_pipeline( unsigned sid, FILE *fout, int print_mem, int mask )
-{
-    m_core[m_config->sid_to_cid(sid)]->display_pipeline(fout,print_mem,mask);
-
-    fprintf(fout,"\n");
-    fprintf(fout,"Cluster %u pipeline state\n", m_cluster_id );
-    fprintf(fout,"Response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
-    for( std::list<mem_fetch*>::const_iterator i=m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
-        const mem_fetch *mf = *i;
-        mf->print(fout);
-    }
-}
+void simt_core_cluster::display_pipeline(unsigned sid, FILE *fout,
+                                         int print_mem, int mask) {
+  m_core[m_config->sid_to_cid(sid)]->display_pipeline(fout, print_mem, mask);
 
-void simt_core_cluster::print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) const {
-   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-      m_core[ i ]->print_cache_stats( fp, dl1_accesses, dl1_misses );
-   }
+  fprintf(fout, "\n");
+  fprintf(fout, "Cluster %u pipeline state\n", m_cluster_id);
+  fprintf(fout, "Response FIFO (occupancy = %zu):\n", m_response_fifo.size());
+  for (std::list<mem_fetch *>::const_iterator i = m_response_fifo.begin();
+       i != m_response_fifo.end(); i++) {
+    const mem_fetch *mf = *i;
+    mf->print(fout);
+  }
 }
 
-void simt_core_cluster::get_icnt_stats(long &n_simt_to_mem, long &n_mem_to_simt) const {
-	long simt_to_mem=0;
-	long mem_to_simt=0;
-	for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-		m_core[i]->get_icnt_power_stats(simt_to_mem, mem_to_simt);
-	}
-	n_simt_to_mem = simt_to_mem;
-	n_mem_to_simt = mem_to_simt;
+void simt_core_cluster::print_cache_stats(FILE *fp, unsigned &dl1_accesses,
+                                          unsigned &dl1_misses) const {
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i) {
+    m_core[i]->print_cache_stats(fp, dl1_accesses, dl1_misses);
+  }
 }
 
-void simt_core_cluster::get_cache_stats(cache_stats &cs) const{
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_cache_stats(cs);
-    }
+void simt_core_cluster::get_icnt_stats(long &n_simt_to_mem,
+                                       long &n_mem_to_simt) const {
+  long simt_to_mem = 0;
+  long mem_to_simt = 0;
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i) {
+    m_core[i]->get_icnt_power_stats(simt_to_mem, mem_to_simt);
+  }
+  n_simt_to_mem = simt_to_mem;
+  n_mem_to_simt = mem_to_simt;
 }
 
-void simt_core_cluster::get_L1I_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1I_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
-}
-void simt_core_cluster::get_L1D_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1D_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
-}
-void simt_core_cluster::get_L1C_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1C_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
-}
-void simt_core_cluster::get_L1T_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1T_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
+void simt_core_cluster::get_cache_stats(cache_stats &cs) const {
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i) {
+    m_core[i]->get_cache_stats(cs);
+  }
 }
 
-void shader_core_ctx::checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t, unsigned tid)
-{
-    if(inst.isatomic())
-           m_warp[inst.warp_id()].inc_n_atomic();
-        if (inst.space.is_local() && (inst.is_load() || inst.is_store())) {
-            new_addr_type localaddrs[MAX_ACCESSES_PER_INSN_PER_THREAD];
-            unsigned num_addrs;
-            num_addrs = translate_local_memaddr(inst.get_addr(t), tid, m_config->n_simt_clusters*m_config->n_simt_cores_per_cluster,
-                   inst.data_size, (new_addr_type*) localaddrs );
-            inst.set_addr(t, (new_addr_type*) localaddrs, num_addrs);
-        }
-        if ( ptx_thread_done(tid) ) {
-            m_warp[inst.warp_id()].set_completed(t);
-            m_warp[inst.warp_id()].ibuffer_flush();
-        }
+void simt_core_cluster::get_L1I_sub_stats(struct cache_sub_stats &css) const {
+  struct cache_sub_stats temp_css;
+  struct cache_sub_stats total_css;
+  temp_css.clear();
+  total_css.clear();
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i) {
+    m_core[i]->get_L1I_sub_stats(temp_css);
+    total_css += temp_css;
+  }
+  css = total_css;
+}
+void simt_core_cluster::get_L1D_sub_stats(struct cache_sub_stats &css) const {
+  struct cache_sub_stats temp_css;
+  struct cache_sub_stats total_css;
+  temp_css.clear();
+  total_css.clear();
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i) {
+    m_core[i]->get_L1D_sub_stats(temp_css);
+    total_css += temp_css;
+  }
+  css = total_css;
+}
+void simt_core_cluster::get_L1C_sub_stats(struct cache_sub_stats &css) const {
+  struct cache_sub_stats temp_css;
+  struct cache_sub_stats total_css;
+  temp_css.clear();
+  total_css.clear();
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i) {
+    m_core[i]->get_L1C_sub_stats(temp_css);
+    total_css += temp_css;
+  }
+  css = total_css;
+}
+void simt_core_cluster::get_L1T_sub_stats(struct cache_sub_stats &css) const {
+  struct cache_sub_stats temp_css;
+  struct cache_sub_stats total_css;
+  temp_css.clear();
+  total_css.clear();
+  for (unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i) {
+    m_core[i]->get_L1T_sub_stats(temp_css);
+    total_css += temp_css;
+  }
+  css = total_css;
+}
+
+void exec_shader_core_ctx::checkExecutionStatusAndUpdate(warp_inst_t &inst,
+                                                         unsigned t,
+                                                         unsigned tid) {
+  if (inst.isatomic()) m_warp[inst.warp_id()]->inc_n_atomic();
+  if (inst.space.is_local() && (inst.is_load() || inst.is_store())) {
+    new_addr_type localaddrs[MAX_ACCESSES_PER_INSN_PER_THREAD];
+    unsigned num_addrs;
+    num_addrs = translate_local_memaddr(
+        inst.get_addr(t), tid,
+        m_config->n_simt_clusters * m_config->n_simt_cores_per_cluster,
+        inst.data_size, (new_addr_type *)localaddrs);
+    inst.set_addr(t, (new_addr_type *)localaddrs, num_addrs);
+  }
+  if (ptx_thread_done(tid)) {
+    m_warp[inst.warp_id()]->set_completed(t);
+    m_warp[inst.warp_id()]->ibuffer_flush();
+  }
 
-    // PC-Histogram Update 
-    unsigned warp_id = inst.warp_id(); 
-    unsigned pc = inst.pc; 
-    for (unsigned t = 0; t < m_config->warp_size; t++) {
-        if (inst.active(t)) {
-            int tid = warp_id * m_config->warp_size + t; 
-            cflog_update_thread_pc(m_sid, tid, pc);  
-        }
+  // PC-Histogram Update
+  unsigned warp_id = inst.warp_id();
+  unsigned pc = inst.pc;
+  for (unsigned t = 0; t < m_config->warp_size; t++) {
+    if (inst.active(t)) {
+      int tid = warp_id * m_config->warp_size + t;
+      cflog_update_thread_pc(m_sid, tid, pc);
     }
+  }
 }
-
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.h
index 84a3d56c1f..3ea7f60ad5 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader.h
@@ -1,1106 +1,1177 @@
-// Copyright (c) 2009-2011, Tor M. Aamodt, Wilson W.L. Fung, Andrew Turner,
-// Ali Bakhoda 
-// The University of British Columbia
+// Copyright (c) 2009-2021, Tor M. Aamodt, Wilson W.L. Fung, Andrew Turner,
+// Ali Bakhoda, Vijay Kandiah, Nikos Hardavellas
+// The University of British Columbia, Northwestern University
 // All rights reserved.
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
 //
-// Redistributions of source code must retain the above copyright notice, this
-// list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
+// 1. Redistributions of source code must retain the above copyright notice, this
+//    list of conditions and the following disclaimer;
+// 2. Redistributions in binary form must reproduce the above copyright notice,
+//    this list of conditions and the following disclaimer in the documentation
+//    and/or other materials provided with the distribution;
+// 3. Neither the names of The University of British Columbia, Northwestern 
+//    University nor the names of their contributors may be used to
+//    endorse or promote products derived from this software without specific
+//    prior written permission.
 //
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
 
 #ifndef SHADER_H
 #define SHADER_H
 
+#include <assert.h>
+#include <math.h>
 #include <stdio.h>
 #include <stdlib.h>
-#include <math.h>
-#include <assert.h>
+#include <algorithm>
+#include <bitset>
+#include <deque>
+#include <list>
 #include <map>
 #include <set>
-#include <vector>
-#include <list>
-#include <bitset>
 #include <utility>
-#include <algorithm>
-#include <deque>
+#include <vector>
 
 //#include "../cuda-sim/ptx.tab.h"
 
+#include "../abstract_hardware_model.h"
 #include "delayqueue.h"
-#include "stack.h"
 #include "dram.h"
-#include "../abstract_hardware_model.h"
-#include "scoreboard.h"
+#include "gpu-cache.h"
 #include "mem_fetch.h"
+#include "scoreboard.h"
+#include "stack.h"
 #include "stats.h"
-#include "gpu-cache.h"
 #include "traffic_breakdown.h"
 
-
-#define NO_OP_FLAG            0xFF
+#define NO_OP_FLAG 0xFF
 
 /* READ_PACKET_SIZE:
-   bytes: 6 address (flit can specify chanel so this gives up to ~2GB/channel, so good for now),
-          2 bytes   [shaderid + mshrid](14 bits) + req_size(0-2 bits if req_size variable) - so up to 2^14 = 16384 mshr total 
+   bytes: 6 address (flit can specify chanel so this gives up to ~2GB/channel,
+   so good for now), 2 bytes   [shaderid + mshrid](14 bits) + req_size(0-2 bits
+   if req_size variable) - so up to 2^14 = 16384 mshr total
  */
 
 #define READ_PACKET_SIZE 8
 
-//WRITE_PACKET_SIZE: bytes: 6 address, 2 miscelaneous. 
+// WRITE_PACKET_SIZE: bytes: 6 address, 2 miscelaneous.
 #define WRITE_PACKET_SIZE 8
 
 #define WRITE_MASK_SIZE 8
 
-enum exec_unit_type_t
-{
+class gpgpu_context;
+
+enum exec_unit_type_t {
   NONE = 0,
   SP = 1,
   SFU = 2,
   MEM = 3,
   DP = 4,
   INT = 5,
-  TENSOR = 6
+  TENSOR = 6,
+  SPECIALIZED = 7
 };
 
 class thread_ctx_t {
-public:
-   unsigned m_cta_id; // hardware CTA this thread belongs
+ public:
+  unsigned m_cta_id;  // hardware CTA this thread belongs
 
-   // per thread stats (ac stands for accumulative).
-   unsigned n_insn;
-   unsigned n_insn_ac;
-   unsigned n_l1_mis_ac;
-   unsigned n_l1_mrghit_ac;
-   unsigned n_l1_access_ac; 
+  // per thread stats (ac stands for accumulative).
+  unsigned n_insn;
+  unsigned n_insn_ac;
+  unsigned n_l1_mis_ac;
+  unsigned n_l1_mrghit_ac;
+  unsigned n_l1_access_ac;
 
-   bool m_active; 
+  bool m_active;
 };
 
 class shd_warp_t {
-public:
-    shd_warp_t( class shader_core_ctx *shader, unsigned warp_size) 
-        : m_shader(shader), m_warp_size(warp_size)
-    {
-        m_stores_outstanding=0;
-        m_inst_in_pipeline=0;
-        reset(); 
-    }
-    void reset()
-    {
-        assert( m_stores_outstanding==0);
-        assert( m_inst_in_pipeline==0);
-        m_imiss_pending=false;
-        m_warp_id=(unsigned)-1;
-        m_dynamic_warp_id = (unsigned)-1;
-        n_completed = m_warp_size; 
-        m_n_atomic=0;
-        m_membar=false;
-        m_done_exit=true;
-        m_last_fetch=0;
-        m_next=0;
-        m_inst_at_barrier=NULL;
-
-        //Jin: cdp support
-        m_cdp_latency = 0;
-        m_cdp_dummy = false;
-    }
-    void init( address_type start_pc,
-               unsigned cta_id,
-               unsigned wid,
-               const std::bitset<MAX_WARP_SIZE> &active,
-               unsigned dynamic_warp_id )
-    {
-        m_cta_id=cta_id;
-        m_warp_id=wid;
-        m_dynamic_warp_id=dynamic_warp_id;
-        m_next_pc=start_pc;
-        assert( n_completed >= active.count() );
-        assert( n_completed <= m_warp_size);
-        n_completed   -= active.count(); // active threads are not yet completed
-        m_active_threads = active;
-        m_done_exit=false;
-
-        //Jin: cdp support
-        m_cdp_latency = 0;
-        m_cdp_dummy = false;
-    }
-
-    bool functional_done() const;
-    bool waiting(); // not const due to membar
-    bool hardware_done() const;
-
-    bool done_exit() const { return m_done_exit; }
-    void set_done_exit() { m_done_exit=true; }
-
-    void print( FILE *fout ) const;
-    void print_ibuffer( FILE *fout ) const;
-
-    unsigned get_n_completed() const { return n_completed; }
-    void set_completed( unsigned lane ) 
-    { 
-        assert( m_active_threads.test(lane) );
-        m_active_threads.reset(lane);
-        n_completed++; 
-    }
-
-    void set_last_fetch( unsigned long long sim_cycle ) { m_last_fetch=sim_cycle; }
-
-    unsigned get_n_atomic() const { return m_n_atomic; }
-    void inc_n_atomic() { m_n_atomic++; }
-    void dec_n_atomic(unsigned n) { m_n_atomic-=n; }
-
-    void set_membar() { m_membar=true; }
-    void clear_membar() { m_membar=false; }
-    bool get_membar() const { return m_membar; }
-    address_type get_pc() const { return m_next_pc; }
-    void set_next_pc( address_type pc ) { m_next_pc = pc; }
-
-    void store_info_of_last_inst_at_barrier(const warp_inst_t *pI){ m_inst_at_barrier = pI;}
-    const warp_inst_t * restore_info_of_last_inst_at_barrier(){ return m_inst_at_barrier;}
-
-    void ibuffer_fill( unsigned slot, const warp_inst_t *pI )
-    {
-       assert(slot < IBUFFER_SIZE );
-       m_ibuffer[slot].m_inst=pI;
-       m_ibuffer[slot].m_valid=true;
-       m_next=0; 
-    }
-    bool ibuffer_empty() const
-    {
-        for( unsigned i=0; i < IBUFFER_SIZE; i++) 
-            if(m_ibuffer[i].m_valid) 
-                return false;
-        return true;
-    }
-    void ibuffer_flush()
-    {
-        for(unsigned i=0;i<IBUFFER_SIZE;i++) {
-            if( m_ibuffer[i].m_valid )
-                dec_inst_in_pipeline();
-            m_ibuffer[i].m_inst=NULL; 
-            m_ibuffer[i].m_valid=false; 
-        }
-    }
-    const warp_inst_t *ibuffer_next_inst() { return m_ibuffer[m_next].m_inst; }
-    bool ibuffer_next_valid() { return m_ibuffer[m_next].m_valid; }
-    void ibuffer_free()
-    {
-        m_ibuffer[m_next].m_inst = NULL;
-        m_ibuffer[m_next].m_valid = false;
+ public:
+  shd_warp_t(class shader_core_ctx *shader, unsigned warp_size)
+      : m_shader(shader), m_warp_size(warp_size) {
+    m_stores_outstanding = 0;
+    m_inst_in_pipeline = 0;
+    reset();
+  }
+  void reset() {
+    assert(m_stores_outstanding == 0);
+    assert(m_inst_in_pipeline == 0);
+    m_imiss_pending = false;
+    m_warp_id = (unsigned)-1;
+    m_dynamic_warp_id = (unsigned)-1;
+    n_completed = m_warp_size;
+    m_n_atomic = 0;
+    m_membar = false;
+    m_done_exit = true;
+    m_last_fetch = 0;
+    m_next = 0;
+
+    // Jin: cdp support
+    m_cdp_latency = 0;
+    m_cdp_dummy = false;
+  }
+  void init(address_type start_pc, unsigned cta_id, unsigned wid,
+            const std::bitset<MAX_WARP_SIZE> &active,
+            unsigned dynamic_warp_id) {
+    m_cta_id = cta_id;
+    m_warp_id = wid;
+    m_dynamic_warp_id = dynamic_warp_id;
+    m_next_pc = start_pc;
+    assert(n_completed >= active.count());
+    assert(n_completed <= m_warp_size);
+    n_completed -= active.count();  // active threads are not yet completed
+    m_active_threads = active;
+    m_done_exit = false;
+
+    // Jin: cdp support
+    m_cdp_latency = 0;
+    m_cdp_dummy = false;
+  }
+
+  bool functional_done() const;
+  bool waiting();  // not const due to membar
+  bool hardware_done() const;
+
+  bool done_exit() const { return m_done_exit; }
+  void set_done_exit() { m_done_exit = true; }
+
+  void print(FILE *fout) const;
+  void print_ibuffer(FILE *fout) const;
+
+  unsigned get_n_completed() const { return n_completed; }
+  void set_completed(unsigned lane) {
+    assert(m_active_threads.test(lane));
+    m_active_threads.reset(lane);
+    n_completed++;
+  }
+
+  void set_last_fetch(unsigned long long sim_cycle) {
+    m_last_fetch = sim_cycle;
+  }
+
+  unsigned get_n_atomic() const { return m_n_atomic; }
+  void inc_n_atomic() { m_n_atomic++; }
+  void dec_n_atomic(unsigned n) { m_n_atomic -= n; }
+
+  void set_membar() { m_membar = true; }
+  void clear_membar() { m_membar = false; }
+  bool get_membar() const { return m_membar; }
+  virtual address_type get_pc() const { return m_next_pc; }
+  void set_next_pc(address_type pc) { m_next_pc = pc; }
+
+  void store_info_of_last_inst_at_barrier(const warp_inst_t *pI) {
+    m_inst_at_barrier = *pI;
+  }
+  warp_inst_t *restore_info_of_last_inst_at_barrier() {
+    return &m_inst_at_barrier;
+  }
+
+  void ibuffer_fill(unsigned slot, const warp_inst_t *pI) {
+    assert(slot < IBUFFER_SIZE);
+    m_ibuffer[slot].m_inst = pI;
+    m_ibuffer[slot].m_valid = true;
+    m_next = 0;
+  }
+  bool ibuffer_empty() const {
+    for (unsigned i = 0; i < IBUFFER_SIZE; i++)
+      if (m_ibuffer[i].m_valid) return false;
+    return true;
+  }
+  void ibuffer_flush() {
+    for (unsigned i = 0; i < IBUFFER_SIZE; i++) {
+      if (m_ibuffer[i].m_valid) dec_inst_in_pipeline();
+      m_ibuffer[i].m_inst = NULL;
+      m_ibuffer[i].m_valid = false;
     }
-    void ibuffer_step() { m_next = (m_next+1)%IBUFFER_SIZE; }
-
-    bool imiss_pending() const { return m_imiss_pending; }
-    void set_imiss_pending() { m_imiss_pending=true; }
-    void clear_imiss_pending() { m_imiss_pending=false; }
-
-    bool stores_done() const { return m_stores_outstanding == 0; }
-    void inc_store_req() { m_stores_outstanding++; }
-    void dec_store_req() 
-    {
-        assert( m_stores_outstanding > 0 );
-        m_stores_outstanding--;
+  }
+  const warp_inst_t *ibuffer_next_inst() { return m_ibuffer[m_next].m_inst; }
+  bool ibuffer_next_valid() { return m_ibuffer[m_next].m_valid; }
+  void ibuffer_free() {
+    m_ibuffer[m_next].m_inst = NULL;
+    m_ibuffer[m_next].m_valid = false;
+  }
+  void ibuffer_step() { m_next = (m_next + 1) % IBUFFER_SIZE; }
+
+  bool imiss_pending() const { return m_imiss_pending; }
+  void set_imiss_pending() { m_imiss_pending = true; }
+  void clear_imiss_pending() { m_imiss_pending = false; }
+
+  bool stores_done() const { return m_stores_outstanding == 0; }
+  void inc_store_req() { m_stores_outstanding++; }
+  void dec_store_req() {
+    assert(m_stores_outstanding > 0);
+    m_stores_outstanding--;
+  }
+
+  unsigned num_inst_in_buffer() const {
+    unsigned count = 0;
+    for (unsigned i = 0; i < IBUFFER_SIZE; i++) {
+      if (m_ibuffer[i].m_valid) count++;
     }
-
-    unsigned num_inst_in_buffer() const
-    {
-    	unsigned count=0;
-        for(unsigned i=0;i<IBUFFER_SIZE;i++) {
-            if( m_ibuffer[i].m_valid )
-            	count++;
-        }
-    	return count;
+    return count;
+  }
+  unsigned num_inst_in_pipeline() const { return m_inst_in_pipeline; }
+  unsigned num_issued_inst_in_pipeline() const {
+    return (num_inst_in_pipeline() - num_inst_in_buffer());
+  }
+  bool inst_in_pipeline() const { return m_inst_in_pipeline > 0; }
+  void inc_inst_in_pipeline() { m_inst_in_pipeline++; }
+  void dec_inst_in_pipeline() {
+    assert(m_inst_in_pipeline > 0);
+    m_inst_in_pipeline--;
+  }
+
+  unsigned get_cta_id() const { return m_cta_id; }
+
+  unsigned get_dynamic_warp_id() const { return m_dynamic_warp_id; }
+  unsigned get_warp_id() const { return m_warp_id; }
+
+  class shader_core_ctx *get_shader() {
+    return m_shader;
+  }
+
+ private:
+  static const unsigned IBUFFER_SIZE = 2;
+  class shader_core_ctx *m_shader;
+  unsigned m_cta_id;
+  unsigned m_warp_id;
+  unsigned m_warp_size;
+  unsigned m_dynamic_warp_id;
+
+  address_type m_next_pc;
+  unsigned n_completed;  // number of threads in warp completed
+  std::bitset<MAX_WARP_SIZE> m_active_threads;
+
+  bool m_imiss_pending;
+
+  struct ibuffer_entry {
+    ibuffer_entry() {
+      m_valid = false;
+      m_inst = NULL;
     }
-    unsigned num_inst_in_pipeline() const { return m_inst_in_pipeline;}
-    unsigned num_issued_inst_in_pipeline() const {return (num_inst_in_pipeline()-num_inst_in_buffer());}
-    bool inst_in_pipeline() const { return m_inst_in_pipeline > 0; }
-    void inc_inst_in_pipeline() { m_inst_in_pipeline++; }
-    void dec_inst_in_pipeline() 
-    {
-        assert( m_inst_in_pipeline > 0 );
-        m_inst_in_pipeline--;
-    }
-
-    unsigned get_cta_id() const { return m_cta_id; }
-
-    unsigned get_dynamic_warp_id() const { return m_dynamic_warp_id; }
-    unsigned get_warp_id() const { return m_warp_id; }
-
-private:
-    static const unsigned IBUFFER_SIZE=2;
-    class shader_core_ctx *m_shader;
-    unsigned m_cta_id;
-    unsigned m_warp_id;
-    unsigned m_warp_size;
-    unsigned m_dynamic_warp_id;
-
-    address_type m_next_pc;
-    unsigned n_completed;          // number of threads in warp completed
-    std::bitset<MAX_WARP_SIZE> m_active_threads;
+    const warp_inst_t *m_inst;
+    bool m_valid;
+  };
 
-    bool m_imiss_pending;
-    
-    struct ibuffer_entry {
-       ibuffer_entry() { m_valid = false; m_inst = NULL; }
-       const warp_inst_t *m_inst;
-       bool m_valid;
-    };
+  warp_inst_t m_inst_at_barrier;
+  ibuffer_entry m_ibuffer[IBUFFER_SIZE];
+  unsigned m_next;
 
-    const warp_inst_t *m_inst_at_barrier;
-    ibuffer_entry m_ibuffer[IBUFFER_SIZE]; 
-    unsigned m_next;
-                                   
-    unsigned m_n_atomic;           // number of outstanding atomic operations 
-    bool     m_membar;             // if true, warp is waiting at memory barrier
+  unsigned m_n_atomic;  // number of outstanding atomic operations
+  bool m_membar;        // if true, warp is waiting at memory barrier
 
-    bool m_done_exit; // true once thread exit has been registered for threads in this warp
+  bool m_done_exit;  // true once thread exit has been registered for threads in
+                     // this warp
 
-    unsigned long long m_last_fetch;
+  unsigned long long m_last_fetch;
 
-    unsigned m_stores_outstanding; // number of store requests sent but not yet acknowledged
-    unsigned m_inst_in_pipeline;
+  unsigned m_stores_outstanding;  // number of store requests sent but not yet
+                                  // acknowledged
+  unsigned m_inst_in_pipeline;
 
-    //Jin: cdp support
-public:
-    unsigned int m_cdp_latency;
-    bool m_cdp_dummy;
+  // Jin: cdp support
+ public:
+  unsigned int m_cdp_latency;
+  bool m_cdp_dummy;
 };
 
-
-
-inline unsigned hw_tid_from_wid(unsigned wid, unsigned warp_size, unsigned i){return wid * warp_size + i;};
-inline unsigned wid_from_hw_tid(unsigned tid, unsigned warp_size){return tid/warp_size;};
+inline unsigned hw_tid_from_wid(unsigned wid, unsigned warp_size, unsigned i) {
+  return wid * warp_size + i;
+};
+inline unsigned wid_from_hw_tid(unsigned tid, unsigned warp_size) {
+  return tid / warp_size;
+};
 
 const unsigned WARP_PER_CTA_MAX = 64;
 typedef std::bitset<WARP_PER_CTA_MAX> warp_set_t;
 
-int register_bank(int regnum, int wid, unsigned num_banks, unsigned bank_warp_shift, bool sub_core_model, unsigned banks_per_sched, unsigned sched_id );
+int register_bank(int regnum, int wid, unsigned num_banks,
+                  unsigned bank_warp_shift, bool sub_core_model,
+                  unsigned banks_per_sched, unsigned sched_id);
 
 class shader_core_ctx;
-struct shader_core_config;
+class shader_core_config;
 class shader_core_stats;
 
-enum scheduler_prioritization_type
-{
-    SCHEDULER_PRIORITIZATION_LRR = 0, // Loose Round Robin
-    SCHEDULER_PRIORITIZATION_SRR, // Strict Round Robin
-    SCHEDULER_PRIORITIZATION_GTO, // Greedy Then Oldest
-    SCHEDULER_PRIORITIZATION_GTLRR, // Greedy Then Loose Round Robin
-    SCHEDULER_PRIORITIZATION_GTY, // Greedy Then Youngest
-    SCHEDULER_PRIORITIZATION_OLDEST, // Oldest First
-    SCHEDULER_PRIORITIZATION_YOUNGEST, // Youngest First
+enum scheduler_prioritization_type {
+  SCHEDULER_PRIORITIZATION_LRR = 0,   // Loose Round Robin
+  SCHEDULER_PRIORITIZATION_SRR,       // Strict Round Robin
+  SCHEDULER_PRIORITIZATION_GTO,       // Greedy Then Oldest
+  SCHEDULER_PRIORITIZATION_GTLRR,     // Greedy Then Loose Round Robin
+  SCHEDULER_PRIORITIZATION_GTY,       // Greedy Then Youngest
+  SCHEDULER_PRIORITIZATION_OLDEST,    // Oldest First
+  SCHEDULER_PRIORITIZATION_YOUNGEST,  // Youngest First
 };
 
 // Each of these corresponds to a string value in the gpgpsim.config file
 // For example - to specify the LRR scheudler the config must contain lrr
-enum concrete_scheduler
-{
-    CONCRETE_SCHEDULER_LRR = 0,
-    CONCRETE_SCHEDULER_GTO,
-    CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE,
-    CONCRETE_SCHEDULER_WARP_LIMITING,
-    CONCRETE_SCHEDULER_OLDEST_FIRST,
-    NUM_CONCRETE_SCHEDULERS
+enum concrete_scheduler {
+  CONCRETE_SCHEDULER_LRR = 0,
+  CONCRETE_SCHEDULER_GTO,
+  CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE,
+  CONCRETE_SCHEDULER_RRR,
+  CONCRETE_SCHEDULER_WARP_LIMITING,
+  CONCRETE_SCHEDULER_OLDEST_FIRST,
+  NUM_CONCRETE_SCHEDULERS
 };
 
-class scheduler_unit { //this can be copied freely, so can be used in std containers.
-public:
-    scheduler_unit(shader_core_stats* stats, shader_core_ctx* shader, 
-                   Scoreboard* scoreboard, simt_stack** simt, 
-                   std::vector<shd_warp_t>* warp, 
-                   register_set* sp_out,
-				   register_set* dp_out,
-                   register_set* sfu_out,
-				   register_set* int_out,
-                   register_set* tensor_core_out,
-                   register_set* mem_out,
-                   int id) 
-        : m_supervised_warps(), m_stats(stats), m_shader(shader),
-        m_scoreboard(scoreboard), m_simt_stack(simt), /*m_pipeline_reg(pipe_regs),*/ m_warp(warp),
-        m_sp_out(sp_out),m_dp_out(dp_out),m_sfu_out(sfu_out),m_int_out(int_out),m_tensor_core_out(tensor_core_out),m_mem_out(mem_out), m_id(id){}
-    virtual ~scheduler_unit(){}
-    virtual void add_supervised_warp_id(int i) {
-        m_supervised_warps.push_back(&warp(i));
-    }
-    virtual void done_adding_supervised_warps() {
-        m_last_supervised_issued = m_supervised_warps.end();
-    }
-
-
-    // The core scheduler cycle method is meant to be common between
-    // all the derived schedulers.  The scheduler's behaviour can be
-    // modified by changing the contents of the m_next_cycle_prioritized_warps list.
-    void cycle();
-
-    // These are some common ordering fucntions that the
-    // higher order schedulers can take advantage of
-    template < typename T >
-    void order_lrr( typename std::vector< T >& result_list,
-                    const typename std::vector< T >& input_list,
-                    const typename std::vector< T >::const_iterator& last_issued_from_input,
-                    unsigned num_warps_to_add );
-    
-    enum OrderingType 
-    {
-        // The item that issued last is prioritized first then the sorted result
-        // of the priority_function
-        ORDERING_GREEDY_THEN_PRIORITY_FUNC = 0,
-        // No greedy scheduling based on last to issue. Only the priority function determines
-        // priority
-        ORDERED_PRIORITY_FUNC_ONLY,
-        NUM_ORDERING,
-    };
-    template < typename U >
-    void order_by_priority( std::vector< U >& result_list,
-                            const typename std::vector< U >& input_list,
-                            const typename std::vector< U >::const_iterator& last_issued_from_input,
-                            unsigned num_warps_to_add,
-                            OrderingType age_ordering,
-                            bool (*priority_func)(U lhs, U rhs) );
-    static bool sort_warps_by_oldest_dynamic_id(shd_warp_t* lhs, shd_warp_t* rhs);
-
-    // Derived classes can override this function to populate
-    // m_supervised_warps with their scheduling policies
-    virtual void order_warps() = 0;
-
-    int get_schd_id() const {return m_id;}
-
-protected:
-    virtual void do_on_warp_issued( unsigned warp_id,
-                                    unsigned num_issued,
-                                    const std::vector< shd_warp_t* >::const_iterator& prioritized_iter );
-    inline int get_sid() const;
-protected:
-    shd_warp_t& warp(int i);
-
-    // This is the prioritized warp list that is looped over each cycle to determine
-    // which warp gets to issue.
-    std::vector< shd_warp_t* > m_next_cycle_prioritized_warps;
-    // The m_supervised_warps list is all the warps this scheduler is supposed to
-    // arbitrate between.  This is useful in systems where there is more than
-    // one warp scheduler. In a single scheduler system, this is simply all
-    // the warps assigned to this core.
-    std::vector< shd_warp_t* > m_supervised_warps;
-    // This is the iterator pointer to the last supervised warp you issued
-    std::vector< shd_warp_t* >::const_iterator m_last_supervised_issued;
-    shader_core_stats *m_stats;
-    shader_core_ctx* m_shader;
-    // these things should become accessors: but would need a bigger rearchitect of how shader_core_ctx interacts with its parts.
-    Scoreboard* m_scoreboard; 
-    simt_stack** m_simt_stack;
-    //warp_inst_t** m_pipeline_reg;
-    std::vector<shd_warp_t>* m_warp;
-    register_set* m_sp_out;
-    register_set* m_dp_out;
-    register_set* m_sfu_out;
-    register_set* m_int_out;
-    register_set* m_tensor_core_out;
-    register_set* m_mem_out;
-
-    int m_id;
+class scheduler_unit {  // this can be copied freely, so can be used in std
+                        // containers.
+ public:
+  scheduler_unit(shader_core_stats *stats, shader_core_ctx *shader,
+                 Scoreboard *scoreboard, simt_stack **simt,
+                 std::vector<shd_warp_t *> *warp, register_set *sp_out,
+                 register_set *dp_out, register_set *sfu_out,
+                 register_set *int_out, register_set *tensor_core_out,
+                 std::vector<register_set *> &spec_cores_out,
+                 register_set *mem_out, int id)
+      : m_supervised_warps(),
+        m_stats(stats),
+        m_shader(shader),
+        m_scoreboard(scoreboard),
+        m_simt_stack(simt),
+        /*m_pipeline_reg(pipe_regs),*/ m_warp(warp),
+        m_sp_out(sp_out),
+        m_dp_out(dp_out),
+        m_sfu_out(sfu_out),
+        m_int_out(int_out),
+        m_tensor_core_out(tensor_core_out),
+        m_spec_cores_out(spec_cores_out),
+        m_mem_out(mem_out),
+        m_id(id) {}
+  virtual ~scheduler_unit() {}
+  virtual void add_supervised_warp_id(int i) {
+    m_supervised_warps.push_back(&warp(i));
+  }
+  virtual void done_adding_supervised_warps() {
+    m_last_supervised_issued = m_supervised_warps.end();
+  }
+
+  // The core scheduler cycle method is meant to be common between
+  // all the derived schedulers.  The scheduler's behaviour can be
+  // modified by changing the contents of the m_next_cycle_prioritized_warps
+  // list.
+  void cycle();
+
+  // These are some common ordering fucntions that the
+  // higher order schedulers can take advantage of
+  template <typename T>
+  void order_lrr(
+      typename std::vector<T> &result_list,
+      const typename std::vector<T> &input_list,
+      const typename std::vector<T>::const_iterator &last_issued_from_input,
+      unsigned num_warps_to_add);
+  template <typename T>
+  void order_rrr(
+      typename std::vector<T> &result_list,
+      const typename std::vector<T> &input_list,
+      const typename std::vector<T>::const_iterator &last_issued_from_input,
+      unsigned num_warps_to_add);
+
+  enum OrderingType {
+    // The item that issued last is prioritized first then the sorted result
+    // of the priority_function
+    ORDERING_GREEDY_THEN_PRIORITY_FUNC = 0,
+    // No greedy scheduling based on last to issue. Only the priority function
+    // determines priority
+    ORDERED_PRIORITY_FUNC_ONLY,
+    NUM_ORDERING,
+  };
+  template <typename U>
+  void order_by_priority(
+      std::vector<U> &result_list, const typename std::vector<U> &input_list,
+      const typename std::vector<U>::const_iterator &last_issued_from_input,
+      unsigned num_warps_to_add, OrderingType age_ordering,
+      bool (*priority_func)(U lhs, U rhs));
+  static bool sort_warps_by_oldest_dynamic_id(shd_warp_t *lhs, shd_warp_t *rhs);
+
+  // Derived classes can override this function to populate
+  // m_supervised_warps with their scheduling policies
+  virtual void order_warps() = 0;
+
+  int get_schd_id() const { return m_id; }
+
+ protected:
+  virtual void do_on_warp_issued(
+      unsigned warp_id, unsigned num_issued,
+      const std::vector<shd_warp_t *>::const_iterator &prioritized_iter);
+  inline int get_sid() const;
+
+ protected:
+  shd_warp_t &warp(int i);
+
+  // This is the prioritized warp list that is looped over each cycle to
+  // determine which warp gets to issue.
+  std::vector<shd_warp_t *> m_next_cycle_prioritized_warps;
+  // The m_supervised_warps list is all the warps this scheduler is supposed to
+  // arbitrate between.  This is useful in systems where there is more than
+  // one warp scheduler. In a single scheduler system, this is simply all
+  // the warps assigned to this core.
+  std::vector<shd_warp_t *> m_supervised_warps;
+  // This is the iterator pointer to the last supervised warp you issued
+  std::vector<shd_warp_t *>::const_iterator m_last_supervised_issued;
+  shader_core_stats *m_stats;
+  shader_core_ctx *m_shader;
+  // these things should become accessors: but would need a bigger rearchitect
+  // of how shader_core_ctx interacts with its parts.
+  Scoreboard *m_scoreboard;
+  simt_stack **m_simt_stack;
+  // warp_inst_t** m_pipeline_reg;
+  std::vector<shd_warp_t *> *m_warp;
+  register_set *m_sp_out;
+  register_set *m_dp_out;
+  register_set *m_sfu_out;
+  register_set *m_int_out;
+  register_set *m_tensor_core_out;
+  register_set *m_mem_out;
+  std::vector<register_set *> &m_spec_cores_out;
+  unsigned m_num_issued_last_cycle;
+  unsigned m_current_turn_warp;
+
+  int m_id;
 };
 
 class lrr_scheduler : public scheduler_unit {
-public:
-	lrr_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                    Scoreboard* scoreboard, simt_stack** simt,
-                    std::vector<shd_warp_t>* warp,
-                    register_set* sp_out,
-					register_set* dp_out,
-                    register_set* sfu_out,
-					register_set* int_out,
-                    register_set* tensor_core_out,
-                    register_set* mem_out,
-                    int id )
-	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, dp_out, sfu_out, int_out, tensor_core_out, mem_out, id ){}
-	virtual ~lrr_scheduler () {}
-	virtual void order_warps ();
-    virtual void done_adding_supervised_warps() {
-        m_last_supervised_issued = m_supervised_warps.end();
-    }
+ public:
+  lrr_scheduler(shader_core_stats *stats, shader_core_ctx *shader,
+                Scoreboard *scoreboard, simt_stack **simt,
+                std::vector<shd_warp_t *> *warp, register_set *sp_out,
+                register_set *dp_out, register_set *sfu_out,
+                register_set *int_out, register_set *tensor_core_out,
+                std::vector<register_set *> &spec_cores_out,
+                register_set *mem_out, int id)
+      : scheduler_unit(stats, shader, scoreboard, simt, warp, sp_out, dp_out,
+                       sfu_out, int_out, tensor_core_out, spec_cores_out,
+                       mem_out, id) {}
+  virtual ~lrr_scheduler() {}
+  virtual void order_warps();
+  virtual void done_adding_supervised_warps() {
+    m_last_supervised_issued = m_supervised_warps.end();
+  }
 };
 
-class gto_scheduler : public scheduler_unit {
-public:
-	gto_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                    Scoreboard* scoreboard, simt_stack** simt,
-                    std::vector<shd_warp_t>* warp,
-                    register_set* sp_out,
-					register_set* dp_out,
-                    register_set* sfu_out,
-					register_set* int_out,
-                    register_set* tensor_core_out,
-                    register_set* mem_out,
-                    int id )
-	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, dp_out, sfu_out, int_out, tensor_core_out, mem_out, id ){}
-	virtual ~gto_scheduler () {}
-	virtual void order_warps ();
-    virtual void done_adding_supervised_warps() {
-        m_last_supervised_issued = m_supervised_warps.begin();
-    }
+class rrr_scheduler : public scheduler_unit {
+ public:
+  rrr_scheduler(shader_core_stats *stats, shader_core_ctx *shader,
+                Scoreboard *scoreboard, simt_stack **simt,
+                std::vector<shd_warp_t *> *warp, register_set *sp_out,
+                register_set *dp_out, register_set *sfu_out,
+                register_set *int_out, register_set *tensor_core_out,
+                std::vector<register_set *> &spec_cores_out,
+                register_set *mem_out, int id)
+      : scheduler_unit(stats, shader, scoreboard, simt, warp, sp_out, dp_out,
+                       sfu_out, int_out, tensor_core_out, spec_cores_out,
+                       mem_out, id) {}
+  virtual ~rrr_scheduler() {}
+  virtual void order_warps();
+  virtual void done_adding_supervised_warps() {
+    m_last_supervised_issued = m_supervised_warps.end();
+  }
+};
 
+class gto_scheduler : public scheduler_unit {
+ public:
+  gto_scheduler(shader_core_stats *stats, shader_core_ctx *shader,
+                Scoreboard *scoreboard, simt_stack **simt,
+                std::vector<shd_warp_t *> *warp, register_set *sp_out,
+                register_set *dp_out, register_set *sfu_out,
+                register_set *int_out, register_set *tensor_core_out,
+                std::vector<register_set *> &spec_cores_out,
+                register_set *mem_out, int id)
+      : scheduler_unit(stats, shader, scoreboard, simt, warp, sp_out, dp_out,
+                       sfu_out, int_out, tensor_core_out, spec_cores_out,
+                       mem_out, id) {}
+  virtual ~gto_scheduler() {}
+  virtual void order_warps();
+  virtual void done_adding_supervised_warps() {
+    m_last_supervised_issued = m_supervised_warps.begin();
+  }
 };
 
 class oldest_scheduler : public scheduler_unit {
-public:
-	oldest_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                    Scoreboard* scoreboard, simt_stack** simt,
-                    std::vector<shd_warp_t>* warp,
-                    register_set* sp_out,
-					register_set* dp_out,
-                    register_set* sfu_out,
-					register_set* int_out,
-                    register_set* tensor_core_out,
-                    register_set* mem_out,
-                    int id )
-	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, dp_out, sfu_out, int_out, tensor_core_out, mem_out, id ){}
-	virtual ~oldest_scheduler () {}
-	virtual void order_warps ();
-        virtual void done_adding_supervised_warps() {
-        m_last_supervised_issued = m_supervised_warps.begin();
-    }
-
+ public:
+  oldest_scheduler(shader_core_stats *stats, shader_core_ctx *shader,
+                   Scoreboard *scoreboard, simt_stack **simt,
+                   std::vector<shd_warp_t *> *warp, register_set *sp_out,
+                   register_set *dp_out, register_set *sfu_out,
+                   register_set *int_out, register_set *tensor_core_out,
+                   std::vector<register_set *> &spec_cores_out,
+                   register_set *mem_out, int id)
+      : scheduler_unit(stats, shader, scoreboard, simt, warp, sp_out, dp_out,
+                       sfu_out, int_out, tensor_core_out, spec_cores_out,
+                       mem_out, id) {}
+  virtual ~oldest_scheduler() {}
+  virtual void order_warps();
+  virtual void done_adding_supervised_warps() {
+    m_last_supervised_issued = m_supervised_warps.begin();
+  }
 };
 
 class two_level_active_scheduler : public scheduler_unit {
-public:
-	two_level_active_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                          Scoreboard* scoreboard, simt_stack** simt,
-                          std::vector<shd_warp_t>* warp,
-                          register_set* sp_out,
-						  register_set* dp_out,
-                          register_set* sfu_out,
-						  register_set* int_out,
-                          register_set* tensor_core_out,
-                          register_set* mem_out,
-                          int id,
-                          char* config_str )
-	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, dp_out, sfu_out, int_out, tensor_core_out, mem_out, id ),
-	  m_pending_warps() 
-    {
-        unsigned inner_level_readin;
-        unsigned outer_level_readin; 
-        int ret = sscanf( config_str,
-                          "two_level_active:%d:%d:%d",
-                          &m_max_active_warps,
-                          &inner_level_readin,
-                          &outer_level_readin);
-        assert( 3 == ret );
-        m_inner_level_prioritization=(scheduler_prioritization_type)inner_level_readin;
-        m_outer_level_prioritization=(scheduler_prioritization_type)outer_level_readin;
-    }
-	virtual ~two_level_active_scheduler () {}
-    virtual void order_warps();
-	void add_supervised_warp_id(int i) {
-        if ( m_next_cycle_prioritized_warps.size() < m_max_active_warps ) {
-            m_next_cycle_prioritized_warps.push_back( &warp(i) );
-        } else {
-		    m_pending_warps.push_back(&warp(i));
-        }
-	}
-    virtual void done_adding_supervised_warps() {
-        m_last_supervised_issued = m_supervised_warps.begin();
+ public:
+  two_level_active_scheduler(shader_core_stats *stats, shader_core_ctx *shader,
+                             Scoreboard *scoreboard, simt_stack **simt,
+                             std::vector<shd_warp_t *> *warp,
+                             register_set *sp_out, register_set *dp_out,
+                             register_set *sfu_out, register_set *int_out,
+                             register_set *tensor_core_out,
+                             std::vector<register_set *> &spec_cores_out,
+                             register_set *mem_out, int id, char *config_str)
+      : scheduler_unit(stats, shader, scoreboard, simt, warp, sp_out, dp_out,
+                       sfu_out, int_out, tensor_core_out, spec_cores_out,
+                       mem_out, id),
+        m_pending_warps() {
+    unsigned inner_level_readin;
+    unsigned outer_level_readin;
+    int ret =
+        sscanf(config_str, "two_level_active:%d:%d:%d", &m_max_active_warps,
+               &inner_level_readin, &outer_level_readin);
+    assert(3 == ret);
+    m_inner_level_prioritization =
+        (scheduler_prioritization_type)inner_level_readin;
+    m_outer_level_prioritization =
+        (scheduler_prioritization_type)outer_level_readin;
+  }
+  virtual ~two_level_active_scheduler() {}
+  virtual void order_warps();
+  void add_supervised_warp_id(int i) {
+    if (m_next_cycle_prioritized_warps.size() < m_max_active_warps) {
+      m_next_cycle_prioritized_warps.push_back(&warp(i));
+    } else {
+      m_pending_warps.push_back(&warp(i));
     }
-
-protected:
-    virtual void do_on_warp_issued( unsigned warp_id,
-                                    unsigned num_issued,
-                                    const std::vector< shd_warp_t* >::const_iterator& prioritized_iter );
-
-private:
-	std::deque< shd_warp_t* > m_pending_warps;
-    scheduler_prioritization_type m_inner_level_prioritization;
-    scheduler_prioritization_type m_outer_level_prioritization;
-	unsigned m_max_active_warps;
+  }
+  virtual void done_adding_supervised_warps() {
+    m_last_supervised_issued = m_supervised_warps.begin();
+  }
+
+ protected:
+  virtual void do_on_warp_issued(
+      unsigned warp_id, unsigned num_issued,
+      const std::vector<shd_warp_t *>::const_iterator &prioritized_iter);
+
+ private:
+  std::deque<shd_warp_t *> m_pending_warps;
+  scheduler_prioritization_type m_inner_level_prioritization;
+  scheduler_prioritization_type m_outer_level_prioritization;
+  unsigned m_max_active_warps;
 };
 
 // Static Warp Limiting Scheduler
 class swl_scheduler : public scheduler_unit {
-public:
-	swl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                    Scoreboard* scoreboard, simt_stack** simt,
-                    std::vector<shd_warp_t>* warp,
-                    register_set* sp_out,
-					register_set* dp_out,
-                    register_set* sfu_out,
-					register_set* int_out,
-                    register_set* tensor_core_out,
-                    register_set* mem_out,
-                    int id,
-                    char* config_string );
-	virtual ~swl_scheduler () {}
-	virtual void order_warps ();
-    virtual void done_adding_supervised_warps() {
-        m_last_supervised_issued = m_supervised_warps.begin();
-    }
-
-protected:
-    scheduler_prioritization_type m_prioritization;
-    unsigned m_num_warps_to_limit;
+ public:
+  swl_scheduler(shader_core_stats *stats, shader_core_ctx *shader,
+                Scoreboard *scoreboard, simt_stack **simt,
+                std::vector<shd_warp_t *> *warp, register_set *sp_out,
+                register_set *dp_out, register_set *sfu_out,
+                register_set *int_out, register_set *tensor_core_out,
+                std::vector<register_set *> &spec_cores_out,
+                register_set *mem_out, int id, char *config_string);
+  virtual ~swl_scheduler() {}
+  virtual void order_warps();
+  virtual void done_adding_supervised_warps() {
+    m_last_supervised_issued = m_supervised_warps.begin();
+  }
+
+ protected:
+  scheduler_prioritization_type m_prioritization;
+  unsigned m_num_warps_to_limit;
 };
 
+class opndcoll_rfu_t {  // operand collector based register file unit
+ public:
+  // constructors
+  opndcoll_rfu_t() {
+    m_num_banks = 0;
+    m_shader = NULL;
+    m_initialized = false;
+  }
+  void add_cu_set(unsigned cu_set, unsigned num_cu, unsigned num_dispatch);
+  typedef std::vector<register_set *> port_vector_t;
+  typedef std::vector<unsigned int> uint_vector_t;
+  void add_port(port_vector_t &input, port_vector_t &ouput,
+                uint_vector_t cu_sets);
+  void init(unsigned num_banks, shader_core_ctx *shader);
+
+  // modifiers
+  bool writeback(warp_inst_t &warp);
+
+  void step() {
+    dispatch_ready_cu();
+    allocate_reads();
+    for (unsigned p = 0; p < m_in_ports.size(); p++) allocate_cu(p);
+    process_banks();
+  }
+
+  void dump(FILE *fp) const {
+    fprintf(fp, "\n");
+    fprintf(fp, "Operand Collector State:\n");
+    for (unsigned n = 0; n < m_cu.size(); n++) {
+      fprintf(fp, "   CU-%2u: ", n);
+      m_cu[n]->dump(fp, m_shader);
+    }
+    m_arbiter.dump(fp);
+  }
 
+  shader_core_ctx *shader_core() { return m_shader; }
 
-class opndcoll_rfu_t { // operand collector based register file unit
-public:
-   // constructors
-   opndcoll_rfu_t()
-   {
-      m_num_banks=0;
-      m_shader=NULL;
-      m_initialized=false;
-   }
-   void add_cu_set(unsigned cu_set, unsigned num_cu, unsigned num_dispatch);
-   typedef std::vector<register_set*> port_vector_t;
-   typedef std::vector<unsigned int> uint_vector_t;
-   void add_port( port_vector_t & input, port_vector_t & ouput, uint_vector_t cu_sets);
-   void init( unsigned num_banks, shader_core_ctx *shader );
-
-   // modifiers
-   bool writeback( warp_inst_t &warp ); 
-
-   void step()
-   {
-        dispatch_ready_cu();   
-        allocate_reads();
-        for( unsigned p = 0 ; p < m_in_ports.size(); p++ ) 
-            allocate_cu( p );
-        process_banks();
-   }
-
-   void dump( FILE *fp ) const
-   {
-      fprintf(fp,"\n");
-      fprintf(fp,"Operand Collector State:\n");
-      for( unsigned n=0; n < m_cu.size(); n++ ) {
-         fprintf(fp,"   CU-%2u: ", n);
-         m_cu[n]->dump(fp,m_shader);
-      }
-      m_arbiter.dump(fp);
-   }
-
-   shader_core_ctx *shader_core() { return m_shader; }
-
-private:
-
-   void process_banks()
-   {
-      m_arbiter.reset_alloction();
-   }
+ private:
+  void process_banks() { m_arbiter.reset_alloction(); }
 
-   void dispatch_ready_cu();
-   void allocate_cu( unsigned port );
-   void allocate_reads();
+  void dispatch_ready_cu();
+  void allocate_cu(unsigned port);
+  void allocate_reads();
 
-   // types
+  // types
 
-   class collector_unit_t;
+  class collector_unit_t;
 
-   class op_t {
+  class op_t {
    public:
+    op_t() { m_valid = false; }
+    op_t(collector_unit_t *cu, unsigned op, unsigned reg, unsigned num_banks,
+         unsigned bank_warp_shift, bool sub_core_model,
+         unsigned banks_per_sched, unsigned sched_id) {
+      m_valid = true;
+      m_warp = NULL;
+      m_cu = cu;
+      m_operand = op;
+      m_register = reg;
+      m_shced_id = sched_id;
+      m_bank = register_bank(reg, cu->get_warp_id(), num_banks, bank_warp_shift,
+                             sub_core_model, banks_per_sched, sched_id);
+    }
+    op_t(const warp_inst_t *warp, unsigned reg, unsigned num_banks,
+         unsigned bank_warp_shift, bool sub_core_model,
+         unsigned banks_per_sched, unsigned sched_id) {
+      m_valid = true;
+      m_warp = warp;
+      m_register = reg;
+      m_cu = NULL;
+      m_operand = -1;
+      m_shced_id = sched_id;
+      m_bank = register_bank(reg, warp->warp_id(), num_banks, bank_warp_shift,
+                             sub_core_model, banks_per_sched, sched_id);
+    }
 
-      op_t() { m_valid = false; }
-      op_t( collector_unit_t *cu, unsigned op, unsigned reg, unsigned num_banks, unsigned bank_warp_shift, bool sub_core_model, unsigned banks_per_sched, unsigned sched_id )
-      {
-         m_valid = true;
-         m_warp=NULL;
-         m_cu = cu;
-         m_operand = op;
-         m_register = reg;
-         m_shced_id = sched_id;
-         m_bank = register_bank(reg,cu->get_warp_id(),num_banks,bank_warp_shift, sub_core_model, banks_per_sched, sched_id);
-      }
-      op_t( const warp_inst_t *warp, unsigned reg, unsigned num_banks, unsigned bank_warp_shift, bool sub_core_model, unsigned banks_per_sched, unsigned sched_id )
-      {
-         m_valid=true;
-         m_warp=warp;
-         m_register=reg;
-         m_cu=NULL;
-         m_operand = -1;
-         m_shced_id = sched_id;
-         m_bank = register_bank(reg,warp->warp_id(),num_banks,bank_warp_shift, sub_core_model, banks_per_sched, sched_id);
-      }
+    // accessors
+    bool valid() const { return m_valid; }
+    unsigned get_reg() const {
+      assert(m_valid);
+      return m_register;
+    }
+    unsigned get_wid() const {
+      if (m_warp)
+        return m_warp->warp_id();
+      else if (m_cu)
+        return m_cu->get_warp_id();
+      else
+        abort();
+    }
+    unsigned get_sid() const { return m_shced_id; }
+    unsigned get_active_count() const {
+      if (m_warp)
+        return m_warp->active_count();
+      else if (m_cu)
+        return m_cu->get_active_count();
+      else
+        abort();
+    }
+    const active_mask_t &get_active_mask() {
+      if (m_warp)
+        return m_warp->get_active_mask();
+      else if (m_cu)
+        return m_cu->get_active_mask();
+      else
+        abort();
+    }
+    unsigned get_sp_op() const {
+      if (m_warp)
+        return m_warp->sp_op;
+      else if (m_cu)
+        return m_cu->get_sp_op();
+      else
+        abort();
+    }
+    unsigned get_oc_id() const { return m_cu->get_id(); }
+    unsigned get_bank() const { return m_bank; }
+    unsigned get_operand() const { return m_operand; }
+    void dump(FILE *fp) const {
+      if (m_cu)
+        fprintf(fp, " <R%u, CU:%u, w:%02u> ", m_register, m_cu->get_id(),
+                m_cu->get_warp_id());
+      else if (!m_warp->empty())
+        fprintf(fp, " <R%u, wid:%02u> ", m_register, m_warp->warp_id());
+    }
+    std::string get_reg_string() const {
+      char buffer[64];
+      snprintf(buffer, 64, "R%u", m_register);
+      return std::string(buffer);
+    }
 
-      // accessors
-      bool valid() const { return m_valid; }
-      unsigned get_reg() const
-      {
-         assert( m_valid );
-         return m_register;
-      }
-      unsigned get_wid() const
-      {
-          if( m_warp ) return m_warp->warp_id();
-          else if( m_cu ) return m_cu->get_warp_id();
-          else abort();
-      }
-      unsigned get_sid() const
-	  {
-		 return m_shced_id;
-	  }
-      unsigned get_active_count() const
-      {
-          if( m_warp ) return m_warp->active_count();
-          else if( m_cu ) return m_cu->get_active_count();
-          else abort();
-      }
-      const active_mask_t & get_active_mask()
-      {
-          if( m_warp ) return m_warp->get_active_mask();
-          else if( m_cu ) return m_cu->get_active_mask();
-          else abort();
-      }
-      unsigned get_sp_op() const
-      {
-          if( m_warp ) return m_warp->sp_op;
-          else if( m_cu ) return m_cu->get_sp_op();
-          else abort();
-      }
-      unsigned get_oc_id() const { return m_cu->get_id(); }
-      unsigned get_bank() const { return m_bank; }
-      unsigned get_operand() const { return m_operand; }
-      void dump(FILE *fp) const 
-      {
-         if(m_cu) 
-            fprintf(fp," <R%u, CU:%u, w:%02u> ", m_register,m_cu->get_id(),m_cu->get_warp_id());
-         else if( !m_warp->empty() )
-            fprintf(fp," <R%u, wid:%02u> ", m_register,m_warp->warp_id() );
-      }
-      std::string get_reg_string() const
-      {
-         char buffer[64];
-         snprintf(buffer,64,"R%u", m_register);
-         return std::string(buffer);
-      }
+    // modifiers
+    void reset() { m_valid = false; }
 
-      // modifiers
-      void reset() { m_valid = false; }
    private:
-      bool m_valid;
-      collector_unit_t  *m_cu; 
-      const warp_inst_t *m_warp;
-      unsigned  m_operand; // operand offset in instruction. e.g., add r1,r2,r3; r2 is oprd 0, r3 is 1 (r1 is dst)
-      unsigned  m_register;
-      unsigned  m_bank;
-      unsigned  m_shced_id; //scheduler id that has issued this inst
-   };
-
-   enum alloc_t {
-      NO_ALLOC,
-      READ_ALLOC,
-      WRITE_ALLOC,
-   };
-
-   class allocation_t {
+    bool m_valid;
+    collector_unit_t *m_cu;
+    const warp_inst_t *m_warp;
+    unsigned m_operand;  // operand offset in instruction. e.g., add r1,r2,r3;
+                         // r2 is oprd 0, r3 is 1 (r1 is dst)
+    unsigned m_register;
+    unsigned m_bank;
+    unsigned m_shced_id;  // scheduler id that has issued this inst
+  };
+
+  enum alloc_t {
+    NO_ALLOC,
+    READ_ALLOC,
+    WRITE_ALLOC,
+  };
+
+  class allocation_t {
    public:
-      allocation_t() { m_allocation = NO_ALLOC; }
-      bool is_read() const { return m_allocation==READ_ALLOC; }
-      bool is_write() const {return m_allocation==WRITE_ALLOC; }
-      bool is_free() const {return m_allocation==NO_ALLOC; }
-      void dump(FILE *fp) const {
-         if( m_allocation == NO_ALLOC ) { fprintf(fp,"<free>"); }
-         else if( m_allocation == READ_ALLOC ) { fprintf(fp,"rd: "); m_op.dump(fp); }
-         else if( m_allocation == WRITE_ALLOC ) { fprintf(fp,"wr: "); m_op.dump(fp); }
-         fprintf(fp,"\n");
+    allocation_t() { m_allocation = NO_ALLOC; }
+    bool is_read() const { return m_allocation == READ_ALLOC; }
+    bool is_write() const { return m_allocation == WRITE_ALLOC; }
+    bool is_free() const { return m_allocation == NO_ALLOC; }
+    void dump(FILE *fp) const {
+      if (m_allocation == NO_ALLOC) {
+        fprintf(fp, "<free>");
+      } else if (m_allocation == READ_ALLOC) {
+        fprintf(fp, "rd: ");
+        m_op.dump(fp);
+      } else if (m_allocation == WRITE_ALLOC) {
+        fprintf(fp, "wr: ");
+        m_op.dump(fp);
       }
-      void alloc_read( const op_t &op )  { assert(is_free()); m_allocation=READ_ALLOC; m_op=op;  }
-      void alloc_write( const op_t &op ) { assert(is_free()); m_allocation=WRITE_ALLOC; m_op=op; }
-      void reset() { m_allocation = NO_ALLOC; }
+      fprintf(fp, "\n");
+    }
+    void alloc_read(const op_t &op) {
+      assert(is_free());
+      m_allocation = READ_ALLOC;
+      m_op = op;
+    }
+    void alloc_write(const op_t &op) {
+      assert(is_free());
+      m_allocation = WRITE_ALLOC;
+      m_op = op;
+    }
+    void reset() { m_allocation = NO_ALLOC; }
+
    private:
-      enum alloc_t m_allocation;
-      op_t m_op;
-   };
+    enum alloc_t m_allocation;
+    op_t m_op;
+  };
 
-   class arbiter_t {
+  class arbiter_t {
    public:
-      // constructors
-      arbiter_t()
-      {
-         m_queue=NULL;
-         m_allocated_bank=NULL;
-         m_allocator_rr_head=NULL;
-         _inmatch=NULL;
-         _outmatch=NULL;
-         _request=NULL;
-         m_last_cu=0;
-      }
-      void init( unsigned num_cu, unsigned num_banks ) 
-      { 
-         assert(num_cu > 0);
-         assert(num_banks > 0);
-         m_num_collectors = num_cu;
-         m_num_banks = num_banks;
-         _inmatch = new int[ m_num_banks ];
-         _outmatch = new int[ m_num_collectors ];
-         _request = new int*[ m_num_banks ];
-         for(unsigned i=0; i<m_num_banks;i++) 
-             _request[i] = new int[m_num_collectors];
-         m_queue = new std::list<op_t>[num_banks];
-         m_allocated_bank = new allocation_t[num_banks];
-         m_allocator_rr_head = new unsigned[num_cu];
-         for( unsigned n=0; n<num_cu;n++ ) 
-            m_allocator_rr_head[n] = n%num_banks;
-         reset_alloction();
-      }
-
-      // accessors
-      void dump(FILE *fp) const
-      {
-         fprintf(fp,"\n");
-         fprintf(fp,"  Arbiter State:\n");
-         fprintf(fp,"  requests:\n");
-         for( unsigned b=0; b<m_num_banks; b++ ) {
-            fprintf(fp,"    bank %u : ", b );
-            std::list<op_t>::const_iterator o = m_queue[b].begin();
-            for(; o != m_queue[b].end(); o++ ) {
-               o->dump(fp);
-            }
-            fprintf(fp,"\n");
-         }
-         fprintf(fp,"  grants:\n");
-         for(unsigned b=0;b<m_num_banks;b++) {
-            fprintf(fp,"    bank %u : ", b );
-            m_allocated_bank[b].dump(fp);
-         }
-         fprintf(fp,"\n");
-      }
+    // constructors
+    arbiter_t() {
+      m_queue = NULL;
+      m_allocated_bank = NULL;
+      m_allocator_rr_head = NULL;
+      _inmatch = NULL;
+      _outmatch = NULL;
+      _request = NULL;
+      m_last_cu = 0;
+    }
+    void init(unsigned num_cu, unsigned num_banks) {
+      assert(num_cu > 0);
+      assert(num_banks > 0);
+      m_num_collectors = num_cu;
+      m_num_banks = num_banks;
+      _inmatch = new int[m_num_banks];
+      _outmatch = new int[m_num_collectors];
+      _request = new int *[m_num_banks];
+      for (unsigned i = 0; i < m_num_banks; i++)
+        _request[i] = new int[m_num_collectors];
+      m_queue = new std::list<op_t>[num_banks];
+      m_allocated_bank = new allocation_t[num_banks];
+      m_allocator_rr_head = new unsigned[num_cu];
+      for (unsigned n = 0; n < num_cu; n++)
+        m_allocator_rr_head[n] = n % num_banks;
+      reset_alloction();
+    }
 
-      // modifiers
-      std::list<op_t> allocate_reads(); 
-
-      void add_read_requests( collector_unit_t *cu ) 
-      {
-         const op_t *src = cu->get_operands();
-         for( unsigned i=0; i<MAX_REG_OPERANDS*2; i++) {
-            const op_t &op = src[i];
-            if( op.valid() ) {
-               unsigned bank = op.get_bank();
-               m_queue[bank].push_back(op);
-            }
-         }
-      }
-      bool bank_idle( unsigned bank ) const
-      {
-          return m_allocated_bank[bank].is_free();
-      }
-      void allocate_bank_for_write( unsigned bank, const op_t &op )
-      {
-         assert( bank < m_num_banks );
-         m_allocated_bank[bank].alloc_write(op);
+    // accessors
+    void dump(FILE *fp) const {
+      fprintf(fp, "\n");
+      fprintf(fp, "  Arbiter State:\n");
+      fprintf(fp, "  requests:\n");
+      for (unsigned b = 0; b < m_num_banks; b++) {
+        fprintf(fp, "    bank %u : ", b);
+        std::list<op_t>::const_iterator o = m_queue[b].begin();
+        for (; o != m_queue[b].end(); o++) {
+          o->dump(fp);
+        }
+        fprintf(fp, "\n");
       }
-      void allocate_for_read( unsigned bank, const op_t &op )
-      {
-         assert( bank < m_num_banks );
-         m_allocated_bank[bank].alloc_read(op);
+      fprintf(fp, "  grants:\n");
+      for (unsigned b = 0; b < m_num_banks; b++) {
+        fprintf(fp, "    bank %u : ", b);
+        m_allocated_bank[b].dump(fp);
       }
-      void reset_alloction()
-      {
-         for( unsigned b=0; b < m_num_banks; b++ ) 
-            m_allocated_bank[b].reset();
+      fprintf(fp, "\n");
+    }
+
+    // modifiers
+    std::list<op_t> allocate_reads();
+
+    void add_read_requests(collector_unit_t *cu) {
+      const op_t *src = cu->get_operands();
+      for (unsigned i = 0; i < MAX_REG_OPERANDS * 2; i++) {
+        const op_t &op = src[i];
+        if (op.valid()) {
+          unsigned bank = op.get_bank();
+          m_queue[bank].push_back(op);
+        }
       }
+    }
+    bool bank_idle(unsigned bank) const {
+      return m_allocated_bank[bank].is_free();
+    }
+    void allocate_bank_for_write(unsigned bank, const op_t &op) {
+      assert(bank < m_num_banks);
+      m_allocated_bank[bank].alloc_write(op);
+    }
+    void allocate_for_read(unsigned bank, const op_t &op) {
+      assert(bank < m_num_banks);
+      m_allocated_bank[bank].alloc_read(op);
+    }
+    void reset_alloction() {
+      for (unsigned b = 0; b < m_num_banks; b++) m_allocated_bank[b].reset();
+    }
 
    private:
-      unsigned m_num_banks;
-      unsigned m_num_collectors;
+    unsigned m_num_banks;
+    unsigned m_num_collectors;
 
-      allocation_t *m_allocated_bank; // bank # -> register that wins
-      std::list<op_t> *m_queue;
+    allocation_t *m_allocated_bank;  // bank # -> register that wins
+    std::list<op_t> *m_queue;
 
-      unsigned *m_allocator_rr_head; // cu # -> next bank to check for request (rr-arb)
-      unsigned  m_last_cu; // first cu to check while arb-ing banks (rr)
+    unsigned *
+        m_allocator_rr_head;  // cu # -> next bank to check for request (rr-arb)
+    unsigned m_last_cu;       // first cu to check while arb-ing banks (rr)
 
-      int *_inmatch;
-      int *_outmatch;
-      int **_request;
-   };
+    int *_inmatch;
+    int *_outmatch;
+    int **_request;
+  };
 
-   class input_port_t {
+  class input_port_t {
    public:
-       input_port_t(port_vector_t & input, port_vector_t & output, uint_vector_t cu_sets)
-       : m_in(input),m_out(output), m_cu_sets(cu_sets)
-       {
-           assert(input.size() == output.size());
-           assert(not m_cu_sets.empty());
-       }
-   //private:
-       port_vector_t m_in,m_out;
-       uint_vector_t m_cu_sets;
-   };
-
-   class collector_unit_t {
+    input_port_t(port_vector_t &input, port_vector_t &output,
+                 uint_vector_t cu_sets)
+        : m_in(input), m_out(output), m_cu_sets(cu_sets) {
+      assert(input.size() == output.size());
+      assert(not m_cu_sets.empty());
+    }
+    // private:
+    port_vector_t m_in, m_out;
+    uint_vector_t m_cu_sets;
+  };
+
+  class collector_unit_t {
    public:
-      // constructors
-      collector_unit_t()
-      { 
-         m_free = true;
-         m_warp = NULL;
-         m_output_register = NULL;
-         m_src_op = new op_t[MAX_REG_OPERANDS*2];
-         m_not_ready.reset();
-         m_warp_id = -1;
-         m_num_banks = 0;
-         m_bank_warp_shift = 0;
-      }
-      // accessors
-      bool ready() const;
-      const op_t *get_operands() const { return m_src_op; }
-      void dump(FILE *fp, const shader_core_ctx *shader ) const;
-
-      unsigned get_warp_id() const { return m_warp_id; }
-      unsigned get_active_count() const { return m_warp->active_count(); }
-      const active_mask_t & get_active_mask() const { return m_warp->get_active_mask(); }
-      unsigned get_sp_op() const { return m_warp->sp_op; }
-      unsigned get_id() const { return m_cuid; } // returns CU hw id
-      unsigned get_inst_uniq_id() { return m_warp->get_uid(); }
-
-      // modifiers
-      void init(unsigned n, 
-                unsigned num_banks, 
-                unsigned log2_warp_size,
-                const core_config *config,
-                opndcoll_rfu_t *rfu,
-				bool m_sub_core_model,
-				unsigned num_banks_per_sched);
-      bool allocate( register_set* pipeline_reg, register_set* output_reg );
-
-      void collect_operand( unsigned op )
-      {
-         m_not_ready.reset(op);
-      }
-      unsigned get_num_operands() const{
-    	  return m_warp->get_num_operands();
-      }
-      unsigned get_num_regs() const{
-    	  return m_warp->get_num_regs();
-      }
-      void dispatch();
-      bool is_free(){return m_free;}
+    // constructors
+    collector_unit_t() {
+      m_free = true;
+      m_warp = NULL;
+      m_output_register = NULL;
+      m_src_op = new op_t[MAX_REG_OPERANDS * 2];
+      m_not_ready.reset();
+      m_warp_id = -1;
+      m_num_banks = 0;
+      m_bank_warp_shift = 0;
+    }
+    // accessors
+    bool ready() const;
+    const op_t *get_operands() const { return m_src_op; }
+    void dump(FILE *fp, const shader_core_ctx *shader) const;
+
+    unsigned get_warp_id() const { return m_warp_id; }
+    unsigned get_active_count() const { return m_warp->active_count(); }
+    const active_mask_t &get_active_mask() const {
+      return m_warp->get_active_mask();
+    }
+    unsigned get_sp_op() const { return m_warp->sp_op; }
+    unsigned get_id() const { return m_cuid; }  // returns CU hw id
+    unsigned get_reg_id() const { return m_reg_id; }
+    unsigned get_inst_uniq_id() { return m_warp->get_uid(); }
+
+    // modifiers
+    void init(unsigned n, unsigned num_banks, unsigned log2_warp_size,
+              const core_config *config, opndcoll_rfu_t *rfu,
+              bool m_sub_core_model, unsigned reg_id,
+              unsigned num_banks_per_sched);
+    bool allocate(register_set *pipeline_reg, register_set *output_reg);
+
+    void collect_operand(unsigned op) { m_not_ready.reset(op); }
+    unsigned get_num_operands() const { return m_warp->get_num_operands(); }
+    unsigned get_num_regs() const { return m_warp->get_num_regs(); }
+    void dispatch();
+    bool is_free() { return m_free; }
 
    private:
-      bool m_free;
-      unsigned m_cuid; // collector unit hw id
-      unsigned m_warp_id;
-      warp_inst_t  *m_warp;
-      register_set* m_output_register; // pipeline register to issue to when ready
-      op_t *m_src_op;
-      std::bitset<MAX_REG_OPERANDS*2> m_not_ready;
-      unsigned m_num_banks;
-      unsigned m_bank_warp_shift;
-      opndcoll_rfu_t *m_rfu;
-
-      unsigned m_num_banks_per_sched;
-      bool m_sub_core_model;
-
-   };
-
-   class dispatch_unit_t {
+    bool m_free;
+    unsigned m_cuid;  // collector unit hw id
+    unsigned m_warp_id;
+    warp_inst_t *m_warp;
+    register_set
+        *m_output_register;  // pipeline register to issue to when ready
+    op_t *m_src_op;
+    std::bitset<MAX_REG_OPERANDS * 2> m_not_ready;
+    unsigned m_num_banks;
+    unsigned m_bank_warp_shift;
+    opndcoll_rfu_t *m_rfu;
+
+    unsigned m_num_banks_per_sched;
+    bool m_sub_core_model;
+    unsigned m_reg_id;  // if sub_core_model enabled, limit regs this cu can r/w
+  };
+
+  class dispatch_unit_t {
    public:
-      dispatch_unit_t(std::vector<collector_unit_t>* cus) 
-      { 
-         m_last_cu=0;
-         m_collector_units=cus;
-         m_num_collectors = (*cus).size();
-         m_next_cu=0;
-      }
-#if 0
-      collector_unit_t *find_ready()
-      {
-         for( unsigned n=0; n < m_num_collectors; n++ ) {
-            unsigned c=(m_last_cu+n+1)%m_num_collectors;
-            if( (*m_collector_units)[c].ready() ) {
-               m_last_cu=c;
-               return &((*m_collector_units)[c]);
-            }
-         }
-         return NULL;
+    dispatch_unit_t(std::vector<collector_unit_t> *cus) {
+      m_last_cu = 0;
+      m_collector_units = cus;
+      m_num_collectors = (*cus).size();
+      m_next_cu = 0;
+    }
+    void init(bool sub_core_model, unsigned num_warp_scheds) {
+      m_sub_core_model = sub_core_model;
+      m_num_warp_scheds = num_warp_scheds;
+      if (m_sub_core_model) {
+        m_last_cu_set = new unsigned(m_num_warp_scheds);
+        for (unsigned i = 0; i < m_num_warp_scheds; i++)
+        {
+          m_last_cu_set[i] = i * m_num_collectors / m_num_warp_scheds;
+        }
       }
-#endif
-      collector_unit_t *find_ready()
-      {
+      
+    }
+
+    collector_unit_t *find_ready() {
          unsigned least_inst_uid = (unsigned)-1;
          collector_unit_t *to_return = NULL;
+      if (m_sub_core_model) {
+        assert(m_num_collectors % m_num_warp_scheds == 0 &&
+                 m_num_collectors >= m_num_warp_scheds);
+        unsigned cusPerSched = m_num_collectors / m_num_warp_scheds;
+        for (unsigned i = 0; i < m_num_warp_scheds; i++) {
+          unsigned cuLowerBound = i * cusPerSched;
+          unsigned cuUpperBound = cuLowerBound + cusPerSched;
+          assert(0 <= cuLowerBound && cuUpperBound <= m_num_collectors);
+          assert(cuLowerBound <= m_last_cu_set[i] && m_last_cu_set[i] <= cuUpperBound);
+          for (unsigned j = cuLowerBound; j < cuUpperBound; j++) {
+            unsigned c = cuLowerBound + (m_last_cu_set[i] + j + 1) % cusPerSched;
+            if ((*m_collector_units)[c].ready()) {
+
          // gem5-gpu NOTE: gem5-gpu requires that all memory instructions be
          // issued in-order to the load-store queues to correctly enforce
          // fences. GPGPU-Sim did not have this requirement, so this ready
          // instruction select code is different than GPGPU-Sim.
-         for ( unsigned n=0; n < m_num_collectors; n++ ) {
+            // m_last_cu_set[i] = c;
+            // return &((*m_collector_units)[c]);
+            collector_unit_t *cu = &((*m_collector_units)[c]);
+            if ( !cu->is_free() ) {
+               if ( cu->get_inst_uniq_id() < least_inst_uid ) {
+                  least_inst_uid = cu->get_inst_uniq_id();
+                  m_last_cu_set[i] = c;
+                  to_return = cu;
+               }
+            }
+            }
+          }
+        }
+      } else {
+        for (unsigned n = 0; n < m_num_collectors; n++) {
+          unsigned c = (m_last_cu + n + 1) % m_num_collectors;
+          if ((*m_collector_units)[c].ready()) {
+            // m_last_cu = c;
+            // return &((*m_collector_units)[c]);
             collector_unit_t *cu = &((*m_collector_units)[n]);
             if ( !cu->is_free() ) {
                if ( cu->get_inst_uniq_id() < least_inst_uid ) {
                   least_inst_uid = cu->get_inst_uniq_id();
-                  if ( cu->ready() ) {
-                     to_return = cu;
-                  } else {
-                     to_return = NULL;
-                  }
+                  m_last_cu = c;
+                  to_return = cu;
                }
             }
-         }
-         return to_return;
-      }
 
+          }
+        }
+      }
+      return to_return;
+    }
 
    private:
-      unsigned m_num_collectors;
-      std::vector<collector_unit_t>* m_collector_units;
-      unsigned m_last_cu; // dispatch ready cu's rr
-      unsigned m_next_cu;  // for initialization
-   };
-
-   // opndcoll_rfu_t data members
-   bool m_initialized;
-
-   unsigned m_num_collector_sets;
-   //unsigned m_num_collectors;
-   unsigned m_num_banks;
-   unsigned m_bank_warp_shift;
-   unsigned m_warp_size;
-   std::vector<collector_unit_t *> m_cu;
-   arbiter_t m_arbiter;
-
-   unsigned m_num_banks_per_sched;
-   unsigned m_num_warp_sceds;
-   bool sub_core_model;
-
-   //unsigned m_num_ports;
-   //std::vector<warp_inst_t**> m_input;
-   //std::vector<warp_inst_t**> m_output;
-   //std::vector<unsigned> m_num_collector_units;
-   //warp_inst_t **m_alu_port;
-
-   std::vector<input_port_t> m_in_ports;
-   typedef std::map<unsigned /* collector set */, std::vector<collector_unit_t> /*collector sets*/ > cu_sets_t;
-   cu_sets_t m_cus;
-   std::vector<dispatch_unit_t> m_dispatch_units;
-
-   //typedef std::map<warp_inst_t**/*port*/,dispatch_unit_t> port_to_du_t;
-   //port_to_du_t                     m_dispatch_units;
-   //std::map<warp_inst_t**,std::list<collector_unit_t*> > m_free_cu;
-   shader_core_ctx                 *m_shader;
+    unsigned m_num_collectors;
+    std::vector<collector_unit_t> *m_collector_units;
+    unsigned m_last_cu;  // dispatch ready cu's rr
+    unsigned *m_last_cu_set;
+    unsigned m_next_cu;  // for initialization
+
+    bool m_sub_core_model;
+    unsigned m_num_warp_scheds;
+  };
+
+  // opndcoll_rfu_t data members
+  bool m_initialized;
+
+  unsigned m_num_collector_sets;
+  //unsigned m_num_collectors;
+  unsigned m_num_banks;
+  unsigned m_bank_warp_shift;
+  unsigned m_warp_size;
+  std::vector<collector_unit_t *> m_cu;
+  arbiter_t m_arbiter;
+
+  unsigned m_num_banks_per_sched;
+  unsigned m_num_warp_scheds;
+  bool sub_core_model;
+
+  //unsigned m_num_ports;
+  //std::vector<warp_inst_t**> m_input;
+  //std::vector<warp_inst_t**> m_output;
+  //std::vector<unsigned> m_num_collector_units;
+  //warp_inst_t **m_alu_port;
+
+  std::vector<input_port_t> m_in_ports;
+  typedef std::map<unsigned /* collector set */,
+                   std::vector<collector_unit_t> /*collector sets*/>
+      cu_sets_t;
+  cu_sets_t m_cus;
+  std::vector<dispatch_unit_t> m_dispatch_units;
+
+  //typedef std::map<warp_inst_t**/*port*/,dispatch_unit_t> port_to_du_t;
+  //port_to_du_t                     m_dispatch_units;
+  //std::map<warp_inst_t**,std::list<collector_unit_t*> > m_free_cu;
+  shader_core_ctx                 *m_shader;
 };
 
 class barrier_set_t {
 public:
-   barrier_set_t(shader_core_ctx * shader, unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size);
-
-   // during cta allocation
-   void allocate_barrier( unsigned cta_id, warp_set_t warps );
-
-   // during cta deallocation
-   void deallocate_barrier( unsigned cta_id );
-
-   typedef std::map<unsigned, warp_set_t >  cta_to_warp_t;
-   typedef std::map<unsigned, warp_set_t >  bar_id_to_warp_t; /*set of warps reached a specific barrier id*/
-
-
-   // individual warp hits barrier
-   void warp_reaches_barrier( unsigned cta_id, unsigned warp_id, warp_inst_t* inst);
-
-
-   // warp reaches exit 
-   void warp_exit( unsigned warp_id );
-
-   // assertions
-   bool warp_waiting_at_barrier( unsigned warp_id ) const;
-
-   // debug
-   void dump();
-
-private:
-   unsigned m_max_cta_per_core;
-   unsigned m_max_warps_per_core;
-   unsigned m_max_barriers_per_cta;
-   unsigned m_warp_size;
-   cta_to_warp_t m_cta_to_warps;
-   bar_id_to_warp_t m_bar_id_to_warps;
-   warp_set_t m_warp_active;
-   warp_set_t m_warp_at_barrier;
-   shader_core_ctx *m_shader;
-
+  barrier_set_t(shader_core_ctx *shader, unsigned max_warps_per_core,
+                unsigned max_cta_per_core, unsigned max_barriers_per_cta,
+                unsigned warp_size);
+
+  // during cta allocation
+  void allocate_barrier(unsigned cta_id, warp_set_t warps);
+
+  // during cta deallocation
+  void deallocate_barrier(unsigned cta_id);
+
+  typedef std::map<unsigned, warp_set_t> cta_to_warp_t;
+  typedef std::map<unsigned, warp_set_t>
+      bar_id_to_warp_t; /*set of warps reached a specific barrier id*/
+
+  // individual warp hits barrier
+  void warp_reaches_barrier(unsigned cta_id, unsigned warp_id,
+                            warp_inst_t *inst);
+
+  // warp reaches exit
+  void warp_exit(unsigned warp_id);
+
+  // assertions
+  bool warp_waiting_at_barrier(unsigned warp_id) const;
+
+  // debug
+  void dump();
+
+ private:
+  unsigned m_max_cta_per_core;
+  unsigned m_max_warps_per_core;
+  unsigned m_max_barriers_per_cta;
+  unsigned m_warp_size;
+  cta_to_warp_t m_cta_to_warps;
+  bar_id_to_warp_t m_bar_id_to_warps;
+  warp_set_t m_warp_active;
+  warp_set_t m_warp_at_barrier;
+  shader_core_ctx *m_shader;
 };
 
 struct insn_latency_info {
-   unsigned pc;
-   unsigned long latency;
+  unsigned pc;
+  unsigned long latency;
 };
 
 struct ifetch_buffer_t {
-    ifetch_buffer_t() { m_valid=false; }
-
-    ifetch_buffer_t( address_type pc, unsigned nbytes, unsigned warp_id ) 
-    { 
-        m_valid=true; 
-        m_pc=pc; 
-        m_nbytes=nbytes; 
-        m_warp_id=warp_id;
-    }
-
-    bool m_valid;
-    address_type m_pc;
-    unsigned m_nbytes;
-    unsigned m_warp_id;
+  ifetch_buffer_t() { m_valid = false; }
+
+  ifetch_buffer_t(address_type pc, unsigned nbytes, unsigned warp_id) {
+    m_valid = true;
+    m_pc = pc;
+    m_nbytes = nbytes;
+    m_warp_id = warp_id;
+  }
+
+  bool m_valid;
+  address_type m_pc;
+  unsigned m_nbytes;
+  unsigned m_warp_id;
 };
 
-struct shader_core_config;
+class shader_core_config;
 
 class simd_function_unit {
 public:
-    simd_function_unit( const shader_core_config *config );
-    ~simd_function_unit() { delete m_dispatch_reg; }
-
-    // modifiers
-    virtual void issue( register_set& source_reg ) { source_reg.move_out_to(m_dispatch_reg); occupied.set(m_dispatch_reg->latency);}
-    virtual void cycle() = 0;
-    virtual void active_lanes_in_pipeline() = 0;
-
-    // accessors
-    virtual unsigned clock_multiplier() const { return 1; }
-    virtual bool can_issue( const warp_inst_t &inst ) const { return m_dispatch_reg->empty() && !occupied.test(inst.latency); }
-    virtual bool stallable() const = 0;
-    virtual void print( FILE *fp ) const
-    {
-        fprintf(fp,"%s dispatch= ", m_name.c_str() );
-        m_dispatch_reg->print(fp);
-    }
-    const char* get_name() {
-    	return m_name.c_str();
-    }
-protected:
-    std::string m_name;
-    const shader_core_config *m_config;
-    warp_inst_t *m_dispatch_reg;
-    static const unsigned MAX_ALU_LATENCY = 512;
-    std::bitset<MAX_ALU_LATENCY> occupied;
+  simd_function_unit( const shader_core_config *config );
+  ~simd_function_unit() { delete m_dispatch_reg; }
+
+  // modifiers
+  virtual void issue( register_set& source_reg );
+  virtual void cycle() = 0;
+  virtual void active_lanes_in_pipeline() = 0;
+
+  // accessors
+  virtual unsigned clock_multiplier() const { return 1; }
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    return m_dispatch_reg->empty() && !occupied.test(inst.latency);
+  }
+  virtual bool is_issue_partitioned() = 0;
+  virtual unsigned get_issue_reg_id() = 0;
+  virtual bool stallable() const = 0;
+  virtual void print( FILE *fp ) const {
+    fprintf(fp,"%s dispatch= ", m_name.c_str() );
+    m_dispatch_reg->print(fp);
+  }
+  const char *get_name() { return m_name.c_str(); }
+
+ protected:
+  std::string m_name;
+  const shader_core_config *m_config;
+  warp_inst_t *m_dispatch_reg;
+  static const unsigned MAX_ALU_LATENCY = 512;
+  std::bitset<MAX_ALU_LATENCY> occupied;
 };
 
 class pipelined_simd_unit : public simd_function_unit {
-public:
-    pipelined_simd_unit( register_set* result_port, const shader_core_config *config, unsigned max_latency, shader_core_ctx *core );
+ public:
+  pipelined_simd_unit(register_set *result_port,
+                      const shader_core_config *config, unsigned max_latency,
+                      shader_core_ctx *core, unsigned issue_reg_id);
 
-    //modifiers
-    virtual void cycle();
-    virtual void issue( register_set& source_reg );
-    virtual unsigned get_active_lanes_in_pipeline();
+  // modifiers
+  virtual void cycle();
+  virtual void issue(register_set &source_reg);
+  virtual unsigned get_active_lanes_in_pipeline();
 
-    virtual void active_lanes_in_pipeline() = 0;
+  virtual void active_lanes_in_pipeline() = 0;
 /*
     virtual void issue( register_set& source_reg )
     {
@@ -1109,126 +1180,171 @@ public:
         simd_function_unit::issue(source_reg);
     }
 */
-    // accessors
-    virtual bool stallable() const { return false; }
-    virtual bool can_issue( const warp_inst_t &inst ) const
-    {
-        return simd_function_unit::can_issue(inst);
-    }
-    virtual void print(FILE *fp) const
-    {
-        simd_function_unit::print(fp);
-        for( int s=m_pipeline_depth-1; s>=0; s-- ) {
-            if( !m_pipeline_reg[s]->empty() ) { 
-                fprintf(fp,"      %s[%2d] ", m_name.c_str(), s );
-                m_pipeline_reg[s]->print(fp);
-            }
-        }
+  // accessors
+  virtual bool stallable() const { return false; }
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    return simd_function_unit::can_issue(inst);
+  }
+  virtual bool is_issue_partitioned() = 0;
+  unsigned get_issue_reg_id() { return m_issue_reg_id; }
+  virtual void print(FILE *fp) const {
+    simd_function_unit::print(fp);
+    for (int s = m_pipeline_depth - 1; s >= 0; s--) {
+      if (!m_pipeline_reg[s]->empty()) {
+        fprintf(fp, "      %s[%2d] ", m_name.c_str(), s);
+        m_pipeline_reg[s]->print(fp);
+      }
     }
-protected:
-    unsigned m_pipeline_depth;
-    warp_inst_t **m_pipeline_reg;
-    register_set *m_result_port;
-    class shader_core_ctx *m_core;
-
-    unsigned active_insts_in_pipeline;
-
+  }
+ protected:
+  unsigned m_pipeline_depth;
+  warp_inst_t **m_pipeline_reg;
+  register_set *m_result_port;
+  class shader_core_ctx *m_core;
+  unsigned m_issue_reg_id;  // if sub_core_model is enabled we can only issue
+                            // from a subset of operand collectors
+
+  unsigned active_insts_in_pipeline;
 };
 
-class sfu : public pipelined_simd_unit
-{
-public:
-    sfu( register_set* result_port, const shader_core_config *config, shader_core_ctx *core );
-    virtual bool can_issue( const warp_inst_t &inst ) const
-    {
-        switch(inst.op) {
-        case SFU_OP: break;
-        case ALU_SFU_OP: break;
-        case DP_OP: break;     //for compute <= 29 (i..e Fermi and GT200)
-        default: return false;
-        }
-        return pipelined_simd_unit::can_issue(inst);
+class sfu : public pipelined_simd_unit {
+ public:
+  sfu(register_set *result_port, const shader_core_config *config,
+      shader_core_ctx *core, unsigned issue_reg_id);
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    switch (inst.op) {
+      case SFU_OP:
+        break;
+      case ALU_SFU_OP:
+        break;
+      case DP_OP:
+        break;  // for compute <= 29 (i..e Fermi and GT200)
+      default:
+        return false;
     }
-    virtual void active_lanes_in_pipeline();
-    virtual void issue(  register_set& source_reg );
+    return pipelined_simd_unit::can_issue(inst);
+  }
+  virtual void active_lanes_in_pipeline();
+  virtual void issue(register_set &source_reg);
+  bool is_issue_partitioned() { return true; }
 };
 
-class dp_unit : public pipelined_simd_unit
-{
-public:
-	dp_unit( register_set* result_port, const shader_core_config *config, shader_core_ctx *core );
-    virtual bool can_issue( const warp_inst_t &inst ) const
-    {
-        switch(inst.op) {
-        case DP_OP: break;
-        default: return false;
-        }
-        return pipelined_simd_unit::can_issue(inst);
+class dp_unit : public pipelined_simd_unit {
+ public:
+  dp_unit(register_set *result_port, const shader_core_config *config,
+          shader_core_ctx *core, unsigned issue_reg_id);
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    switch (inst.op) {
+      case DP_OP:
+        break;
+      default:
+        return false;
     }
-    virtual void active_lanes_in_pipeline();
-    virtual void issue(  register_set& source_reg );
+    return pipelined_simd_unit::can_issue(inst);
+  }
+  virtual void active_lanes_in_pipeline();
+  virtual void issue(register_set &source_reg);
+  bool is_issue_partitioned() { return true; }
 };
 
-class tensor_core : public pipelined_simd_unit
-{
-public:
-    tensor_core( register_set* result_port, const shader_core_config *config, shader_core_ctx *core );
-    virtual bool can_issue( const warp_inst_t &inst ) const
-    {
-        switch(inst.op) {
-        case TENSOR_CORE_OP: break;
-        default: return false;
-        }
-        return pipelined_simd_unit::can_issue(inst);
+class tensor_core : public pipelined_simd_unit {
+ public:
+  tensor_core(register_set *result_port, const shader_core_config *config,
+              shader_core_ctx *core, unsigned issue_reg_id);
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    switch (inst.op) {
+      case TENSOR_CORE_OP:
+        break;
+      default:
+        return false;
     }
-    virtual void active_lanes_in_pipeline();
-    virtual void issue(  register_set& source_reg );
+    return pipelined_simd_unit::can_issue(inst);
+  }
+  virtual void active_lanes_in_pipeline();
+  virtual void issue(register_set &source_reg);
+  bool is_issue_partitioned() { return true; }
 };
 
 
-class int_unit : public pipelined_simd_unit
-{
-public:
-	int_unit( register_set* result_port, const shader_core_config *config, shader_core_ctx *core );
-    virtual bool can_issue( const warp_inst_t &inst ) const
-    {
-        switch(inst.op) {
-        case SFU_OP: return false;
-	    case LOAD_OP: return false;
-	    case TENSOR_CORE_LOAD_OP: return false;
-		case STORE_OP: return false;
-		case TENSOR_CORE_STORE_OP: return false;
-		case MEMORY_BARRIER_OP: return false;
-	    case SP_OP: return false;
-	    case DP_OP: return false;
-        default: break;
-        }
-        return pipelined_simd_unit::can_issue(inst);
+class int_unit : public pipelined_simd_unit {
+ public:
+  int_unit(register_set *result_port, const shader_core_config *config,
+           shader_core_ctx *core, unsigned issue_reg_id);
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    switch (inst.op) {
+      case SFU_OP:
+        return false;
+      case LOAD_OP:
+        return false;
+      case TENSOR_CORE_LOAD_OP:
+        return false;
+      case STORE_OP:
+        return false;
+      case TENSOR_CORE_STORE_OP:
+        return false;
+      case MEMORY_BARRIER_OP:
+        return false;
+      case SP_OP:
+        return false;
+      case DP_OP:
+        return false;
+      default:
+        break;
     }
-    virtual void active_lanes_in_pipeline();
-    virtual void issue(  register_set& source_reg );
+    return pipelined_simd_unit::can_issue(inst);
+  }
+  virtual void active_lanes_in_pipeline();
+  virtual void issue(register_set &source_reg);
+  bool is_issue_partitioned() { return true; }
 };
 
-class sp_unit : public pipelined_simd_unit
-{
-public:
-    sp_unit( register_set* result_port, const shader_core_config *config, shader_core_ctx *core );
-    virtual bool can_issue( const warp_inst_t &inst ) const
-    {
-        switch(inst.op) {
-        case SFU_OP: return false; 
-        case LOAD_OP: return false;
-        case TENSOR_CORE_LOAD_OP: return false;
-        case STORE_OP: return false;
-        case TENSOR_CORE_STORE_OP: return false;
-        case MEMORY_BARRIER_OP: return false;
-        case DP_OP: return false;
-        default: break;
-        }
-        return pipelined_simd_unit::can_issue(inst);
+class sp_unit : public pipelined_simd_unit {
+ public:
+  sp_unit(register_set *result_port, const shader_core_config *config,
+          shader_core_ctx *core, unsigned issue_reg_id);
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    switch (inst.op) {
+      case SFU_OP:
+        return false;
+      case LOAD_OP:
+        return false;
+      case TENSOR_CORE_LOAD_OP:
+        return false;
+      case STORE_OP:
+        return false;
+      case TENSOR_CORE_STORE_OP:
+        return false;
+      case MEMORY_BARRIER_OP:
+        return false;
+      case DP_OP:
+        return false;
+      default:
+        break;
+    }
+    return pipelined_simd_unit::can_issue(inst);
+  }
+  virtual void active_lanes_in_pipeline();
+  virtual void issue(register_set &source_reg);
+  bool is_issue_partitioned() { return true; }
+};
+
+class specialized_unit : public pipelined_simd_unit {
+ public:
+  specialized_unit(register_set *result_port, const shader_core_config *config,
+                   shader_core_ctx *core, unsigned supported_op,
+                   char *unit_name, unsigned latency, unsigned issue_reg_id);
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    if (inst.op != m_supported_op) {
+      return false;
     }
-    virtual void active_lanes_in_pipeline();
-    virtual void issue( register_set& source_reg );
+    return pipelined_simd_unit::can_issue(inst);
+  }
+  virtual void active_lanes_in_pipeline();
+  virtual void issue(register_set &source_reg);
+  bool is_issue_partitioned() { return true; }
+
+ private:
+  unsigned m_supported_op;
 };
 
 class simt_core_cluster;
@@ -1238,596 +1354,760 @@ class cache_t;
 
 class ldst_unit: public pipelined_simd_unit {
 public:
-    ldst_unit( mem_fetch_interface *icnt,
-               shader_core_mem_fetch_allocator *mf_allocator,
-               shader_core_ctx *core, 
-               opndcoll_rfu_t *operand_collector,
-               Scoreboard *scoreboard,
-               const shader_core_config *config, 
-               const memory_config *mem_config,  
-               class shader_core_stats *stats, 
-               unsigned sid, unsigned tpc );
-
-    // modifiers
-    virtual void issue( register_set &inst );
-    virtual void cycle();
-     
-    void fill( mem_fetch *mf );
-    void flush();
-    void invalidate();
-    void writeback();
+  ldst_unit(mem_fetch_interface *icnt,
+            shader_core_mem_fetch_allocator *mf_allocator,
+            shader_core_ctx *core, opndcoll_rfu_t *operand_collector,
+            Scoreboard *scoreboard, const shader_core_config *config,
+            const memory_config *mem_config, class shader_core_stats *stats,
+            unsigned sid, unsigned tpc);
+
+  // modifiers
+  virtual void issue(register_set &inst);
+  bool is_issue_partitioned() { return false; }
+  virtual void cycle();
+
+  void fill(mem_fetch *mf);
+  void flush();
+  void invalidate();
+  void writeback();
 
     // TODO schi add
     /// Inserts this instruction into the writeback stage of the pipeline
     /// Returns true if successful, false if there is an instruction blocking
     bool writebackInst(warp_inst_t &inst);
 
-    // accessors
-    virtual unsigned clock_multiplier() const;
-
-    virtual bool can_issue( const warp_inst_t &inst ) const
-    {
-        switch(inst.op) {
-        case LOAD_OP: break;
-        case TENSOR_CORE_LOAD_OP: break;
-        case STORE_OP: break;
-        case TENSOR_CORE_STORE_OP: break;
-        case MEMORY_BARRIER_OP: break;
-        default: return false;
-        }
-        return m_dispatch_reg->empty();
+  // accessors
+  virtual unsigned clock_multiplier() const;
+  virtual bool can_issue(const warp_inst_t &inst) const {
+    switch (inst.op) {
+      case LOAD_OP:
+        break;
+      case TENSOR_CORE_LOAD_OP:
+        break;
+      case STORE_OP:
+        break;
+      case TENSOR_CORE_STORE_OP:
+        break;
+      case MEMORY_BARRIER_OP:
+        break;
+      default:
+        return false;
     }
-
-    virtual void active_lanes_in_pipeline();
-    virtual bool stallable() const { return true; }
-    bool response_buffer_full() const;
-    void print(FILE *fout) const;
-    void print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses );
-    void get_cache_stats(unsigned &read_accesses, unsigned &write_accesses, unsigned &read_misses, unsigned &write_misses, unsigned cache_type);
-    void get_cache_stats(cache_stats &cs);
-
-    void get_L1D_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1C_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1T_sub_stats(struct cache_sub_stats &css) const;
+    return m_dispatch_reg->empty();
+  }
+
+  virtual void active_lanes_in_pipeline();
+  virtual bool stallable() const { return true; }
+  bool response_buffer_full() const;
+  void print(FILE *fout) const;
+  void print_cache_stats(FILE *fp, unsigned &dl1_accesses,
+                         unsigned &dl1_misses);
+  void get_cache_stats(unsigned &read_accesses, unsigned &write_accesses,
+                       unsigned &read_misses, unsigned &write_misses,
+                       unsigned cache_type);
+  void get_cache_stats(cache_stats &cs);
+
+  void get_L1D_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1C_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1T_sub_stats(struct cache_sub_stats &css) const;
 
 protected:
-    ldst_unit( mem_fetch_interface *icnt,
-               shader_core_mem_fetch_allocator *mf_allocator,
-               shader_core_ctx *core, 
-               opndcoll_rfu_t *operand_collector,
-               Scoreboard *scoreboard,
-               const shader_core_config *config,
-               const memory_config *mem_config,  
-               shader_core_stats *stats,
-               unsigned sid,
-               unsigned tpc,
-               l1_cache* new_l1d_cache );
-    void init( mem_fetch_interface *icnt,
-               shader_core_mem_fetch_allocator *mf_allocator,
-               shader_core_ctx *core, 
-               opndcoll_rfu_t *operand_collector,
-               Scoreboard *scoreboard,
-               const shader_core_config *config,
-               const memory_config *mem_config,  
-               shader_core_stats *stats,
-               unsigned sid,
-               unsigned tpc );
+  ldst_unit(mem_fetch_interface *icnt,
+            shader_core_mem_fetch_allocator *mf_allocator,
+            shader_core_ctx *core, opndcoll_rfu_t *operand_collector,
+            Scoreboard *scoreboard, const shader_core_config *config,
+            const memory_config *mem_config, shader_core_stats *stats,
+            unsigned sid, unsigned tpc, l1_cache *new_l1d_cache);
+  void init(mem_fetch_interface *icnt,
+            shader_core_mem_fetch_allocator *mf_allocator,
+            shader_core_ctx *core, opndcoll_rfu_t *operand_collector,
+            Scoreboard *scoreboard, const shader_core_config *config,
+            const memory_config *mem_config, shader_core_stats *stats,
+            unsigned sid, unsigned tpc);
+
+
+ protected:
+  bool shared_cycle(warp_inst_t &inst, mem_stage_stall_type &rc_fail,
+                    mem_stage_access_type &fail_type);
+  bool constant_cycle(warp_inst_t &inst, mem_stage_stall_type &rc_fail,
+                      mem_stage_access_type &fail_type);
+  bool texture_cycle(warp_inst_t &inst, mem_stage_stall_type &rc_fail,
+                     mem_stage_access_type &fail_type);
+  bool memory_cycle(warp_inst_t &inst, mem_stage_stall_type &rc_fail,
+                    mem_stage_access_type &fail_type);
 
-protected:
-   bool shared_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
-   bool constant_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
-   bool texture_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
-   bool memory_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
    bool memory_cycle_gem5( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
 
-   virtual mem_stage_stall_type process_cache_access( cache_t* cache,
-                                                      new_addr_type address,
-                                                      warp_inst_t &inst,
-                                                      std::list<cache_event>& events,
-                                                      mem_fetch *mf,
-                                                      enum cache_request_status status );
-   mem_stage_stall_type process_memory_access_queue( cache_t *cache, warp_inst_t &inst );
-   mem_stage_stall_type process_memory_access_queue_l1cache( l1_cache *cache, warp_inst_t &inst );
-
-   const memory_config *m_memory_config;
-   class mem_fetch_interface *m_icnt;
-   shader_core_mem_fetch_allocator *m_mf_allocator;
-   class shader_core_ctx *m_core;
-   unsigned m_sid;
-   unsigned m_tpc;
-
-   tex_cache *m_L1T; // texture cache
-   read_only_cache *m_L1C; // constant cache
-   l1_cache *m_L1D; // data cache
-   std::map<unsigned/*warp_id*/, std::map<unsigned/*regnum*/,unsigned/*count*/> > m_pending_writes;
-   std::list<mem_fetch*> m_response_fifo;
-   opndcoll_rfu_t *m_operand_collector;
-   Scoreboard *m_scoreboard;
-
-   mem_fetch *m_next_global;
-   warp_inst_t m_next_wb;
-   unsigned m_writeback_arb; // round-robin arbiter for writeback contention between L1T, L1C, shared
-   unsigned m_num_writeback_clients;
-
-   enum mem_stage_stall_type m_mem_rc;
-
-   shader_core_stats *m_stats; 
-
-   // for debugging
-   unsigned long long m_last_inst_gpu_sim_cycle;
-   unsigned long long m_last_inst_gpu_tot_sim_cycle;
-
-   std::deque<mem_fetch* > l1_latency_queue;
-   void L1_latency_queue_cycle();
+  virtual mem_stage_stall_type process_cache_access(
+      cache_t *cache, new_addr_type address, warp_inst_t &inst,
+      std::list<cache_event> &events, mem_fetch *mf,
+      enum cache_request_status status);
+  mem_stage_stall_type process_memory_access_queue(cache_t *cache,
+                                                   warp_inst_t &inst);
+  mem_stage_stall_type process_memory_access_queue_l1cache(l1_cache *cache,
+                                                           warp_inst_t &inst);
+
+  const memory_config *m_memory_config;
+  class mem_fetch_interface *m_icnt;
+  shader_core_mem_fetch_allocator *m_mf_allocator;
+  class shader_core_ctx *m_core;
+  unsigned m_sid;
+  unsigned m_tpc;
+
+  tex_cache *m_L1T; // texture cache
+  read_only_cache *m_L1C; // constant cache
+  l1_cache *m_L1D; // data cache
+  std::map<unsigned /*warp_id*/,
+           std::map<unsigned /*regnum*/, unsigned /*count*/>>
+      m_pending_writes;
+  std::list<mem_fetch*> m_response_fifo;
+  opndcoll_rfu_t *m_operand_collector;
+  Scoreboard *m_scoreboard;
+
+  mem_fetch *m_next_global;
+  warp_inst_t m_next_wb;
+  unsigned m_writeback_arb;  // round-robin arbiter for writeback contention
+                             // between L1T, L1C, shared
+  unsigned m_num_writeback_clients;
+
+  enum mem_stage_stall_type m_mem_rc;
+
+  shader_core_stats *m_stats; 
+
+  // for debugging
+  unsigned long long m_last_inst_gpu_sim_cycle;
+  unsigned long long m_last_inst_gpu_tot_sim_cycle;
+
+  std::vector<std::deque<mem_fetch *>> l1_latency_queue;
+  void L1_latency_queue_cycle();
 };
 
 enum pipeline_stage_name_t {
-    ID_OC_SP=0,
-	ID_OC_DP,
-	ID_OC_INT,
-    ID_OC_SFU,  
-    ID_OC_MEM,  
-    OC_EX_SP,
-	OC_EX_DP,
-	OC_EX_INT,
-    OC_EX_SFU,
-    OC_EX_MEM,
-    EX_WB,
-    ID_OC_TENSOR_CORE,  
-    OC_EX_TENSOR_CORE,
-    N_PIPELINE_STAGES 
-    };
-
-const char* const pipeline_stage_name_decode[] = {
-    "ID_OC_SP",
-	"ID_OC_DP",
-	"ID_OC_INT",
-    "ID_OC_SFU",  
-    "ID_OC_MEM",  
-    "OC_EX_SP",
-	"OC_EX_DP",
-	"OC_EX_INT",
-    "OC_EX_SFU",
-    "OC_EX_MEM",
-    "EX_WB",
-    "ID_OC_TENSOR_CORE",  
-    "OC_EX_TENSOR_CORE",
-    "N_PIPELINE_STAGES" 
+  ID_OC_SP = 0,
+  ID_OC_DP,
+  ID_OC_INT,
+  ID_OC_SFU,
+  ID_OC_MEM,
+  OC_EX_SP,
+  OC_EX_DP,
+  OC_EX_INT,
+  OC_EX_SFU,
+  OC_EX_MEM,
+  EX_WB,
+  ID_OC_TENSOR_CORE,
+  OC_EX_TENSOR_CORE,
+  N_PIPELINE_STAGES
 };
 
-struct shader_core_config : public core_config
-{
-    shader_core_config(){
-	pipeline_widths_string = NULL;
-    }
-
-    void init()
-    {
-        int ntok = sscanf(gpgpu_shader_core_pipeline_opt,"%d:%d", 
-                          &n_thread_per_shader,
-                          &warp_size);
-        if(ntok != 2) {
-           printf("GPGPU-Sim uArch: error while parsing configuration string gpgpu_shader_core_pipeline_opt\n");
-           abort();
-	}
-
-	char* toks = new char[100];
-	char* tokd = toks;
-	strcpy(toks,pipeline_widths_string);
-                                  
-	toks = strtok(toks,",");
-
-	/*	Removing the tensorcore pipeline while reading the config files if the tensor core is not available.
-	 	If we won't remove it, old regression will be broken.
-		So to support the legacy config files it's best to handle in this way.
-         */  
-	int num_config_to_read=N_PIPELINE_STAGES-2*(!gpgpu_tensor_core_avail);
-
-        for (unsigned i = 0; i <num_config_to_read; i++) { 
-	    assert(toks);
-	    ntok = sscanf(toks,"%d", &pipe_widths[i]);
-	    assert(ntok == 1); 
-	    toks = strtok(NULL,",");
-	}
-
-	delete[] tokd;
-	
-        if (n_thread_per_shader > MAX_THREAD_PER_SM) {
-           printf("GPGPU-Sim uArch: Error ** increase MAX_THREAD_PER_SM in abstract_hardware_model.h from %u to %u\n", 
-                  MAX_THREAD_PER_SM, n_thread_per_shader);
-           abort();
-        }
-        max_warps_per_shader =  n_thread_per_shader/warp_size;
-        assert( !(n_thread_per_shader % warp_size) );
-
-        set_pipeline_latency();
-        
-	m_L1I_config.init(m_L1I_config.m_config_string,FuncCachePreferNone);
-        m_L1T_config.init(m_L1T_config.m_config_string,FuncCachePreferNone);
-        m_L1C_config.init(m_L1C_config.m_config_string,FuncCachePreferNone);
-        m_L1D_config.init(m_L1D_config.m_config_string,FuncCachePreferNone);
-        gpgpu_cache_texl1_linesize = m_L1T_config.get_line_sz();
-        gpgpu_cache_constl1_linesize = m_L1C_config.get_line_sz();
-        m_valid = true;
-    }
-    void reg_options(class OptionParser * opp );
-    unsigned max_cta( const kernel_info_t &k ) const;
-    unsigned num_shader() const { return n_simt_clusters*n_simt_cores_per_cluster; }
-    unsigned sid_to_cluster( unsigned sid ) const { return sid / n_simt_cores_per_cluster; }
-    unsigned sid_to_cid( unsigned sid )     const { return sid % n_simt_cores_per_cluster; }
-    unsigned cid_to_sid( unsigned cid, unsigned cluster_id ) const { return cluster_id*n_simt_cores_per_cluster + cid; }
-    void set_pipeline_latency();
-
-// data
-    char *gpgpu_shader_core_pipeline_opt;
-    bool gpgpu_perfect_mem;
-    bool gpgpu_clock_gated_reg_file;
-    bool gpgpu_clock_gated_lanes;
-    enum divergence_support_t model;
-    unsigned n_thread_per_shader;
-    unsigned n_regfile_gating_group;
-    unsigned max_warps_per_shader; 
-    unsigned max_cta_per_core; //Limit on number of concurrent CTAs in shader core
-    unsigned max_barriers_per_cta;
-    char * gpgpu_scheduler_string;
-    unsigned gpgpu_shmem_per_block;
-    unsigned gpgpu_registers_per_block;
-    char* pipeline_widths_string;
-    int pipe_widths[N_PIPELINE_STAGES];
-
-    mutable cache_config m_L1I_config;
-    mutable cache_config m_L1T_config;
-    mutable cache_config m_L1C_config;
-    mutable l1d_cache_config m_L1D_config;
-
-    bool gpgpu_dwf_reg_bankconflict;
-
-    int gpgpu_num_sched_per_core;
-    int gpgpu_max_insn_issue_per_warp;
-    bool gpgpu_dual_issue_diff_exec_units;
-
-    //op collector
-    bool enable_specialized_operand_collector;
-    int gpgpu_operand_collector_num_units_sp;
-    int gpgpu_operand_collector_num_units_dp;
-    int gpgpu_operand_collector_num_units_sfu;
-    int gpgpu_operand_collector_num_units_tensor_core;
-    int gpgpu_operand_collector_num_units_mem;
-    int gpgpu_operand_collector_num_units_gen;
-    int gpgpu_operand_collector_num_units_int;
-
-    unsigned int gpgpu_operand_collector_num_in_ports_sp;
-    unsigned int gpgpu_operand_collector_num_in_ports_dp;
-    unsigned int gpgpu_operand_collector_num_in_ports_sfu;
-    unsigned int gpgpu_operand_collector_num_in_ports_tensor_core;
-    unsigned int gpgpu_operand_collector_num_in_ports_mem;
-    unsigned int gpgpu_operand_collector_num_in_ports_gen;
-    unsigned int gpgpu_operand_collector_num_in_ports_int;
-
-    unsigned int gpgpu_operand_collector_num_out_ports_sp;
-    unsigned int gpgpu_operand_collector_num_out_ports_dp;
-    unsigned int gpgpu_operand_collector_num_out_ports_sfu;
-    unsigned int gpgpu_operand_collector_num_out_ports_tensor_core;
-    unsigned int gpgpu_operand_collector_num_out_ports_mem;
-    unsigned int gpgpu_operand_collector_num_out_ports_gen;
-    unsigned int gpgpu_operand_collector_num_out_ports_int;
-
-    int gpgpu_num_sp_units;
-    int gpgpu_tensor_core_avail;
-    int gpgpu_num_dp_units;
-    int gpgpu_num_sfu_units;
-    int gpgpu_num_tensor_core_units;
-    int gpgpu_num_mem_units;
-    int gpgpu_num_int_units;
-
-    //Shader core resources
-    unsigned gpgpu_shader_registers;
-    int gpgpu_warpdistro_shader;
-    int gpgpu_warp_issue_shader;
-    unsigned gpgpu_num_reg_banks;
-    bool gpgpu_reg_bank_use_warp_id;
-    bool gpgpu_local_mem_map;
-    bool gpgpu_ignore_resources_limitation;
-    bool sub_core_model;
-    
-    unsigned max_sp_latency;
-    unsigned max_int_latency;
-    unsigned max_sfu_latency;
-    unsigned max_dp_latency;
-    unsigned max_tensor_core_latency;
-    
-    unsigned n_simt_cores_per_cluster;
-    unsigned n_simt_clusters;
-    unsigned n_simt_ejection_buffer_size;
-    unsigned ldst_unit_response_queue_size;
-
-    int simt_core_sim_order; 
-    
-    unsigned smem_latency;
-
-    unsigned mem2device(unsigned memid) const { return memid + n_simt_clusters; }
-
-    //Jin: concurrent kernel on sm
-    bool gpgpu_concurrent_kernel_sm;
-
-    bool adpative_volta_cache_config;
-
+const char *const pipeline_stage_name_decode[] = {
+    "ID_OC_SP",          "ID_OC_DP",         "ID_OC_INT", "ID_OC_SFU",
+    "ID_OC_MEM",         "OC_EX_SP",         "OC_EX_DP",  "OC_EX_INT",
+    "OC_EX_SFU",         "OC_EX_MEM",        "EX_WB",     "ID_OC_TENSOR_CORE",
+    "OC_EX_TENSOR_CORE", "N_PIPELINE_STAGES"};
+
+struct specialized_unit_params {
+  unsigned latency;
+  unsigned num_units;
+  unsigned id_oc_spec_reg_width;
+  unsigned oc_ex_spec_reg_width;
+  char name[20];
+  unsigned ID_OC_SPEC_ID;
+  unsigned OC_EX_SPEC_ID;
 };
 
-struct shader_core_stats_pod {
-
-	void* shader_core_stats_pod_start[0]; // DO NOT MOVE FROM THE TOP - spaceless pointer to the start of this structure
-	unsigned long long *shader_cycles;
-    unsigned *m_num_sim_insn; // number of scalar thread instructions committed by this shader core
-    unsigned *m_num_sim_winsn; // number of warp instructions committed by this shader core
-	unsigned *m_last_num_sim_insn;
-	unsigned *m_last_num_sim_winsn;
-    unsigned *m_num_decoded_insn; // number of instructions decoded by this shader core
-    float *m_pipeline_duty_cycle;
-    unsigned *m_num_FPdecoded_insn;
-    unsigned *m_num_INTdecoded_insn;
-    unsigned *m_num_storequeued_insn;
-    unsigned *m_num_loadqueued_insn;
-    unsigned *m_num_ialu_acesses;
-    unsigned *m_num_fp_acesses;
-    unsigned *m_num_imul_acesses;
-    unsigned *m_num_tex_inst;
-    unsigned *m_num_fpmul_acesses;
-    unsigned *m_num_idiv_acesses;
-    unsigned *m_num_fpdiv_acesses;
-    unsigned *m_num_sp_acesses;
-    unsigned *m_num_sfu_acesses;
-    unsigned *m_num_tensor_core_acesses;
-    unsigned *m_num_trans_acesses;
-    unsigned *m_num_mem_acesses;
-    unsigned *m_num_sp_committed;
-    unsigned *m_num_tlb_hits;
-    unsigned *m_num_tlb_accesses;
-    unsigned *m_num_sfu_committed;
-    unsigned *m_num_tensor_core_committed;
-    unsigned *m_num_mem_committed;
-    unsigned *m_read_regfile_acesses;
-    unsigned *m_write_regfile_acesses;
-    unsigned *m_non_rf_operands;
-    unsigned *m_num_imul24_acesses;
-    unsigned *m_num_imul32_acesses;
-    unsigned *m_active_sp_lanes;
-    unsigned *m_active_sfu_lanes;
-    unsigned *m_active_tensor_core_lanes;
-    unsigned *m_active_fu_lanes;
-    unsigned *m_active_fu_mem_lanes;
-    unsigned *m_n_diverge;    // number of divergence occurring in this shader
-    unsigned gpgpu_n_load_insn;
-    unsigned gpgpu_n_store_insn;
-    unsigned gpgpu_n_shmem_insn;
-    unsigned gpgpu_n_sstarr_insn;
-    unsigned gpgpu_n_tex_insn;
-    unsigned gpgpu_n_const_insn;
-    unsigned gpgpu_n_param_insn;
-    unsigned gpgpu_n_shmem_bkconflict;
-    unsigned gpgpu_n_cache_bkconflict;
-    int      gpgpu_n_intrawarp_mshr_merge;
-    unsigned gpgpu_n_cmem_portconflict;
-    unsigned gpu_stall_shd_mem_breakdown[N_MEM_STAGE_ACCESS_TYPE][N_MEM_STAGE_STALL_TYPE];
-    unsigned gpu_reg_bank_conflict_stalls;
-    unsigned *shader_cycle_distro;
-    unsigned *last_shader_cycle_distro;
-    unsigned *num_warps_issuable;
-    unsigned gpgpu_n_stall_shd_mem;
-    unsigned* single_issue_nums;
-    unsigned* dual_issue_nums;
-
-    //memory access classification
-    int gpgpu_n_mem_read_local;
-    int gpgpu_n_mem_write_local;
-    int gpgpu_n_mem_texture;
-    int gpgpu_n_mem_const;
-    int gpgpu_n_mem_read_global;
-    int gpgpu_n_mem_write_global;
-    int gpgpu_n_mem_read_inst;
-    
-    int gpgpu_n_mem_l2_writeback;
-    int gpgpu_n_mem_l1_write_allocate; 
-    int gpgpu_n_mem_l2_write_allocate;
-
-    unsigned made_write_mfs;
-    unsigned made_read_mfs;
-
-    unsigned *gpgpu_n_shmem_bank_access;
-    long *n_simt_to_mem; // Interconnect power stats
-    long *n_mem_to_simt;
-};
-
-class shader_core_stats : public shader_core_stats_pod {
-public:
-    shader_core_stats( const shader_core_config *config )
-    {
-        m_config = config;
-        shader_core_stats_pod *pod = reinterpret_cast< shader_core_stats_pod * > ( this->shader_core_stats_pod_start );
-        memset(pod,0,sizeof(shader_core_stats_pod));
-        shader_cycles=(unsigned long long *) calloc(config->num_shader(),sizeof(unsigned long long ));
-        m_num_sim_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_sim_winsn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_last_num_sim_winsn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_last_num_sim_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_pipeline_duty_cycle=(float*) calloc(config->num_shader(),sizeof(float));
-        m_num_decoded_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_FPdecoded_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_storequeued_insn=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_loadqueued_insn=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_INTdecoded_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_ialu_acesses = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_fp_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_tex_inst= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_imul_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_imul24_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_imul32_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_fpmul_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_idiv_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_fpdiv_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_sp_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_sfu_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_tensor_core_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_trans_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_mem_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_sp_committed= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_tlb_hits=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_tlb_accesses=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_active_sp_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_active_sfu_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_active_tensor_core_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_active_fu_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_active_fu_mem_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_sfu_committed= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_tensor_core_committed= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_num_mem_committed= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_read_regfile_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_write_regfile_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_non_rf_operands=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        m_n_diverge = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
-        shader_cycle_distro = (unsigned*) calloc(config->warp_size+3, sizeof(unsigned));
-        last_shader_cycle_distro = (unsigned*) calloc(m_config->warp_size+3, sizeof(unsigned));
-        single_issue_nums = (unsigned*) calloc(config->gpgpu_num_sched_per_core,sizeof(unsigned));
-        dual_issue_nums = (unsigned*) calloc(config->gpgpu_num_sched_per_core, sizeof(unsigned));
-
-        n_simt_to_mem = (long *)calloc(config->num_shader(), sizeof(long));
-        n_mem_to_simt = (long *)calloc(config->num_shader(), sizeof(long));
-
-        m_outgoing_traffic_stats = new traffic_breakdown("coretomem"); 
-        m_incoming_traffic_stats = new traffic_breakdown("memtocore"); 
-
-        gpgpu_n_shmem_bank_access = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
-
-        m_shader_dynamic_warp_issue_distro.resize( config->num_shader() );
-        m_shader_warp_slot_issue_distro.resize( config->num_shader() );
-    }
-
-    ~shader_core_stats()
-    {
-        delete m_outgoing_traffic_stats; 
-        delete m_incoming_traffic_stats; 
-        free(m_num_sim_insn); 
-        free(m_num_sim_winsn);
-        free(m_n_diverge); 
-        free(shader_cycle_distro);
-        free(last_shader_cycle_distro);
-    }
-
-    void new_grid()
-    {
+class shader_core_config : public core_config {
+ public:
+  shader_core_config(gpgpu_context *ctx) : core_config(ctx) {
+    pipeline_widths_string = NULL;
+    gpgpu_ctx = ctx;
+  }
+
+  void init() {
+    int ntok = sscanf(gpgpu_shader_core_pipeline_opt, "%d:%d",
+                      &n_thread_per_shader, &warp_size);
+    if (ntok != 2) {
+      printf(
+          "GPGPU-Sim uArch: error while parsing configuration string "
+          "gpgpu_shader_core_pipeline_opt\n");
+      abort();
     }
 
-    void event_warp_issued( unsigned s_id, unsigned warp_id, unsigned num_issued, unsigned dynamic_warp_id );
+    char *toks = new char[100];
+    char *tokd = toks;
+    strcpy(toks, pipeline_widths_string);
 
-    void visualizer_print( gzFile visualizer_file );
+    toks = strtok(toks, ",");
 
-    void print( FILE *fout ) const;
+    /*	Removing the tensorcore pipeline while reading the config files if the
+       tensor core is not available. If we won't remove it, old regression will
+       be broken. So to support the legacy config files it's best to handle in
+       this way.
+     */
+    int num_config_to_read = N_PIPELINE_STAGES - 2 * (!gpgpu_tensor_core_avail);
 
-    const std::vector< std::vector<unsigned> >& get_dynamic_warp_issue() const
-    {
-        return m_shader_dynamic_warp_issue_distro;
+    for (int i = 0; i < num_config_to_read; i++) {
+      assert(toks);
+      ntok = sscanf(toks, "%d", &pipe_widths[i]);
+      assert(ntok == 1);
+      toks = strtok(NULL, ",");
     }
 
-    const std::vector< std::vector<unsigned> >& get_warp_slot_issue() const
-    {
-        return m_shader_warp_slot_issue_distro;
-    }
+    delete[] tokd;
 
-private:
-    const shader_core_config *m_config;
+    if (n_thread_per_shader > MAX_THREAD_PER_SM) {
+      printf(
+          "GPGPU-Sim uArch: Error ** increase MAX_THREAD_PER_SM in "
+          "abstract_hardware_model.h from %u to %u\n",
+          MAX_THREAD_PER_SM, n_thread_per_shader);
+      abort();
+    }
+    max_warps_per_shader = n_thread_per_shader / warp_size;
+    assert(!(n_thread_per_shader % warp_size));
+
+    set_pipeline_latency();
+
+    m_L1I_config.init(m_L1I_config.m_config_string, FuncCachePreferNone);
+    m_L1T_config.init(m_L1T_config.m_config_string, FuncCachePreferNone);
+    m_L1C_config.init(m_L1C_config.m_config_string, FuncCachePreferNone);
+    m_L1D_config.init(m_L1D_config.m_config_string, FuncCachePreferNone);
+    gpgpu_cache_texl1_linesize = m_L1T_config.get_line_sz();
+    gpgpu_cache_constl1_linesize = m_L1C_config.get_line_sz();
+    m_valid = true;
+
+    m_specialized_unit_num = 0;
+    // parse the specialized units
+    for (unsigned i = 0; i < SPECIALIZED_UNIT_NUM; ++i) {
+      unsigned enabled;
+      specialized_unit_params sparam;
+      sscanf(specialized_unit_string[i], "%u,%u,%u,%u,%u,%s", &enabled,
+             &sparam.num_units, &sparam.latency, &sparam.id_oc_spec_reg_width,
+             &sparam.oc_ex_spec_reg_width, sparam.name);
+
+      if (enabled) {
+        m_specialized_unit.push_back(sparam);
+        strncpy(m_specialized_unit.back().name, sparam.name,
+                sizeof(m_specialized_unit.back().name));
+        m_specialized_unit_num += sparam.num_units;
+      } else
+        break;  // we only accept continuous specialized_units, i.e., 1,2,3,4
+    }
 
-    traffic_breakdown *m_outgoing_traffic_stats; // core to memory partitions
-    traffic_breakdown *m_incoming_traffic_stats; // memory partition to core 
+    // parse gpgpu_shmem_option for adpative cache config
+    if (adaptive_cache_config) {
+      std::stringstream ss(gpgpu_shmem_option);
+      while (ss.good()) {
+        std::string option;
+        std::getline(ss, option, ',');
+        shmem_opt_list.push_back((unsigned)std::stoi(option) * 1024);
+      }
+      std::sort(shmem_opt_list.begin(), shmem_opt_list.end());
+    }
+  }
+  void reg_options(class OptionParser *opp);
+  unsigned max_cta(const kernel_info_t &k) const;
+  unsigned num_shader() const {
+    return n_simt_clusters * n_simt_cores_per_cluster;
+  }
+  unsigned sid_to_cluster(unsigned sid) const {
+    return sid / n_simt_cores_per_cluster;
+  }
+  unsigned sid_to_cid(unsigned sid) const {
+    return sid % n_simt_cores_per_cluster;
+  }
+  unsigned cid_to_sid(unsigned cid, unsigned cluster_id) const {
+    return cluster_id * n_simt_cores_per_cluster + cid;
+  }
+  void set_pipeline_latency();
+
+  // backward pointer
+  class gpgpu_context *gpgpu_ctx;
+  // data
+  char *gpgpu_shader_core_pipeline_opt;
+  bool gpgpu_perfect_mem;
+  bool gpgpu_clock_gated_reg_file;
+  bool gpgpu_clock_gated_lanes;
+  enum divergence_support_t model;
+  unsigned n_thread_per_shader;
+  unsigned n_regfile_gating_group;
+  unsigned max_warps_per_shader;
+  unsigned
+      max_cta_per_core;  // Limit on number of concurrent CTAs in shader core
+  unsigned max_barriers_per_cta;
+  char *gpgpu_scheduler_string;
+  unsigned gpgpu_shmem_per_block;
+  unsigned gpgpu_registers_per_block;
+  char *pipeline_widths_string;
+  int pipe_widths[N_PIPELINE_STAGES];
+
+  mutable cache_config m_L1I_config;
+  mutable cache_config m_L1T_config;
+  mutable cache_config m_L1C_config;
+  mutable l1d_cache_config m_L1D_config;
+
+  bool gpgpu_dwf_reg_bankconflict;
+
+  unsigned gpgpu_num_sched_per_core;
+  int gpgpu_max_insn_issue_per_warp;
+  bool gpgpu_dual_issue_diff_exec_units;
+
+  // op collector
+  bool enable_specialized_operand_collector;
+  int gpgpu_operand_collector_num_units_sp;
+  int gpgpu_operand_collector_num_units_dp;
+  int gpgpu_operand_collector_num_units_sfu;
+  int gpgpu_operand_collector_num_units_tensor_core;
+  int gpgpu_operand_collector_num_units_mem;
+  int gpgpu_operand_collector_num_units_gen;
+  int gpgpu_operand_collector_num_units_int;
+
+  unsigned int gpgpu_operand_collector_num_in_ports_sp;
+  unsigned int gpgpu_operand_collector_num_in_ports_dp;
+  unsigned int gpgpu_operand_collector_num_in_ports_sfu;
+  unsigned int gpgpu_operand_collector_num_in_ports_tensor_core;
+  unsigned int gpgpu_operand_collector_num_in_ports_mem;
+  unsigned int gpgpu_operand_collector_num_in_ports_gen;
+  unsigned int gpgpu_operand_collector_num_in_ports_int;
+
+  unsigned int gpgpu_operand_collector_num_out_ports_sp;
+  unsigned int gpgpu_operand_collector_num_out_ports_dp;
+  unsigned int gpgpu_operand_collector_num_out_ports_sfu;
+  unsigned int gpgpu_operand_collector_num_out_ports_tensor_core;
+  unsigned int gpgpu_operand_collector_num_out_ports_mem;
+  unsigned int gpgpu_operand_collector_num_out_ports_gen;
+  unsigned int gpgpu_operand_collector_num_out_ports_int;
+
+  int gpgpu_num_sp_units;
+  int gpgpu_tensor_core_avail;
+  int gpgpu_num_dp_units;
+  int gpgpu_num_sfu_units;
+  int gpgpu_num_tensor_core_units;
+  int gpgpu_num_mem_units;
+  int gpgpu_num_int_units;
+
+  // Shader core resources
+  unsigned gpgpu_shader_registers;
+  int gpgpu_warpdistro_shader;
+  int gpgpu_warp_issue_shader;
+  unsigned gpgpu_num_reg_banks;
+  bool gpgpu_reg_bank_use_warp_id;
+  bool gpgpu_local_mem_map;
+  bool gpgpu_ignore_resources_limitation;
+  bool sub_core_model;
+
+  unsigned max_sp_latency;
+  unsigned max_int_latency;
+  unsigned max_sfu_latency;
+  unsigned max_dp_latency;
+  unsigned max_tensor_core_latency;
+
+  unsigned n_simt_cores_per_cluster;
+  unsigned n_simt_clusters;
+  unsigned n_simt_ejection_buffer_size;
+  unsigned ldst_unit_response_queue_size;
+
+  int simt_core_sim_order;
+
+  unsigned smem_latency;
+
+  unsigned mem2device(unsigned memid) const { return memid + n_simt_clusters; }
+
+  // Jin: concurrent kernel on sm
+  bool gpgpu_concurrent_kernel_sm;
+
+  bool perfect_inst_const_cache;
+  unsigned inst_fetch_throughput;
+  unsigned reg_file_port_throughput;
+
+  // specialized unit config strings
+  char *specialized_unit_string[SPECIALIZED_UNIT_NUM];
+  mutable std::vector<specialized_unit_params> m_specialized_unit;
+  unsigned m_specialized_unit_num;
+};
 
-    // Counts the instructions issued for each dynamic warp.
-    std::vector< std::vector<unsigned> > m_shader_dynamic_warp_issue_distro;
-    std::vector<unsigned> m_last_shader_dynamic_warp_issue_distro;
-    std::vector< std::vector<unsigned> > m_shader_warp_slot_issue_distro;
-    std::vector<unsigned> m_last_shader_warp_slot_issue_distro;
+struct shader_core_stats_pod {
+  void *
+      shader_core_stats_pod_start[0];  // DO NOT MOVE FROM THE TOP - spaceless
+                                       // pointer to the start of this structure
+  unsigned long long *shader_cycles;
+  unsigned *m_num_sim_insn;   // number of scalar thread instructions committed
+                              // by this shader core
+  unsigned *m_num_sim_winsn;  // number of warp instructions committed by this
+                              // shader core
+  unsigned *m_last_num_sim_insn;
+  unsigned *m_last_num_sim_winsn;
+  unsigned *
+      m_num_decoded_insn;  // number of instructions decoded by this shader core
+  float *m_pipeline_duty_cycle;
+  unsigned *m_num_FPdecoded_insn;
+  unsigned *m_num_INTdecoded_insn;
+  unsigned *m_num_storequeued_insn;
+  unsigned *m_num_loadqueued_insn;
+  unsigned *m_num_tex_inst;
+  double *m_num_ialu_acesses;
+  double *m_num_fp_acesses;
+  double *m_num_imul_acesses;
+  double *m_num_fpmul_acesses;
+  double *m_num_idiv_acesses;
+  double *m_num_fpdiv_acesses;
+  double *m_num_sp_acesses;
+  double *m_num_sfu_acesses;
+  double *m_num_tensor_core_acesses;
+  double *m_num_tex_acesses;
+  double *m_num_const_acesses;
+  double *m_num_dp_acesses;
+  double *m_num_dpmul_acesses;
+  double *m_num_dpdiv_acesses;
+  double *m_num_sqrt_acesses;
+  double *m_num_log_acesses;
+  double *m_num_sin_acesses;
+  double *m_num_exp_acesses;
+  double *m_num_mem_acesses;
+  unsigned *m_num_sp_committed;
+  unsigned *m_num_tlb_hits;
+  unsigned *m_num_tlb_accesses;
+  unsigned *m_num_sfu_committed;
+  unsigned *m_num_tensor_core_committed;
+  unsigned *m_num_mem_committed;
+  unsigned *m_read_regfile_acesses;
+  unsigned *m_write_regfile_acesses;
+  unsigned *m_non_rf_operands;
+  double *m_num_imul24_acesses;
+  double *m_num_imul32_acesses;
+  unsigned *m_active_sp_lanes;
+  unsigned *m_active_sfu_lanes;
+  unsigned *m_active_tensor_core_lanes;
+  unsigned *m_active_fu_lanes;
+  unsigned *m_active_fu_mem_lanes;
+  double *m_active_exu_threads; //For power model
+  double *m_active_exu_warps; //For power model
+  unsigned *m_n_diverge;  // number of divergence occurring in this shader
+  unsigned gpgpu_n_load_insn;
+  unsigned gpgpu_n_store_insn;
+  unsigned gpgpu_n_shmem_insn;
+  unsigned gpgpu_n_sstarr_insn;
+  unsigned gpgpu_n_tex_insn;
+  unsigned gpgpu_n_const_insn;
+  unsigned gpgpu_n_param_insn;
+  unsigned gpgpu_n_shmem_bkconflict;
+  unsigned gpgpu_n_cache_bkconflict;
+  int gpgpu_n_intrawarp_mshr_merge;
+  unsigned gpgpu_n_cmem_portconflict;
+  unsigned gpu_stall_shd_mem_breakdown[N_MEM_STAGE_ACCESS_TYPE]
+                                      [N_MEM_STAGE_STALL_TYPE];
+  unsigned gpu_reg_bank_conflict_stalls;
+  unsigned *shader_cycle_distro;
+  unsigned *last_shader_cycle_distro;
+  unsigned *num_warps_issuable;
+  unsigned gpgpu_n_stall_shd_mem;
+  unsigned *single_issue_nums;
+  unsigned *dual_issue_nums;
+
+  unsigned ctas_completed;
+  // memory access classification
+  int gpgpu_n_mem_read_local;
+  int gpgpu_n_mem_write_local;
+  int gpgpu_n_mem_texture;
+  int gpgpu_n_mem_const;
+  int gpgpu_n_mem_read_global;
+  int gpgpu_n_mem_write_global;
+  int gpgpu_n_mem_read_inst;
+
+  int gpgpu_n_mem_l2_writeback;
+  int gpgpu_n_mem_l1_write_allocate;
+  int gpgpu_n_mem_l2_write_allocate;
+
+  unsigned made_write_mfs;
+  unsigned made_read_mfs;
+
+  unsigned *gpgpu_n_shmem_bank_access;
+  long *n_simt_to_mem;  // Interconnect power stats
+  long *n_mem_to_simt;
+};
 
-    friend class power_stat_t;
-    friend class shader_core_ctx;
-    friend class ldst_unit;
-    friend class simt_core_cluster;
-    friend class scheduler_unit;
-    friend class TwoLevelScheduler;
-    friend class LooseRoundRobbinScheduler;
+class shader_core_stats : public shader_core_stats_pod {
+ public:
+  shader_core_stats(const shader_core_config *config) {
+    m_config = config;
+    shader_core_stats_pod *pod = reinterpret_cast<shader_core_stats_pod *>(
+        this->shader_core_stats_pod_start);
+    memset(pod, 0, sizeof(shader_core_stats_pod));
+    shader_cycles = (unsigned long long *)calloc(config->num_shader(),
+                                                 sizeof(unsigned long long));
+    m_num_sim_insn = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_sim_winsn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_last_num_sim_winsn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_last_num_sim_insn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_pipeline_duty_cycle =
+        (float *)calloc(config->num_shader(), sizeof(float));
+    m_num_decoded_insn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_FPdecoded_insn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_storequeued_insn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_loadqueued_insn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_tex_inst = 
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_INTdecoded_insn =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_ialu_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_fp_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_imul_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_imul24_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_imul32_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_fpmul_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_idiv_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_fpdiv_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_dp_acesses = 
+        (double*) calloc(config->num_shader(),sizeof(double));
+    m_num_dpmul_acesses = 
+        (double*) calloc(config->num_shader(),sizeof(double));
+    m_num_dpdiv_acesses = 
+        (double*) calloc(config->num_shader(),sizeof(double));
+    m_num_sp_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_sfu_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_tensor_core_acesses = 
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_const_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_tex_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_sqrt_acesses = 
+        (double*) calloc(config->num_shader(),sizeof(double));
+    m_num_log_acesses = 
+        (double*) calloc(config->num_shader(),sizeof(double));
+    m_num_sin_acesses = 
+        (double*) calloc(config->num_shader(),sizeof(double));
+    m_num_exp_acesses = 
+        (double*) calloc(config->num_shader(),sizeof(double));
+    m_num_mem_acesses =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_num_sp_committed =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_tlb_hits = 
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_tlb_accesses =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_active_sp_lanes =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_active_sfu_lanes =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_active_tensor_core_lanes =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_active_fu_lanes =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_active_exu_threads =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_active_exu_warps =
+        (double *)calloc(config->num_shader(), sizeof(double));
+    m_active_fu_mem_lanes =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_sfu_committed =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_tensor_core_committed =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_num_mem_committed =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_read_regfile_acesses =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_write_regfile_acesses =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_non_rf_operands =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    m_n_diverge = 
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+    shader_cycle_distro =
+        (unsigned *)calloc(config->warp_size + 3, sizeof(unsigned));
+    last_shader_cycle_distro =
+        (unsigned *)calloc(m_config->warp_size + 3, sizeof(unsigned));
+    single_issue_nums =
+        (unsigned *)calloc(config->gpgpu_num_sched_per_core, sizeof(unsigned));
+    dual_issue_nums =
+        (unsigned *)calloc(config->gpgpu_num_sched_per_core, sizeof(unsigned));
+
+    ctas_completed = 0;
+    n_simt_to_mem = (long *)calloc(config->num_shader(), sizeof(long));
+    n_mem_to_simt = (long *)calloc(config->num_shader(), sizeof(long));
+
+    m_outgoing_traffic_stats = new traffic_breakdown("coretomem");
+    m_incoming_traffic_stats = new traffic_breakdown("memtocore");
+
+    gpgpu_n_shmem_bank_access =
+        (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+
+    m_shader_dynamic_warp_issue_distro.resize(config->num_shader());
+    m_shader_warp_slot_issue_distro.resize(config->num_shader());
+  }
+
+  ~shader_core_stats() {
+    delete m_outgoing_traffic_stats;
+    delete m_incoming_traffic_stats;
+    free(m_num_sim_insn);
+    free(m_num_sim_winsn);
+    free(m_num_FPdecoded_insn);
+    free(m_num_INTdecoded_insn);
+    free(m_num_storequeued_insn);
+    free(m_num_loadqueued_insn);
+    free(m_num_ialu_acesses);
+    free(m_num_fp_acesses);
+    free(m_num_imul_acesses);
+    free(m_num_tex_inst);
+    free(m_num_fpmul_acesses);
+    free(m_num_idiv_acesses);
+    free(m_num_fpdiv_acesses);
+    free(m_num_sp_acesses);
+    free(m_num_sfu_acesses);
+    free(m_num_tensor_core_acesses);
+    free(m_num_tex_acesses);
+    free(m_num_const_acesses);
+    free(m_num_dp_acesses);
+    free(m_num_dpmul_acesses);
+    free(m_num_dpdiv_acesses);
+    free(m_num_sqrt_acesses);
+    free(m_num_log_acesses);
+    free(m_num_sin_acesses);
+    free(m_num_exp_acesses);
+    free(m_num_mem_acesses);
+    free(m_num_sp_committed);
+    free(m_num_tlb_hits);
+    free(m_num_tlb_accesses);
+    free(m_num_sfu_committed);
+    free(m_num_tensor_core_committed);
+    free(m_num_mem_committed);
+    free(m_read_regfile_acesses);
+    free(m_write_regfile_acesses);
+    free(m_non_rf_operands);
+    free(m_num_imul24_acesses);
+    free(m_num_imul32_acesses);
+    free(m_active_sp_lanes);
+    free(m_active_sfu_lanes);
+    free(m_active_tensor_core_lanes);
+    free(m_active_fu_lanes);
+    free(m_active_exu_threads);
+    free(m_active_exu_warps);
+    free(m_active_fu_mem_lanes);
+    free(m_n_diverge);
+    free(shader_cycle_distro);
+    free(last_shader_cycle_distro);
+  }
+
+  void new_grid() {}
+
+  void event_warp_issued(unsigned s_id, unsigned warp_id, unsigned num_issued,
+                         unsigned dynamic_warp_id);
+
+  void visualizer_print(gzFile visualizer_file);
+
+  void print(FILE *fout) const;
+
+  const std::vector<std::vector<unsigned>> &get_dynamic_warp_issue() const {
+    return m_shader_dynamic_warp_issue_distro;
+  }
+
+  const std::vector<std::vector<unsigned>> &get_warp_slot_issue() const {
+    return m_shader_warp_slot_issue_distro;
+  }
+
+ private:
+  const shader_core_config *m_config;
+
+  traffic_breakdown *m_outgoing_traffic_stats;  // core to memory partitions
+  traffic_breakdown *m_incoming_traffic_stats;  // memory partition to core
+
+  // Counts the instructions issued for each dynamic warp.
+  std::vector<std::vector<unsigned>> m_shader_dynamic_warp_issue_distro;
+  std::vector<unsigned> m_last_shader_dynamic_warp_issue_distro;
+  std::vector<std::vector<unsigned>> m_shader_warp_slot_issue_distro;
+  std::vector<unsigned> m_last_shader_warp_slot_issue_distro;
+
+  friend class power_stat_t;
+  friend class shader_core_ctx;
+  friend class ldst_unit;
+  friend class simt_core_cluster;
+  friend class scheduler_unit;
+  friend class TwoLevelScheduler;
+  friend class LooseRoundRobbinScheduler;
 };
 
+class memory_config;
 class shader_core_mem_fetch_allocator : public mem_fetch_allocator {
-public:
-    shader_core_mem_fetch_allocator( unsigned core_id, unsigned cluster_id, const memory_config *config )
-    {
-    	m_core_id = core_id;
-    	m_cluster_id = cluster_id;
-    	m_memory_config = config;
-    }
-    mem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const 
-    {
-    	mem_access_t access( type, addr, size, wr );
-    	mem_fetch *mf = new mem_fetch( access, 
-    				       NULL,
-    				       wr?WRITE_PACKET_SIZE:READ_PACKET_SIZE, 
-    				       -1, 
-    				       m_core_id, 
-    				       m_cluster_id,
-    				       m_memory_config );
-    	return mf;
-    }
-    
-    mem_fetch *alloc( const warp_inst_t &inst, const mem_access_t &access ) const
-    {
-        warp_inst_t inst_copy = inst;
-        mem_fetch *mf = new mem_fetch(access, 
-                                      &inst_copy, 
-                                      access.is_write()?WRITE_PACKET_SIZE:READ_PACKET_SIZE,
-                                      inst.warp_id(),
-                                      m_core_id, 
-                                      m_cluster_id, 
-                                      m_memory_config);
-        return mf;
-    }
+ public:
+  shader_core_mem_fetch_allocator(unsigned core_id, unsigned cluster_id,
+                                  const memory_config *config) {
+    m_core_id = core_id;
+    m_cluster_id = cluster_id;
+    m_memory_config = config;
+  }
+  mem_fetch *alloc(new_addr_type addr, mem_access_type type, unsigned size,
+                   bool wr, unsigned long long cycle) const;
+  mem_fetch *alloc(new_addr_type addr, mem_access_type type,
+                   const active_mask_t &active_mask,
+                   const mem_access_byte_mask_t &byte_mask,
+                   const mem_access_sector_mask_t &sector_mask, unsigned size,
+                   bool wr, unsigned long long cycle, unsigned wid,
+                   unsigned sid, unsigned tpc, mem_fetch *original_mf) const;
+  mem_fetch *alloc(const warp_inst_t &inst, const mem_access_t &access,
+                   unsigned long long cycle) const {
+    warp_inst_t inst_copy = inst;
+    mem_fetch *mf = new mem_fetch(
+        access, &inst_copy,
+        access.is_write() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE,
+        inst.warp_id(), m_core_id, m_cluster_id, m_memory_config, cycle);
+    return mf;
+  }
 
 private:
-    unsigned m_core_id;
-    unsigned m_cluster_id;
-    const memory_config *m_memory_config;
+  unsigned m_core_id;
+  unsigned m_cluster_id;
+  const memory_config *m_memory_config;
 };
 
 class shader_core_ctx : public core_t {
-public:
-    // creator:
-    shader_core_ctx( class gpgpu_sim *gpu,
-                     class simt_core_cluster *cluster,
-                     unsigned shader_id,
-                     unsigned tpc_id,
-                     const struct shader_core_config *config,
-                     const struct memory_config *mem_config,
-                     shader_core_stats *stats );
-
-// used by simt_core_cluster:
-    // modifiers
-    void cycle();
-    void reinit(unsigned start_thread, unsigned end_thread, bool reset_not_completed );
-    void issue_block2core( class kernel_info_t &kernel );
-
-    void cache_flush();
-    void cache_invalidate();
-    void accept_fetch_response( mem_fetch *mf );
-    void accept_ldst_unit_response( class mem_fetch * mf );
-    void broadcast_barrier_reduction(unsigned cta_id, unsigned bar_id,warp_set_t warps);
+ public:
+  // creator:
+  shader_core_ctx(class gpgpu_sim *gpu, class simt_core_cluster *cluster,
+                  unsigned shader_id, unsigned tpc_id,
+                  const shader_core_config *config,
+                  const memory_config *mem_config, shader_core_stats *stats);
+
+  // used by simt_core_cluster:
+  // modifiers
+  void cycle();
+  void reinit(unsigned start_thread, unsigned end_thread,
+              bool reset_not_completed);
+  void issue_block2core(class kernel_info_t &kernel);
+
+  void cache_flush();
+  void cache_invalidate();
+  void accept_fetch_response(mem_fetch *mf);
+  void accept_ldst_unit_response(class mem_fetch *mf);
+  void broadcast_barrier_reduction(unsigned cta_id, unsigned bar_id,
+                                   warp_set_t warps);
 
     // TODO schi add
     bool ldst_unit_wb_inst(warp_inst_t &inst) { return m_ldst_unit->writebackInst(inst); }
 
-    void set_kernel( kernel_info_t *k ) 
-    {
-        assert(k);
-        m_kernel=k; 
-//        k->inc_running(); 
-        printf("GPGPU-Sim uArch: Shader %d bind to kernel %u \'%s\'\n", m_sid, m_kernel->get_uid(),
-                 m_kernel->name().c_str() );
-    }
+  void set_kernel(kernel_info_t *k) {
+    assert(k);
+    m_kernel = k;
+    //        k->inc_running();
+    printf("GPGPU-Sim uArch: Shader %d bind to kernel %u \'%s\'\n", m_sid,
+           m_kernel->get_uid(), m_kernel->name().c_str());
+  }
 
 
     // TODO schi 
@@ -1836,135 +2116,259 @@ public:
     void start_kernel_finish();
     void finish_kernel();
     bool kernel_finish_issued() { return m_kernel_finishing; }
-    // accessors
-    bool fetch_unit_response_buffer_full() const;
-    bool ldst_unit_response_buffer_full() const;
-    unsigned get_not_completed() const { return m_not_completed; }
-    unsigned get_n_active_cta() const { return m_n_active_cta; }
-    unsigned isactive() const {if(m_n_active_cta>0) return 1; else return 0;}
-    kernel_info_t *get_kernel() { return m_kernel; }
-    unsigned get_sid() const {return m_sid;}
-
-// used by functional simulation:
-    // modifiers
-    virtual void warp_exit( unsigned warp_id );
-    
-    // accessors
-    virtual bool warp_waiting_at_barrier( unsigned warp_id ) const;
+  PowerscalingCoefficients *scaling_coeffs;
+  // accessors
+  bool fetch_unit_response_buffer_full() const;
+  bool ldst_unit_response_buffer_full() const;
+  unsigned get_not_completed() const { return m_not_completed; }
+  unsigned get_n_active_cta() const { return m_n_active_cta; }
+  unsigned isactive() const {
+    if (m_n_active_cta > 0)
+      return 1;
+    else
+      return 0;
+  }
+  kernel_info_t *get_kernel() { return m_kernel; }
+  unsigned get_sid() const { return m_sid; }
+
+  // used by functional simulation:
+  // modifiers
+  virtual void warp_exit(unsigned warp_id);
+
+  // accessors
+  virtual bool warp_waiting_at_barrier(unsigned warp_id) const;
 
     // TODO schi add
     void warp_reaches_barrier(warp_inst_t &inst);
     bool fence_unblock_needed(unsigned warp_id) {
-        return m_warp[warp_id].get_membar();
+        return m_warp[warp_id]->get_membar();
     }
     void complete_fence(unsigned warp_id) {
-        assert(m_warp[warp_id].get_membar());
-        m_warp[warp_id].clear_membar();
+        assert(m_warp[warp_id]->get_membar());
+        m_warp[warp_id]->clear_membar();
     }
 
 
-    void get_pdom_stack_top_info( unsigned tid, unsigned *pc, unsigned *rpc ) const;
-    float get_current_occupancy( unsigned long long & active, unsigned long long & total ) const;
-
-// used by pipeline timing model components:
-    // modifiers
-    void mem_instruction_stats(const warp_inst_t &inst);
-    void decrement_atomic_count( unsigned wid, unsigned n );
-    void inc_store_req( unsigned warp_id) { m_warp[warp_id].inc_store_req(); }
-    void dec_inst_in_pipeline( unsigned warp_id ) { m_warp[warp_id].dec_inst_in_pipeline(); } // also used in writeback()
-    void store_ack( class mem_fetch *mf );
-    bool warp_waiting_at_mem_barrier( unsigned warp_id );
-    void set_max_cta( const kernel_info_t &kernel );
-    void warp_inst_complete(const warp_inst_t &inst);
-    
-    // accessors
-    std::list<unsigned> get_regs_written( const inst_t &fvt ) const;
-    const shader_core_config *get_config() const { return m_config; }
-    void print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses );
+  void get_pdom_stack_top_info(unsigned tid, unsigned *pc, unsigned *rpc) const;
+  float get_current_occupancy(unsigned long long &active,
+                              unsigned long long &total) const;
+
+  // used by pipeline timing model components:
+  // modifiers
+  void mem_instruction_stats(const warp_inst_t &inst);
+  void decrement_atomic_count(unsigned wid, unsigned n);
+  void inc_store_req(unsigned warp_id) { m_warp[warp_id]->inc_store_req(); }
+  void dec_inst_in_pipeline(unsigned warp_id) {
+    m_warp[warp_id]->dec_inst_in_pipeline();
+  }  // also used in writeback()
+  void store_ack(class mem_fetch *mf);
+  bool warp_waiting_at_mem_barrier(unsigned warp_id);
+  void set_max_cta(const kernel_info_t &kernel);
+  void warp_inst_complete(const warp_inst_t &inst);
+
+  // accessors
+  std::list<unsigned> get_regs_written(const inst_t &fvt) const;
+  const shader_core_config *get_config() const { return m_config; }
+  void print_cache_stats(FILE *fp, unsigned &dl1_accesses,
+                         unsigned &dl1_misses);
+
+  void get_cache_stats(cache_stats &cs);
+  void get_L1I_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1D_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1C_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1T_sub_stats(struct cache_sub_stats &css) const;
+
+  void get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const;
+
+
+  // debug:
+  void display_simt_state(FILE *fout, int mask) const;
+  void display_pipeline(FILE *fout, int print_mem, int mask3bit) const;
+
+  void incload_stat() { m_stats->m_num_loadqueued_insn[m_sid]++; }
+  void incstore_stat() { m_stats->m_num_storequeued_insn[m_sid]++; }
+  void incialu_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_ialu_acesses[m_sid]=m_stats->m_num_ialu_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_nonsfu(active_count, latency);
+    }else {
+      m_stats->m_num_ialu_acesses[m_sid]=m_stats->m_num_ialu_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+  }
+  void incimul_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_imul_acesses[m_sid]=m_stats->m_num_imul_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_nonsfu(active_count, latency);
+    }else {
+      m_stats->m_num_imul_acesses[m_sid]=m_stats->m_num_imul_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+  }
+  void incimul24_stat(unsigned active_count,double latency) {
+  if(m_config->gpgpu_clock_gated_lanes==false){
+    m_stats->m_num_imul24_acesses[m_sid]=m_stats->m_num_imul24_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_nonsfu(active_count, latency);
+    }else {
+      m_stats->m_num_imul24_acesses[m_sid]=m_stats->m_num_imul24_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;    
+   }
+   void incimul32_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_imul32_acesses[m_sid]=m_stats->m_num_imul32_acesses[m_sid]+(double)active_count*latency
+         + inactive_lanes_accesses_sfu(active_count, latency);          
+    }else{
+      m_stats->m_num_imul32_acesses[m_sid]=m_stats->m_num_imul32_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+  }
+   void incidiv_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_idiv_acesses[m_sid]=m_stats->m_num_idiv_acesses[m_sid]+(double)active_count*latency
+         + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else {
+      m_stats->m_num_idiv_acesses[m_sid]=m_stats->m_num_idiv_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;    
+  }
+   void incfpalu_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_fp_acesses[m_sid]=m_stats->m_num_fp_acesses[m_sid]+(double)active_count*latency
+         + inactive_lanes_accesses_nonsfu(active_count, latency);
+    }else {
+    m_stats->m_num_fp_acesses[m_sid]=m_stats->m_num_fp_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;     
+  }
+   void incfpmul_stat(unsigned active_count,double latency) {
+              // printf("FP MUL stat increament\n");
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_fpmul_acesses[m_sid]=m_stats->m_num_fpmul_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_nonsfu(active_count, latency);
+    }else {
+    m_stats->m_num_fpmul_acesses[m_sid]=m_stats->m_num_fpmul_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+   }
+   void incfpdiv_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_fpdiv_acesses[m_sid]=m_stats->m_num_fpdiv_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else {
+      m_stats->m_num_fpdiv_acesses[m_sid]=m_stats->m_num_fpdiv_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+   }
+   void incdpalu_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_dp_acesses[m_sid]=m_stats->m_num_dp_acesses[m_sid]+(double)active_count*latency
+         + inactive_lanes_accesses_nonsfu(active_count, latency);
+    }else {
+    m_stats->m_num_dp_acesses[m_sid]=m_stats->m_num_dp_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++; 
+   }
+   void incdpmul_stat(unsigned active_count,double latency) {
+              // printf("FP MUL stat increament\n");
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_dpmul_acesses[m_sid]=m_stats->m_num_dpmul_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_nonsfu(active_count, latency);
+    }else {
+    m_stats->m_num_dpmul_acesses[m_sid]=m_stats->m_num_dpmul_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+   }
+   void incdpdiv_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_dpdiv_acesses[m_sid]=m_stats->m_num_dpdiv_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else {
+      m_stats->m_num_dpdiv_acesses[m_sid]=m_stats->m_num_dpdiv_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+   }
+   void incsqrt_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_sqrt_acesses[m_sid]=m_stats->m_num_sqrt_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else{
+      m_stats->m_num_sqrt_acesses[m_sid]=m_stats->m_num_sqrt_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+   }
+   void inclog_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_log_acesses[m_sid]=m_stats->m_num_log_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else{
+      m_stats->m_num_log_acesses[m_sid]=m_stats->m_num_log_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+   }
 
-    void get_cache_stats(cache_stats &cs);
-    void get_L1I_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1D_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1C_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1T_sub_stats(struct cache_sub_stats &css) const;
+   void incexp_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_exp_acesses[m_sid]=m_stats->m_num_exp_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else{
+      m_stats->m_num_exp_acesses[m_sid]=m_stats->m_num_exp_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+   }
 
-    void get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const;
+   void incsin_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_sin_acesses[m_sid]=m_stats->m_num_sin_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else{
+      m_stats->m_num_sin_acesses[m_sid]=m_stats->m_num_sin_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+  }
 
-// debug:
-    void display_simt_state(FILE *fout, int mask ) const;
-    void display_pipeline( FILE *fout, int print_mem, int mask3bit ) const;
 
-    void incload_stat() {m_stats->m_num_loadqueued_insn[m_sid]++;}
-    void incstore_stat() {m_stats->m_num_storequeued_insn[m_sid]++;}
-    void incialu_stat(unsigned active_count,double latency) {
-		if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_ialu_acesses[m_sid]=m_stats->m_num_ialu_acesses[m_sid]+active_count*latency
-		    + inactive_lanes_accesses_nonsfu(active_count, latency);
-		}else {
-        m_stats->m_num_ialu_acesses[m_sid]=m_stats->m_num_ialu_acesses[m_sid]+active_count*latency;
-		}
-	 }
-    void inctex_stat(unsigned active_count,double latency){
-    	m_stats->m_num_tex_inst[m_sid]=m_stats->m_num_tex_inst[m_sid]+active_count*latency;
+   void inctensor_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_tensor_core_acesses[m_sid]=m_stats->m_num_tensor_core_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else{
+      m_stats->m_num_tensor_core_acesses[m_sid]=m_stats->m_num_tensor_core_acesses[m_sid]+(double)active_count*latency;
     }
-    void incimul_stat(unsigned active_count,double latency) {
-		if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_imul_acesses[m_sid]=m_stats->m_num_imul_acesses[m_sid]+active_count*latency
-		    + inactive_lanes_accesses_nonsfu(active_count, latency);
-		}else {
-        m_stats->m_num_imul_acesses[m_sid]=m_stats->m_num_imul_acesses[m_sid]+active_count*latency;
-		}
-	 }
-    void incimul24_stat(unsigned active_count,double latency) {
-      if(m_config->gpgpu_clock_gated_lanes==false){
-   		m_stats->m_num_imul24_acesses[m_sid]=m_stats->m_num_imul24_acesses[m_sid]+active_count*latency
-		    + inactive_lanes_accesses_nonsfu(active_count, latency);
-		}else {
-		  m_stats->m_num_imul24_acesses[m_sid]=m_stats->m_num_imul24_acesses[m_sid]+active_count*latency;
-		}
-	 }
-	 void incimul32_stat(unsigned active_count,double latency) {
-		if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_imul32_acesses[m_sid]=m_stats->m_num_imul32_acesses[m_sid]+active_count*latency
-			 + inactive_lanes_accesses_sfu(active_count, latency);			
-		}else{
-		  m_stats->m_num_imul32_acesses[m_sid]=m_stats->m_num_imul32_acesses[m_sid]+active_count*latency;
-		}
-		//printf("Int_Mul -- Active_count: %d\n",active_count);
-	 }
-	 void incidiv_stat(unsigned active_count,double latency) {
-		if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_idiv_acesses[m_sid]=m_stats->m_num_idiv_acesses[m_sid]+active_count*latency
-			 + inactive_lanes_accesses_sfu(active_count, latency); 
-		}else {
-		  m_stats->m_num_idiv_acesses[m_sid]=m_stats->m_num_idiv_acesses[m_sid]+active_count*latency;
-		}
-	 }
-	 void incfpalu_stat(unsigned active_count,double latency) {
-		if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_fp_acesses[m_sid]=m_stats->m_num_fp_acesses[m_sid]+active_count*latency
-			 + inactive_lanes_accesses_nonsfu(active_count, latency);
-		}else {
-        m_stats->m_num_fp_acesses[m_sid]=m_stats->m_num_fp_acesses[m_sid]+active_count*latency;
-		} 
-	 }
-	 void incfpmul_stat(unsigned active_count,double latency) {
-		 		// printf("FP MUL stat increament\n");
-      if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_fpmul_acesses[m_sid]=m_stats->m_num_fpmul_acesses[m_sid]+active_count*latency
-		    + inactive_lanes_accesses_nonsfu(active_count, latency);
-		}else {
-        m_stats->m_num_fpmul_acesses[m_sid]=m_stats->m_num_fpmul_acesses[m_sid]+active_count*latency;
-		}
-	 }
-	 void incfpdiv_stat(unsigned active_count,double latency) {
-		if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_fpdiv_acesses[m_sid]=m_stats->m_num_fpdiv_acesses[m_sid]+active_count*latency
-			+ inactive_lanes_accesses_sfu(active_count, latency); 
-		}else {
-		  m_stats->m_num_fpdiv_acesses[m_sid]=m_stats->m_num_fpdiv_acesses[m_sid]+active_count*latency;
-		}
-	 }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+  }
+
+  void inctex_stat(unsigned active_count,double latency) {
+    if(m_config->gpgpu_clock_gated_lanes==false){
+      m_stats->m_num_tex_acesses[m_sid]=m_stats->m_num_tex_acesses[m_sid]+(double)active_count*latency
+        + inactive_lanes_accesses_sfu(active_count, latency); 
+    }else{
+      m_stats->m_num_tex_acesses[m_sid]=m_stats->m_num_tex_acesses[m_sid]+(double)active_count*latency;
+    }
+    m_stats->m_active_exu_threads[m_sid]+=active_count;
+    m_stats->m_active_exu_warps[m_sid]++;
+  }
+
+  void inc_const_accesses(unsigned active_count) {
+    m_stats->m_num_const_acesses[m_sid]=m_stats->m_num_const_acesses[m_sid]+active_count;
+  }
+#if 0
 	 void inctrans_stat(unsigned active_count,double latency) {
 		if(m_config->gpgpu_clock_gated_lanes==false){
 		  m_stats->m_num_trans_acesses[m_sid]=m_stats->m_num_trans_acesses[m_sid]+active_count*latency
@@ -1973,249 +2377,368 @@ public:
 		  m_stats->m_num_trans_acesses[m_sid]=m_stats->m_num_trans_acesses[m_sid]+active_count*latency;
 		}
 	 }
-
-	 void incsfu_stat(unsigned active_count,double latency) {m_stats->m_num_sfu_acesses[m_sid]=m_stats->m_num_sfu_acesses[m_sid]+active_count*latency;}
-	 void incsp_stat(unsigned active_count,double latency) {m_stats->m_num_sp_acesses[m_sid]=m_stats->m_num_sp_acesses[m_sid]+active_count*latency;}
-	 void incmem_stat(unsigned active_count,double latency) {
-		if(m_config->gpgpu_clock_gated_lanes==false){
-		  m_stats->m_num_mem_acesses[m_sid]=m_stats->m_num_mem_acesses[m_sid]+active_count*latency
-		    + inactive_lanes_accesses_nonsfu(active_count, latency);
-		}else {
-		  m_stats->m_num_mem_acesses[m_sid]=m_stats->m_num_mem_acesses[m_sid]+active_count*latency;
-		}
-	 }
-	 void incexecstat(warp_inst_t *&inst);
-
-	 void incregfile_reads(unsigned active_count) {m_stats->m_read_regfile_acesses[m_sid]=m_stats->m_read_regfile_acesses[m_sid]+active_count;}
-	 void incregfile_writes(unsigned active_count){m_stats->m_write_regfile_acesses[m_sid]=m_stats->m_write_regfile_acesses[m_sid]+active_count;}
-	 void incnon_rf_operands(unsigned active_count){m_stats->m_non_rf_operands[m_sid]=m_stats->m_non_rf_operands[m_sid]+active_count;}
-
-	 void incspactivelanes_stat(unsigned active_count) {m_stats->m_active_sp_lanes[m_sid]=m_stats->m_active_sp_lanes[m_sid]+active_count;}
-	 void incsfuactivelanes_stat(unsigned active_count) {m_stats->m_active_sfu_lanes[m_sid]=m_stats->m_active_sfu_lanes[m_sid]+active_count;}
-	 void incfuactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_lanes[m_sid]=m_stats->m_active_fu_lanes[m_sid]+active_count;}
-	 void incfumemactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_mem_lanes[m_sid]=m_stats->m_active_fu_mem_lanes[m_sid]+active_count;}
-
-	 void inc_simt_to_mem(unsigned n_flits){ m_stats->n_simt_to_mem[m_sid] += n_flits; }
-	 bool check_if_non_released_reduction_barrier(warp_inst_t &inst);
-
-	private:
-	 unsigned inactive_lanes_accesses_sfu(unsigned active_count,double latency){
-      return  ( ((32-active_count)>>1)*latency) + ( ((32-active_count)>>3)*latency) + ( ((32-active_count)>>3)*latency);
-	 }
-	 unsigned inactive_lanes_accesses_nonsfu(unsigned active_count,double latency){
-      return  ( ((32-active_count)>>1)*latency);
-	 }
-
-    int test_res_bus(int latency);
-    void init_warps(unsigned cta_id, unsigned start_thread, unsigned end_thread,unsigned ctaid, int cta_size, unsigned kernel_id);
-    virtual void checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t, unsigned tid);
-    address_type next_pc( int tid ) const;
-    void fetch();
-    void register_cta_thread_exit(unsigned cta_num, kernel_info_t * kernel );
-
-    void decode();
-    
-    void issue();
-    friend class scheduler_unit; //this is needed to use private issue warp.
-    friend class TwoLevelScheduler;
-    friend class LooseRoundRobbinScheduler;
+#endif
+  void incsfu_stat(unsigned active_count, double latency) {
+    m_stats->m_num_sfu_acesses[m_sid] =
+        m_stats->m_num_sfu_acesses[m_sid] + (double)active_count*latency;
+  }
+  void incsp_stat(unsigned active_count, double latency) {
+    m_stats->m_num_sp_acesses[m_sid] =
+        m_stats->m_num_sp_acesses[m_sid] + (double)active_count*latency;
+  }
+  void incmem_stat(unsigned active_count, double latency) {
+    if (m_config->gpgpu_clock_gated_lanes == false) {
+      m_stats->m_num_mem_acesses[m_sid] =
+          m_stats->m_num_mem_acesses[m_sid] + (double)active_count*latency +
+          inactive_lanes_accesses_nonsfu(active_count, latency);
+    } else {
+      m_stats->m_num_mem_acesses[m_sid] =
+          m_stats->m_num_mem_acesses[m_sid] + (double)active_count*latency;
+    }
+  }
+  void incexecstat(warp_inst_t *&inst);
+
+  void incregfile_reads(unsigned active_count) {
+    m_stats->m_read_regfile_acesses[m_sid] =
+        m_stats->m_read_regfile_acesses[m_sid] + active_count;
+  }
+  void incregfile_writes(unsigned active_count) {
+    m_stats->m_write_regfile_acesses[m_sid] =
+        m_stats->m_write_regfile_acesses[m_sid] + active_count;
+  }
+  void incnon_rf_operands(unsigned active_count) {
+    m_stats->m_non_rf_operands[m_sid] =
+        m_stats->m_non_rf_operands[m_sid] + active_count;
+  }
+
+  void incspactivelanes_stat(unsigned active_count) {
+    m_stats->m_active_sp_lanes[m_sid] =
+        m_stats->m_active_sp_lanes[m_sid] + active_count;
+  }
+  void incsfuactivelanes_stat(unsigned active_count) {
+    m_stats->m_active_sfu_lanes[m_sid] =
+        m_stats->m_active_sfu_lanes[m_sid] + active_count;
+  }
+  void incfuactivelanes_stat(unsigned active_count) {
+    m_stats->m_active_fu_lanes[m_sid] =
+        m_stats->m_active_fu_lanes[m_sid] + active_count;
+  }
+  void incfumemactivelanes_stat(unsigned active_count) {
+    m_stats->m_active_fu_mem_lanes[m_sid] =
+        m_stats->m_active_fu_mem_lanes[m_sid] + active_count;
+  }
+
+  void inc_simt_to_mem(unsigned n_flits) {
+    m_stats->n_simt_to_mem[m_sid] += n_flits;
+  }
+  bool check_if_non_released_reduction_barrier(warp_inst_t &inst);
+
+ protected:
+  unsigned inactive_lanes_accesses_sfu(unsigned active_count, double latency) {
+    return (((32 - active_count) >> 1) * latency) +
+           (((32 - active_count) >> 3) * latency) +
+           (((32 - active_count) >> 3) * latency);
+  }
+  unsigned inactive_lanes_accesses_nonsfu(unsigned active_count,
+                                          double latency) {
+    return (((32 - active_count) >> 1) * latency);
+  }
+
+  int test_res_bus(int latency);
+  address_type next_pc(int tid) const;
+  void fetch();
+  void register_cta_thread_exit(unsigned cta_num, kernel_info_t *kernel);
+
+  void decode();
+
+  void issue();
+  friend class scheduler_unit;  // this is needed to use private issue warp.
+  friend class TwoLevelScheduler;
+  friend class LooseRoundRobbinScheduler;
     friend class ldst_unit;
-    void issue_warp( register_set& warp, const warp_inst_t *pI, const active_mask_t &active_mask, unsigned warp_id, unsigned sch_id );
-    void func_exec_inst( warp_inst_t &inst );
-
-     // Returns numbers of addresses in translated_addrs
-    unsigned translate_local_memaddr( address_type localaddr, unsigned tid, unsigned num_shader, unsigned datasize, new_addr_type* translated_addrs );
-
-    void read_operands();
-    
-    void execute();
-    
-    void writeback();
-    
-    // used in display_pipeline():
-    void dump_warp_state( FILE *fout ) const;
-    void print_stage(unsigned int stage, FILE *fout) const;
-    unsigned long long m_last_inst_gpu_sim_cycle;
-    unsigned long long m_last_inst_gpu_tot_sim_cycle;
-
-    // general information
-    unsigned m_sid; // shader id
-    unsigned m_tpc; // texture processor cluster id (aka, node id when using interconnect concentration)
-    const shader_core_config *m_config;
-    const memory_config *m_memory_config;
-    class simt_core_cluster *m_cluster;
-
-    // statistics 
-    shader_core_stats *m_stats;
-
-    // CTA scheduling / hardware thread allocation
-    unsigned m_n_active_cta; // number of Cooperative Thread Arrays (blocks) currently running on this shader.
-    unsigned m_cta_status[MAX_CTA_PER_SHADER]; // CTAs status 
-    unsigned m_not_completed; // number of threads to be completed (==0 when all thread on this core completed) 
-    std::bitset<MAX_THREAD_PER_SM> m_active_threads;
-    
-    // thread contexts 
-    thread_ctx_t             *m_threadState;
-    
-    // interconnect interface
-    mem_fetch_interface *m_icnt;
-    shader_core_mem_fetch_allocator *m_mem_fetch_allocator;
-    
-    // fetch
-    read_only_cache *m_L1I; // instruction cache
-    int  m_last_warp_fetched;
-
-    // decode/dispatch
-    std::vector<shd_warp_t>   m_warp;   // per warp information array
-    barrier_set_t             m_barriers;
-    ifetch_buffer_t           m_inst_fetch_buffer;
-    std::vector<register_set> m_pipeline_reg;
-    Scoreboard               *m_scoreboard;
-    opndcoll_rfu_t            m_operand_collector;
-    int m_active_warps;
-
-    //schedule
-    std::vector<scheduler_unit*>  schedulers;
-
-    //issue
-    unsigned int Issue_Prio;
-
-    // execute
-    unsigned m_num_function_units;
-    std::vector<pipeline_stage_name_t> m_dispatch_port;
-    std::vector<pipeline_stage_name_t> m_issue_port;
-    std::vector<simd_function_unit*> m_fu; // stallable pipelines should be last in this array
-    ldst_unit *m_ldst_unit;
-    static const unsigned MAX_ALU_LATENCY = 512;
-    unsigned num_result_bus;
-    std::vector< std::bitset<MAX_ALU_LATENCY>* > m_result_bus;
-
-    // used for local address mapping with single kernel launch
-    unsigned kernel_max_cta_per_shader;
-    unsigned kernel_padded_threads_per_cta;
-    // Used for handing out dynamic warp_ids to new warps.
-    // the differnece between a warp_id and a dynamic_warp_id
-    // is that the dynamic_warp_id is a running number unique to every warp
-    // run on this shader, where the warp_id is the static warp slot.
-    unsigned m_dynamic_warp_id;
-
-    //Jin: concurrent kernels on a sm
-public:
-    bool can_issue_1block(kernel_info_t & kernel);
-    bool occupy_shader_resource_1block(kernel_info_t & kernel, bool occupy);
-    void release_shader_resource_1block(unsigned hw_ctaid, kernel_info_t & kernel);
-    int find_available_hwtid(unsigned int cta_size, bool occupy);
-private:
-    unsigned int m_occupied_n_threads; 
-    unsigned int m_occupied_shmem; 
-    unsigned int m_occupied_regs;
-    unsigned int m_occupied_ctas;
-    std::bitset<MAX_THREAD_PER_SM> m_occupied_hwtid;
-    std::map<unsigned int, unsigned int> m_occupied_cta_to_hwtid; 
-
+  virtual void issue_warp(register_set &warp, const warp_inst_t *pI,
+                          const active_mask_t &active_mask, unsigned warp_id,
+                          unsigned sch_id);
+
+  void create_front_pipeline();
+  void create_schedulers();
+  void create_exec_pipeline();
+
+  // pure virtual methods implemented based on the current execution mode
+  // (execution-driven vs trace-driven)
+  virtual void init_warps(unsigned cta_id, unsigned start_thread,
+                          unsigned end_thread, unsigned ctaid, int cta_size,
+                          kernel_info_t &kernel);
+  virtual void checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t,
+                                             unsigned tid) = 0;
+  virtual void func_exec_inst(warp_inst_t &inst) = 0;
+
+  virtual unsigned sim_init_thread(kernel_info_t &kernel,
+                                   ptx_thread_info **thread_info, int sid,
+                                   unsigned tid, unsigned threads_left,
+                                   unsigned num_threads, core_t *core,
+                                   unsigned hw_cta_id, unsigned hw_warp_id,
+                                   gpgpu_t *gpu) = 0;
+
+  virtual void create_shd_warp() = 0;
+
+  virtual const warp_inst_t *get_next_inst(unsigned warp_id,
+                                           address_type pc) = 0;
+  virtual void get_pdom_stack_top_info(unsigned warp_id, const warp_inst_t *pI,
+                                       unsigned *pc, unsigned *rpc) = 0;
+  virtual const active_mask_t &get_active_mask(unsigned warp_id,
+                                               const warp_inst_t *pI) = 0;
+
+  // Returns numbers of addresses in translated_addrs
+  unsigned translate_local_memaddr(address_type localaddr, unsigned tid,
+                                   unsigned num_shader, unsigned datasize,
+                                   new_addr_type *translated_addrs);
+
+  void read_operands();
+
+  void execute();
+
+  void writeback();
+
+  // used in display_pipeline():
+  void dump_warp_state(FILE *fout) const;
+  void print_stage(unsigned int stage, FILE *fout) const;
+
+  unsigned long long m_last_inst_gpu_sim_cycle;
+  unsigned long long m_last_inst_gpu_tot_sim_cycle;
+
+  // general information
+  unsigned m_sid;  // shader id
+  unsigned m_tpc;  // texture processor cluster id (aka, node id when using
+                   // interconnect concentration)
+  const shader_core_config *m_config;
+  const memory_config *m_memory_config;
+  class simt_core_cluster *m_cluster;
+
+  // statistics 
+  shader_core_stats *m_stats;
+
+  // CTA scheduling / hardware thread allocation
+  unsigned m_n_active_cta;  // number of Cooperative Thread Arrays (blocks)
+                            // currently running on this shader.
+  unsigned m_cta_status[MAX_CTA_PER_SHADER];  // CTAs status
+  unsigned m_not_completed;  // number of threads to be completed (==0 when all
+                             // thread on this core completed)
+  std::bitset<MAX_THREAD_PER_SM> m_active_threads;
+
+  // thread contexts
+  thread_ctx_t *m_threadState;
+
+  // interconnect interface
+  mem_fetch_interface *m_icnt;
+  shader_core_mem_fetch_allocator *m_mem_fetch_allocator;
+
+  // fetch
+  read_only_cache *m_L1I;  // instruction cache
+  int m_last_warp_fetched;
+
+  // decode/dispatch
+  std::vector<shd_warp_t *> m_warp;  // per warp information array
+  barrier_set_t m_barriers;
+  ifetch_buffer_t m_inst_fetch_buffer;
+  std::vector<register_set> m_pipeline_reg;
+  Scoreboard *m_scoreboard;
+  opndcoll_rfu_t m_operand_collector;
+  int m_active_warps;
+  std::vector<register_set *> m_specilized_dispatch_reg;
+
+  // schedule
+  std::vector<scheduler_unit *> schedulers;
+
+  // issue
+  unsigned int Issue_Prio;
+
+  // execute
+  unsigned m_num_function_units;
+  std::vector<unsigned> m_dispatch_port;
+  std::vector<unsigned> m_issue_port;
+  std::vector<simd_function_unit *>
+      m_fu;  // stallable pipelines should be last in this array
+  ldst_unit *m_ldst_unit;
+  static const unsigned MAX_ALU_LATENCY = 512;
+  unsigned num_result_bus;
+  std::vector<std::bitset<MAX_ALU_LATENCY> *> m_result_bus;
+
+  // used for local address mapping with single kernel launch
+  unsigned kernel_max_cta_per_shader;
+  unsigned kernel_padded_threads_per_cta;
+  // Used for handing out dynamic warp_ids to new warps.
+  // the differnece between a warp_id and a dynamic_warp_id
+  // is that the dynamic_warp_id is a running number unique to every warp
+  // run on this shader, where the warp_id is the static warp slot.
+  unsigned m_dynamic_warp_id;
+
+  // Jin: concurrent kernels on a sm
+ public:
+  bool can_issue_1block(kernel_info_t &kernel);
+  bool occupy_shader_resource_1block(kernel_info_t &kernel, bool occupy);
+  void release_shader_resource_1block(unsigned hw_ctaid, kernel_info_t &kernel);
+  int find_available_hwtid(unsigned int cta_size, bool occupy);
+
+ private:
+  unsigned int m_occupied_n_threads;
+  unsigned int m_occupied_shmem;
+  unsigned int m_occupied_regs;
+  unsigned int m_occupied_ctas;
+  std::bitset<MAX_THREAD_PER_SM> m_occupied_hwtid;
+  std::map<unsigned int, unsigned int> m_occupied_cta_to_hwtid;
+};
 
+class exec_shader_core_ctx : public shader_core_ctx {
+ public:
+  exec_shader_core_ctx(class gpgpu_sim *gpu, class simt_core_cluster *cluster,
+                       unsigned shader_id, unsigned tpc_id,
+                       const shader_core_config *config,
+                       const memory_config *mem_config,
+                       shader_core_stats *stats)
+      : shader_core_ctx(gpu, cluster, shader_id, tpc_id, config, mem_config,
+                        stats) {
+    create_front_pipeline();
+    create_shd_warp();
+    create_schedulers();
+    create_exec_pipeline();
+  }
+
+  virtual void checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t,
+                                             unsigned tid);
+  virtual void func_exec_inst(warp_inst_t &inst);
+  virtual unsigned sim_init_thread(kernel_info_t &kernel,
+                                   ptx_thread_info **thread_info, int sid,
+                                   unsigned tid, unsigned threads_left,
+                                   unsigned num_threads, core_t *core,
+                                   unsigned hw_cta_id, unsigned hw_warp_id,
+                                   gpgpu_t *gpu);
+  virtual void create_shd_warp();
+  virtual const warp_inst_t *get_next_inst(unsigned warp_id, address_type pc);
+  virtual void get_pdom_stack_top_info(unsigned warp_id, const warp_inst_t *pI,
+                                       unsigned *pc, unsigned *rpc);
+  virtual const active_mask_t &get_active_mask(unsigned warp_id,
+                                               const warp_inst_t *pI);
 };
 
 class simt_core_cluster {
-public:
-    simt_core_cluster( class gpgpu_sim *gpu, 
-                       unsigned cluster_id, 
-                       const struct shader_core_config *config, 
-                       const struct memory_config *mem_config,
-                       shader_core_stats *stats,
-                       memory_stats_t *mstats );
-
-    void core_cycle();
-    void icnt_cycle();
-
-    void reinit();
-    unsigned issue_block2core();
-    void cache_flush();
-    void cache_invalidate();
-    bool icnt_injection_buffer_full(unsigned size, bool write);
-    void icnt_inject_request_packet(class mem_fetch *mf);
-
-
-    // for perfect memory interface
-    bool response_queue_full() {
-        return ( m_response_fifo.size() >= m_config->n_simt_ejection_buffer_size );
-    }
-    void push_response_fifo(class mem_fetch *mf) {
-        m_response_fifo.push_back(mf);
-    }
-
-    void get_pdom_stack_top_info( unsigned sid, unsigned tid, unsigned *pc, unsigned *rpc ) const;
-    unsigned max_cta( const kernel_info_t &kernel );
-    unsigned get_not_completed() const;
-    void print_not_completed( FILE *fp ) const;
-    unsigned get_n_active_cta() const;
-    unsigned get_n_active_sms() const;
-    gpgpu_sim *get_gpu() { return m_gpu; }
-
-    void display_pipeline( unsigned sid, FILE *fout, int print_mem, int mask );
-    void print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) const;
-
-    void get_cache_stats(cache_stats &cs) const;
-    void get_L1I_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1D_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1C_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1T_sub_stats(struct cache_sub_stats &css) const;
-
-    void get_icnt_stats(long &n_simt_to_mem, long &n_mem_to_simt) const;
-    float get_current_occupancy( unsigned long long& active, unsigned long long & total ) const;
-
-    shader_core_ctx *get_core(int id_in_cluster) { return m_core[id_in_cluster]; }
+ public:
+  simt_core_cluster(class gpgpu_sim *gpu, unsigned cluster_id,
+                    const shader_core_config *config,
+                    const memory_config *mem_config, shader_core_stats *stats,
+                    memory_stats_t *mstats);
+
+  void core_cycle();
+  void icnt_cycle();
+
+  void reinit();
+  unsigned issue_block2core();
+  void cache_flush();
+  void cache_invalidate();
+  bool icnt_injection_buffer_full(unsigned size, bool write);
+  void icnt_inject_request_packet(class mem_fetch *mf);
+
+  // for perfect memory interface
+  bool response_queue_full() {
+    return (m_response_fifo.size() >= m_config->n_simt_ejection_buffer_size);
+  }
+  void push_response_fifo(class mem_fetch *mf) {
+    m_response_fifo.push_back(mf);
+  }
+
+  void get_pdom_stack_top_info(unsigned sid, unsigned tid, unsigned *pc,
+                               unsigned *rpc) const;
+  unsigned max_cta(const kernel_info_t &kernel);
+  unsigned get_not_completed() const;
+  void print_not_completed(FILE *fp) const;
+  unsigned get_n_active_cta() const;
+  unsigned get_n_active_sms() const;
+  gpgpu_sim *get_gpu() { return m_gpu; }
+
+  void display_pipeline(unsigned sid, FILE *fout, int print_mem, int mask);
+  void print_cache_stats(FILE *fp, unsigned &dl1_accesses,
+                         unsigned &dl1_misses) const;
+
+  void get_cache_stats(cache_stats &cs) const;
+  void get_L1I_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1D_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1C_sub_stats(struct cache_sub_stats &css) const;
+  void get_L1T_sub_stats(struct cache_sub_stats &css) const;
+
+  void get_icnt_stats(long &n_simt_to_mem, long &n_mem_to_simt) const;
+  float get_current_occupancy(unsigned long long &active,
+                              unsigned long long &total) const;
+  virtual void create_shader_core_ctx() = 0;
+
+  shader_core_ctx *get_core(int id_in_cluster) { return m_core[id_in_cluster]; }
+ protected:
+  unsigned m_cluster_id;
+  gpgpu_sim *m_gpu;
+  const shader_core_config *m_config;
+  shader_core_stats *m_stats;
+  memory_stats_t *m_memory_stats;
+  shader_core_ctx **m_core;
+  const memory_config *m_mem_config;
+
+  unsigned m_cta_issue_next_core;
+  std::list<unsigned> m_core_sim_order;
+  std::list<mem_fetch *> m_response_fifo;
+};
 
-private:
-    unsigned m_cluster_id;
-    gpgpu_sim *m_gpu;
-    const shader_core_config *m_config;
-    shader_core_stats *m_stats;
-    memory_stats_t *m_memory_stats;
-    shader_core_ctx **m_core;
-
-    unsigned m_cta_issue_next_core;
-    std::list<unsigned> m_core_sim_order;
-    std::list<mem_fetch*> m_response_fifo;
+class exec_simt_core_cluster : public simt_core_cluster {
+ public:
+  exec_simt_core_cluster(class gpgpu_sim *gpu, unsigned cluster_id,
+                         const shader_core_config *config,
+                         const memory_config *mem_config,
+                         class shader_core_stats *stats,
+                         class memory_stats_t *mstats)
+      : simt_core_cluster(gpu, cluster_id, config, mem_config, stats, mstats) {
+    create_shader_core_ctx();
+  }
+
+  virtual void create_shader_core_ctx();
 };
 
 class shader_memory_interface : public mem_fetch_interface {
-public:
-    shader_memory_interface( shader_core_ctx *core, simt_core_cluster *cluster ) { m_core=core; m_cluster=cluster; }
-    virtual bool full( unsigned size, bool write ) const 
-    {
-        return m_cluster->icnt_injection_buffer_full(size,write);
-    }
-    virtual void push(mem_fetch *mf) 
-    {
-    	m_core->inc_simt_to_mem(mf->get_num_flits(true));
-        m_cluster->icnt_inject_request_packet(mf);        
-    }
-private:
-    shader_core_ctx *m_core;
-    simt_core_cluster *m_cluster;
+ public:
+  shader_memory_interface(shader_core_ctx *core, simt_core_cluster *cluster) {
+    m_core = core;
+    m_cluster = cluster;
+  }
+  virtual bool full(unsigned size, bool write) const {
+    return m_cluster->icnt_injection_buffer_full(size, write);
+  }
+  virtual void push(mem_fetch *mf) {
+    m_core->inc_simt_to_mem(mf->get_num_flits(true));
+    m_cluster->icnt_inject_request_packet(mf);
+  }
+
+ private:
+  shader_core_ctx *m_core;
+  simt_core_cluster *m_cluster;
 };
 
 class perfect_memory_interface : public mem_fetch_interface {
-public:
-    perfect_memory_interface( shader_core_ctx *core, simt_core_cluster *cluster ) { m_core=core; m_cluster=cluster; }
-    virtual bool full( unsigned size, bool write) const
-    {
-        return m_cluster->response_queue_full();
-    }
-    virtual void push(mem_fetch *mf)
-    {
-        if ( mf && mf->isatomic() )
-            mf->do_atomic(); // execute atomic inside the "memory subsystem"
-        m_core->inc_simt_to_mem(mf->get_num_flits(true));
-        m_cluster->push_response_fifo(mf);        
-    }
-private:
-    shader_core_ctx *m_core;
-    simt_core_cluster *m_cluster;
+ public:
+  perfect_memory_interface(shader_core_ctx *core, simt_core_cluster *cluster) {
+    m_core = core;
+    m_cluster = cluster;
+  }
+  virtual bool full(unsigned size, bool write) const {
+    return m_cluster->response_queue_full();
+  }
+  virtual void push(mem_fetch *mf) {
+    if (mf && mf->isatomic())
+      mf->do_atomic();  // execute atomic inside the "memory subsystem"
+    m_core->inc_simt_to_mem(mf->get_num_flits(true));
+    m_cluster->push_response_fifo(mf);
+  }
+
+ private:
+  shader_core_ctx *m_core;
+  simt_core_cluster *m_cluster;
 };
 
-
 inline int scheduler_unit::get_sid() const { return m_shader->get_sid(); }
 
 #endif /* SHADER_H */
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader_trace.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader_trace.h
index 8c1b30f4ed..a73c6e038d 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader_trace.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/shader_trace.h
@@ -40,28 +40,29 @@
 
 // Intended to be called from inside components of a shader core.
 // Depends on a get_sid() function
-#define SHADER_DPRINTF(x, ...) do {\
-    if (SHADER_DTRACE(x)) {\
-        printf( SHADER_PRINT_STR,\
-                gpu_sim_cycle + gpu_tot_sim_cycle,\
-                Trace_gpgpu::trace_streams_str[Trace_gpgpu::x],\
-                get_sid() );\
-        printf(__VA_ARGS__);\
-    }\
-} while (0)
+#define SHADER_DPRINTF(x, ...)                                \
+  do {                                                        \
+    if (SHADER_DTRACE(x)) {                                   \
+      printf(SHADER_PRINT_STR,                                \
+             m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle, \
+             Trace_gpgpu::trace_streams_str[Trace_gpgpu::x], get_sid());  \
+      printf(__VA_ARGS__);                                    \
+    }                                                         \
+  } while (0)
 
 // Intended to be called from inside a scheduler_unit.
 // Depends on a m_id member
-#define SCHED_DPRINTF(...) do {\
-    if (SHADER_DTRACE(WARP_SCHEDULER)) {\
-        printf( SCHED_PRINT_STR,\
-                gpu_sim_cycle + gpu_tot_sim_cycle,\
-                Trace_gpgpu::trace_streams_str[Trace_gpgpu::WARP_SCHEDULER],\
-                get_sid(),\
-                m_id );\
-        printf(__VA_ARGS__);\
-    }\
-} while (0)
+#define SCHED_DPRINTF(...)                                               \
+  do {                                                                   \
+    if (SHADER_DTRACE(WARP_SCHEDULER)) {                                 \
+      printf(SCHED_PRINT_STR,                                            \
+             m_shader->get_gpu()->gpu_sim_cycle +                        \
+                 m_shader->get_gpu()->gpu_tot_sim_cycle,                 \
+             Trace_gpgpu::trace_streams_str[Trace_gpgpu::WARP_SCHEDULER], get_sid(), \
+             m_id);                                                      \
+      printf(__VA_ARGS__);                                               \
+    }                                                                    \
+  } while (0)
 
 #else
 
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.cc b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.cc
index 6a4c75b793..619e6e60e5 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.cc
@@ -37,6 +37,7 @@
 #include <map>
 #include <algorithm>
 #include <string>
+#include "../../libcuda_sim/gpgpu_context.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
@@ -44,144 +45,141 @@ static unsigned long long  min_snap_shot_interval = 0;
 static unsigned long long  next_snap_shot_cycle = 0;
 static std::list<snap_shot_trigger*> list_ss_trigger;
 
-void add_snap_shot_trigger (snap_shot_trigger* ss_trigger)
-{
-   // quick optimization assuming that all snap shot intervals are perfect multiples of each other
-   if (min_snap_shot_interval == 0 || min_snap_shot_interval > ss_trigger->get_interval()) {
-      min_snap_shot_interval = ss_trigger->get_interval();
-      next_snap_shot_cycle = min_snap_shot_interval; // assume that snap shots haven't started yet
-   }
-   list_ss_trigger.push_back(ss_trigger);
+void add_snap_shot_trigger(snap_shot_trigger *ss_trigger) {
+  // quick optimization assuming that all snap shot intervals are perfect
+  // multiples of each other
+  if (min_snap_shot_interval == 0 ||
+      min_snap_shot_interval > ss_trigger->get_interval()) {
+    min_snap_shot_interval = ss_trigger->get_interval();
+    next_snap_shot_cycle =
+        min_snap_shot_interval;  // assume that snap shots haven't started yet
+  }
+  list_ss_trigger.push_back(ss_trigger);
 }
 
-void remove_snap_shot_trigger (snap_shot_trigger* ss_trigger)
-{
-   list_ss_trigger.remove(ss_trigger);
+void remove_snap_shot_trigger(snap_shot_trigger *ss_trigger) {
+  list_ss_trigger.remove(ss_trigger);
 }
 
-void try_snap_shot (unsigned long long  current_cycle)
-{
-   if (min_snap_shot_interval == 0) return;
-   if (current_cycle != next_snap_shot_cycle) return;
-   
-   std::list<snap_shot_trigger*>::iterator ss_trigger_iter = list_ss_trigger.begin();
-   for(; ss_trigger_iter != list_ss_trigger.end(); ++ss_trigger_iter) {
-      (*ss_trigger_iter)->snap_shot(current_cycle); // WF: should be try_snap_shot
-   }
-   next_snap_shot_cycle = current_cycle + min_snap_shot_interval; // WF: stateful testing, maybe bad
+void try_snap_shot(unsigned long long current_cycle) {
+  if (min_snap_shot_interval == 0) return;
+  if (current_cycle != next_snap_shot_cycle) return;
+
+  std::list<snap_shot_trigger *>::iterator ss_trigger_iter =
+      list_ss_trigger.begin();
+  for (; ss_trigger_iter != list_ss_trigger.end(); ++ss_trigger_iter) {
+    (*ss_trigger_iter)
+        ->snap_shot(current_cycle);  // WF: should be try_snap_shot
+  }
+  next_snap_shot_cycle =
+      current_cycle +
+      min_snap_shot_interval;  // WF: stateful testing, maybe bad
 }
 
 ////////////////////////////////////////////////////////////////////////////////
- 
-static unsigned long long  spill_interval = 0;
-static unsigned long long  next_spill_cycle = 0;
-static std::list<spill_log_interface*> list_spill_log;
 
-void add_spill_log (spill_log_interface* spill_log)
-{
-   list_spill_log.push_back(spill_log);
+static unsigned long long spill_interval = 0;
+static unsigned long long next_spill_cycle = 0;
+static std::list<spill_log_interface *> list_spill_log;
+
+void add_spill_log(spill_log_interface *spill_log) {
+  list_spill_log.push_back(spill_log);
 }
 
-void remove_spill_log (spill_log_interface* spill_log)
-{
-   list_spill_log.remove(spill_log);
+void remove_spill_log(spill_log_interface *spill_log) {
+  list_spill_log.remove(spill_log);
 }
 
-void set_spill_interval (unsigned long long  interval)
-{
-   spill_interval = interval;
-   next_spill_cycle = spill_interval;
+void set_spill_interval(unsigned long long interval) {
+  spill_interval = interval;
+  next_spill_cycle = spill_interval;
 }
 
-void spill_log_to_file (FILE *fout, int final, unsigned long long  current_cycle)
-{
-   if (!final && spill_interval == 0) return;
-   if (!final && current_cycle <= next_spill_cycle) return;
+void spill_log_to_file(FILE *fout, int final,
+                       unsigned long long current_cycle) {
+  if (!final && spill_interval == 0) return;
+  if (!final && current_cycle <= next_spill_cycle) return;
 
-   fprintf(fout, "\n"); // ensure that the spill occurs at a new line
-   std::list<spill_log_interface*>::iterator i_spill_log = list_spill_log.begin();
-   for(; i_spill_log != list_spill_log.end(); ++i_spill_log) {
-      (*i_spill_log)->spill(fout, final); 
-   }
-   fflush(fout);
+  fprintf(fout, "\n");  // ensure that the spill occurs at a new line
+  std::list<spill_log_interface *>::iterator i_spill_log =
+      list_spill_log.begin();
+  for (; i_spill_log != list_spill_log.end(); ++i_spill_log) {
+    (*i_spill_log)->spill(fout, final);
+  }
+  fflush(fout);
 
-   next_spill_cycle = current_cycle + spill_interval; // WF: stateful testing, maybe bad
+  next_spill_cycle =
+      current_cycle + spill_interval;  // WF: stateful testing, maybe bad
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-unsigned translate_pc_to_ptxlineno(unsigned pc);
-
 static int n_thread_CFloggers = 0;
-static thread_CFlocality** thread_CFlogger = NULL;
+static thread_CFlocality **thread_CFlogger = NULL;
 
-void create_thread_CFlogger( int n_loggers, int n_threads, address_type start_pc, unsigned long long  logging_interval) 
-{
-   destroy_thread_CFlogger();
-   
-   n_thread_CFloggers = n_loggers;
-   thread_CFlogger = new thread_CFlocality*[n_loggers];
-
-   std::string name_tpl("CFLog");
-   char buffer[32];
-   for (int i = 0; i < n_thread_CFloggers; i++) {
-      snprintf(buffer, 32, "%02d", i);
-      thread_CFlogger[i] = new thread_CFlocality( name_tpl + buffer, logging_interval, n_threads, start_pc);
-      if (logging_interval != 0) {
-         add_snap_shot_trigger(thread_CFlogger[i]);
-         add_spill_log(thread_CFlogger[i]);
-      }
-   }
-}
-
-void destroy_thread_CFlogger( ) 
-{
-   if (thread_CFlogger != NULL) {
-      for (int i = 0; i < n_thread_CFloggers; i++) {
-         remove_snap_shot_trigger(thread_CFlogger[i]);
-         remove_spill_log(thread_CFlogger[i]);
-         delete thread_CFlogger[i];
-      }
-      delete [] thread_CFlogger;
-      thread_CFlogger = NULL;
-   }
-}
-
-void cflog_update_thread_pc( int logger_id, int thread_id, address_type pc ) 
-{
-   if (thread_CFlogger == NULL) return;  // this means no visualizer output 
-   if (thread_id < 0) return;
-   thread_CFlogger[logger_id]->update_thread_pc(thread_id, pc);
+void create_thread_CFlogger(gpgpu_context *ctx, int n_loggers, int n_threads,
+                            address_type start_pc,
+                            unsigned long long logging_interval) {
+  destroy_thread_CFlogger();
+
+  n_thread_CFloggers = n_loggers;
+  thread_CFlogger = new thread_CFlocality *[n_loggers];
+
+  std::string name_tpl("CFLog");
+  char buffer[32];
+  for (int i = 0; i < n_thread_CFloggers; i++) {
+    snprintf(buffer, 32, "%02d", i);
+    thread_CFlogger[i] = new thread_CFlocality(
+        ctx, name_tpl + buffer, logging_interval, n_threads, start_pc);
+    if (logging_interval != 0) {
+      add_snap_shot_trigger(thread_CFlogger[i]);
+      add_spill_log(thread_CFlogger[i]);
+    }
+  }
 }
 
-// deprecated 
-void cflog_snapshot( int logger_id, unsigned long long  cycle ) 
-{
-   thread_CFlogger[logger_id]->snap_shot(cycle);
+void destroy_thread_CFlogger() {
+  if (thread_CFlogger != NULL) {
+    for (int i = 0; i < n_thread_CFloggers; i++) {
+      remove_snap_shot_trigger(thread_CFlogger[i]);
+      remove_spill_log(thread_CFlogger[i]);
+      delete thread_CFlogger[i];
+    }
+    delete[] thread_CFlogger;
+    thread_CFlogger = NULL;
+  }
 }
 
-void cflog_print(FILE *fout) 
-{
-   if (thread_CFlogger == NULL) return;  // this means no visualizer output 
-   for (int i = 0; i < n_thread_CFloggers; i++) {
-      thread_CFlogger[i]->print_histo(fout);
-   }
+void cflog_update_thread_pc(int logger_id, int thread_id, address_type pc) {
+  if (thread_CFlogger == NULL) return;  // this means no visualizer output
+  if (thread_id < 0) return;
+  thread_CFlogger[logger_id]->update_thread_pc(thread_id, pc);
 }
 
-void cflog_visualizer_print(FILE *fout) 
-{
-   if (thread_CFlogger == NULL) return;  // this means no visualizer output 
-   for (int i = 0; i < n_thread_CFloggers; i++) {
-      thread_CFlogger[i]->print_visualizer(fout);
-   }
+// deprecated
+void cflog_snapshot(int logger_id, unsigned long long cycle) {
+  thread_CFlogger[logger_id]->snap_shot(cycle);
 }
 
-void cflog_visualizer_gzprint(gzFile fout) 
-{
-   if (thread_CFlogger == NULL) return;  // this means no visualizer output 
-   for (int i = 0; i < n_thread_CFloggers; i++) {
-      thread_CFlogger[i]->print_visualizer(fout);
-   }
+void cflog_print(FILE *fout) {
+  if (thread_CFlogger == NULL) return;  // this means no visualizer output
+  for (int i = 0; i < n_thread_CFloggers; i++) {
+    thread_CFlogger[i]->print_histo(fout);
+  }
+}
+
+void cflog_visualizer_print(FILE *fout) {
+  if (thread_CFlogger == NULL) return;  // this means no visualizer output
+  for (int i = 0; i < n_thread_CFloggers; i++) {
+    thread_CFlogger[i]->print_visualizer(fout);
+  }
+}
+
+void cflog_visualizer_gzprint(gzFile fout) {
+  if (thread_CFlogger == NULL) return;  // this means no visualizer output
+  for (int i = 0; i < n_thread_CFloggers; i++) {
+    thread_CFlogger[i]->print_visualizer(fout);
+  }
 }
 
 ////////////////////////////////////////////////////////////////////////////////
@@ -190,26 +188,23 @@ int insn_warp_occ_logger::s_ids = 0;
 
 static std::vector<insn_warp_occ_logger> iwo_logger;
 
-void insn_warp_occ_create( int n_loggers, int simd_width )
-{
-   iwo_logger.clear();
-   iwo_logger.assign(n_loggers, insn_warp_occ_logger(simd_width));
-   for (unsigned i = 0; i < iwo_logger.size(); i++) {
-      iwo_logger[i].set_id(i);
-   }
+void insn_warp_occ_create(int n_loggers, int simd_width) {
+  iwo_logger.clear();
+  iwo_logger.assign(n_loggers, insn_warp_occ_logger(simd_width));
+  for (unsigned i = 0; i < iwo_logger.size(); i++) {
+    iwo_logger[i].set_id(i);
+  }
 }
 
-void insn_warp_occ_log( int logger_id, address_type pc, int warp_occ)
-{
-   if (warp_occ <= 0) return;
-   iwo_logger[logger_id].log(pc, warp_occ);
+void insn_warp_occ_log(int logger_id, address_type pc, int warp_occ) {
+  if (warp_occ <= 0) return;
+  iwo_logger[logger_id].log(pc, warp_occ);
 }
 
-void insn_warp_occ_print( FILE *fout )
-{
-   for (unsigned i = 0; i < iwo_logger.size(); i++) {
-      iwo_logger[i].print(fout);
-   }
+void insn_warp_occ_print(FILE *fout) {
+  for (unsigned i = 0; i < iwo_logger.size(); i++) {
+    iwo_logger[i].print(fout);
+  }
 }
 
 ////////////////////////////////////////////////////////////////////////////////
@@ -222,36 +217,33 @@ int linear_histogram_logger::s_ids = 0;
 
 static std::vector<linear_histogram_logger> s_warp_occ_logger;
 
-void shader_warp_occ_create( int n_loggers, int simd_width, unsigned long long  logging_interval)
-{
-   // simd_width + 1 to include the case with full warp
-   s_warp_occ_logger.assign(n_loggers, 
-                            linear_histogram_logger(simd_width + 1, logging_interval, "ShdrWarpOcc"));
-   for (unsigned i = 0; i < s_warp_occ_logger.size(); i++) {
-      s_warp_occ_logger[i].set_id(i);
-      add_snap_shot_trigger(&(s_warp_occ_logger[i]));
-      add_spill_log(&(s_warp_occ_logger[i]));
-   }
+void shader_warp_occ_create(int n_loggers, int simd_width,
+                            unsigned long long logging_interval) {
+  // simd_width + 1 to include the case with full warp
+  s_warp_occ_logger.assign(
+      n_loggers,
+      linear_histogram_logger(simd_width + 1, logging_interval, "ShdrWarpOcc"));
+  for (unsigned i = 0; i < s_warp_occ_logger.size(); i++) {
+    s_warp_occ_logger[i].set_id(i);
+    add_snap_shot_trigger(&(s_warp_occ_logger[i]));
+    add_spill_log(&(s_warp_occ_logger[i]));
+  }
 }
 
-void shader_warp_occ_log( int logger_id, int warp_occ)
-{
-   s_warp_occ_logger[logger_id].log(warp_occ);
+void shader_warp_occ_log(int logger_id, int warp_occ) {
+  s_warp_occ_logger[logger_id].log(warp_occ);
 }
 
-void shader_warp_occ_snapshot( int logger_id, unsigned long long  current_cycle)
-{
-   s_warp_occ_logger[logger_id].snap_shot(current_cycle);
+void shader_warp_occ_snapshot(int logger_id, unsigned long long current_cycle) {
+  s_warp_occ_logger[logger_id].snap_shot(current_cycle);
 }
 
-void shader_warp_occ_print( FILE *fout )
-{
-   for (unsigned i = 0; i < s_warp_occ_logger.size(); i++) {
-      s_warp_occ_logger[i].print(fout);
-   }
+void shader_warp_occ_print(FILE *fout) {
+  for (unsigned i = 0; i < s_warp_occ_logger.size(); i++) {
+    s_warp_occ_logger[i].print(fout);
+  }
 }
 
-
 /////////////////////////////////////////////////////////////////////////////////////
 // per-shadercore memory-access logger
 /////////////////////////////////////////////////////////////////////////////////////
@@ -260,105 +252,115 @@ static int s_mem_acc_logger_n_dram = 0;
 static int s_mem_acc_logger_n_bank = 0;
 static std::vector<linear_histogram_logger> s_mem_acc_logger;
 
-void shader_mem_acc_create( int n_loggers, int n_dram, int n_bank, unsigned long long  logging_interval)
-{
-   // (n_bank + 1) to space data out; 2x to separate read and write
-   s_mem_acc_logger.assign(n_loggers, 
-                           linear_histogram_logger(2 * n_dram * (n_bank + 1), logging_interval, "ShdrMemAcc"));
-
-   s_mem_acc_logger_n_dram = n_dram;
-   s_mem_acc_logger_n_bank = n_bank;
-   for (unsigned i = 0; i < s_mem_acc_logger.size(); i++) {
-      s_mem_acc_logger[i].set_id(i);
-      add_snap_shot_trigger(&(s_mem_acc_logger[i]));
-      add_spill_log(&(s_mem_acc_logger[i]));
-   }
-}
-
-void shader_mem_acc_log( int logger_id, int dram_id, int bank, char rw)
-{
-   if (s_mem_acc_logger_n_dram == 0) return;
-   int write_offset = 0;
-   switch(rw) {
-   case 'r': write_offset = 0; break;
-   case 'w': write_offset = (s_mem_acc_logger_n_bank + 1) * s_mem_acc_logger_n_dram; break;
-   default: assert(0); break;
-   }
-   s_mem_acc_logger[logger_id].log(dram_id * s_mem_acc_logger_n_bank + bank + write_offset);
-}
-
-void shader_mem_acc_snapshot( int logger_id, unsigned long long  current_cycle)
-{
-   s_mem_acc_logger[logger_id].snap_shot(current_cycle);
+void shader_mem_acc_create(int n_loggers, int n_dram, int n_bank,
+                           unsigned long long logging_interval) {
+  // (n_bank + 1) to space data out; 2x to separate read and write
+  s_mem_acc_logger.assign(
+      n_loggers, linear_histogram_logger(2 * n_dram * (n_bank + 1),
+                                         logging_interval, "ShdrMemAcc"));
+
+  s_mem_acc_logger_n_dram = n_dram;
+  s_mem_acc_logger_n_bank = n_bank;
+  for (unsigned i = 0; i < s_mem_acc_logger.size(); i++) {
+    s_mem_acc_logger[i].set_id(i);
+    add_snap_shot_trigger(&(s_mem_acc_logger[i]));
+    add_spill_log(&(s_mem_acc_logger[i]));
+  }
+}
+
+void shader_mem_acc_log(int logger_id, int dram_id, int bank, char rw) {
+  if (s_mem_acc_logger_n_dram == 0) return;
+  int write_offset = 0;
+  switch (rw) {
+    case 'r':
+      write_offset = 0;
+      break;
+    case 'w':
+      write_offset = (s_mem_acc_logger_n_bank + 1) * s_mem_acc_logger_n_dram;
+      break;
+    default:
+      assert(0);
+      break;
+  }
+  s_mem_acc_logger[logger_id].log(dram_id * s_mem_acc_logger_n_bank + bank +
+                                  write_offset);
+}
+
+void shader_mem_acc_snapshot(int logger_id, unsigned long long current_cycle) {
+  s_mem_acc_logger[logger_id].snap_shot(current_cycle);
+}
+
+void shader_mem_acc_print(FILE *fout) {
+  for (unsigned i = 0; i < s_mem_acc_logger.size(); i++) {
+    s_mem_acc_logger[i].print(fout);
+  }
 }
 
-void shader_mem_acc_print( FILE *fout )
-{
-   for (unsigned i = 0; i < s_mem_acc_logger.size(); i++) {
-      s_mem_acc_logger[i].print(fout);
-   }
-}
-
-
 /////////////////////////////////////////////////////////////////////////////////////
 // per-shadercore memory-latency logger
 /////////////////////////////////////////////////////////////////////////////////////
 
 static bool s_mem_lat_logger_used = false;
-static int s_mem_lat_logger_nbins = 48;     // up to 2^24 = 16M
+static int s_mem_lat_logger_nbins = 48;  // up to 2^24 = 16M
 static std::vector<linear_histogram_logger> s_mem_lat_logger;
 
-void shader_mem_lat_create( int n_loggers, unsigned long long  logging_interval)
-{
-   s_mem_lat_logger.assign(n_loggers, 
-                           linear_histogram_logger(s_mem_lat_logger_nbins, logging_interval, "ShdrMemLat"));
+void shader_mem_lat_create(int n_loggers, unsigned long long logging_interval) {
+  s_mem_lat_logger.assign(
+      n_loggers, linear_histogram_logger(s_mem_lat_logger_nbins,
+                                         logging_interval, "ShdrMemLat"));
+
+  for (unsigned i = 0; i < s_mem_lat_logger.size(); i++) {
+    s_mem_lat_logger[i].set_id(i);
+    add_snap_shot_trigger(&(s_mem_lat_logger[i]));
+    add_spill_log(&(s_mem_lat_logger[i]));
+  }
 
-   for (unsigned i = 0; i < s_mem_lat_logger.size(); i++) {
-      s_mem_lat_logger[i].set_id(i);
-      add_snap_shot_trigger(&(s_mem_lat_logger[i]));
-      add_spill_log(&(s_mem_lat_logger[i]));
-   }
-   
-   s_mem_lat_logger_used = true;
+  s_mem_lat_logger_used = true;
 }
 
-void shader_mem_lat_log( int logger_id, int latency)
-{
-   if (s_mem_lat_logger_used == false) return;
-   if (latency > (1<<(s_mem_lat_logger_nbins/2))) assert(0); // guard for out of bound bin
-   assert(latency > 0);
-   
-   int latency_bin;
-   
-   int bin; // LOG_2(latency)
-   int v = latency;
-   register unsigned int shift;
-
-   bin =   (v > 0xFFFF) << 4; v >>= bin;
-   shift = (v > 0xFF  ) << 3; v >>= shift; bin |= shift;
-   shift = (v > 0xF   ) << 2; v >>= shift; bin |= shift;
-   shift = (v > 0x3   ) << 1; v >>= shift; bin |= shift;
-                                           bin |= (v >> 1);
-   latency_bin = 2 * bin;
-   if (bin > 0) {
-      latency_bin += ((latency & (1 << (bin - 1))) != 0)? 1 : 0; // approx. for LOG_sqrt2(latency)
-   }
-
-   s_mem_lat_logger[logger_id].log(latency_bin);
-}
-
-void shader_mem_lat_snapshot( int logger_id, unsigned long long  current_cycle)
-{
-   s_mem_lat_logger[logger_id].snap_shot(current_cycle);
+void shader_mem_lat_log(int logger_id, int latency) {
+  if (s_mem_lat_logger_used == false) return;
+  if (latency > (1 << (s_mem_lat_logger_nbins / 2)))
+    assert(0);  // guard for out of bound bin
+  assert(latency > 0);
+
+  int latency_bin;
+
+  int bin;  // LOG_2(latency)
+  int v = latency;
+  register unsigned int shift;
+
+  bin = (v > 0xFFFF) << 4;
+  v >>= bin;
+  shift = (v > 0xFF) << 3;
+  v >>= shift;
+  bin |= shift;
+  shift = (v > 0xF) << 2;
+  v >>= shift;
+  bin |= shift;
+  shift = (v > 0x3) << 1;
+  v >>= shift;
+  bin |= shift;
+  bin |= (v >> 1);
+  latency_bin = 2 * bin;
+  if (bin > 0) {
+    latency_bin += ((latency & (1 << (bin - 1))) != 0)
+                       ? 1
+                       : 0;  // approx. for LOG_sqrt2(latency)
+  }
+
+  s_mem_lat_logger[logger_id].log(latency_bin);
 }
 
-void shader_mem_lat_print( FILE *fout )
-{
-   for (unsigned i = 0; i < s_mem_lat_logger.size(); i++) {
-      s_mem_lat_logger[i].print(fout);
-   }
+void shader_mem_lat_snapshot(int logger_id, unsigned long long current_cycle) {
+  s_mem_lat_logger[logger_id].snap_shot(current_cycle);
 }
 
+void shader_mem_lat_print(FILE *fout) {
+  for (unsigned i = 0; i < s_mem_lat_logger.size(); i++) {
+    s_mem_lat_logger[i].print(fout);
+  }
+}
 
 /////////////////////////////////////////////////////////////////////////////////////
 // per-shadercore cache-miss logger
@@ -367,423 +369,391 @@ void shader_mem_lat_print( FILE *fout )
 static int s_cache_access_logger_n_types = 0;
 static std::vector<linear_histogram_logger> s_cache_access_logger;
 
-enum cache_access_logger_types {
-   NORMAL, TEXTURE, CONSTANT, INSTRUCTION
-};
-
-int get_shader_normal_cache_id() { return NORMAL; }
+int get_shader_normal_cache_id() { return NORMALS; }
 int get_shader_texture_cache_id() { return TEXTURE; }
 int get_shader_constant_cache_id() { return CONSTANT; }
 int get_shader_instruction_cache_id() { return INSTRUCTION; }
 
-void shader_cache_access_create( int n_loggers, int n_types, unsigned long long  logging_interval)
-{
-   // There are different type of cache (x2 for recording accesses and misses)
-   s_cache_access_logger.assign(n_loggers, 
-                                linear_histogram_logger(n_types * 2, logging_interval, "ShdrCacheMiss"));
+void shader_cache_access_create(int n_loggers, int n_types,
+                                unsigned long long logging_interval) {
+  // There are different type of cache (x2 for recording accesses and misses)
+  s_cache_access_logger.assign(
+      n_loggers,
+      linear_histogram_logger(n_types * 2, logging_interval, "ShdrCacheMiss"));
 
-   s_cache_access_logger_n_types = n_types;
-   for (unsigned i = 0; i < s_cache_access_logger.size(); i++) {
-      s_cache_access_logger[i].set_id(i);
-      add_snap_shot_trigger(&(s_cache_access_logger[i]));
-      add_spill_log(&(s_cache_access_logger[i]));
-   }
+  s_cache_access_logger_n_types = n_types;
+  for (unsigned i = 0; i < s_cache_access_logger.size(); i++) {
+    s_cache_access_logger[i].set_id(i);
+    add_snap_shot_trigger(&(s_cache_access_logger[i]));
+    add_spill_log(&(s_cache_access_logger[i]));
+  }
 }
 
-void shader_cache_access_log( int logger_id, int type, int miss)
-{
-   if (s_cache_access_logger_n_types == 0) return;
-   if (logger_id < 0) return;
-   assert(type == NORMAL || type == TEXTURE || type == CONSTANT || type == INSTRUCTION);
-   assert(miss == 0 || miss == 1);
-   
-   s_cache_access_logger[logger_id].log(2 * type + miss);
-}
+void shader_cache_access_log(int logger_id, int type, int miss) {
+  if (s_cache_access_logger_n_types == 0) return;
+  if (logger_id < 0) return;
+  assert(type == NORMALS || type == TEXTURE || type == CONSTANT ||
+         type == INSTRUCTION);
+  assert(miss == 0 || miss == 1);
 
-void shader_cache_access_unlog( int logger_id, int type, int miss)
-{
-   if (s_cache_access_logger_n_types == 0) return;
-   if (logger_id < 0) return;
-   assert(type == NORMAL || type == TEXTURE || type == CONSTANT || type == INSTRUCTION);
-   assert(miss == 0 || miss == 1);
-   
-   s_cache_access_logger[logger_id].unlog(2 * type + miss);
+  s_cache_access_logger[logger_id].log(2 * type + miss);
 }
 
-void shader_cache_access_print( FILE *fout )
-{
-   for (unsigned i = 0; i < s_cache_access_logger.size(); i++) {
-      s_cache_access_logger[i].print(fout);
-   }
+void shader_cache_access_unlog(int logger_id, int type, int miss) {
+  if (s_cache_access_logger_n_types == 0) return;
+  if (logger_id < 0) return;
+  assert(type == NORMALS || type == TEXTURE || type == CONSTANT ||
+         type == INSTRUCTION);
+  assert(miss == 0 || miss == 1);
+
+  s_cache_access_logger[logger_id].unlog(2 * type + miss);
 }
 
+void shader_cache_access_print(FILE *fout) {
+  for (unsigned i = 0; i < s_cache_access_logger.size(); i++) {
+    s_cache_access_logger[i].print(fout);
+  }
+}
 
 /////////////////////////////////////////////////////////////////////////////////////
-// per-shadercore CTA count logger (only make sense with gpgpu_spread_blocks_across_cores)
+// per-shadercore CTA count logger (only make sense with
+// gpgpu_spread_blocks_across_cores)
 /////////////////////////////////////////////////////////////////////////////////////
 
 static linear_histogram_logger *s_CTA_count_logger = NULL;
 
-void shader_CTA_count_create( int n_shaders, unsigned long long  logging_interval)
-{
-   // only need one logger to track all the shaders
-   if (s_CTA_count_logger != NULL) delete s_CTA_count_logger;
-   s_CTA_count_logger = new linear_histogram_logger(n_shaders, logging_interval, "ShdrCTACount", false);
+void shader_CTA_count_create(int n_shaders,
+                             unsigned long long logging_interval) {
+  // only need one logger to track all the shaders
+  if (s_CTA_count_logger != NULL) delete s_CTA_count_logger;
+  s_CTA_count_logger = new linear_histogram_logger(n_shaders, logging_interval,
+                                                   "ShdrCTACount", false);
 
-   s_CTA_count_logger->set_id(-1);
-   if (logging_interval != 0) {
-      add_snap_shot_trigger(s_CTA_count_logger);
-      add_spill_log(s_CTA_count_logger);
-   }
+  s_CTA_count_logger->set_id(-1);
+  if (logging_interval != 0) {
+    add_snap_shot_trigger(s_CTA_count_logger);
+    add_spill_log(s_CTA_count_logger);
+  }
 }
 
-void shader_CTA_count_log( int shader_id, int nCTAadded )
-{
-   if (s_CTA_count_logger == NULL) return;
-   
-   for (int i = 0; i < nCTAadded; i++) {
-      s_CTA_count_logger->log(shader_id);
-   }
-}
+void shader_CTA_count_log(int shader_id, int nCTAadded) {
+  if (s_CTA_count_logger == NULL) return;
 
-void shader_CTA_count_unlog( int shader_id, int nCTAdone )
-{
-   if (s_CTA_count_logger == NULL) return;
-   
-   for (int i = 0; i < nCTAdone; i++) {
-      s_CTA_count_logger->unlog(shader_id);
-   }
+  for (int i = 0; i < nCTAadded; i++) {
+    s_CTA_count_logger->log(shader_id);
+  }
 }
 
-void shader_CTA_count_print( FILE *fout )
-{
-   if (s_CTA_count_logger == NULL) return;
-   s_CTA_count_logger->print(fout);
+void shader_CTA_count_unlog(int shader_id, int nCTAdone) {
+  if (s_CTA_count_logger == NULL) return;
+
+  for (int i = 0; i < nCTAdone; i++) {
+    s_CTA_count_logger->unlog(shader_id);
+  }
 }
 
-void shader_CTA_count_visualizer_print( FILE *fout )
-{
-   if (s_CTA_count_logger == NULL) return;
-   s_CTA_count_logger->print_visualizer(fout);
+void shader_CTA_count_print(FILE *fout) {
+  if (s_CTA_count_logger == NULL) return;
+  s_CTA_count_logger->print(fout);
 }
 
-void shader_CTA_count_visualizer_gzprint( gzFile fout )
-{
-   if (s_CTA_count_logger == NULL) return;
-   s_CTA_count_logger->print_visualizer(fout);
+void shader_CTA_count_visualizer_print(FILE *fout) {
+  if (s_CTA_count_logger == NULL) return;
+  s_CTA_count_logger->print_visualizer(fout);
 }
 
+void shader_CTA_count_visualizer_gzprint(gzFile fout) {
+  if (s_CTA_count_logger == NULL) return;
+  s_CTA_count_logger->print_visualizer(fout);
+}
 
 ////////////////////////////////////////////////////////////////////////////////
 ////////////////////////////////////////////////////////////////////////////////
 
-thread_insn_span::thread_insn_span(unsigned long long  cycle)
-  : m_cycle(cycle),
+thread_insn_span::thread_insn_span(unsigned long long cycle, gpgpu_context *ctx)
+    : m_cycle(cycle),
 #if (tr1_hash_map_ismap == 1)
-     m_insn_span_count() 
-#else 
-     m_insn_span_count(32*1024) 
+      m_insn_span_count()
+#else
+      m_insn_span_count(32 * 1024)
 #endif
-{ 
-}
-
-thread_insn_span::~thread_insn_span() { }
-   
-thread_insn_span::thread_insn_span(const thread_insn_span& other)
-      : m_cycle(other.m_cycle),
-        m_insn_span_count(other.m_insn_span_count) 
-{ 
-}
-      
-thread_insn_span& thread_insn_span::operator=(const thread_insn_span& other)
-{
-   printf("thread_insn_span& operator=\n");
-   if (this != &other) {
-      m_insn_span_count = other.m_insn_span_count;
-      m_cycle = other.m_cycle;
-   }
-   return *this;
-}
-   
-thread_insn_span& thread_insn_span::operator+=(const thread_insn_span& other)
-{
-   span_count_map::const_iterator i_sc = other.m_insn_span_count.begin();
-   for (; i_sc != other.m_insn_span_count.end(); ++i_sc) {
-      m_insn_span_count[i_sc->first] += i_sc->second;
-   }
-   return *this;
-}
-   
-void thread_insn_span::set_span( address_type pc ) 
 {
-   if( ((int)pc) >= 0 )
-      m_insn_span_count[pc] += 1;
+  gpgpu_ctx = ctx;
 }
-   
-void thread_insn_span::reset(unsigned long long  cycle) 
-{
-   m_cycle = cycle;
-   m_insn_span_count.clear(); 
-}
-   
-void thread_insn_span::print_span(FILE *fout) const
-{
-   fprintf(fout, "%d: ", (int)m_cycle);
-   span_count_map::const_iterator i_sc = m_insn_span_count.begin();
-   for (; i_sc != m_insn_span_count.end(); ++i_sc) {
-      fprintf(fout, "%d ", i_sc->first);
-   }
-   fprintf(fout, "\n");
+
+thread_insn_span::~thread_insn_span() {}
+
+thread_insn_span::thread_insn_span(const thread_insn_span &other,
+                                   gpgpu_context *ctx)
+    : m_cycle(other.m_cycle), m_insn_span_count(other.m_insn_span_count) {
+  gpgpu_ctx = ctx;
 }
 
-void thread_insn_span::print_histo(FILE *fout) const
-{
-   fprintf(fout, "%d:", (int)m_cycle);
-   span_count_map::const_iterator i_sc = m_insn_span_count.begin();
-   for (; i_sc != m_insn_span_count.end(); ++i_sc) {
-      fprintf(fout, "%d ", i_sc->second);
-   }
-   fprintf(fout, "\n");
+thread_insn_span &thread_insn_span::operator=(const thread_insn_span &other) {
+  printf("thread_insn_span& operator=\n");
+  if (this != &other) {
+    m_insn_span_count = other.m_insn_span_count;
+    m_cycle = other.m_cycle;
+  }
+  return *this;
 }
 
-void thread_insn_span::print_sparse_histo(FILE *fout) const
-{
-   int n_printed_entries = 0;
-   span_count_map::const_iterator i_sc = m_insn_span_count.begin();
-   for (; i_sc != m_insn_span_count.end(); ++i_sc) {
-      unsigned ptx_lineno = translate_pc_to_ptxlineno(i_sc->first);
-      fprintf(fout, "%u %d ", ptx_lineno, i_sc->second);
-      n_printed_entries++;
-   }
-   if (n_printed_entries == 0) {
-      fprintf(fout, "0 0 ");
-   }
-   fprintf(fout, "\n");
-}
-
-void thread_insn_span::print_sparse_histo(gzFile fout) const
-{
-   int n_printed_entries = 0;
-   span_count_map::const_iterator i_sc = m_insn_span_count.begin();
-   for (; i_sc != m_insn_span_count.end(); ++i_sc) {
-      unsigned ptx_lineno = translate_pc_to_ptxlineno(i_sc->first);
-      gzprintf(fout, "%u %d ", ptx_lineno, i_sc->second);
-      n_printed_entries++;
-   }
-   if (n_printed_entries == 0) {
-      gzprintf(fout, "0 0 ");
-   }
-   gzprintf(fout, "\n");
+thread_insn_span &thread_insn_span::operator+=(const thread_insn_span &other) {
+  span_count_map::const_iterator i_sc = other.m_insn_span_count.begin();
+  for (; i_sc != other.m_insn_span_count.end(); ++i_sc) {
+    m_insn_span_count[i_sc->first] += i_sc->second;
+  }
+  return *this;
 }
 
-////////////////////////////////////////////////////////////////////////////////
+void thread_insn_span::set_span(address_type pc) {
+  if (((int)pc) >= 0) m_insn_span_count[pc] += 1;
+}
 
-thread_CFlocality::thread_CFlocality(std::string name, 
-                                     unsigned long long  snap_shot_interval, 
-                                     int nthreads, 
-                                     address_type start_pc, 
-                                     unsigned long long  start_cycle)
-      : snap_shot_trigger(snap_shot_interval), m_name(name),
-        m_nthreads(nthreads), m_thread_pc(nthreads, start_pc), m_cycle(start_cycle),
-        m_thd_span(start_cycle)
-{
-   std::fill(m_thread_pc.begin(), m_thread_pc.end(), -1); // so that hw thread with no work assigned will not clobber results
+void thread_insn_span::reset(unsigned long long cycle) {
+  m_cycle = cycle;
+  m_insn_span_count.clear();
 }
-   
-thread_CFlocality::~thread_CFlocality() 
-{
-} 
-   
-void thread_CFlocality::update_thread_pc( int thread_id, address_type pc ) 
-{
-   m_thread_pc[thread_id] = pc;
-   m_thd_span.set_span(pc);
+
+void thread_insn_span::print_span(FILE *fout) const {
+  fprintf(fout, "%d: ", (int)m_cycle);
+  span_count_map::const_iterator i_sc = m_insn_span_count.begin();
+  for (; i_sc != m_insn_span_count.end(); ++i_sc) {
+    fprintf(fout, "%d ", i_sc->first);
+  }
+  fprintf(fout, "\n");
 }
-   
-void thread_CFlocality::snap_shot(unsigned long long  current_cycle) 
-{
-   m_thd_span_archive.push_back(m_thd_span);
-   m_thd_span.reset(current_cycle);
-   for (int i = 0; i < (int)m_thread_pc.size(); i++) {
-      m_thd_span.set_span(m_thread_pc[i]);
-   }
+
+void thread_insn_span::print_histo(FILE *fout) const {
+  fprintf(fout, "%d:", (int)m_cycle);
+  span_count_map::const_iterator i_sc = m_insn_span_count.begin();
+  for (; i_sc != m_insn_span_count.end(); ++i_sc) {
+    fprintf(fout, "%d ", i_sc->second);
+  }
+  fprintf(fout, "\n");
 }
-   
-void thread_CFlocality::spill(FILE *fout, bool final) 
-{
-   std::list<thread_insn_span>::iterator lit = m_thd_span_archive.begin();
-   for (; lit != m_thd_span_archive.end(); lit = m_thd_span_archive.erase(lit) ) {
-      fprintf(fout, "%s-", m_name.c_str());
-      lit->print_histo(fout);
-   }
-   assert( m_thd_span_archive.empty() );
-   if (final) {
-      fprintf(fout, "%s-", m_name.c_str());
-      m_thd_span.print_histo(fout);
-   }
-}
-   
-      
-void thread_CFlocality::print_visualizer(FILE *fout)  
-{
-   fprintf(fout, "%s: ", m_name.c_str());
-   if (m_thd_span_archive.empty()) {
-   
-      // visualizer do no require snap_shots
-      m_thd_span.print_sparse_histo(fout);
-      
-      // clean the thread span
-      m_thd_span.reset(0);
-      for (int i = 0; i < (int)m_thread_pc.size(); i++) 
-         m_thd_span.set_span(m_thread_pc[i]);
-   } else { 
-      assert(0); // TODO: implement fall back so that visualizer can work with snap shots
-   }
-}
-   
-void thread_CFlocality::print_visualizer(gzFile fout)
-{
-   gzprintf(fout, "%s: ", m_name.c_str());
-   if (m_thd_span_archive.empty()) {
-   
-      // visualizer do no require snap_shots
-      m_thd_span.print_sparse_histo(fout);
-      
-      // clean the thread span
-      m_thd_span.reset(0);
-      for (int i = 0; i < (int)m_thread_pc.size(); i++) {
-         m_thd_span.set_span(m_thread_pc[i]);
-      }
-   } else { 
-      assert(0); // TODO: implement fall back so that visualizer can work with snap shots
-   }
-}
-   
-void thread_CFlocality::print_span(FILE *fout) const
-{
-   std::list<thread_insn_span>::const_iterator lit = m_thd_span_archive.begin();
-   for (; lit != m_thd_span_archive.end(); ++lit) {
-      fprintf(fout, "%s-", m_name.c_str());
-      lit->print_span(fout);
-   }
-   fprintf(fout, "%s-", m_name.c_str());
-   m_thd_span.print_span(fout);
+
+void thread_insn_span::print_sparse_histo(FILE *fout) const {
+  int n_printed_entries = 0;
+  span_count_map::const_iterator i_sc = m_insn_span_count.begin();
+  for (; i_sc != m_insn_span_count.end(); ++i_sc) {
+    unsigned ptx_lineno = gpgpu_ctx->translate_pc_to_ptxlineno(i_sc->first);
+    fprintf(fout, "%u %d ", ptx_lineno, i_sc->second);
+    n_printed_entries++;
+  }
+  if (n_printed_entries == 0) {
+    fprintf(fout, "0 0 ");
+  }
+  fprintf(fout, "\n");
 }
 
-void thread_CFlocality::print_histo(FILE *fout) const
-{
-   std::list<thread_insn_span>::const_iterator lit = m_thd_span_archive.begin();
-   for (; lit != m_thd_span_archive.end(); ++lit) {
-      fprintf(fout, "%s-", m_name.c_str());
-      lit->print_histo(fout);
-   }
-   fprintf(fout, "%s-", m_name.c_str());
-   m_thd_span.print_histo(fout);
+void thread_insn_span::print_sparse_histo(gzFile fout) const {
+  int n_printed_entries = 0;
+  span_count_map::const_iterator i_sc = m_insn_span_count.begin();
+  for (; i_sc != m_insn_span_count.end(); ++i_sc) {
+    unsigned ptx_lineno = gpgpu_ctx->translate_pc_to_ptxlineno(i_sc->first);
+    gzprintf(fout, "%u %d ", ptx_lineno, i_sc->second);
+    n_printed_entries++;
+  }
+  if (n_printed_entries == 0) {
+    gzprintf(fout, "0 0 ");
+  }
+  gzprintf(fout, "\n");
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-linear_histogram_logger::linear_histogram_logger(int n_bins, 
-                           unsigned long long  snap_shot_interval, 
-                           const char *name, 
-                           bool reset_at_snap_shot, 
-                           unsigned long long  start_cycle )
-      : snap_shot_trigger(snap_shot_interval), 
-        m_n_bins(n_bins), 
-        m_curr_lin_hist(m_n_bins, start_cycle),
-        m_lin_hist_archive(),
-        m_cycle(start_cycle),
-        m_reset_at_snap_shot(reset_at_snap_shot), 
-        m_name(name),
-        m_id(s_ids++) 
-{
+thread_CFlocality::thread_CFlocality(gpgpu_context *ctx, std::string name,
+                                     unsigned long long snap_shot_interval,
+                                     int nthreads, address_type start_pc,
+                                     unsigned long long start_cycle)
+    : snap_shot_trigger(snap_shot_interval),
+      m_name(name),
+      m_nthreads(nthreads),
+      m_thread_pc(nthreads, start_pc),
+      m_cycle(start_cycle),
+      m_thd_span(start_cycle, ctx) {
+  std::fill(
+      m_thread_pc.begin(), m_thread_pc.end(),
+      -1);  // so that hw thread with no work assigned will not clobber results
+}
+
+thread_CFlocality::~thread_CFlocality() {}
+
+void thread_CFlocality::update_thread_pc(int thread_id, address_type pc) {
+  m_thread_pc[thread_id] = pc;
+  m_thd_span.set_span(pc);
+}
+
+void thread_CFlocality::snap_shot(unsigned long long current_cycle) {
+  m_thd_span_archive.push_back(m_thd_span);
+  m_thd_span.reset(current_cycle);
+  for (int i = 0; i < (int)m_thread_pc.size(); i++) {
+    m_thd_span.set_span(m_thread_pc[i]);
+  }
+}
+
+void thread_CFlocality::spill(FILE *fout, bool final) {
+  std::list<thread_insn_span>::iterator lit = m_thd_span_archive.begin();
+  for (; lit != m_thd_span_archive.end(); lit = m_thd_span_archive.erase(lit)) {
+    fprintf(fout, "%s-", m_name.c_str());
+    lit->print_histo(fout);
+  }
+  assert(m_thd_span_archive.empty());
+  if (final) {
+    fprintf(fout, "%s-", m_name.c_str());
+    m_thd_span.print_histo(fout);
+  }
+}
+
+void thread_CFlocality::print_visualizer(FILE *fout) {
+  fprintf(fout, "%s: ", m_name.c_str());
+  if (m_thd_span_archive.empty()) {
+    // visualizer do no require snap_shots
+    m_thd_span.print_sparse_histo(fout);
+
+    // clean the thread span
+    m_thd_span.reset(0);
+    for (int i = 0; i < (int)m_thread_pc.size(); i++)
+      m_thd_span.set_span(m_thread_pc[i]);
+  } else {
+    assert(0);  // TODO: implement fall back so that visualizer can work with
+                // snap shots
+  }
 }
 
-linear_histogram_logger::linear_histogram_logger(const linear_histogram_logger& other) 
-      : snap_shot_trigger(other.get_interval()), 
-        m_n_bins(other.m_n_bins), 
-        m_curr_lin_hist(m_n_bins, other.m_cycle),
-        m_lin_hist_archive(),
-        m_cycle(other.m_cycle),
-        m_reset_at_snap_shot(other.m_reset_at_snap_shot), 
-        m_name(other.m_name),
-        m_id(s_ids++) 
-{
+void thread_CFlocality::print_visualizer(gzFile fout) {
+  gzprintf(fout, "%s: ", m_name.c_str());
+  if (m_thd_span_archive.empty()) {
+    // visualizer do no require snap_shots
+    m_thd_span.print_sparse_histo(fout);
+
+    // clean the thread span
+    m_thd_span.reset(0);
+    for (int i = 0; i < (int)m_thread_pc.size(); i++) {
+      m_thd_span.set_span(m_thread_pc[i]);
+    }
+  } else {
+    assert(0);  // TODO: implement fall back so that visualizer can work with
+                // snap shots
+  }
 }
 
-linear_histogram_logger::~linear_histogram_logger() 
-{
-      remove_snap_shot_trigger(this);
-      remove_spill_log(this);
-}
-   
-void linear_histogram_logger::snap_shot(unsigned long long  current_cycle) {
-   m_lin_hist_archive.push_back(m_curr_lin_hist);
-   if (m_reset_at_snap_shot) {
-      m_curr_lin_hist.reset(current_cycle);
-   } else {
-      m_curr_lin_hist.set_cycle(current_cycle);
-   }
-}
-   
-void linear_histogram_logger::spill(FILE *fout, bool final) 
-{
-   std::list<linear_histogram_snapshot>::iterator iter = m_lin_hist_archive.begin();
-   for (; iter != m_lin_hist_archive.end(); iter = m_lin_hist_archive.erase(iter) ) {
-      fprintf(fout, "%s%02d-", m_name.c_str(), (m_id >= 0)? m_id : 0);
-      iter->print(fout);
-      fprintf(fout, "\n");
-   }
-   assert( m_lin_hist_archive.empty() );
-   if (final) {
-      fprintf(fout, "%s%02d-", m_name.c_str(), (m_id >= 0)? m_id : 0);
-      m_curr_lin_hist.print(fout);
-      fprintf(fout, "\n");
-   }
-}
-   
-void linear_histogram_logger::print(FILE *fout) const
-{
-   std::list<linear_histogram_snapshot>::const_iterator iter = m_lin_hist_archive.begin();
-   for (; iter != m_lin_hist_archive.end(); ++iter) {
-      fprintf(fout, "%s%02d-", m_name.c_str(), m_id);
-      iter->print(fout);
-      fprintf(fout, "\n");
-   }
-   fprintf(fout, "%s%02d-", m_name.c_str(), m_id);
-   m_curr_lin_hist.print(fout);
-   fprintf(fout, "\n");
-}
-
-void linear_histogram_logger::print_visualizer(FILE *fout)
-{
-   assert(m_lin_hist_archive.empty()); // don't support snapshot for now
-   fprintf(fout, "%s", m_name.c_str());
-   if (m_id >= 0) {
-      fprintf(fout, "%02d: ", m_id);
-   } else {
-      fprintf(fout, ": ");
-   }
-   m_curr_lin_hist.print_visualizer(fout);
-   fprintf(fout, "\n");
-   if (m_reset_at_snap_shot) {
-      m_curr_lin_hist.reset(0);
-   } 
-}
-
-void linear_histogram_logger::print_visualizer(gzFile fout)
-{
-   assert(m_lin_hist_archive.empty()); // don't support snapshot for now
-   gzprintf(fout, "%s", m_name.c_str());
-   if (m_id >= 0) {
-      gzprintf(fout, "%02d: ", m_id);
-   } else {
-      gzprintf(fout, ": ");
-   }
-   m_curr_lin_hist.print_visualizer(fout);
-   gzprintf(fout, "\n");
-   if (m_reset_at_snap_shot) {
-      m_curr_lin_hist.reset(0);
-   } 
+void thread_CFlocality::print_span(FILE *fout) const {
+  std::list<thread_insn_span>::const_iterator lit = m_thd_span_archive.begin();
+  for (; lit != m_thd_span_archive.end(); ++lit) {
+    fprintf(fout, "%s-", m_name.c_str());
+    lit->print_span(fout);
+  }
+  fprintf(fout, "%s-", m_name.c_str());
+  m_thd_span.print_span(fout);
+}
+
+void thread_CFlocality::print_histo(FILE *fout) const {
+  std::list<thread_insn_span>::const_iterator lit = m_thd_span_archive.begin();
+  for (; lit != m_thd_span_archive.end(); ++lit) {
+    fprintf(fout, "%s-", m_name.c_str());
+    lit->print_histo(fout);
+  }
+  fprintf(fout, "%s-", m_name.c_str());
+  m_thd_span.print_histo(fout);
 }
 
+////////////////////////////////////////////////////////////////////////////////
+
+linear_histogram_logger::linear_histogram_logger(
+    int n_bins, unsigned long long snap_shot_interval, const char *name,
+    bool reset_at_snap_shot, unsigned long long start_cycle)
+    : snap_shot_trigger(snap_shot_interval),
+      m_n_bins(n_bins),
+      m_curr_lin_hist(m_n_bins, start_cycle),
+      m_lin_hist_archive(),
+      m_cycle(start_cycle),
+      m_reset_at_snap_shot(reset_at_snap_shot),
+      m_name(name),
+      m_id(s_ids++) {}
+
+linear_histogram_logger::linear_histogram_logger(
+    const linear_histogram_logger &other)
+    : snap_shot_trigger(other.get_interval()),
+      m_n_bins(other.m_n_bins),
+      m_curr_lin_hist(m_n_bins, other.m_cycle),
+      m_lin_hist_archive(),
+      m_cycle(other.m_cycle),
+      m_reset_at_snap_shot(other.m_reset_at_snap_shot),
+      m_name(other.m_name),
+      m_id(s_ids++) {}
+
+linear_histogram_logger::~linear_histogram_logger() {
+  remove_snap_shot_trigger(this);
+  remove_spill_log(this);
+}
+
+void linear_histogram_logger::snap_shot(unsigned long long current_cycle) {
+  m_lin_hist_archive.push_back(m_curr_lin_hist);
+  if (m_reset_at_snap_shot) {
+    m_curr_lin_hist.reset(current_cycle);
+  } else {
+    m_curr_lin_hist.set_cycle(current_cycle);
+  }
+}
+
+void linear_histogram_logger::spill(FILE *fout, bool final) {
+  std::list<linear_histogram_snapshot>::iterator iter =
+      m_lin_hist_archive.begin();
+  for (; iter != m_lin_hist_archive.end();
+       iter = m_lin_hist_archive.erase(iter)) {
+    fprintf(fout, "%s%02d-", m_name.c_str(), (m_id >= 0) ? m_id : 0);
+    iter->print(fout);
+    fprintf(fout, "\n");
+  }
+  assert(m_lin_hist_archive.empty());
+  if (final) {
+    fprintf(fout, "%s%02d-", m_name.c_str(), (m_id >= 0) ? m_id : 0);
+    m_curr_lin_hist.print(fout);
+    fprintf(fout, "\n");
+  }
+}
+
+void linear_histogram_logger::print(FILE *fout) const {
+  std::list<linear_histogram_snapshot>::const_iterator iter =
+      m_lin_hist_archive.begin();
+  for (; iter != m_lin_hist_archive.end(); ++iter) {
+    fprintf(fout, "%s%02d-", m_name.c_str(), m_id);
+    iter->print(fout);
+    fprintf(fout, "\n");
+  }
+  fprintf(fout, "%s%02d-", m_name.c_str(), m_id);
+  m_curr_lin_hist.print(fout);
+  fprintf(fout, "\n");
+}
+
+void linear_histogram_logger::print_visualizer(FILE *fout) {
+  assert(m_lin_hist_archive.empty());  // don't support snapshot for now
+  fprintf(fout, "%s", m_name.c_str());
+  if (m_id >= 0) {
+    fprintf(fout, "%02d: ", m_id);
+  } else {
+    fprintf(fout, ": ");
+  }
+  m_curr_lin_hist.print_visualizer(fout);
+  fprintf(fout, "\n");
+  if (m_reset_at_snap_shot) {
+    m_curr_lin_hist.reset(0);
+  }
+}
+
+void linear_histogram_logger::print_visualizer(gzFile fout) {
+  assert(m_lin_hist_archive.empty());  // don't support snapshot for now
+  gzprintf(fout, "%s", m_name.c_str());
+  if (m_id >= 0) {
+    gzprintf(fout, "%02d: ", m_id);
+  } else {
+    gzprintf(fout, ": ");
+  }
+  m_curr_lin_hist.print_visualizer(fout);
+  gzprintf(fout, "\n");
+  if (m_reset_at_snap_shot) {
+    m_curr_lin_hist.reset(0);
+  }
+}
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.h b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.h
index 5646f0199b..189eca117c 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpu-sim/stat-tool.h
@@ -35,6 +35,7 @@
 #include <stdio.h>
 #include <zlib.h>
 
+class gpgpu_context;
 /////////////////////////////////////////////////////////////////////////////////////
 // logger snapshot trigger: 
 // - automate the snap_shot part of loggers to avoid modifying simulation loop everytime 
@@ -42,24 +43,26 @@
 /////////////////////////////////////////////////////////////////////////////////////
 
 class snap_shot_trigger {
-public:
-   snap_shot_trigger(unsigned long long  interval) : m_snap_shot_interval(interval) {}
-   virtual ~snap_shot_trigger() {}
-   
-   void try_snap_shot(unsigned long long  current_cycle) {
-      if ((current_cycle % m_snap_shot_interval == 0) && current_cycle != 0) {
-         snap_shot(current_cycle);
-      }
-   }
-   
-   virtual void snap_shot(unsigned long long  current_cycle) = 0;
-
-   const unsigned long long & get_interval() const { return m_snap_shot_interval;}
-
-protected:
-   unsigned long long  m_snap_shot_interval;
-};
+ public:
+  snap_shot_trigger(unsigned long long interval)
+      : m_snap_shot_interval(interval) {}
+  virtual ~snap_shot_trigger() {}
+
+  void try_snap_shot(unsigned long long current_cycle) {
+    if ((current_cycle % m_snap_shot_interval == 0) && current_cycle != 0) {
+      snap_shot(current_cycle);
+    }
+  }
 
+  virtual void snap_shot(unsigned long long current_cycle) = 0;
+
+  const unsigned long long &get_interval() const {
+    return m_snap_shot_interval;
+  }
+
+ protected:
+  unsigned long long m_snap_shot_interval;
+};
 
 /////////////////////////////////////////////////////////////////////////////////////
 // spill log interface: 
@@ -67,11 +70,11 @@ protected:
 /////////////////////////////////////////////////////////////////////////////////////
 
 class spill_log_interface {
-public:
-   spill_log_interface() {}
-   virtual ~spill_log_interface() {}
-   
-   virtual void spill(FILE *fout, bool final) = 0;
+ public:
+  spill_log_interface() {}
+  virtual ~spill_log_interface() {}
+
+  virtual void spill(FILE *fout, bool final) = 0;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////
@@ -79,50 +82,53 @@ public:
 /////////////////////////////////////////////////////////////////////////////////////
 
 class thread_insn_span {
-public:
-   thread_insn_span(unsigned long long  cycle);
-   thread_insn_span(const thread_insn_span& other);
-   ~thread_insn_span();
-   
-   thread_insn_span& operator=(const thread_insn_span& other);
-   thread_insn_span& operator+=(const thread_insn_span& other);
-   void set_span( address_type pc );
-   void reset(unsigned long long  cycle);
-   
-   void print_span(FILE *fout) const;
-   void print_histo(FILE *fout) const;
-   void print_sparse_histo(FILE *fout) const;
-   void print_sparse_histo(gzFile fout) const;
-
-private: 
-   typedef tr1_hash_map<address_type, int> span_count_map;
-   unsigned long long  m_cycle;
-   span_count_map m_insn_span_count;
+ public:
+  thread_insn_span(unsigned long long cycle, gpgpu_context *ctx);
+  thread_insn_span(const thread_insn_span &other, gpgpu_context *ctx);
+  ~thread_insn_span();
+
+  thread_insn_span &operator=(const thread_insn_span &other);
+  thread_insn_span &operator+=(const thread_insn_span &other);
+  void set_span(address_type pc);
+  void reset(unsigned long long cycle);
+
+  void print_span(FILE *fout) const;
+  void print_histo(FILE *fout) const;
+  void print_sparse_histo(FILE *fout) const;
+  void print_sparse_histo(gzFile fout) const;
+
+ private:
+  gpgpu_context *gpgpu_ctx;
+  typedef tr1_hash_map<address_type, int> span_count_map;
+  unsigned long long m_cycle;
+  span_count_map m_insn_span_count;
 };
 
 class thread_CFlocality : public snap_shot_trigger, public spill_log_interface {
-public:
-   thread_CFlocality(std::string name, unsigned long long  snap_shot_interval, 
-                     int nthreads, address_type start_pc, unsigned long long  start_cycle = 0);
-   ~thread_CFlocality();
-   
-   void update_thread_pc( int thread_id, address_type pc );
-   void snap_shot(unsigned long long  current_cycle);
-   void spill(FILE *fout, bool final);
-   
-   void print_visualizer(FILE *fout);
-   void print_visualizer(gzFile fout);
-   void print_span(FILE *fout) const;
-   void print_histo(FILE *fout) const;
-private:
-   std::string m_name;
-
-   int m_nthreads;
-   std::vector<address_type> m_thread_pc;
-   
-   unsigned long long  m_cycle;
-   thread_insn_span m_thd_span;
-   std::list<thread_insn_span> m_thd_span_archive;
+ public:
+  thread_CFlocality(gpgpu_context *ctx, std::string name,
+                    unsigned long long snap_shot_interval, int nthreads,
+                    address_type start_pc, unsigned long long start_cycle = 0);
+  ~thread_CFlocality();
+
+  void update_thread_pc(int thread_id, address_type pc);
+  void snap_shot(unsigned long long current_cycle);
+  void spill(FILE *fout, bool final);
+
+  void print_visualizer(FILE *fout);
+  void print_visualizer(gzFile fout);
+  void print_span(FILE *fout) const;
+  void print_histo(FILE *fout) const;
+
+ private:
+  std::string m_name;
+
+  int m_nthreads;
+  std::vector<address_type> m_thread_pc;
+
+  unsigned long long m_cycle;
+  thread_insn_span m_thd_span;
+  std::list<thread_insn_span> m_thd_span_archive;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////
@@ -130,194 +136,190 @@ private:
 /////////////////////////////////////////////////////////////////////////////////////
 
 class insn_warp_occ_logger {
-public:
-   insn_warp_occ_logger(int simd_width)
-      : m_simd_width(simd_width), 
-        m_insn_warp_occ(1,linear_histogram(1, "", m_simd_width)),
+ public:
+  insn_warp_occ_logger(int simd_width)
+      : m_simd_width(simd_width),
+        m_insn_warp_occ(1, linear_histogram(1, "", m_simd_width)),
         m_id(s_ids++) {}
-   
-   insn_warp_occ_logger(const insn_warp_occ_logger& other)
-      : m_simd_width(other.m_simd_width), 
-        m_insn_warp_occ(other.m_insn_warp_occ.size(), linear_histogram(1, "", m_simd_width)),
+
+  insn_warp_occ_logger(const insn_warp_occ_logger &other)
+      : m_simd_width(other.m_simd_width),
+        m_insn_warp_occ(other.m_insn_warp_occ.size(),
+                        linear_histogram(1, "", m_simd_width)),
         m_id(s_ids++) {}
-   
-   ~insn_warp_occ_logger() {}
-
-   insn_warp_occ_logger& operator=(const insn_warp_occ_logger& p) {
-      printf("insn_warp_occ_logger Operator= called: %02d \n", m_id);
-      assert(0);
-      return *this;
-   }   
-   
-   void set_id(int id) { m_id = id; }
-   
-   void log(address_type pc, int warp_occ) {
-       if( pc >= m_insn_warp_occ.size() ) 
-           m_insn_warp_occ.resize(2*pc, linear_histogram(1, "", m_simd_width));
-       m_insn_warp_occ[pc].add2bin(warp_occ - 1);
-   }
-   
-   void print(FILE *fout) const 
-   {
-      for (unsigned i = 0; i < m_insn_warp_occ.size(); i++) {
-         fprintf(fout, "InsnWarpOcc%02d-%d", m_id, i);
-         m_insn_warp_occ[i].fprint(fout);
-         fprintf(fout, "\n");
-      }
-   }
-
-private:
-
-   int m_simd_width;
-   std::vector<linear_histogram> m_insn_warp_occ;
-   int m_id;
-   static int s_ids;
-};
 
+  ~insn_warp_occ_logger() {}
+
+  insn_warp_occ_logger &operator=(const insn_warp_occ_logger &p) {
+    printf("insn_warp_occ_logger Operator= called: %02d \n", m_id);
+    assert(0);
+    return *this;
+  }
+
+  void set_id(int id) { m_id = id; }
+
+  void log(address_type pc, int warp_occ) {
+    if (pc >= m_insn_warp_occ.size())
+      m_insn_warp_occ.resize(2 * pc, linear_histogram(1, "", m_simd_width));
+    m_insn_warp_occ[pc].add2bin(warp_occ - 1);
+  }
+
+  void print(FILE *fout) const {
+    for (unsigned i = 0; i < m_insn_warp_occ.size(); i++) {
+      fprintf(fout, "InsnWarpOcc%02d-%d", m_id, i);
+      m_insn_warp_occ[i].fprint(fout);
+      fprintf(fout, "\n");
+    }
+  }
+
+ private:
+  int m_simd_width;
+  std::vector<linear_histogram> m_insn_warp_occ;
+  int m_id;
+  static int s_ids;
+};
 
 /////////////////////////////////////////////////////////////////////////////////////
 // generic linear histogram logger
 /////////////////////////////////////////////////////////////////////////////////////
 
 class linear_histogram_snapshot {
-public:
-   linear_histogram_snapshot(int n_bins, unsigned long long  cycle) 
-      : m_cycle(cycle), 
-        m_linear_histogram(n_bins,0) 
-   { }
-   
-   linear_histogram_snapshot(const linear_histogram_snapshot& other) 
-      : m_cycle(other.m_cycle), 
-        m_linear_histogram(other.m_linear_histogram)
-   { }
-   
-   ~linear_histogram_snapshot() { }
-   
-   void addsample(int pos) {
-      assert((size_t)pos < m_linear_histogram.size());
-      m_linear_histogram[pos] += 1;
-   }
-   
-   void subsample(int pos) {
-      assert((size_t)pos < m_linear_histogram.size());
-      m_linear_histogram[pos] -= 1;
-   }
-   
-   void reset(unsigned long long  cycle) {
-      m_cycle = cycle;
-      m_linear_histogram.assign(m_linear_histogram.size(), 0);
-   }
-   
-   void set_cycle(unsigned long long  cycle) { m_cycle = cycle; }
-   
-   void print(FILE *fout) const {
-      fprintf(fout, "%d = ", (int)m_cycle);
-      for (unsigned int i = 0; i < m_linear_histogram.size(); i++) {
-         fprintf(fout, "%d ", m_linear_histogram[i]);
-      }
-   }
-
-   void print_visualizer(FILE *fout) const {
-      for (unsigned int i = 0; i < m_linear_histogram.size(); i++) {
-         fprintf(fout, "%d ", m_linear_histogram[i]);
-      }
-   }
-
-   void print_visualizer(gzFile fout) const {
-      for (unsigned int i = 0; i < m_linear_histogram.size(); i++) {
-         gzprintf(fout, "%d ", m_linear_histogram[i]);
-      }
-   }
-
-private:
-   unsigned long long  m_cycle;
-   std::vector<int> m_linear_histogram;
+ public:
+  linear_histogram_snapshot(int n_bins, unsigned long long cycle)
+      : m_cycle(cycle), m_linear_histogram(n_bins, 0) {}
+
+  linear_histogram_snapshot(const linear_histogram_snapshot &other)
+      : m_cycle(other.m_cycle), m_linear_histogram(other.m_linear_histogram) {}
+
+  ~linear_histogram_snapshot() {}
+
+  void addsample(int pos) {
+    assert((size_t)pos < m_linear_histogram.size());
+    m_linear_histogram[pos] += 1;
+  }
+
+  void subsample(int pos) {
+    assert((size_t)pos < m_linear_histogram.size());
+    m_linear_histogram[pos] -= 1;
+  }
+
+  void reset(unsigned long long cycle) {
+    m_cycle = cycle;
+    m_linear_histogram.assign(m_linear_histogram.size(), 0);
+  }
+
+  void set_cycle(unsigned long long cycle) { m_cycle = cycle; }
+
+  void print(FILE *fout) const {
+    fprintf(fout, "%d = ", (int)m_cycle);
+    for (unsigned int i = 0; i < m_linear_histogram.size(); i++) {
+      fprintf(fout, "%d ", m_linear_histogram[i]);
+    }
+  }
+
+  void print_visualizer(FILE *fout) const {
+    for (unsigned int i = 0; i < m_linear_histogram.size(); i++) {
+      fprintf(fout, "%d ", m_linear_histogram[i]);
+    }
+  }
+
+  void print_visualizer(gzFile fout) const {
+    for (unsigned int i = 0; i < m_linear_histogram.size(); i++) {
+      gzprintf(fout, "%d ", m_linear_histogram[i]);
+    }
+  }
+
+ private:
+  unsigned long long m_cycle;
+  std::vector<int> m_linear_histogram;
 };
 
-class linear_histogram_logger : public snap_shot_trigger, public spill_log_interface {
-public:
-   linear_histogram_logger(int n_bins, 
-                           unsigned long long  snap_shot_interval, 
-                           const char *name, 
-                           bool reset_at_snap_shot = true, 
-                           unsigned long long  start_cycle = 0);
-   linear_histogram_logger(const linear_histogram_logger& other);
-   
-   ~linear_histogram_logger();
-   
-   void set_id(int id) { m_id = id; }
-   void log(int pos) { m_curr_lin_hist.addsample(pos); }
-   void unlog(int pos) { m_curr_lin_hist.subsample(pos); }
-   void snap_shot(unsigned long long  current_cycle);
-   void spill(FILE *fout, bool final); 
-
-   void print(FILE *fout) const;
-   void print_visualizer(FILE *fout);
-   void print_visualizer(gzFile fout);
-
-private:
-   int m_n_bins;
-   linear_histogram_snapshot m_curr_lin_hist;
-   std::list<linear_histogram_snapshot> m_lin_hist_archive;
-   unsigned long long  m_cycle;
-   bool m_reset_at_snap_shot;
-   std::string m_name;
-   int m_id;
-   static int s_ids;
+class linear_histogram_logger : public snap_shot_trigger,
+                                public spill_log_interface {
+ public:
+  linear_histogram_logger(int n_bins, unsigned long long snap_shot_interval,
+                          const char *name, bool reset_at_snap_shot = true,
+                          unsigned long long start_cycle = 0);
+  linear_histogram_logger(const linear_histogram_logger &other);
+
+  ~linear_histogram_logger();
+
+  void set_id(int id) { m_id = id; }
+  void log(int pos) { m_curr_lin_hist.addsample(pos); }
+  void unlog(int pos) { m_curr_lin_hist.subsample(pos); }
+  void snap_shot(unsigned long long current_cycle);
+  void spill(FILE *fout, bool final);
+
+  void print(FILE *fout) const;
+  void print_visualizer(FILE *fout);
+  void print_visualizer(gzFile fout);
+
+ private:
+  int m_n_bins;
+  linear_histogram_snapshot m_curr_lin_hist;
+  std::list<linear_histogram_snapshot> m_lin_hist_archive;
+  unsigned long long m_cycle;
+  bool m_reset_at_snap_shot;
+  std::string m_name;
+  int m_id;
+  static int s_ids;
 };
 
-void try_snap_shot (unsigned long long  current_cycle);
-void set_spill_interval (unsigned long long  interval);
-void spill_log_to_file (FILE *fout, int final, unsigned long long  current_cycle);
+enum cache_access_logger_types { NORMALS, TEXTURE, CONSTANT, INSTRUCTION };
 
-void create_thread_CFlogger( int n_loggers, int n_threads, address_type start_pc, unsigned long long  logging_interval);
-void destroy_thread_CFlogger( );
-void cflog_update_thread_pc( int logger_id, int thread_id, address_type pc );
-void cflog_snapshot( int logger_id, unsigned long long  cycle );
+void try_snap_shot(unsigned long long current_cycle);
+void set_spill_interval(unsigned long long interval);
+void spill_log_to_file(FILE *fout, int final, unsigned long long current_cycle);
+
+void create_thread_CFlogger(gpgpu_context *ctx, int n_loggers, int n_threads,
+                            address_type start_pc,
+                            unsigned long long logging_interval);
+void destroy_thread_CFlogger();
+void cflog_update_thread_pc(int logger_id, int thread_id, address_type pc);
+void cflog_snapshot(int logger_id, unsigned long long cycle);
 void cflog_print(FILE *fout);
 void cflog_print_path_expression(FILE *fout);
 void cflog_visualizer_print(FILE *fout);
 void cflog_visualizer_gzprint(gzFile fout);
 
-void insn_warp_occ_create( int n_loggers, int simd_width );
-void insn_warp_occ_log( int logger_id, address_type pc, int warp_occ );
-void insn_warp_occ_print( FILE *fout );
-
-
-void shader_warp_occ_create( int n_loggers, int simd_width, unsigned long long  logging_interval );
-void shader_warp_occ_log( int logger_id, int warp_occ );
-void shader_warp_occ_snapshot( int logger_id, unsigned long long  current_cycle );
-void shader_warp_occ_print( FILE *fout );
-
-
-void shader_mem_acc_create( int n_loggers, int n_dram, int n_bank, unsigned long long  logging_interval );
-void shader_mem_acc_log( int logger_id, int dram_id, int bank, char rw );
-void shader_mem_acc_snapshot( int logger_id, unsigned long long  current_cycle );
-void shader_mem_acc_print( FILE *fout );
+void insn_warp_occ_create(int n_loggers, int simd_width);
+void insn_warp_occ_log(int logger_id, address_type pc, int warp_occ);
+void insn_warp_occ_print(FILE *fout);
 
+void shader_warp_occ_create(int n_loggers, int simd_width,
+                            unsigned long long logging_interval);
+void shader_warp_occ_log(int logger_id, int warp_occ);
+void shader_warp_occ_snapshot(int logger_id, unsigned long long current_cycle);
+void shader_warp_occ_print(FILE *fout);
 
-void shader_mem_lat_create( int n_loggers, unsigned long long  logging_interval );
-void shader_mem_lat_log( int logger_id, int latency );
-void shader_mem_lat_snapshot( int logger_id, unsigned long long  current_cycle );
-void shader_mem_lat_print( FILE *fout );
+void shader_mem_acc_create(int n_loggers, int n_dram, int n_bank,
+                           unsigned long long logging_interval);
+void shader_mem_acc_log(int logger_id, int dram_id, int bank, char rw);
+void shader_mem_acc_snapshot(int logger_id, unsigned long long current_cycle);
+void shader_mem_acc_print(FILE *fout);
 
+void shader_mem_lat_create(int n_loggers, unsigned long long logging_interval);
+void shader_mem_lat_log(int logger_id, int latency);
+void shader_mem_lat_snapshot(int logger_id, unsigned long long current_cycle);
+void shader_mem_lat_print(FILE *fout);
 
 int get_shader_normal_cache_id();
 int get_shader_texture_cache_id();
 int get_shader_constant_cache_id();
 int get_shader_instruction_cache_id();
-void shader_cache_access_create( int n_loggers, int n_types, unsigned long long  logging_interval );
-void shader_cache_access_log( int logger_id, int type, int miss);
-void shader_cache_access_unlog( int logger_id, int type, int miss);
-void shader_cache_access_print( FILE *fout );
-
-
-void shader_CTA_count_create( int n_shaders, unsigned long long  logging_interval);
-void shader_CTA_count_log( int shader_id, int nCTAadded );
-void shader_CTA_count_unlog( int shader_id, int nCTAdone );
-void shader_CTA_count_resetnow( );
-void shader_CTA_count_print( FILE *fout );
-void shader_CTA_count_visualizer_print( FILE *fout );
+void shader_cache_access_create(int n_loggers, int n_types,
+                                unsigned long long logging_interval);
+void shader_cache_access_log(int logger_id, int type, int miss);
+void shader_cache_access_unlog(int logger_id, int type, int miss);
+void shader_cache_access_print(FILE *fout);
+
+void shader_CTA_count_create(int n_shaders,
+                             unsigned long long logging_interval);
+void shader_CTA_count_log(int shader_id, int nCTAadded);
+void shader_CTA_count_unlog(int shader_id, int nCTAdone);
+void shader_CTA_count_resetnow();
+void shader_CTA_count_print(FILE *fout);
+void shader_CTA_count_visualizer_print(FILE *fout);
 void shader_CTA_count_visualizer_gzprint(gzFile fout);
 
 #endif /* CFLOGGER_H */
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.cc b/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.cc
index 8ec3521f8d..c3ed3ce147 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.cc
@@ -28,311 +28,330 @@
 #include "gpgpusim_entrypoint.h"
 #include <stdio.h>
 
-#include "option_parser.h"
+#include "../libcuda_sim/gpgpu_context.h"
 #include "cuda-sim/cuda-sim.h"
 #include "cuda-sim/ptx_ir.h"
 #include "cuda-sim/ptx_parser.h"
 #include "gpgpu-sim/gpu-sim.h"
 #include "gpgpu-sim/icnt_wrapper.h"
+#include "option_parser.h"
 #include "stream_manager.h"
+#include <dlfcn.h>
+//#include "opu/isasim/executor/inc/IsaSim.h"
+class IsaSim;
 
-// #include <pthread.h>
-// #include <semaphore.h>
-
-#define MAX(a,b) (((a)>(b))?(a):(b))
-
-
-
-struct gpgpu_ptx_sim_arg *grid_params;
-
-// sem_t g_sim_signal_start;
-// sem_t g_sim_signal_finish;
-// sem_t g_sim_signal_exit;
-time_t g_simulation_starttime;
-// pthread_t g_simulation_thread;
-
-gpgpu_sim_config g_the_gpu_config;
-gpgpu_sim *g_the_gpu;
-stream_manager *g_stream_manager;
-
-
+#define MAX(a, b) (((a) > (b)) ? (a) : (b))
 
 static int sg_argc = 3;
-static const char *sg_argv[] = {"", "-config","gpgpusim.config"};
-
-
-
-static void print_simulation_time();
-
-void *gpgpu_sim_thread_sequential(void*)
-{
-   // at most one kernel running at a time
-   bool done;
-   do {
-      // sem_wait(&g_sim_signal_start);
-      done = true;
-      if( g_the_gpu->get_more_cta_left() ) {
-          done = false;
-          g_the_gpu->init();
-          while( g_the_gpu->active() ) {
-              // TODO schi 
-              // g_the_gpu->cycle();
-              g_the_gpu->deadlock_check();
-          }
-          g_the_gpu->print_stats();
-          g_the_gpu->update_stats();
-          print_simulation_time();
+static const char *sg_argv[] = {"", "-config", "gpgpusim.config"};
+
+void *gpgpu_sim_thread_sequential(void *ctx_ptr) {
+  gpgpu_context *ctx = (gpgpu_context *)ctx_ptr;
+  // at most one kernel running at a time
+  bool done;
+  do {
+    sem_wait(&(ctx->the_gpgpusim->g_sim_signal_start));
+    done = true;
+    if (ctx->the_gpgpusim->g_the_gpu->get_more_cta_left()) {
+      done = false;
+      ctx->the_gpgpusim->g_the_gpu->init();
+      while (ctx->the_gpgpusim->g_the_gpu->active()) {
+        // TODO schi
+        // ctx->the_gpgpusim->g_the_gpu->cycle();
+        ctx->the_gpgpusim->g_the_gpu->deadlock_check();
       }
-      // sem_post(&g_sim_signal_finish);
-   } while(!done);
-   // sem_post(&g_sim_signal_exit);
-   return NULL;
+      ctx->the_gpgpusim->g_the_gpu->print_stats();
+      ctx->the_gpgpusim->g_the_gpu->update_stats();
+      ctx->print_simulation_time();
+    }
+    sem_post(&(ctx->the_gpgpusim->g_sim_signal_finish));
+  } while (!done);
+  sem_post(&(ctx->the_gpgpusim->g_sim_signal_exit));
+  return NULL;
 }
 
-// pthread_mutex_t g_sim_lock = PTHREAD_MUTEX_INITIALIZER;
-bool g_sim_active = false;
-bool g_sim_done = true;
-bool break_limit = false;
-
-static void termination_callback()
-{
-    printf("GPGPU-Sim: *** exit detected ***\n");
-    fflush(stdout);
+static void termination_callback() {
+  printf("GPGPU-Sim: *** exit detected ***\n");
+  fflush(stdout);
 }
 
-void *gpgpu_sim_thread_concurrent(void*)
-{
-    atexit(termination_callback);
-    // concurrent kernel execution simulation thread
+void *gpgpu_sim_thread_concurrent(void *ctx_ptr) {
+  gpgpu_context *ctx = (gpgpu_context *)ctx_ptr;
+  atexit(termination_callback);
+  // concurrent kernel execution simulation thread
+  do {
+    if (g_debug_execution >= 3) {
+      printf(
+          "GPGPU-Sim: *** simulation thread starting and spinning waiting for "
+          "work ***\n");
+      fflush(stdout);
+    }
+    while (ctx->the_gpgpusim->g_stream_manager->empty_protected() &&
+           !ctx->the_gpgpusim->g_sim_done)
+      ;
+    if (g_debug_execution >= 3) {
+      printf("GPGPU-Sim: ** START simulation thread (detected work) **\n");
+      ctx->the_gpgpusim->g_stream_manager->print(stdout);
+      fflush(stdout);
+    }
+    pthread_mutex_lock(&(ctx->the_gpgpusim->g_sim_lock));
+    ctx->the_gpgpusim->g_sim_active = true;
+    pthread_mutex_unlock(&(ctx->the_gpgpusim->g_sim_lock));
+    bool active = false;
+    bool sim_cycles = false;
+    ctx->the_gpgpusim->g_the_gpu->init();
     do {
-        if(g_debug_execution >= 3) {
-           printf("GPGPU-Sim: *** simulation thread starting and spinning waiting for work ***\n");
-           fflush(stdout);
-        }
-        while( g_stream_manager->empty_protected() && !g_sim_done )
-            ;
-        if(g_debug_execution >= 3) {
-           printf("GPGPU-Sim: ** START simulation thread (detected work) **\n");
-           g_stream_manager->print(stdout);
-           fflush(stdout);
-        }
-        // pthread_mutex_lock(&g_sim_lock);
-        g_sim_active = true;
-        // pthread_mutex_unlock(&g_sim_lock);
-        bool active = false;
-        bool sim_cycles = false;
-        g_the_gpu->init();
-        do {
-            // check if a kernel has completed
-            // launch operation on device if one is pending and can be run
-
-            // Need to break this loop when a kernel completes. This was a
-            // source of non-deterministic behaviour in GPGPU-Sim (bug 147).
-            // If another stream operation is available, g_the_gpu remains active,
-            // causing this loop to not break. If the next operation happens to be
-            // another kernel, the gpu is not re-initialized and the inter-kernel
-            // behaviour may be incorrect. Check that a kernel has finished and
-            // no other kernel is currently running.
-            if(g_stream_manager->operation(&sim_cycles) && !g_the_gpu->active())
-                break;
-
-            //functional simulation
-            /*
-            if( g_the_gpu->is_functional_sim()) {
-                kernel_info_t * kernel = g_the_gpu->get_functional_kernel();
-                assert(kernel);
-                gpgpu_cuda_ptx_sim_main_func(*kernel);
-                g_the_gpu->finish_functional_sim(kernel);
-            }
-            */
-
-            //performance simulation
-            if( g_the_gpu->active() ) {
-                // TODO schi g_the_gpu->cycle();
-                sim_cycles = true;
-                g_the_gpu->deadlock_check();
-            }
-            /* else {
-                if(g_the_gpu->cycle_insn_cta_max_hit()){
-                    g_stream_manager->stop_all_running_kernels();
-                    g_sim_done = true;
-                    break_limit = true;
-                }
-            }
-            */
-
-            active=g_the_gpu->active() || !g_stream_manager->empty_protected();
-
-        } while( active /*&& !g_sim_done*/);
-        if(g_debug_execution >= 3) {
-           printf("GPGPU-Sim: ** STOP simulation thread (no work) **\n");
-           fflush(stdout);
-        }
-        if(sim_cycles) {
-            // g_the_gpu->print_stats();
-            g_the_gpu->update_stats();
-            print_simulation_time();
+      // check if a kernel has completed
+      // launch operation on device if one is pending and can be run
+
+      // Need to break this loop when a kernel completes. This was a
+      // source of non-deterministic behaviour in GPGPU-Sim (bug 147).
+      // If another stream operation is available, g_the_gpu remains active,
+      // causing this loop to not break. If the next operation happens to be
+      // another kernel, the gpu is not re-initialized and the inter-kernel
+      // behaviour may be incorrect. Check that a kernel has finished and
+      // no other kernel is currently running.
+      if (ctx->the_gpgpusim->g_stream_manager->operation(&sim_cycles) &&
+          !ctx->the_gpgpusim->g_the_gpu->active())
+        break;
+
+      // functional simulation
+      /*
+      if (ctx->the_gpgpusim->g_the_gpu->is_functional_sim()) {
+        kernel_info_t *kernel =
+            ctx->the_gpgpusim->g_the_gpu->get_functional_kernel();
+        assert(kernel);
+        ctx->the_gpgpusim->gpgpu_ctx->func_sim->gpgpu_cuda_ptx_sim_main_func(
+            *kernel);
+        ctx->the_gpgpusim->g_the_gpu->finish_functional_sim(kernel);
+      }
+      */
+
+      // performance simulation
+      if (ctx->the_gpgpusim->g_the_gpu->active()) {
+        // TODO schi
+        // ctx->the_gpgpusim->g_the_gpu->cycle();
+        sim_cycles = true;
+        ctx->the_gpgpusim->g_the_gpu->deadlock_check();
+      }
+      /* else {
+        if (ctx->the_gpgpusim->g_the_gpu->cycle_insn_cta_max_hit()) {
+          ctx->the_gpgpusim->g_stream_manager->stop_all_running_kernels();
+          ctx->the_gpgpusim->g_sim_done = true;
+          ctx->the_gpgpusim->break_limit = true;
         }
-        // pthread_mutex_lock(&g_sim_lock);
-        g_sim_active = false;
-        // pthread_mutex_unlock(&g_sim_lock);
-    } while( !g_sim_done );
+      }
+      */
 
-    printf("GPGPU-Sim: *** simulation thread exiting ***\n");
-    fflush(stdout);
+      active = ctx->the_gpgpusim->g_the_gpu->active() ||
+               !(ctx->the_gpgpusim->g_stream_manager->empty_protected());
 
-    if(break_limit) {
-    	printf("GPGPU-Sim: ** break due to reaching the maximum cycles (or instructions) **\n");
-    	exit(1);
+    } while (active && !ctx->the_gpgpusim->g_sim_done);
+    if (g_debug_execution >= 3) {
+      printf("GPGPU-Sim: ** STOP simulation thread (no work) **\n");
+      fflush(stdout);
     }
-
-    // sem_post(&g_sim_signal_exit);
-    return NULL;
-}
-
-void synchronize()
-{
-    printf("GPGPU-Sim: synchronize waiting for inactive GPU simulation\n");
-    g_stream_manager->print(stdout);
-    fflush(stdout);
-//    sem_wait(&g_sim_signal_finish);
-    bool done = false;
-    do {
-//        pthread_mutex_lock(&g_sim_lock);
-        done = ( g_stream_manager->empty() && !g_sim_active ); //  || g_sim_done;
-//        pthread_mutex_unlock(&g_sim_lock);
-    } while (!done);
-    printf("GPGPU-Sim: detected inactive GPU simulation thread\n");
-    fflush(stdout);
-//    sem_post(&g_sim_signal_start);
-}
-
-void exit_simulation()
-{
-    g_sim_done=true;
-    printf("GPGPU-Sim: exit_simulation called\n");
-    fflush(stdout);
-//     sem_wait(&g_sim_signal_exit);
-    printf("GPGPU-Sim: simulation thread signaled exit\n");
-    fflush(stdout);
+    if (sim_cycles) {
+      ctx->the_gpgpusim->g_the_gpu->print_stats();
+      ctx->the_gpgpusim->g_the_gpu->update_stats();
+      ctx->print_simulation_time();
+    }
+    pthread_mutex_lock(&(ctx->the_gpgpusim->g_sim_lock));
+    ctx->the_gpgpusim->g_sim_active = false;
+    pthread_mutex_unlock(&(ctx->the_gpgpusim->g_sim_lock));
+  } while (!ctx->the_gpgpusim->g_sim_done);
+
+  printf("GPGPU-Sim: *** simulation thread exiting ***\n");
+  fflush(stdout);
+
+  if (ctx->the_gpgpusim->break_limit) {
+    printf(
+        "GPGPU-Sim: ** break due to reaching the maximum cycles (or "
+        "instructions) **\n");
+    exit(1);
+  }
+
+  sem_post(&(ctx->the_gpgpusim->g_sim_signal_exit));
+  return NULL;
 }
 
-extern bool g_cuda_launch_blocking;
-
-gpgpu_sim *gpgpu_ptx_sim_init_perf()
-{
-   srand(1);
-   print_splash();
-   read_sim_environment_variables();
-   read_parser_environment_variables();
-   option_parser_t opp = option_parser_create();
-
-   ptx_reg_options(opp);
-   ptx_opcocde_latency_options(opp);
+typedef IsaSim* (*pfn_make_isasim)(gpgpu_t* gpu, gpgpu_context *ctx);
 
-   icnt_reg_options(opp);
-   g_the_gpu_config.reg_options(opp); // register GPU microrachitecture options
-
-   option_parser_cmdline(opp, sg_argc, sg_argv); // parse configuration options
-   fprintf(stdout, "GPGPU-Sim: Configuration options:\n\n");
-   option_parser_print(opp, stdout);
-   // Set the Numeric locale to a standard locale where a decimal point is a "dot" not a "comma"
-   // so it does the parsing correctly independent of the system environment variables
-   assert(setlocale(LC_NUMERIC,"C"));
-   g_the_gpu_config.init();
+IsaSim* gpgpu_context::get_isasim() {
+    static IsaSim* isasim = nullptr;
+    if (isasim == nullptr) {
+        void* lib_handle = dlopen("libisasim.so", RTLD_NOW | RTLD_GLOBAL);
+        if (lib_handle == nullptr) {
+            printf("Failed to load libisasim.so, error - %sn\n", dlerror());
+            exit(-1);
+        }
+        pfn_make_isasim make_isasim = (pfn_make_isasim)dlsym(lib_handle, "make_isasim");
 
-   g_the_gpu = new gpgpu_sim(g_the_gpu_config);
-   g_stream_manager = new stream_manager(g_the_gpu,g_cuda_launch_blocking);
+        if (make_isasim == nullptr) {
+            printf("Failed to dlsym make_isasim, error - %sn\n", dlerror());
+            exit(-1);
+        }
+        isasim = make_isasim(this->the_gpgpusim->g_the_gpu, this);
+    }
+    return isasim;
+}
 
-   g_simulation_starttime = time((time_t *)NULL);
+void gpgpu_context::synchronize() {
+  printf("GPGPU-Sim: synchronize waiting for inactive GPU simulation\n");
+  the_gpgpusim->g_stream_manager->print(stdout);
+  fflush(stdout);
+  //    sem_wait(&g_sim_signal_finish);
+  bool done = false;
+  do {
+    pthread_mutex_lock(&(the_gpgpusim->g_sim_lock));
+    done = (the_gpgpusim->g_stream_manager->empty() &&
+            !the_gpgpusim->g_sim_active) ||
+           the_gpgpusim->g_sim_done;
+    pthread_mutex_unlock(&(the_gpgpusim->g_sim_lock));
+  } while (!done);
+  printf("GPGPU-Sim: detected inactive GPU simulation thread\n");
+  fflush(stdout);
+  //    sem_post(&g_sim_signal_start);
+}
 
-//   sem_init(&g_sim_signal_start,0,0);
-//   sem_init(&g_sim_signal_finish,0,0);
-//   sem_init(&g_sim_signal_exit,0,0);
+void gpgpu_context::exit_simulation() {
+  the_gpgpusim->g_sim_done = true;
+  printf("GPGPU-Sim: exit_simulation called\n");
+  fflush(stdout);
+  sem_wait(&(the_gpgpusim->g_sim_signal_exit));
+  printf("GPGPU-Sim: simulation thread signaled exit\n");
+  fflush(stdout);
+}
 
-   return g_the_gpu;
+gpgpu_sim *gpgpu_context::gpgpu_ptx_sim_init_perf() {
+  srand(1);
+  print_splash();
+  func_sim->read_sim_environment_variables();
+  ptx_parser->read_parser_environment_variables();
+  option_parser_t opp = option_parser_create();
+
+  ptx_reg_options(opp);
+  func_sim->ptx_opcocde_latency_options(opp);
+
+  icnt_reg_options(opp);
+  the_gpgpusim->g_the_gpu_config = new gpgpu_sim_config(this);
+  the_gpgpusim->g_the_gpu_config->reg_options(
+      opp);  // register GPU microrachitecture options
+
+  option_parser_cmdline(opp, sg_argc, sg_argv);  // parse configuration options
+  fprintf(stdout, "GPGPU-Sim: Configuration options:\n\n");
+  option_parser_print(opp, stdout);
+  // Set the Numeric locale to a standard locale where a decimal point is a
+  // "dot" not a "comma" so it does the parsing correctly independent of the
+  // system environment variables
+  assert(setlocale(LC_NUMERIC, "C"));
+  the_gpgpusim->g_the_gpu_config->init();
+
+  the_gpgpusim->g_the_gpu =
+      new exec_gpgpu_sim(*(the_gpgpusim->g_the_gpu_config), this);
+  the_gpgpusim->g_stream_manager = new stream_manager(
+      (the_gpgpusim->g_the_gpu), func_sim->g_cuda_launch_blocking);
+
+  the_gpgpusim->g_simulation_starttime = time((time_t *)NULL);
+
+  sem_init(&(the_gpgpusim->g_sim_signal_start), 0, 0);
+  sem_init(&(the_gpgpusim->g_sim_signal_finish), 0, 0);
+  sem_init(&(the_gpgpusim->g_sim_signal_exit), 0, 0);
+
+  return the_gpgpusim->g_the_gpu;
 }
-// TODO schi 
-gpgpu_sim *gem5_ptx_sim_init_perf(stream_manager **p_stream_manager, CudaGPU *cuda_gpu, const char *config_path)
+
+gpgpu_sim *gpgpu_context::gem5_ptx_sim_init_perf(stream_manager **p_stream_manager, CudaGPU *cuda_gpu, const char *config_path)
 {
-   print_splash();
-   read_sim_environment_variables();
-   read_parser_environment_variables();
-   option_parser_t opp = option_parser_create();
-
-   icnt_reg_options(opp);
-   g_the_gpu_config.reg_options(opp); // register GPU microrachitecture options
-   ptx_reg_options(opp);
-   ptx_opcocde_latency_options(opp);
-   sg_argv[2] = config_path;
-   option_parser_cmdline(opp, sg_argc, sg_argv); // parse configuration options
-   fprintf(stdout, "GPGPU-Sim: Configuration options:\n\n");
-   option_parser_print(opp, stdout);
-   option_parser_destroy(opp);
-   // Set the Numeric locale to a standard locale where a decimal point is a "dot" not a "comma"
-   // so it does the parsing correctly independent of the system environment variables
-   assert(setlocale(LC_NUMERIC,"C"));
-   g_the_gpu_config.init();
-
-   g_the_gpu = new gpgpu_sim(g_the_gpu_config, cuda_gpu);
-   g_stream_manager = new stream_manager(g_the_gpu,g_cuda_launch_blocking);
-
-   g_simulation_starttime = time((time_t *)NULL);
-
-   *p_stream_manager = g_stream_manager;
-   return g_the_gpu;
+  srand(1);
+  print_splash();
+  func_sim->read_sim_environment_variables();
+  ptx_parser->read_parser_environment_variables();
+  option_parser_t opp = option_parser_create();
+
+  ptx_reg_options(opp);
+  func_sim->ptx_opcocde_latency_options(opp);
+
+  icnt_reg_options(opp);
+  the_gpgpusim->g_the_gpu_config = new gpgpu_sim_config(this);
+  the_gpgpusim->g_the_gpu_config->reg_options(
+      opp);  // register GPU microrachitecture options
+
+  option_parser_cmdline(opp, sg_argc, sg_argv);  // parse configuration options
+  fprintf(stdout, "GPGPU-Sim: Configuration options:\n\n");
+  option_parser_print(opp, stdout);
+  // Set the Numeric locale to a standard locale where a decimal point is a
+  // "dot" not a "comma" so it does the parsing correctly independent of the
+  // system environment variables
+  assert(setlocale(LC_NUMERIC, "C"));
+  the_gpgpusim->g_the_gpu_config->init();
+
+  the_gpgpusim->g_the_gpu =
+      new exec_gpgpu_sim(*(the_gpgpusim->g_the_gpu_config), this, cuda_gpu);
+  the_gpgpusim->g_stream_manager = new stream_manager(
+      (the_gpgpusim->g_the_gpu), func_sim->g_cuda_launch_blocking);
+
+  the_gpgpusim->g_simulation_starttime = time((time_t *)NULL);
+
+  *p_stream_manager = the_gpgpusim->g_stream_manager;
+
+  return the_gpgpusim->g_the_gpu;
 }
 
-/*
-void start_sim_thread(int api)
-{
-    if( g_sim_done ) {
-        g_sim_done = false;
-        if( api == 1 ) {
-           pthread_create(&g_simulation_thread,NULL,gpgpu_sim_thread_concurrent,NULL);
-        } else {
-           pthread_create(&g_simulation_thread,NULL,gpgpu_sim_thread_sequential,NULL);
-        }
+void gpgpu_context::start_sim_thread(int api) {
+  if (the_gpgpusim->g_sim_done) {
+    the_gpgpusim->g_sim_done = false;
+    if (api == 1) {
+      pthread_create(&(the_gpgpusim->g_simulation_thread), NULL,
+                     gpgpu_sim_thread_concurrent, (void *)this);
+    } else {
+      pthread_create(&(the_gpgpusim->g_simulation_thread), NULL,
+                     gpgpu_sim_thread_sequential, (void *)this);
     }
+  }
 }
-*/
 
-void print_simulation_time()
-{
-   time_t current_time, difference, d, h, m, s;
-   current_time = time((time_t *)NULL);
-   difference = MAX(current_time - g_simulation_starttime, 1);
-
-   d = difference/(3600*24);
-   h = difference/3600 - 24*d;
-   m = difference/60 - 60*(h + 24*d);
-   s = difference - 60*(m + 60*(h + 24*d));
-
-   fflush(stderr);
-   printf("\n\ngpgpu_simulation_time = %u days, %u hrs, %u min, %u sec (%u sec)\n",
-          (unsigned)d, (unsigned)h, (unsigned)m, (unsigned)s, (unsigned)difference );
-   printf("gpgpu_simulation_rate = %u (inst/sec)\n", (unsigned)(g_the_gpu->gpu_tot_sim_insn / difference) );
-   printf("gpgpu_simulation_rate = %u (cycle/sec)\n", (unsigned)(gpu_tot_sim_cycle / difference) );
-   fflush(stdout);
+void gpgpu_context::print_simulation_time() {
+  time_t current_time, difference, d, h, m, s;
+  current_time = time((time_t *)NULL);
+  difference = MAX(current_time - the_gpgpusim->g_simulation_starttime, 1);
+
+  d = difference / (3600 * 24);
+  h = difference / 3600 - 24 * d;
+  m = difference / 60 - 60 * (h + 24 * d);
+  s = difference - 60 * (m + 60 * (h + 24 * d));
+
+  fflush(stderr);
+  printf(
+      "\n\ngpgpu_simulation_time = %u days, %u hrs, %u min, %u sec (%u sec)\n",
+      (unsigned)d, (unsigned)h, (unsigned)m, (unsigned)s, (unsigned)difference);
+  printf("gpgpu_simulation_rate = %u (inst/sec)\n",
+         (unsigned)(the_gpgpusim->g_the_gpu->gpu_tot_sim_insn / difference));
+  const unsigned cycles_per_sec =
+      (unsigned)(the_gpgpusim->g_the_gpu->gpu_tot_sim_cycle / difference);
+  printf("gpgpu_simulation_rate = %u (cycle/sec)\n", cycles_per_sec);
+  printf("gpgpu_silicon_slowdown = %ux\n",
+         the_gpgpusim->g_the_gpu->shader_clock() * 1000 / cycles_per_sec);
+  fflush(stdout);
 }
 
-int gpgpu_opencl_ptx_sim_main_perf( kernel_info_t *grid )
-{
-   g_the_gpu->launch(grid);
-//   sem_post(&g_sim_signal_start);
-//   sem_wait(&g_sim_signal_finish);
-   return 0;
+int gpgpu_context::gpgpu_opencl_ptx_sim_main_perf(kernel_info_t *grid) {
+  the_gpgpusim->g_the_gpu->launch(grid);
+  sem_post(&(the_gpgpusim->g_sim_signal_start));
+  sem_wait(&(the_gpgpusim->g_sim_signal_finish));
+  return 0;
 }
 
 //! Functional simulation of OpenCL
 /*!
  * This function call the CUDA PTX functional simulator
  */
-int gpgpu_opencl_ptx_sim_main_func( kernel_info_t *grid )
-{
-    //calling the CUDA PTX simulator, sending the kernel by reference and a flag set to true,
-    //the flag used by the function to distinguish OpenCL calls from the CUDA simulation calls which
-    //it is needed by the called function to not register the exit the exit of OpenCL kernel as it doesn't register entering in the first place as the CUDA kernels does
-   gpgpu_cuda_ptx_sim_main_func( *grid, true );
-   return 0;
+int cuda_sim::gpgpu_opencl_ptx_sim_main_func(kernel_info_t *grid) {
+  // calling the CUDA PTX simulator, sending the kernel by reference and a flag
+  // set to true, the flag used by the function to distinguish OpenCL calls from
+  // the CUDA simulation calls which it is needed by the called function to not
+  // register the exit the exit of OpenCL kernel as it doesn't register entering
+  // in the first place as the CUDA kernels does
+  gpgpu_cuda_ptx_sim_main_func(*grid, true);
+  return 0;
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.h b/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.h
index b3e45543ba..c12b5f797a 100644
--- a/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.h
+++ b/design/gpgpu/gpgpu-sim/src/gpgpusim_entrypoint.h
@@ -27,21 +27,50 @@
 
 #ifndef GPGPUSIM_ENTRYPOINT_H_INCLUDED
 #define GPGPUSIM_ENTRYPOINT_H_INCLUDED
-
+#include <pthread.h>
+#include <semaphore.h>
+#include <time.h>
 #include "abstract_hardware_model.h"
 
-#include <time.h>
-extern time_t g_simulation_starttime;
+// extern time_t g_simulation_starttime;
+class gpgpu_context;
+
+class GPGPUsim_ctx {
+ public:
+  GPGPUsim_ctx(gpgpu_context *ctx) {
+    g_sim_active = false;
+    g_sim_done = true;
+    break_limit = false;
+    g_sim_lock = PTHREAD_MUTEX_INITIALIZER;
+
+    g_the_gpu_config = NULL;
+    g_the_gpu = NULL;
+    g_stream_manager = NULL;
+    the_cude_device = NULL;
+    the_context = NULL;
+    gpgpu_ctx = ctx;
+  }
 
+  // struct gpgpu_ptx_sim_arg *grid_params;
 
+  sem_t g_sim_signal_start;
+  sem_t g_sim_signal_finish;
+  sem_t g_sim_signal_exit;
+  time_t g_simulation_starttime;
+  pthread_t g_simulation_thread;
 
-class gpgpu_sim *gpgpu_ptx_sim_init_perf();
-// TODO schi 
-class gpgpu_sim *gem5_ptx_sim_init_perf(stream_manager **p_stream_manager, gem5::CudaGPU *cuda_gpu, const char *config_path);
+  class gpgpu_sim_config *g_the_gpu_config;
+  class gpgpu_sim *g_the_gpu;
+  class stream_manager *g_stream_manager;
 
-// void start_sim_thread(int api);
+  struct _cuda_device_id *the_cude_device;
+  struct CUctx_st *the_context;
+  gpgpu_context *gpgpu_ctx;
 
-int gpgpu_opencl_ptx_sim_main_perf( kernel_info_t *grid );
-int gpgpu_opencl_ptx_sim_main_func( kernel_info_t *grid );
+  pthread_mutex_t g_sim_lock;
+  bool g_sim_active;
+  bool g_sim_done;
+  bool break_limit;
+};
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.cc b/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.cc
index efb3e6b62b..e1dd64fe73 100644
--- a/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.cc
+++ b/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.cc
@@ -30,732 +30,1112 @@
 #define SP_BASE_POWER 0
 #define SFU_BASE_POWER  0
 
-
-static const char * pwr_cmp_label[] = {"IBP,", "ICP,", "DCP,", "TCP,", "CCP,", "SHRDP,", "RFP,", "SPP,",
-								"SFUP,", "FPUP,", "SCHEDP,", "L2CP,", "MCP,", "NOCP,", "DRAMP,",
-								"PIPEP,", "IDLE_COREP,", "CONST_DYNAMICP"};
+static const char* pwr_cmp_label[] = {
+    "IBP,", "ICP,", "DCP,", "TCP,", "CCP,", "SHRDP,", "RFP,", "INTP,", 
+    "FPUP,", "DPUP,", "INT_MUL24P,", "INT_MUL32P,", "INT_MULP,", "INT_DIVP,", 
+    "FP_MULP,", "FP_DIVP,", "FP_SQRTP,", "FP_LGP,", "FP_SINP,", "FP_EXP,", 
+    "DP_MULP,", "DP_DIVP,", "TENSORP,", "TEXP,", "SCHEDP,", "L2CP,", "MCP,", "NOCP,", 
+    "DRAMP,", "PIPEP,", "IDLE_COREP,", "CONSTP", "STATICP"};
 
 enum pwr_cmp_t {
-   IBP=0,
-   ICP,
-   DCP,
-   TCP,
-   CCP,
-   SHRDP,
-   RFP,
-   SPP,
-   SFUP,
-   FPUP,
-   SCHEDP,
-   L2CP,
-   MCP,
-   NOCP,
-   DRAMP,
-   PIPEP,
-   IDLE_COREP,
-   CONST_DYNAMICP,
-   NUM_COMPONENTS_MODELLED
+  IBP=0,
+  ICP,
+  DCP,
+  TCP,
+  CCP,
+  SHRDP,
+  RFP,
+  INTP,
+  FPUP,
+  DPUP,
+  INT_MUL24P,
+  INT_MUL32P,
+  INT_MULP,
+  INT_DIVP,
+  FP_MULP,
+  FP_DIVP,
+  FP_SQRTP,
+  FP_LGP,
+  FP_SINP,
+  FP_EXP,
+  DP_MULP,
+  DP_DIVP,
+  TENSORP,
+  TEXP,
+  SCHEDP,
+  L2CP,
+  MCP,
+  NOCP,
+  DRAMP,
+  PIPEP,
+  IDLE_COREP,
+  CONSTP,
+  STATICP,
+  NUM_COMPONENTS_MODELLED
 };
 
+gpgpu_sim_wrapper::gpgpu_sim_wrapper(bool power_simulation_enabled,
+                                     char* xmlfile, int power_simulation_mode, bool dvfs_enabled) {
+  kernel_sample_count = 0;
+  total_sample_count = 0;
+
+  kernel_tot_power = 0;
+  avg_threads_per_warp_tot = 0;
+  num_pwr_cmps = NUM_COMPONENTS_MODELLED;
+  num_perf_counters = NUM_PERFORMANCE_COUNTERS;
+
+  // Initialize per-component counter/power vectors
+  avg_max_min_counters<double> init;
+  kernel_cmp_pwr.resize(NUM_COMPONENTS_MODELLED, init);
+  kernel_cmp_perf_counters.resize(NUM_PERFORMANCE_COUNTERS, init);
+
+  kernel_power = init;   // Per-kernel powers
+  gpu_tot_power = init;  // Global powers
+
+  sample_cmp_pwr.resize(NUM_COMPONENTS_MODELLED, 0);
+
+  sample_perf_counters.resize(NUM_PERFORMANCE_COUNTERS, 0);
+  initpower_coeff.resize(NUM_PERFORMANCE_COUNTERS, 0);
+  effpower_coeff.resize(NUM_PERFORMANCE_COUNTERS, 0);
+
+  const_dynamic_power = 0;
+  proc_power = 0;
+
+  g_power_filename = NULL;
+  g_power_trace_filename = NULL;
+  g_metric_trace_filename = NULL;
+  g_steady_state_tracking_filename = NULL;
+  xml_filename = xmlfile;
+  g_power_simulation_enabled = power_simulation_enabled;
+  g_power_simulation_mode = power_simulation_mode;
+  g_dvfs_enabled = dvfs_enabled;
+  g_power_trace_enabled = false;
+  g_steady_power_levels_enabled = false;
+  g_power_trace_zlevel = 0;
+  g_power_per_cycle_dump = false;
+  gpu_steady_power_deviation = 0;
+  gpu_steady_min_period = 0;
+
+  gpu_stat_sample_freq = 0;
+  p = new ParseXML();
+  if (g_power_simulation_enabled) {
+    p->parse(xml_filename);
+  }
+  proc = new Processor(p);
+  power_trace_file = NULL;
+  metric_trace_file = NULL;
+  steady_state_tacking_file = NULL;
+  has_written_avg = false;
+  init_inst_val = false;
+}
 
-gpgpu_sim_wrapper::gpgpu_sim_wrapper( bool power_simulation_enabled, char* xmlfile) {
-	   kernel_sample_count=0;
-	   total_sample_count=0;
-
-	   kernel_tot_power=0;
-
-	   num_pwr_cmps=NUM_COMPONENTS_MODELLED;
-	   num_perf_counters=NUM_PERFORMANCE_COUNTERS;
+gpgpu_sim_wrapper::~gpgpu_sim_wrapper() {}
 
-	   // Initialize per-component counter/power vectors
-	   avg_max_min_counters<double> init;
-	   kernel_cmp_pwr.resize(NUM_COMPONENTS_MODELLED, init);
-	   kernel_cmp_perf_counters.resize(NUM_PERFORMANCE_COUNTERS, init);
+bool gpgpu_sim_wrapper::sanity_check(double a, double b) {
+  if (b == 0)
+    return (abs(a - b) < 0.00001);
+  else
+    return (abs(a - b) / abs(b) < 0.00001);
 
-	   kernel_power = init; // Per-kernel powers
-	   gpu_tot_power = init; // Global powers
+  return false;
+}
+void gpgpu_sim_wrapper::init_mcpat_hw_mode(unsigned gpu_sim_cycle) {
+   p->sys.total_cycles = gpu_sim_cycle; //total simulated cycles for current kernel
+}
 
-	   sample_cmp_pwr.resize(NUM_COMPONENTS_MODELLED, 0);
+void gpgpu_sim_wrapper::init_mcpat(
+    char* xmlfile, char* powerfilename, char* power_trace_filename,
+    char* metric_trace_filename, char* steady_state_filename,
+    bool power_sim_enabled, bool trace_enabled, bool steady_state_enabled,
+    bool power_per_cycle_dump, double steady_power_deviation,
+    double steady_min_period, int zlevel, double init_val,
+    int stat_sample_freq, int power_sim_mode, bool dvfs_enabled,
+    unsigned clock_freq, unsigned num_shaders) {
+  // Write File Headers for (-metrics trace, -power trace)
+
+  reset_counters();
+  static bool mcpat_init = true;
+
+  // initialize file name if it is not set
+  time_t curr_time;
+  time(&curr_time);
+  char* date = ctime(&curr_time);
+  char* s = date;
+  while (*s) {
+    if (*s == ' ' || *s == '\t' || *s == ':') *s = '-';
+    if (*s == '\n' || *s == '\r') *s = 0;
+    s++;
+  }
+
+  if (mcpat_init) {
+    g_power_filename = powerfilename;
+    g_power_trace_filename = power_trace_filename;
+    g_metric_trace_filename = metric_trace_filename;
+    g_steady_state_tracking_filename = steady_state_filename;
+    xml_filename = xmlfile;
+    g_power_simulation_enabled = power_sim_enabled;
+    g_power_simulation_mode = power_sim_mode;
+    g_dvfs_enabled = dvfs_enabled;
+    g_power_trace_enabled = trace_enabled;
+    g_steady_power_levels_enabled = steady_state_enabled;
+    g_power_trace_zlevel = zlevel;
+    g_power_per_cycle_dump = power_per_cycle_dump;
+    gpu_steady_power_deviation = steady_power_deviation;
+    gpu_steady_min_period = steady_min_period;
+
+    gpu_stat_sample_freq = stat_sample_freq;
+
+    // p->sys.total_cycles=gpu_stat_sample_freq*4;
+    p->sys.total_cycles = gpu_stat_sample_freq;
+    p->sys.target_core_clockrate = clock_freq;
+    p->sys.number_of_cores = num_shaders;
+    p->sys.core[0].clock_rate = clock_freq;
+    power_trace_file = NULL;
+    metric_trace_file = NULL;
+    steady_state_tacking_file = NULL;
+
+    if (g_power_trace_enabled) {
+      power_trace_file = gzopen(g_power_trace_filename, "w");
+      metric_trace_file = gzopen(g_metric_trace_filename, "w");
+      if ((power_trace_file == NULL) || (metric_trace_file == NULL)) {
+        printf("error - could not open trace files \n");
+        exit(1);
+      }
+      gzsetparams(power_trace_file, g_power_trace_zlevel, Z_DEFAULT_STRATEGY);
+
+      gzprintf(power_trace_file, "power,");
+      for (unsigned i = 0; i < num_pwr_cmps; i++) {
+        gzprintf(power_trace_file, pwr_cmp_label[i]);
+      }
+      gzprintf(power_trace_file, "\n");
+
+      gzsetparams(metric_trace_file, g_power_trace_zlevel, Z_DEFAULT_STRATEGY);
+      for (unsigned i = 0; i < num_perf_counters; i++) {
+        gzprintf(metric_trace_file, perf_count_label[i]);
+      }
+      gzprintf(metric_trace_file, "\n");
+
+      gzclose(power_trace_file);
+      gzclose(metric_trace_file);
+    }
+    if (g_steady_power_levels_enabled) {
+      steady_state_tacking_file = gzopen(g_steady_state_tracking_filename, "w");
+      if ((steady_state_tacking_file == NULL)) {
+        printf("error - could not open trace files \n");
+        exit(1);
+      }
+      gzsetparams(steady_state_tacking_file, g_power_trace_zlevel,
+                  Z_DEFAULT_STRATEGY);
+      gzprintf(steady_state_tacking_file, "start,end,power,IPC,");
+      for (unsigned i = 0; i < num_perf_counters; i++) {
+        gzprintf(steady_state_tacking_file, perf_count_label[i]);
+      }
+      gzprintf(steady_state_tacking_file, "\n");
+
+      gzclose(steady_state_tacking_file);
+    }
 
-	   sample_perf_counters.resize(NUM_PERFORMANCE_COUNTERS, 0);
-	   initpower_coeff.resize(NUM_PERFORMANCE_COUNTERS, 0);
-	   effpower_coeff.resize(NUM_PERFORMANCE_COUNTERS, 0);
+    mcpat_init = false;
+    has_written_avg = false;
+    powerfile.open(g_power_filename);
+    int flg = chmod(g_power_filename, S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH);
+    assert(flg == 0);
+  }
+  sample_val = 0;
+  init_inst_val = init_val;  // gpu_tot_sim_insn+gpu_sim_insn;
+}
 
-	   const_dynamic_power=0;
-	   proc_power=0;
+void gpgpu_sim_wrapper::reset_counters() {
+  avg_max_min_counters<double> init;
+  for (unsigned i = 0; i < num_perf_counters; ++i) {
+    sample_perf_counters[i] = 0;
+    kernel_cmp_perf_counters[i] = init;
+  }
+  for (unsigned i = 0; i < num_pwr_cmps; ++i) {
+    sample_cmp_pwr[i] = 0;
+    kernel_cmp_pwr[i] = init;
+  }
+
+  // Reset per-kernel counters
+  kernel_sample_count = 0;
+  kernel_tot_power = 0;
+  kernel_power = init;
+  avg_threads_per_warp_tot = 0;
+  return;
+}
 
-	   g_power_filename = NULL;
-	   g_power_trace_filename = NULL;
-	   g_metric_trace_filename = NULL;
-	   g_steady_state_tracking_filename = NULL;
-	   xml_filename= xmlfile;
-	   g_power_simulation_enabled= power_simulation_enabled;
-	   g_power_trace_enabled= false;
-	   g_steady_power_levels_enabled= false;
-	   g_power_trace_zlevel= 0;
-	   g_power_per_cycle_dump= false;
-	   gpu_steady_power_deviation= 0;
-	   gpu_steady_min_period= 0;
+void gpgpu_sim_wrapper::set_inst_power(bool clk_gated_lanes, double tot_cycles,
+                                       double busy_cycles, double tot_inst,
+                                       double int_inst, double fp_inst,
+                                       double load_inst, double store_inst,
+                                       double committed_inst) {
+  p->sys.core[0].gpgpu_clock_gated_lanes = clk_gated_lanes;
+  p->sys.core[0].total_cycles = tot_cycles;
+  p->sys.core[0].busy_cycles = busy_cycles;
+  p->sys.core[0].total_instructions =
+      tot_inst * p->sys.scaling_coefficients[TOT_INST];
+  p->sys.core[0].int_instructions =
+      int_inst * p->sys.scaling_coefficients[FP_INT];
+  p->sys.core[0].fp_instructions =
+      fp_inst * p->sys.scaling_coefficients[FP_INT];
+  p->sys.core[0].load_instructions = load_inst;
+  p->sys.core[0].store_instructions = store_inst;
+  p->sys.core[0].committed_instructions = committed_inst;
+  sample_perf_counters[FP_INT] = int_inst + fp_inst;
+  sample_perf_counters[TOT_INST] = tot_inst;
+}
 
-	   gpu_stat_sample_freq=0;
-	   p=new ParseXML();
-	   if (g_power_simulation_enabled){
-	       p->parse(xml_filename);
-	   }
-	   proc = new Processor(p);
-	   power_trace_file = NULL;
-	   metric_trace_file = NULL;
-	   steady_state_tacking_file = NULL;
-	   has_written_avg=false;
-	   init_inst_val=false;
+void gpgpu_sim_wrapper::set_regfile_power(double reads, double writes,
+                                          double ops) {
+  p->sys.core[0].int_regfile_reads =
+      reads * p->sys.scaling_coefficients[REG_RD];
+  p->sys.core[0].int_regfile_writes =
+      writes * p->sys.scaling_coefficients[REG_WR];
+  p->sys.core[0].non_rf_operands =
+      ops * p->sys.scaling_coefficients[NON_REG_OPs];
+  sample_perf_counters[REG_RD] = reads;
+  sample_perf_counters[REG_WR] = writes;
+  sample_perf_counters[NON_REG_OPs] = ops;
+}
 
+void gpgpu_sim_wrapper::set_icache_power(double hits, double misses) {
+  p->sys.core[0].icache.read_accesses =
+      hits * p->sys.scaling_coefficients[IC_H] +
+      misses * p->sys.scaling_coefficients[IC_M];
+  p->sys.core[0].icache.read_misses =
+      misses * p->sys.scaling_coefficients[IC_M];
+  sample_perf_counters[IC_H] = hits;
+  sample_perf_counters[IC_M] = misses;
 }
 
-gpgpu_sim_wrapper::~gpgpu_sim_wrapper() { }
+void gpgpu_sim_wrapper::set_ccache_power(double hits, double misses) {
+  p->sys.core[0].ccache.read_accesses =
+      hits * p->sys.scaling_coefficients[CC_H] +
+      misses * p->sys.scaling_coefficients[CC_M];
+  p->sys.core[0].ccache.read_misses =
+      misses * p->sys.scaling_coefficients[CC_M];
+  sample_perf_counters[CC_H] = hits;
+  sample_perf_counters[CC_M] = misses;
+  // TODO: coalescing logic is counted as part of the caches power (this is not
+  // valid for no-caches architectures)
+}
 
-bool gpgpu_sim_wrapper::sanity_check(double a, double b)
-{
-	if (b == 0)
-		return (abs(a-b)<0.00001);
-	else
-		return (abs(a-b)/abs(b)<0.00001);
-
-	return false;
-}
-void gpgpu_sim_wrapper::init_mcpat(char* xmlfile, char* powerfilename, char* power_trace_filename,char* metric_trace_filename,
-								   char * steady_state_filename, bool power_sim_enabled,bool trace_enabled,
-								   bool steady_state_enabled,bool power_per_cycle_dump,double steady_power_deviation,
-								   double steady_min_period, int zlevel, double init_val,int stat_sample_freq ){
-	// Write File Headers for (-metrics trace, -power trace)
-
-	reset_counters();
-   static bool mcpat_init=true;
-
-   // initialize file name if it is not set
-   time_t curr_time;
-   time(&curr_time);
-   char *date = ctime(&curr_time);
-   char *s = date;
-   while (*s) {
-       if (*s == ' ' || *s == '\t' || *s == ':') *s = '-';
-       if (*s == '\n' || *s == '\r' ) *s = 0;
-       s++;
-   }
-
-   if(mcpat_init){
-	   g_power_filename = powerfilename;
-	   g_power_trace_filename =power_trace_filename;
-	   g_metric_trace_filename = metric_trace_filename;
-	   g_steady_state_tracking_filename = steady_state_filename;
-	   xml_filename=xmlfile;
-	   g_power_simulation_enabled=power_sim_enabled;
-	   g_power_trace_enabled=trace_enabled;
-	   g_steady_power_levels_enabled=steady_state_enabled;
-	   g_power_trace_zlevel=zlevel;
-	   g_power_per_cycle_dump=power_per_cycle_dump;
-	   gpu_steady_power_deviation=steady_power_deviation;
-	   gpu_steady_min_period=steady_min_period;
-
-	   gpu_stat_sample_freq=stat_sample_freq;
-
-	   //p->sys.total_cycles=gpu_stat_sample_freq*4;
-	   p->sys.total_cycles=gpu_stat_sample_freq;
-	   power_trace_file = NULL;
-	   metric_trace_file = NULL;
-	   steady_state_tacking_file = NULL;
-
-
-	   if (g_power_trace_enabled ){
-		   power_trace_file = gzopen(g_power_trace_filename, "w");
-		   metric_trace_file = gzopen(g_metric_trace_filename, "w");
-		   if ((power_trace_file == NULL) || (metric_trace_file == NULL)) {
-			   printf("error - could not open trace files \n");
-			   exit(1);
-		   }
-		   gzsetparams(power_trace_file, g_power_trace_zlevel, Z_DEFAULT_STRATEGY);
-
-		   gzprintf(power_trace_file,"power,");
-		   for(unsigned i=0; i<num_pwr_cmps; i++){
-			   gzprintf(power_trace_file,pwr_cmp_label[i]);
-		   }
-		   gzprintf(power_trace_file,"\n");
-
-		   gzsetparams(metric_trace_file, g_power_trace_zlevel, Z_DEFAULT_STRATEGY);
-		   for(unsigned i=0; i<num_perf_counters; i++){
-			   gzprintf(metric_trace_file,perf_count_label[i]);
-		   }
-		   gzprintf(metric_trace_file,"\n");
-
-		   gzclose(power_trace_file);
-		   gzclose(metric_trace_file);
-	   }
-	   if(g_steady_power_levels_enabled){
-		   steady_state_tacking_file = gzopen(g_steady_state_tracking_filename, "w");
-		   if ((steady_state_tacking_file == NULL)) {
-			   printf("error - could not open trace files \n");
-			   exit(1);
-		   }
-		   gzsetparams(steady_state_tacking_file,g_power_trace_zlevel, Z_DEFAULT_STRATEGY);
-		   gzprintf(steady_state_tacking_file,"start,end,power,IPC,");
-		   for(unsigned i=0; i<num_perf_counters; i++){
-			   gzprintf(steady_state_tacking_file,perf_count_label[i]);
-		   }
-		   gzprintf(steady_state_tacking_file,"\n");
-
-		   gzclose(steady_state_tacking_file);
-	   }
-
-	   mcpat_init = false;
-	   has_written_avg=false;
-	   powerfile.open(g_power_filename);
-       int flg=chmod(g_power_filename, S_IRUSR|S_IWUSR|S_IRGRP|S_IROTH);
-       assert(flg==0);
-   }
-   sample_val = 0;
-   init_inst_val=init_val;//gpu_tot_sim_insn+gpu_sim_insn;
-
-}
-
-void gpgpu_sim_wrapper::reset_counters(){
-
-	avg_max_min_counters<double> init;
-	for(unsigned i=0; i<num_perf_counters; ++i){
-		sample_perf_counters[i] = 0;
-		kernel_cmp_perf_counters[i] = init;
-	}
-	for(unsigned i=0; i<num_pwr_cmps; ++i){
-		sample_cmp_pwr[i] = 0;
-		kernel_cmp_pwr[i] = init;
-	}
+void gpgpu_sim_wrapper::set_tcache_power(double hits, double misses) {
+  p->sys.core[0].tcache.read_accesses =
+      hits * p->sys.scaling_coefficients[TC_H] +
+      misses * p->sys.scaling_coefficients[TC_M];
+  p->sys.core[0].tcache.read_misses =
+      misses * p->sys.scaling_coefficients[TC_M];
+  sample_perf_counters[TC_H] = hits;
+  sample_perf_counters[TC_M] = misses;
+  // TODO: coalescing logic is counted as part of the caches power (this is not
+  // valid for no-caches architectures)
+}
 
-	// Reset per-kernel counters
-	kernel_sample_count = 0;
-	kernel_tot_power = 0;
-	kernel_power = init;
+void gpgpu_sim_wrapper::set_shrd_mem_power(double accesses) {
+  p->sys.core[0].sharedmemory.read_accesses =
+      accesses * p->sys.scaling_coefficients[SHRD_ACC];
+  sample_perf_counters[SHRD_ACC] = accesses;
+}
 
-	return;
+void gpgpu_sim_wrapper::set_l1cache_power(double read_hits, double read_misses,
+                                          double write_hits,
+                                          double write_misses) {
+  p->sys.core[0].dcache.read_accesses =
+      read_hits * p->sys.scaling_coefficients[DC_RH] +
+      read_misses * p->sys.scaling_coefficients[DC_RM];
+  p->sys.core[0].dcache.read_misses =
+      read_misses * p->sys.scaling_coefficients[DC_RM];
+  p->sys.core[0].dcache.write_accesses =
+      write_hits * p->sys.scaling_coefficients[DC_WH] +
+      write_misses * p->sys.scaling_coefficients[DC_WM];
+  p->sys.core[0].dcache.write_misses =
+      write_misses * p->sys.scaling_coefficients[DC_WM];
+  sample_perf_counters[DC_RH] = read_hits;
+  sample_perf_counters[DC_RM] = read_misses;
+  sample_perf_counters[DC_WH] = write_hits;
+  sample_perf_counters[DC_WM] = write_misses;
+  // TODO: coalescing logic is counted as part of the caches power (this is not
+  // valid for no-caches architectures)
 }
 
-void gpgpu_sim_wrapper::set_inst_power(bool clk_gated_lanes, double tot_cycles, double busy_cycles, double tot_inst, double int_inst, double fp_inst, double load_inst, double store_inst, double committed_inst)
-{
-	p->sys.core[0].gpgpu_clock_gated_lanes = clk_gated_lanes;
-	p->sys.core[0].total_cycles = tot_cycles;
-	p->sys.core[0].busy_cycles = busy_cycles;
-	p->sys.core[0].total_instructions  = tot_inst * p->sys.scaling_coefficients[TOT_INST];
-	p->sys.core[0].int_instructions    = int_inst * p->sys.scaling_coefficients[FP_INT];
-	p->sys.core[0].fp_instructions     = fp_inst  * p->sys.scaling_coefficients[FP_INT];
-	p->sys.core[0].load_instructions  = load_inst;
-	p->sys.core[0].store_instructions = store_inst;
-	p->sys.core[0].committed_instructions = committed_inst;
-	sample_perf_counters[FP_INT]=int_inst+fp_inst;
-	sample_perf_counters[TOT_INST]=tot_inst;
-}
-
-void gpgpu_sim_wrapper::set_regfile_power(double reads, double writes,double ops)
-{
-	p->sys.core[0].int_regfile_reads = reads * p->sys.scaling_coefficients[REG_RD];
-	p->sys.core[0].int_regfile_writes = writes * p->sys.scaling_coefficients[REG_WR];
-	p->sys.core[0].non_rf_operands =  ops *p->sys.scaling_coefficients[NON_REG_OPs];
-	sample_perf_counters[REG_RD]=reads;
-	sample_perf_counters[REG_WR]=writes;
-	sample_perf_counters[NON_REG_OPs]=ops;
+void gpgpu_sim_wrapper::set_l2cache_power(double read_hits, double read_misses,
+                                          double write_hits,
+                                          double write_misses) {
+  p->sys.l2.total_accesses = read_hits * p->sys.scaling_coefficients[L2_RH] +
+                             read_misses * p->sys.scaling_coefficients[L2_RM] +
+                             write_hits * p->sys.scaling_coefficients[L2_WH] +
+                             write_misses * p->sys.scaling_coefficients[L2_WM];
+  p->sys.l2.read_accesses = read_hits * p->sys.scaling_coefficients[L2_RH] +
+                            read_misses * p->sys.scaling_coefficients[L2_RM];
+  p->sys.l2.write_accesses = write_hits * p->sys.scaling_coefficients[L2_WH] +
+                             write_misses * p->sys.scaling_coefficients[L2_WM];
+  p->sys.l2.read_hits = read_hits * p->sys.scaling_coefficients[L2_RH];
+  p->sys.l2.read_misses = read_misses * p->sys.scaling_coefficients[L2_RM];
+  p->sys.l2.write_hits = write_hits * p->sys.scaling_coefficients[L2_WH];
+  p->sys.l2.write_misses = write_misses * p->sys.scaling_coefficients[L2_WM];
+  sample_perf_counters[L2_RH] = read_hits;
+  sample_perf_counters[L2_RM] = read_misses;
+  sample_perf_counters[L2_WH] = write_hits;
+  sample_perf_counters[L2_WM] = write_misses;
+}
 
+void gpgpu_sim_wrapper::set_num_cores(double num_core) {
+  
+  num_cores = num_core;
+}
 
+void gpgpu_sim_wrapper::set_idle_core_power(double num_idle_core) {
+  p->sys.num_idle_cores = num_idle_core;
+  sample_perf_counters[IDLE_CORE_N] = num_idle_core;
+  num_idle_cores = num_idle_core;
+}
 
+void gpgpu_sim_wrapper::set_duty_cycle_power(double duty_cycle) {
+  p->sys.core[0].pipeline_duty_cycle =
+      duty_cycle * p->sys.scaling_coefficients[PIPE_A];
+  sample_perf_counters[PIPE_A] = duty_cycle;
 }
 
-void gpgpu_sim_wrapper::set_icache_power(double hits, double misses)
-{
-	p->sys.core[0].icache.read_accesses = hits * p->sys.scaling_coefficients[IC_H]+misses * p->sys.scaling_coefficients[IC_M];
-	p->sys.core[0].icache.read_misses = misses * p->sys.scaling_coefficients[IC_M];
-	sample_perf_counters[IC_H]=hits;
-	sample_perf_counters[IC_M]=misses;
+void gpgpu_sim_wrapper::set_mem_ctrl_power(double reads, double writes,
+                                           double dram_precharge) {
+  p->sys.mc.memory_accesses = reads * p->sys.scaling_coefficients[MEM_RD] +
+                              writes * p->sys.scaling_coefficients[MEM_WR];
+  p->sys.mc.memory_reads = reads * p->sys.scaling_coefficients[MEM_RD];
+  p->sys.mc.memory_writes = writes * p->sys.scaling_coefficients[MEM_WR];
+  p->sys.mc.dram_pre = dram_precharge * p->sys.scaling_coefficients[MEM_PRE];
+  sample_perf_counters[MEM_RD] = reads;
+  sample_perf_counters[MEM_WR] = writes;
+  sample_perf_counters[MEM_PRE] = dram_precharge;
+}
 
 
+void gpgpu_sim_wrapper::set_model_voltage(double model_voltage) {
+	modeled_chip_voltage = model_voltage;
 }
 
-void gpgpu_sim_wrapper::set_ccache_power(double hits, double misses)
-{
-	p->sys.core[0].ccache.read_accesses = hits * p->sys.scaling_coefficients[CC_H]+misses * p->sys.scaling_coefficients[CC_M];
-	p->sys.core[0].ccache.read_misses = misses * p->sys.scaling_coefficients[CC_M];
-	sample_perf_counters[CC_H]=hits;
-	sample_perf_counters[CC_M]=misses;
-	// TODO: coalescing logic is counted as part of the caches power (this is not valid for no-caches architectures)
 
-}
+void gpgpu_sim_wrapper::set_exec_unit_power(double fpu_accesses,
+                                            double ialu_accesses,
+                                            double sfu_accesses) {
+  p->sys.core[0].fpu_accesses = fpu_accesses;
+  tot_fpu_accesses = fpu_accesses;
+  //Integer ALU (not present in Tesla)
+  p->sys.core[0].ialu_accesses = ialu_accesses;
 
-void gpgpu_sim_wrapper::set_tcache_power(double hits, double misses)
-{
-	p->sys.core[0].tcache.read_accesses = hits * p->sys.scaling_coefficients[TC_H]+misses * p->sys.scaling_coefficients[TC_M];
-	p->sys.core[0].tcache.read_misses = misses* p->sys.scaling_coefficients[TC_M];
-	sample_perf_counters[TC_H]=hits;
-	sample_perf_counters[TC_M]=misses;
-	// TODO: coalescing logic is counted as part of the caches power (this is not valid for no-caches architectures)
+  //Sfu accesses
+  p->sys.core[0].mul_accesses = sfu_accesses;
+  tot_sfu_accesses = sfu_accesses;
 }
 
-void gpgpu_sim_wrapper::set_shrd_mem_power(double accesses)
+PowerscalingCoefficients * gpgpu_sim_wrapper::get_scaling_coeffs()
 {
-	p->sys.core[0].sharedmemory.read_accesses = accesses * p->sys.scaling_coefficients[SHRD_ACC];
-	sample_perf_counters[SHRD_ACC]=accesses;
 
+  PowerscalingCoefficients * scalingCoeffs = new PowerscalingCoefficients();
+
+  scalingCoeffs->int_coeff = p->sys.scaling_coefficients[INT_ACC];
+  scalingCoeffs->int_mul_coeff = p->sys.scaling_coefficients[INT_MUL_ACC];
+  scalingCoeffs->int_mul24_coeff = p->sys.scaling_coefficients[INT_MUL24_ACC];
+  scalingCoeffs->int_mul32_coeff = p->sys.scaling_coefficients[INT_MUL32_ACC];
+  scalingCoeffs->int_div_coeff = p->sys.scaling_coefficients[INT_DIV_ACC];
+  scalingCoeffs->fp_coeff = p->sys.scaling_coefficients[FP_ACC];
+  scalingCoeffs->dp_coeff = p->sys.scaling_coefficients[DP_ACC];
+  scalingCoeffs->fp_mul_coeff = p->sys.scaling_coefficients[FP_MUL_ACC];
+  scalingCoeffs->fp_div_coeff = p->sys.scaling_coefficients[FP_DIV_ACC];
+  scalingCoeffs->dp_mul_coeff = p->sys.scaling_coefficients[DP_MUL_ACC];
+  scalingCoeffs->dp_div_coeff = p->sys.scaling_coefficients[DP_DIV_ACC];
+  scalingCoeffs->sqrt_coeff = p->sys.scaling_coefficients[FP_SQRT_ACC];
+  scalingCoeffs->log_coeff = p->sys.scaling_coefficients[FP_LG_ACC];
+  scalingCoeffs->sin_coeff = p->sys.scaling_coefficients[FP_SIN_ACC];
+  scalingCoeffs->exp_coeff = p->sys.scaling_coefficients[FP_EXP_ACC];
+  scalingCoeffs->tensor_coeff = p->sys.scaling_coefficients[TENSOR_ACC];
+  scalingCoeffs->tex_coeff = p->sys.scaling_coefficients[TEX_ACC];
+  return scalingCoeffs;
 
 }
 
-void gpgpu_sim_wrapper::set_l1cache_power(double read_hits, double read_misses, double write_hits, double write_misses)
+void gpgpu_sim_wrapper::set_int_accesses(double ialu_accesses, 
+                                        double imul24_accesses, 
+                                        double imul32_accesses, 
+                                        double imul_accesses, 
+                                        double idiv_accesses)
 {
-	p->sys.core[0].dcache.read_accesses = read_hits * p->sys.scaling_coefficients[DC_RH] +read_misses * p->sys.scaling_coefficients[DC_RM];
-	p->sys.core[0].dcache.read_misses =  read_misses * p->sys.scaling_coefficients[DC_RM];
-	p->sys.core[0].dcache.write_accesses = write_hits * p->sys.scaling_coefficients[DC_WH]+write_misses * p->sys.scaling_coefficients[DC_WM];
-	p->sys.core[0].dcache.write_misses = write_misses * p->sys.scaling_coefficients[DC_WM];
-	sample_perf_counters[DC_RH]=read_hits;
-	sample_perf_counters[DC_RM]=read_misses;
-	sample_perf_counters[DC_WH]=write_hits;
-	sample_perf_counters[DC_WM]=write_misses;
-	// TODO: coalescing logic is counted as part of the caches power (this is not valid for no-caches architectures)
-
-
 
+  sample_perf_counters[INT_ACC]=ialu_accesses;
+  sample_perf_counters[INT_MUL24_ACC]=imul24_accesses;
+  sample_perf_counters[INT_MUL32_ACC]=imul32_accesses;
+  sample_perf_counters[INT_MUL_ACC]=imul_accesses;
+  sample_perf_counters[INT_DIV_ACC]=idiv_accesses;
 }
 
-void gpgpu_sim_wrapper::set_l2cache_power(double read_hits, double read_misses, double write_hits, double write_misses)
+void gpgpu_sim_wrapper::set_dp_accesses(double dpu_accesses, 
+                                        double dpmul_accesses, 
+                                        double dpdiv_accesses)
 {
-	p->sys.l2.total_accesses = read_hits* p->sys.scaling_coefficients[L2_RH]+read_misses * p->sys.scaling_coefficients[L2_RM]+ write_hits * p->sys.scaling_coefficients[L2_WH]+write_misses  * p->sys.scaling_coefficients[L2_WM];
-	p->sys.l2.read_accesses = read_hits* p->sys.scaling_coefficients[L2_RH]+read_misses* p->sys.scaling_coefficients[L2_RM];
-	p->sys.l2.write_accesses = write_hits * p->sys.scaling_coefficients[L2_WH]+write_misses * p->sys.scaling_coefficients[L2_WM];
-	p->sys.l2.read_hits = read_hits * p->sys.scaling_coefficients[L2_RH];
-	p->sys.l2.read_misses = read_misses  * p->sys.scaling_coefficients[L2_RM];
-	p->sys.l2.write_hits =write_hits * p->sys.scaling_coefficients[L2_WH];
-	p->sys.l2.write_misses = write_misses * p->sys.scaling_coefficients[L2_WM];
-	sample_perf_counters[L2_RH]=read_hits;
-	sample_perf_counters[L2_RM]=read_misses;
-	sample_perf_counters[L2_WH]=write_hits;
-	sample_perf_counters[L2_WM]=write_misses;
-}
-
-void gpgpu_sim_wrapper::set_idle_core_power(double num_idle_core)
-{
-	p->sys.num_idle_cores = num_idle_core;
-	sample_perf_counters[IDLE_CORE_N]=num_idle_core;
+  sample_perf_counters[DP_ACC]=dpu_accesses;
+  sample_perf_counters[DP_MUL_ACC]=dpmul_accesses;
+  sample_perf_counters[DP_DIV_ACC]=dpdiv_accesses;
 }
 
-void gpgpu_sim_wrapper::set_duty_cycle_power(double duty_cycle)
+void gpgpu_sim_wrapper::set_fp_accesses(double fpu_accesses, 
+                                        double fpmul_accesses, 
+                                        double fpdiv_accesses)
 {
-	p->sys.core[0].pipeline_duty_cycle = duty_cycle  * p->sys.scaling_coefficients[PIPE_A];
-	sample_perf_counters[PIPE_A]=duty_cycle;
-
+  sample_perf_counters[FP_ACC]=fpu_accesses;
+  sample_perf_counters[FP_MUL_ACC]=fpmul_accesses;
+  sample_perf_counters[FP_DIV_ACC]=fpdiv_accesses;
 }
 
-void gpgpu_sim_wrapper::set_mem_ctrl_power(double reads, double writes, double dram_precharge)
+void gpgpu_sim_wrapper::set_trans_accesses(double sqrt_accesses, 
+                                           double log_accesses, 
+                                           double sin_accesses, 
+                                           double exp_accesses)
 {
-	p->sys.mc.memory_accesses = reads  * p->sys.scaling_coefficients[MEM_RD]+ writes * p->sys.scaling_coefficients[MEM_WR];
-	p->sys.mc.memory_reads = reads *p->sys.scaling_coefficients[MEM_RD];
-	p->sys.mc.memory_writes = writes*p->sys.scaling_coefficients[MEM_WR];
-	p->sys.mc.dram_pre = dram_precharge*p->sys.scaling_coefficients[MEM_PRE];
-	sample_perf_counters[MEM_RD]=reads;
-	sample_perf_counters[MEM_WR]=writes;
-	sample_perf_counters[MEM_PRE]=dram_precharge;
+
+  sample_perf_counters[FP_SQRT_ACC]=sqrt_accesses;
+  sample_perf_counters[FP_LG_ACC]=log_accesses;
+  sample_perf_counters[FP_SIN_ACC]=sin_accesses;
+  sample_perf_counters[FP_EXP_ACC]=exp_accesses;
 
 }
 
-void gpgpu_sim_wrapper::set_exec_unit_power(double fpu_accesses, double ialu_accesses, double sfu_accesses)
+void gpgpu_sim_wrapper::set_tensor_accesses(double tensor_accesses)
 {
-	p->sys.core[0].fpu_accesses = fpu_accesses*p->sys.scaling_coefficients[FPU_ACC];
-    //Integer ALU (not present in Tesla)
-	p->sys.core[0].ialu_accesses = ialu_accesses*p->sys.scaling_coefficients[SP_ACC];
-	//Sfu accesses
-	p->sys.core[0].mul_accesses = sfu_accesses*p->sys.scaling_coefficients[SFU_ACC];
-
-	sample_perf_counters[SP_ACC]=ialu_accesses;
-	sample_perf_counters[SFU_ACC]=sfu_accesses;
-	sample_perf_counters[FPU_ACC]=fpu_accesses;
-
+  sample_perf_counters[TENSOR_ACC]=tensor_accesses;
 
 }
 
-void gpgpu_sim_wrapper::set_active_lanes_power(double sp_avg_active_lane, double sfu_avg_active_lane)
+void gpgpu_sim_wrapper::set_tex_accesses(double tex_accesses)
 {
-	p->sys.core[0].sp_average_active_lanes = sp_avg_active_lane;
-	p->sys.core[0].sfu_average_active_lanes = sfu_avg_active_lane;
-}
+  sample_perf_counters[TEX_ACC]=tex_accesses;
 
-void gpgpu_sim_wrapper::set_NoC_power(double noc_tot_reads, double noc_tot_writes )
-{
-	p->sys.NoC[0].total_accesses = noc_tot_reads * p->sys.scaling_coefficients[NOC_A] + noc_tot_writes * p->sys.scaling_coefficients[NOC_A];
-	sample_perf_counters[NOC_A]=noc_tot_reads+noc_tot_writes;
 }
 
-
-void gpgpu_sim_wrapper::power_metrics_calculations()
+void gpgpu_sim_wrapper::set_avg_active_threads(float active_threads)
 {
-    total_sample_count++;
-    kernel_sample_count++;
-
-    // Current sample power
-    double sample_power = proc->rt_power.readOp.dynamic + sample_cmp_pwr[CONST_DYNAMICP];
-
-    // Average power
-    // Previous + new + constant dynamic power (e.g., dynamic clocking power)
-    kernel_tot_power += sample_power;
-    kernel_power.avg = kernel_tot_power / kernel_sample_count;
-	for(unsigned ind=0; ind<num_pwr_cmps; ++ind){
-		kernel_cmp_pwr[ind].avg += (double)sample_cmp_pwr[ind];
-	}
-
-	for(unsigned ind=0; ind<num_perf_counters; ++ind){
-		kernel_cmp_perf_counters[ind].avg += (double)sample_perf_counters[ind];
-	}
-
-	// Max Power
-	if(sample_power > kernel_power.max){
-		kernel_power.max = sample_power;
-		for(unsigned ind=0; ind<num_pwr_cmps; ++ind){
-			kernel_cmp_pwr[ind].max = (double)sample_cmp_pwr[ind];
-		}
-		for(unsigned ind=0; ind<num_perf_counters; ++ind){
-			kernel_cmp_perf_counters[ind].max = sample_perf_counters[ind];
-		}
-
-	}
-
-	// Min Power
-	if(sample_power < kernel_power.min ||(kernel_power.min==0) ){
-	  kernel_power.min = sample_power;
-	  for(unsigned ind=0; ind<num_pwr_cmps; ++ind){
-		  kernel_cmp_pwr[ind].min = (double)sample_cmp_pwr[ind];
-	  }
-	  for(unsigned ind=0; ind<num_perf_counters; ++ind){
-		  kernel_cmp_perf_counters[ind].min = sample_perf_counters[ind];
-	  }
-	}
+  avg_threads_per_warp = (unsigned)ceil(active_threads);
+  avg_threads_per_warp_tot += active_threads;
+}
 
-	gpu_tot_power.avg = (gpu_tot_power.avg + sample_power);
-	gpu_tot_power.max = (sample_power > gpu_tot_power.max) ? sample_power : gpu_tot_power.max;
-	gpu_tot_power.min = ((sample_power < gpu_tot_power.min) || (gpu_tot_power.min == 0)) ? sample_power : gpu_tot_power.min;
+void gpgpu_sim_wrapper::set_active_lanes_power(double sp_avg_active_lane,
+                                               double sfu_avg_active_lane) {
+  p->sys.core[0].sp_average_active_lanes = sp_avg_active_lane;
+  p->sys.core[0].sfu_average_active_lanes = sfu_avg_active_lane;
+}
 
+void gpgpu_sim_wrapper::set_NoC_power(double noc_tot_acc) {
+  p->sys.NoC[0].total_accesses =
+      noc_tot_acc * p->sys.scaling_coefficients[NOC_A];
+  sample_perf_counters[NOC_A] = noc_tot_acc;
 }
 
+void gpgpu_sim_wrapper::power_metrics_calculations() {
+  total_sample_count++;
+  kernel_sample_count++;
+
+  // Current sample power
+  double sample_power = proc->rt_power.readOp.dynamic + sample_cmp_pwr[CONSTP] + sample_cmp_pwr[STATICP];
+  // double sample_power;
+  // for(unsigned i=0; i<num_pwr_cmps; i++){
+  //   sample_power+=sample_cmp_pwr[i]; //fix for dvfs
+  // }
+
+  // Average power
+  // Previous + new + constant dynamic power (e.g., dynamic clocking power)
+  kernel_tot_power += sample_power;
+  kernel_power.avg = kernel_tot_power / kernel_sample_count;
+  for (unsigned ind = 0; ind < num_pwr_cmps; ++ind) {
+    kernel_cmp_pwr[ind].avg += (double)sample_cmp_pwr[ind];
+  }
+
+  for (unsigned ind = 0; ind < num_perf_counters; ++ind) {
+    kernel_cmp_perf_counters[ind].avg += (double)sample_perf_counters[ind];
+  }
+
+  // Max Power
+  if (sample_power > kernel_power.max) {
+    kernel_power.max = sample_power;
+    for (unsigned ind = 0; ind < num_pwr_cmps; ++ind) {
+      kernel_cmp_pwr[ind].max = (double)sample_cmp_pwr[ind];
+    }
+    for (unsigned ind = 0; ind < num_perf_counters; ++ind) {
+      kernel_cmp_perf_counters[ind].max = sample_perf_counters[ind];
+    }
+  }
 
-void gpgpu_sim_wrapper::print_trace_files()
-{
-	open_files();
+  // Min Power
+  if (sample_power < kernel_power.min || (kernel_power.min == 0)) {
+    kernel_power.min = sample_power;
+    for (unsigned ind = 0; ind < num_pwr_cmps; ++ind) {
+      kernel_cmp_pwr[ind].min = (double)sample_cmp_pwr[ind];
+    }
+    for (unsigned ind = 0; ind < num_perf_counters; ++ind) {
+      kernel_cmp_perf_counters[ind].min = sample_perf_counters[ind];
+    }
+  }
+
+  gpu_tot_power.avg = (gpu_tot_power.avg + sample_power);
+  gpu_tot_power.max =
+      (sample_power > gpu_tot_power.max) ? sample_power : gpu_tot_power.max;
+  gpu_tot_power.min =
+      ((sample_power < gpu_tot_power.min) || (gpu_tot_power.min == 0))
+          ? sample_power
+          : gpu_tot_power.min;
+}
 
-	for(unsigned i=0; i<num_perf_counters; ++i){
-		gzprintf(metric_trace_file,"%f,",sample_perf_counters[i]);
-	}
-	gzprintf(metric_trace_file,"\n");
+void gpgpu_sim_wrapper::print_trace_files() {
+  open_files();
 
-	gzprintf(power_trace_file,"%f,",proc_power);
-	for(unsigned i=0; i<num_pwr_cmps; ++i){
-		gzprintf(power_trace_file,"%f,",sample_cmp_pwr[i]);
-	}
-	gzprintf(power_trace_file,"\n");
+  for (unsigned i = 0; i < num_perf_counters; ++i) {
+    gzprintf(metric_trace_file, "%f,", sample_perf_counters[i]);
+  }
+  gzprintf(metric_trace_file, "\n");
 
-	close_files();
+  gzprintf(power_trace_file, "%f,", proc_power);
+  for (unsigned i = 0; i < num_pwr_cmps; ++i) {
+    gzprintf(power_trace_file, "%f,", sample_cmp_pwr[i]);
+  }
+  gzprintf(power_trace_file, "\n");
 
+  close_files();
 }
 
 void gpgpu_sim_wrapper::update_coefficients()
 {
 
-	initpower_coeff[FP_INT]=proc->cores[0]->get_coefficient_fpint_insts();
-	effpower_coeff[FP_INT]=initpower_coeff[FP_INT] * p->sys.scaling_coefficients[FP_INT];
-
-	initpower_coeff[TOT_INST]=proc->cores[0]->get_coefficient_tot_insts();
-	effpower_coeff[TOT_INST]=initpower_coeff[TOT_INST] * p->sys.scaling_coefficients[TOT_INST];
-
-	initpower_coeff[REG_RD]=proc->cores[0]->get_coefficient_regreads_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
-	initpower_coeff[REG_WR]=proc->cores[0]->get_coefficient_regwrites_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
-	initpower_coeff[NON_REG_OPs]=proc->cores[0]->get_coefficient_noregfileops_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
-	effpower_coeff[REG_RD]=initpower_coeff[REG_RD]*p->sys.scaling_coefficients[REG_RD];
-	effpower_coeff[REG_WR]=initpower_coeff[REG_WR]*p->sys.scaling_coefficients[REG_WR];
-	effpower_coeff[NON_REG_OPs]=initpower_coeff[NON_REG_OPs]*p->sys.scaling_coefficients[NON_REG_OPs];
-
-	initpower_coeff[IC_H]=proc->cores[0]->get_coefficient_icache_hits();
-	initpower_coeff[IC_M]=proc->cores[0]->get_coefficient_icache_misses();
-	effpower_coeff[IC_H]=initpower_coeff[IC_H]*p->sys.scaling_coefficients[IC_H];
-	effpower_coeff[IC_M]=initpower_coeff[IC_M]*p->sys.scaling_coefficients[IC_M];
-
-	initpower_coeff[CC_H]=(proc->cores[0]->get_coefficient_ccache_readhits()+proc->get_coefficient_readcoalescing());
-	initpower_coeff[CC_M]=(proc->cores[0]->get_coefficient_ccache_readmisses()+proc->get_coefficient_readcoalescing());
-	effpower_coeff[CC_H]=initpower_coeff[CC_H]*p->sys.scaling_coefficients[CC_H];
-	effpower_coeff[CC_M]=initpower_coeff[CC_M]*p->sys.scaling_coefficients[CC_M];
-
-	initpower_coeff[TC_H]=(proc->cores[0]->get_coefficient_tcache_readhits()+proc->get_coefficient_readcoalescing());
-	initpower_coeff[TC_M]=(proc->cores[0]->get_coefficient_tcache_readmisses()+proc->get_coefficient_readcoalescing());
-	effpower_coeff[TC_H]=initpower_coeff[TC_H]*p->sys.scaling_coefficients[TC_H];
-	effpower_coeff[TC_M]=initpower_coeff[TC_M]*p->sys.scaling_coefficients[TC_M];
-
-	initpower_coeff[SHRD_ACC]=proc->cores[0]->get_coefficient_sharedmemory_readhits();
-	effpower_coeff[SHRD_ACC]=initpower_coeff[SHRD_ACC]*p->sys.scaling_coefficients[SHRD_ACC];
-
-	initpower_coeff[DC_RH]=(proc->cores[0]->get_coefficient_dcache_readhits() + proc->get_coefficient_readcoalescing());
-	initpower_coeff[DC_RM]=(proc->cores[0]->get_coefficient_dcache_readmisses() + proc->get_coefficient_readcoalescing());
-	initpower_coeff[DC_WH]=(proc->cores[0]->get_coefficient_dcache_writehits() + proc->get_coefficient_writecoalescing());
-	initpower_coeff[DC_WM]=(proc->cores[0]->get_coefficient_dcache_writemisses() + proc->get_coefficient_writecoalescing());
-	effpower_coeff[DC_RH]=initpower_coeff[DC_RH]*p->sys.scaling_coefficients[DC_RH];
-	effpower_coeff[DC_RM]=initpower_coeff[DC_RM]*p->sys.scaling_coefficients[DC_RM];
-	effpower_coeff[DC_WH]=initpower_coeff[DC_WH]*p->sys.scaling_coefficients[DC_WH];
-	effpower_coeff[DC_WM]=initpower_coeff[DC_WM]*p->sys.scaling_coefficients[DC_WM];
-
-	initpower_coeff[L2_RH]=proc->get_coefficient_l2_read_hits();
-	initpower_coeff[L2_RM]=proc->get_coefficient_l2_read_misses();
-	initpower_coeff[L2_WH]=proc->get_coefficient_l2_write_hits();
-	initpower_coeff[L2_WM]=proc->get_coefficient_l2_write_misses();
-	effpower_coeff[L2_RH]=initpower_coeff[L2_RH]*p->sys.scaling_coefficients[L2_RH];
-	effpower_coeff[L2_RM]=initpower_coeff[L2_RM]*p->sys.scaling_coefficients[L2_RM];
-	effpower_coeff[L2_WH]=initpower_coeff[L2_WH]*p->sys.scaling_coefficients[L2_WH];
-	effpower_coeff[L2_WM]=initpower_coeff[L2_WM]*p->sys.scaling_coefficients[L2_WM];
-
-	initpower_coeff[IDLE_CORE_N]=p->sys.idle_core_power * proc->cores[0]->executionTime;
-	effpower_coeff[IDLE_CORE_N]=initpower_coeff[IDLE_CORE_N]*p->sys.scaling_coefficients[IDLE_CORE_N];
-
-	initpower_coeff[PIPE_A]=proc->cores[0]->get_coefficient_duty_cycle();
-	effpower_coeff[PIPE_A]=initpower_coeff[PIPE_A]*p->sys.scaling_coefficients[PIPE_A];
-
-	initpower_coeff[MEM_RD]=proc->get_coefficient_mem_reads();
-	initpower_coeff[MEM_WR]=proc->get_coefficient_mem_writes();
-	initpower_coeff[MEM_PRE]=proc->get_coefficient_mem_pre();
-	effpower_coeff[MEM_RD]=initpower_coeff[MEM_RD]*p->sys.scaling_coefficients[MEM_RD];
-	effpower_coeff[MEM_WR]=initpower_coeff[MEM_WR]*p->sys.scaling_coefficients[MEM_WR];
-	effpower_coeff[MEM_PRE]=initpower_coeff[MEM_PRE]*p->sys.scaling_coefficients[MEM_PRE];
-
-	initpower_coeff[SP_ACC]=proc->cores[0]->get_coefficient_ialu_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);;
-	initpower_coeff[SFU_ACC]=proc->cores[0]->get_coefficient_sfu_accesses();
-	initpower_coeff[FPU_ACC]=proc->cores[0]->get_coefficient_fpu_accesses();
-
-	effpower_coeff[SP_ACC]=initpower_coeff[SP_ACC]*p->sys.scaling_coefficients[SP_ACC];
-	effpower_coeff[SFU_ACC]=initpower_coeff[SFU_ACC]*p->sys.scaling_coefficients[SFU_ACC];
-	effpower_coeff[FPU_ACC]=initpower_coeff[FPU_ACC]*p->sys.scaling_coefficients[FPU_ACC];
-
-	initpower_coeff[NOC_A]=proc->get_coefficient_noc_accesses();
-	effpower_coeff[NOC_A]=initpower_coeff[NOC_A]*p->sys.scaling_coefficients[NOC_A];
-
-	const_dynamic_power=proc->get_const_dynamic_power()/(proc->cores[0]->executionTime);
-
-	for(unsigned i=0; i<num_perf_counters; i++){
-		initpower_coeff[i]/=(proc->cores[0]->executionTime);
-		effpower_coeff[i]/=(proc->cores[0]->executionTime);
-	}
+  initpower_coeff[FP_INT]=proc->cores[0]->get_coefficient_fpint_insts();
+  effpower_coeff[FP_INT]=initpower_coeff[FP_INT] * p->sys.scaling_coefficients[FP_INT];
+
+  initpower_coeff[TOT_INST]=proc->cores[0]->get_coefficient_tot_insts();
+  effpower_coeff[TOT_INST]=initpower_coeff[TOT_INST] * p->sys.scaling_coefficients[TOT_INST];
+
+  initpower_coeff[REG_RD]=proc->cores[0]->get_coefficient_regreads_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+  initpower_coeff[REG_WR]=proc->cores[0]->get_coefficient_regwrites_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+  initpower_coeff[NON_REG_OPs]=proc->cores[0]->get_coefficient_noregfileops_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+  effpower_coeff[REG_RD]=initpower_coeff[REG_RD]*p->sys.scaling_coefficients[REG_RD];
+  effpower_coeff[REG_WR]=initpower_coeff[REG_WR]*p->sys.scaling_coefficients[REG_WR];
+  effpower_coeff[NON_REG_OPs]=initpower_coeff[NON_REG_OPs]*p->sys.scaling_coefficients[NON_REG_OPs];
+
+  initpower_coeff[IC_H]=proc->cores[0]->get_coefficient_icache_hits();
+  initpower_coeff[IC_M]=proc->cores[0]->get_coefficient_icache_misses();
+  effpower_coeff[IC_H]=initpower_coeff[IC_H]*p->sys.scaling_coefficients[IC_H];
+  effpower_coeff[IC_M]=initpower_coeff[IC_M]*p->sys.scaling_coefficients[IC_M];
+
+  initpower_coeff[CC_H]=(proc->cores[0]->get_coefficient_ccache_readhits()+proc->get_coefficient_readcoalescing());
+  initpower_coeff[CC_M]=(proc->cores[0]->get_coefficient_ccache_readmisses()+proc->get_coefficient_readcoalescing());
+  effpower_coeff[CC_H]=initpower_coeff[CC_H]*p->sys.scaling_coefficients[CC_H];
+  effpower_coeff[CC_M]=initpower_coeff[CC_M]*p->sys.scaling_coefficients[CC_M];
+
+  initpower_coeff[TC_H]=(proc->cores[0]->get_coefficient_tcache_readhits()+proc->get_coefficient_readcoalescing());
+  initpower_coeff[TC_M]=(proc->cores[0]->get_coefficient_tcache_readmisses()+proc->get_coefficient_readcoalescing());
+  effpower_coeff[TC_H]=initpower_coeff[TC_H]*p->sys.scaling_coefficients[TC_H];
+  effpower_coeff[TC_M]=initpower_coeff[TC_M]*p->sys.scaling_coefficients[TC_M];
+
+  initpower_coeff[SHRD_ACC]=proc->cores[0]->get_coefficient_sharedmemory_readhits();
+  effpower_coeff[SHRD_ACC]=initpower_coeff[SHRD_ACC]*p->sys.scaling_coefficients[SHRD_ACC];
+
+  initpower_coeff[DC_RH]=(proc->cores[0]->get_coefficient_dcache_readhits() + proc->get_coefficient_readcoalescing());
+  initpower_coeff[DC_RM]=(proc->cores[0]->get_coefficient_dcache_readmisses() + proc->get_coefficient_readcoalescing());
+  initpower_coeff[DC_WH]=(proc->cores[0]->get_coefficient_dcache_writehits() + proc->get_coefficient_writecoalescing());
+  initpower_coeff[DC_WM]=(proc->cores[0]->get_coefficient_dcache_writemisses() + proc->get_coefficient_writecoalescing());
+  effpower_coeff[DC_RH]=initpower_coeff[DC_RH]*p->sys.scaling_coefficients[DC_RH];
+  effpower_coeff[DC_RM]=initpower_coeff[DC_RM]*p->sys.scaling_coefficients[DC_RM];
+  effpower_coeff[DC_WH]=initpower_coeff[DC_WH]*p->sys.scaling_coefficients[DC_WH];
+  effpower_coeff[DC_WM]=initpower_coeff[DC_WM]*p->sys.scaling_coefficients[DC_WM];
+
+  initpower_coeff[L2_RH]=proc->get_coefficient_l2_read_hits();
+  initpower_coeff[L2_RM]=proc->get_coefficient_l2_read_misses();
+  initpower_coeff[L2_WH]=proc->get_coefficient_l2_write_hits();
+  initpower_coeff[L2_WM]=proc->get_coefficient_l2_write_misses();
+  effpower_coeff[L2_RH]=initpower_coeff[L2_RH]*p->sys.scaling_coefficients[L2_RH];
+  effpower_coeff[L2_RM]=initpower_coeff[L2_RM]*p->sys.scaling_coefficients[L2_RM];
+  effpower_coeff[L2_WH]=initpower_coeff[L2_WH]*p->sys.scaling_coefficients[L2_WH];
+  effpower_coeff[L2_WM]=initpower_coeff[L2_WM]*p->sys.scaling_coefficients[L2_WM];
+
+  initpower_coeff[IDLE_CORE_N]=p->sys.idle_core_power * proc->cores[0]->executionTime;
+  effpower_coeff[IDLE_CORE_N]=initpower_coeff[IDLE_CORE_N]*p->sys.scaling_coefficients[IDLE_CORE_N];
+
+  initpower_coeff[PIPE_A]=proc->cores[0]->get_coefficient_duty_cycle();
+  effpower_coeff[PIPE_A]=initpower_coeff[PIPE_A]*p->sys.scaling_coefficients[PIPE_A];
+
+  initpower_coeff[MEM_RD]=proc->get_coefficient_mem_reads();
+  initpower_coeff[MEM_WR]=proc->get_coefficient_mem_writes();
+  initpower_coeff[MEM_PRE]=proc->get_coefficient_mem_pre();
+  effpower_coeff[MEM_RD]=initpower_coeff[MEM_RD]*p->sys.scaling_coefficients[MEM_RD];
+  effpower_coeff[MEM_WR]=initpower_coeff[MEM_WR]*p->sys.scaling_coefficients[MEM_WR];
+  effpower_coeff[MEM_PRE]=initpower_coeff[MEM_PRE]*p->sys.scaling_coefficients[MEM_PRE];
+  
+  double fp_coeff = proc->cores[0]->get_coefficient_fpu_accesses();
+  double sfu_coeff = proc->cores[0]->get_coefficient_sfu_accesses();
+
+  initpower_coeff[INT_ACC]= proc->cores[0]->get_coefficient_ialu_accesses()*(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+
+  if(tot_fpu_accesses != 0){
+    initpower_coeff[FP_ACC]= fp_coeff * sample_perf_counters[FP_ACC]/tot_fpu_accesses;
+    initpower_coeff[DP_ACC]= fp_coeff * sample_perf_counters[DP_ACC]/tot_fpu_accesses;
+  }
+  else{
+    initpower_coeff[FP_ACC]= 0;
+    initpower_coeff[DP_ACC]= 0;
+  }
+
+  if(tot_sfu_accesses != 0){
+    initpower_coeff[INT_MUL24_ACC]= sfu_coeff * sample_perf_counters[INT_MUL24_ACC]/tot_sfu_accesses;
+    initpower_coeff[INT_MUL32_ACC]= sfu_coeff * sample_perf_counters[INT_MUL32_ACC]/tot_sfu_accesses;
+    initpower_coeff[INT_MUL_ACC]= sfu_coeff * sample_perf_counters[INT_MUL_ACC]/tot_sfu_accesses;
+    initpower_coeff[INT_DIV_ACC]= sfu_coeff * sample_perf_counters[INT_DIV_ACC]/tot_sfu_accesses;
+    initpower_coeff[DP_MUL_ACC]= sfu_coeff * sample_perf_counters[DP_MUL_ACC]/tot_sfu_accesses;
+    initpower_coeff[DP_DIV_ACC]= sfu_coeff * sample_perf_counters[DP_DIV_ACC]/tot_sfu_accesses;
+    initpower_coeff[FP_MUL_ACC]= sfu_coeff * sample_perf_counters[FP_MUL_ACC]/tot_sfu_accesses;
+    initpower_coeff[FP_DIV_ACC]= sfu_coeff * sample_perf_counters[FP_DIV_ACC]/tot_sfu_accesses;
+    initpower_coeff[FP_SQRT_ACC]= sfu_coeff * sample_perf_counters[FP_SQRT_ACC]/tot_sfu_accesses;
+    initpower_coeff[FP_LG_ACC]= sfu_coeff * sample_perf_counters[FP_LG_ACC]/tot_sfu_accesses;
+    initpower_coeff[FP_SIN_ACC]= sfu_coeff * sample_perf_counters[FP_SIN_ACC]/tot_sfu_accesses;
+    initpower_coeff[FP_EXP_ACC]= sfu_coeff * sample_perf_counters[FP_EXP_ACC]/tot_sfu_accesses;
+    initpower_coeff[TENSOR_ACC]= sfu_coeff * sample_perf_counters[TENSOR_ACC]/tot_sfu_accesses;
+    initpower_coeff[TEX_ACC]= sfu_coeff * sample_perf_counters[TEX_ACC]/tot_sfu_accesses;
+  }
+  else{
+    initpower_coeff[INT_MUL24_ACC]= 0;
+    initpower_coeff[INT_MUL32_ACC]= 0;
+    initpower_coeff[INT_MUL_ACC]= 0;
+    initpower_coeff[INT_DIV_ACC]= 0;
+    initpower_coeff[DP_MUL_ACC]= 0;
+    initpower_coeff[DP_DIV_ACC]= 0;
+    initpower_coeff[FP_MUL_ACC]= 0;
+    initpower_coeff[FP_DIV_ACC]= 0;
+    initpower_coeff[FP_SQRT_ACC]= 0;
+    initpower_coeff[FP_LG_ACC]= 0;
+    initpower_coeff[FP_SIN_ACC]= 0;
+    initpower_coeff[FP_EXP_ACC]= 0;
+    initpower_coeff[TENSOR_ACC]= 0;
+    initpower_coeff[TEX_ACC]= 0;
+  }
+
+  effpower_coeff[INT_ACC]= initpower_coeff[INT_ACC];
+  effpower_coeff[FP_ACC]= initpower_coeff[FP_ACC];
+  effpower_coeff[DP_ACC]= initpower_coeff[DP_ACC];
+  effpower_coeff[INT_MUL24_ACC]= initpower_coeff[INT_MUL24_ACC];
+  effpower_coeff[INT_MUL32_ACC]= initpower_coeff[INT_MUL32_ACC];
+  effpower_coeff[INT_MUL_ACC]= initpower_coeff[INT_MUL_ACC];
+  effpower_coeff[INT_DIV_ACC]= initpower_coeff[INT_DIV_ACC];
+  effpower_coeff[DP_MUL_ACC]= initpower_coeff[DP_MUL_ACC];
+  effpower_coeff[DP_DIV_ACC]= initpower_coeff[DP_DIV_ACC];
+  effpower_coeff[FP_MUL_ACC]= initpower_coeff[FP_MUL_ACC];
+  effpower_coeff[FP_DIV_ACC]= initpower_coeff[FP_DIV_ACC];
+  effpower_coeff[FP_SQRT_ACC]= initpower_coeff[FP_SQRT_ACC];
+  effpower_coeff[FP_LG_ACC]= initpower_coeff[FP_LG_ACC];
+  effpower_coeff[FP_SIN_ACC]= initpower_coeff[FP_SIN_ACC];
+  effpower_coeff[FP_EXP_ACC]= initpower_coeff[FP_EXP_ACC];
+  effpower_coeff[TENSOR_ACC]= initpower_coeff[TENSOR_ACC];
+  effpower_coeff[TEX_ACC]= initpower_coeff[TEX_ACC];
+
+  initpower_coeff[NOC_A]=proc->get_coefficient_noc_accesses();
+  effpower_coeff[NOC_A]=initpower_coeff[NOC_A]*p->sys.scaling_coefficients[NOC_A];
+
+  //const_dynamic_power=proc->get_const_dynamic_power()/(proc->cores[0]->executionTime);
+
+  for(unsigned i=0; i<num_perf_counters; i++){
+    initpower_coeff[i]/=(proc->cores[0]->executionTime);
+    effpower_coeff[i]/=(proc->cores[0]->executionTime);
+  }
 }
 
-void gpgpu_sim_wrapper::update_components_power()
-{
-
-	update_coefficients();
-
-	proc_power=proc->rt_power.readOp.dynamic;
-
-	sample_cmp_pwr[IBP]=(proc->cores[0]->ifu->IB->rt_power.readOp.dynamic
-			    +proc->cores[0]->ifu->IB->rt_power.writeOp.dynamic
-			    +proc->cores[0]->ifu->ID_misc->rt_power.readOp.dynamic
-			    +proc->cores[0]->ifu->ID_operand->rt_power.readOp.dynamic
-			    +proc->cores[0]->ifu->ID_inst->rt_power.readOp.dynamic)/(proc->cores[0]->executionTime);
-
-	sample_cmp_pwr[ICP]=proc->cores[0]->ifu->icache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
-
-	sample_cmp_pwr[DCP]=proc->cores[0]->lsu->dcache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
-
-	sample_cmp_pwr[TCP]=proc->cores[0]->lsu->tcache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
-
-	sample_cmp_pwr[CCP]=proc->cores[0]->lsu->ccache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
-
-	sample_cmp_pwr[SHRDP]=proc->cores[0]->lsu->sharedmemory.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+double gpgpu_sim_wrapper::calculate_static_power(){ 
+	double int_accesses = initpower_coeff[INT_ACC] + initpower_coeff[INT_MUL24_ACC] + initpower_coeff[INT_MUL32_ACC] + initpower_coeff[INT_MUL_ACC] + initpower_coeff[INT_DIV_ACC];
+	double int_add_accesses = initpower_coeff[INT_ACC];
+	double int_mul_accesses = initpower_coeff[INT_MUL24_ACC] + initpower_coeff[INT_MUL32_ACC] + initpower_coeff[INT_MUL_ACC] + initpower_coeff[INT_DIV_ACC];
+	double fp_accesses = initpower_coeff[FP_ACC] + initpower_coeff[FP_MUL_ACC] + initpower_coeff[FP_DIV_ACC];
+	double dp_accesses = initpower_coeff[DP_ACC] + initpower_coeff[DP_MUL_ACC] + initpower_coeff[DP_DIV_ACC];
+	double sfu_accesses = initpower_coeff[FP_SQRT_ACC] + initpower_coeff[FP_LG_ACC] + initpower_coeff[FP_SIN_ACC] + initpower_coeff[FP_EXP_ACC];
+	double tensor_accesses = initpower_coeff[TENSOR_ACC];
+	double tex_accesses = initpower_coeff[TEX_ACC];
+	double total_static_power = 0.0;
+	double base_static_power = 0.0; 
+	double lane_static_power = 0.0;
+	double per_active_core = (num_cores - num_idle_cores)/num_cores;
+
+
+	double l1_accesses = initpower_coeff[DC_RH] + initpower_coeff[DC_RM] + initpower_coeff[DC_WH] + initpower_coeff[DC_WM];
+	double l2_accesses = initpower_coeff[L2_RH] + initpower_coeff[L2_RM] + initpower_coeff[L2_WH] + initpower_coeff[L2_WM];
+	double shared_accesses = initpower_coeff[SHRD_ACC];
+
+
+	if(avg_threads_per_warp == 0){ //no functional unit threads, check for memory or a 'LIGHT_SM'
+		if(l1_accesses != 0.0)
+			return (p->sys.static_l1_flane*per_active_core);
+		else if(shared_accesses != 0.0)
+			return (p->sys.static_shared_flane*per_active_core);
+		else if(l2_accesses != 0.0)
+			return (p->sys.static_l2_flane*per_active_core);
+		else //LIGHT_SM
+			return (p->sys.static_light_flane*per_active_core); //return LIGHT_SM base static power
+	}
 
-	sample_cmp_pwr[RFP]=(proc->cores[0]->exu->rfu->rt_power.readOp.dynamic/(proc->cores[0]->executionTime))
-			   *(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+	/* using a linear model for thread divergence */
+	if((int_accesses != 0.0) && (fp_accesses != 0.0) && (dp_accesses != 0.0) && (sfu_accesses == 0.0) && (tensor_accesses == 0.0) && (tex_accesses == 0.0)){
+		/* INT_FP_DP */
+		base_static_power = p->sys.static_cat3_flane;
+		lane_static_power = p->sys.static_cat3_addlane;
+	}
 
-	sample_cmp_pwr[SPP]=(proc->cores[0]->exu->exeu->rt_power.readOp.dynamic/(proc->cores[0]->executionTime))
-			   *(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+	else if((int_accesses != 0.0) && (fp_accesses != 0.0) && (dp_accesses == 0.0) && (sfu_accesses == 0.0) && (tensor_accesses != 0.0) && (tex_accesses == 0.0)){
+		/* INT_FP_TENSOR */
+		base_static_power = p->sys.static_cat6_flane;
+		lane_static_power = p->sys.static_cat6_addlane;
+	}
 
-	sample_cmp_pwr[SFUP]=(proc->cores[0]->exu->mul->rt_power.readOp.dynamic/(proc->cores[0]->executionTime));
+	else if((int_accesses != 0.0) && (fp_accesses != 0.0) && (dp_accesses == 0.0) && (sfu_accesses != 0.0) && (tensor_accesses == 0.0) && (tex_accesses == 0.0)){
+		/* INT_FP_SFU */
+		base_static_power = p->sys.static_cat4_flane;
+		lane_static_power = p->sys.static_cat4_addlane;
+	}
 
-	sample_cmp_pwr[FPUP]=(proc->cores[0]->exu->fp_u->rt_power.readOp.dynamic/(proc->cores[0]->executionTime));
+	else if((int_accesses != 0.0) && (fp_accesses != 0.0) && (dp_accesses == 0.0) && (sfu_accesses == 0.0) && (tensor_accesses == 0.0) && (tex_accesses != 0.0)){
+		/* INT_FP_TEX */
+		base_static_power = p->sys.static_cat5_flane;
+		lane_static_power = p->sys.static_cat5_addlane;
+	}
 
-	sample_cmp_pwr[SCHEDP]=proc->cores[0]->exu->scheu->rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+	else if((int_accesses != 0.0) && (fp_accesses != 0.0) && (dp_accesses == 0.0) && (sfu_accesses == 0.0) && (tensor_accesses == 0.0) && (tex_accesses == 0.0)){
+		/* INT_FP */
+		base_static_power = p->sys.static_cat2_flane;
+		lane_static_power = p->sys.static_cat2_addlane;
+	}
 
-	sample_cmp_pwr[L2CP]=(proc->XML->sys.number_of_L2s>0)? proc->l2array[0]->rt_power.readOp.dynamic/(proc->cores[0]->executionTime):0;
+	else if((int_accesses != 0.0) && (fp_accesses == 0.0) && (dp_accesses == 0.0) && (sfu_accesses == 0.0) && (tensor_accesses == 0.0) && (tex_accesses == 0.0)){
+		/* INT */
+		/* Seperating INT_ADD only and INT_MUL only from mix of INT instructions */
+		if((int_add_accesses != 0.0) && (int_mul_accesses == 0.0)){ //INT_ADD
+			base_static_power = p->sys.static_intadd_flane;
+			lane_static_power = p->sys.static_intadd_addlane;
+		}
+		else if((int_add_accesses == 0.0) && (int_mul_accesses != 0.0)){ //INT_MUL
+			base_static_power = p->sys.static_intmul_flane;
+			lane_static_power = p->sys.static_intmul_addlane;
+		}
+		else{ //INT_ADD+MUL
+			base_static_power = p->sys.static_cat1_flane;
+			lane_static_power = p->sys.static_cat1_addlane;
+		}
+	}
 
-	sample_cmp_pwr[MCP]=(proc->mc->rt_power.readOp.dynamic-proc->mc->dram->rt_power.readOp.dynamic)/(proc->cores[0]->executionTime);
+	else if((int_accesses == 0.0) && (fp_accesses == 0.0) && (dp_accesses == 0.0) && (sfu_accesses == 0.0) && (tensor_accesses == 0.0) && (tex_accesses == 0.0)){
+		/* LIGHT_SM or memory only sample */
+		lane_static_power = 0.0; //addlane static power is 0 for l1/l2/shared memory only accesses
+		if(l1_accesses != 0.0)
+			base_static_power = p->sys.static_l1_flane;
+		else if(shared_accesses != 0.0)
+			base_static_power = p->sys.static_shared_flane;
+		else if(l2_accesses != 0.0)
+			base_static_power = p->sys.static_l2_flane;
+		else{
+			base_static_power = p->sys.static_light_flane;
+			lane_static_power = p->sys.static_light_addlane;
+		}
+	}
+	else{
+		base_static_power = p->sys.static_geomean_flane; //GEOMEAN except LIGHT_SM if we don't fall into any of the categories above
+		lane_static_power = p->sys.static_geomean_addlane;
+	}
 
-	sample_cmp_pwr[NOCP]=proc->nocs[0]->rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+	total_static_power = base_static_power + (((double)avg_threads_per_warp-1.0)*lane_static_power); //Linear Model
+	return (total_static_power*per_active_core);
+}
 
-	sample_cmp_pwr[DRAMP]=proc->mc->dram->rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+void gpgpu_sim_wrapper::update_components_power()
+{
 
-	sample_cmp_pwr[PIPEP]=proc->cores[0]->Pipeline_energy/(proc->cores[0]->executionTime);
+  update_coefficients();
+
+  proc_power=proc->rt_power.readOp.dynamic;
+  sample_cmp_pwr[IBP]=(proc->cores[0]->ifu->IB->rt_power.readOp.dynamic
+          +proc->cores[0]->ifu->IB->rt_power.writeOp.dynamic
+          +proc->cores[0]->ifu->ID_misc->rt_power.readOp.dynamic
+          +proc->cores[0]->ifu->ID_operand->rt_power.readOp.dynamic
+          +proc->cores[0]->ifu->ID_inst->rt_power.readOp.dynamic)/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[ICP]=proc->cores[0]->ifu->icache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[DCP]=proc->cores[0]->lsu->dcache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[TCP]=proc->cores[0]->lsu->tcache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[CCP]=proc->cores[0]->lsu->ccache.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[SHRDP]=proc->cores[0]->lsu->sharedmemory.rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[RFP]=(proc->cores[0]->exu->rfu->rt_power.readOp.dynamic/(proc->cores[0]->executionTime))
+         *(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+
+  double sample_fp_pwr = (proc->cores[0]->exu->fp_u->rt_power.readOp.dynamic/(proc->cores[0]->executionTime));
+
+  double sample_sfu_pwr = (proc->cores[0]->exu->mul->rt_power.readOp.dynamic/(proc->cores[0]->executionTime));
+
+  sample_cmp_pwr[INTP]=(proc->cores[0]->exu->exeu->rt_power.readOp.dynamic/(proc->cores[0]->executionTime))
+         *(proc->cores[0]->exu->rf_fu_clockRate/proc->cores[0]->exu->clockRate);
+
+  
+  if(tot_fpu_accesses != 0){
+    sample_cmp_pwr[FPUP]= sample_fp_pwr * sample_perf_counters[FP_ACC]/tot_fpu_accesses;
+    sample_cmp_pwr[DPUP]= sample_fp_pwr * sample_perf_counters[DP_ACC]/tot_fpu_accesses;
+  }
+  else{
+    sample_cmp_pwr[FPUP]= 0;
+    sample_cmp_pwr[DPUP]= 0;
+  }
+  if(tot_sfu_accesses != 0){
+    sample_cmp_pwr[INT_MUL24P]= sample_sfu_pwr * sample_perf_counters[INT_MUL24_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[INT_MUL32P]= sample_sfu_pwr * sample_perf_counters[INT_MUL32_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[INT_MULP]= sample_sfu_pwr * sample_perf_counters[INT_MUL_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[INT_DIVP]= sample_sfu_pwr * sample_perf_counters[INT_DIV_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[FP_MULP]= sample_sfu_pwr * sample_perf_counters[FP_MUL_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[FP_DIVP]= sample_sfu_pwr * sample_perf_counters[FP_DIV_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[FP_SQRTP]= sample_sfu_pwr * sample_perf_counters[FP_SQRT_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[FP_LGP]= sample_sfu_pwr * sample_perf_counters[FP_LG_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[FP_SINP]= sample_sfu_pwr * sample_perf_counters[FP_SIN_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[FP_EXP]= sample_sfu_pwr * sample_perf_counters[FP_EXP_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[DP_MULP]= sample_sfu_pwr * sample_perf_counters[DP_MUL_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[DP_DIVP]= sample_sfu_pwr * sample_perf_counters[DP_DIV_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[TENSORP]= sample_sfu_pwr * sample_perf_counters[TENSOR_ACC]/tot_sfu_accesses;
+    sample_cmp_pwr[TEXP]= sample_sfu_pwr * sample_perf_counters[TEX_ACC]/tot_sfu_accesses;
+  }
+  else{
+    sample_cmp_pwr[INT_MUL24P]= 0;
+    sample_cmp_pwr[INT_MUL32P]= 0;
+    sample_cmp_pwr[INT_MULP]= 0;
+    sample_cmp_pwr[INT_DIVP]= 0;
+    sample_cmp_pwr[FP_MULP]= 0;
+    sample_cmp_pwr[FP_DIVP]= 0;
+    sample_cmp_pwr[FP_SQRTP]= 0;
+    sample_cmp_pwr[FP_LGP]= 0;
+    sample_cmp_pwr[FP_SINP]= 0;
+    sample_cmp_pwr[FP_EXP]= 0;
+    sample_cmp_pwr[DP_MULP]= 0;
+    sample_cmp_pwr[DP_DIVP]= 0;
+    sample_cmp_pwr[TENSORP]= 0;
+    sample_cmp_pwr[TEXP]= 0;
+  }
+
+  sample_cmp_pwr[SCHEDP]=proc->cores[0]->exu->scheu->rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[L2CP]=(proc->XML->sys.number_of_L2s>0)? proc->l2array[0]->rt_power.readOp.dynamic/(proc->cores[0]->executionTime):0;
+
+  sample_cmp_pwr[MCP]=(proc->mc->rt_power.readOp.dynamic-proc->mc->dram->rt_power.readOp.dynamic)/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[NOCP]=proc->nocs[0]->rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[DRAMP]=proc->mc->dram->rt_power.readOp.dynamic/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[PIPEP]=proc->cores[0]->Pipeline_energy/(proc->cores[0]->executionTime);
+
+  sample_cmp_pwr[IDLE_COREP]=proc->cores[0]->IdleCoreEnergy/(proc->cores[0]->executionTime);
+
+  // This constant dynamic power (e.g., clock power) part is estimated via regression model.
+  sample_cmp_pwr[CONSTP]=0;
+  sample_cmp_pwr[STATICP]=0;
+  // double cnst_dyn = proc->get_const_dynamic_power()/(proc->cores[0]->executionTime);
+  // // If the regression scaling term is greater than the recorded constant dynamic power
+  // // then use the difference (other portion already added to dynamic power). Else,
+  // // all the constant dynamic power is accounted for, add nothing.
+  // if(p->sys.scaling_coefficients[constant_power] > cnst_dyn)
+  //   sample_cmp_pwr[CONSTP] = (p->sys.scaling_coefficients[constant_power]-cnst_dyn);
+  sample_cmp_pwr[CONSTP] = p->sys.scaling_coefficients[constant_power];
+  sample_cmp_pwr[STATICP] = calculate_static_power();
+
+  if(g_dvfs_enabled){
+  	double voltage_ratio = modeled_chip_voltage/p->sys.modeled_chip_voltage_ref; 
+  	sample_cmp_pwr[IDLE_COREP] *= voltage_ratio; // static power scaled by voltage_ratio
+  	sample_cmp_pwr[STATICP] *= voltage_ratio;  // static power scaled by voltage_ratio
+  	for(unsigned i=0; i<num_pwr_cmps; i++){
+    	if((i != IDLE_COREP) && (i != STATICP)){ 
+    		sample_cmp_pwr[i] *= voltage_ratio*voltage_ratio; // dynamic power scaled by square of voltage_ratio
+    	}
+  	}
+  }
+  
+  proc_power+=sample_cmp_pwr[CONSTP]+sample_cmp_pwr[STATICP];
+  if(!g_dvfs_enabled){ // sanity check will fail when voltage scaling is applied, fix later
+	  double sum_pwr_cmp=0;
+	  for(unsigned i=0; i<num_pwr_cmps; i++){
+	    sum_pwr_cmp+=sample_cmp_pwr[i];
+	  }
+	  bool check=false;
+	  check=sanity_check(sum_pwr_cmp,proc_power);
+	  if(!check)
+	    printf("sum_pwr_cmp %f : proc_power %f \n",sum_pwr_cmp,proc_power);
+	  assert("Total Power does not equal the sum of the components\n" && (check));
+  }
+}
 
-	sample_cmp_pwr[IDLE_COREP]=proc->cores[0]->IdleCoreEnergy/(proc->cores[0]->executionTime);
+void gpgpu_sim_wrapper::compute() { proc->compute(); }
+void gpgpu_sim_wrapper::print_power_kernel_stats(
+    double gpu_sim_cycle, double gpu_tot_sim_cycle, double init_value,
+    const std::string& kernel_info_string, bool print_trace) {
+  detect_print_steady_state(1, init_value);
+  if (g_power_simulation_enabled) {
+    powerfile << kernel_info_string << std::endl;
+
+    sanity_check((kernel_power.avg * kernel_sample_count), kernel_tot_power);
+    powerfile << "Kernel Average Power Data:" << std::endl;
+    powerfile << "kernel_avg_power = " << kernel_power.avg << std::endl;
+
+    for (unsigned i = 0; i < num_pwr_cmps; ++i) {
+      powerfile << "gpu_avg_" << pwr_cmp_label[i] << " = "
+                << kernel_cmp_pwr[i].avg / kernel_sample_count << std::endl;
+    }
+    for (unsigned i = 0; i < num_perf_counters; ++i) {
+      powerfile << "gpu_avg_" << perf_count_label[i] << " = "
+                << kernel_cmp_perf_counters[i].avg / kernel_sample_count
+                << std::endl;
+    }
 
-	// This constant dynamic power (e.g., clock power) part is estimated via regression model.
-	sample_cmp_pwr[CONST_DYNAMICP]=0;
-	double cnst_dyn = proc->get_const_dynamic_power()/(proc->cores[0]->executionTime);
-	// If the regression scaling term is greater than the recorded constant dynamic power
-	// then use the difference (other portion already added to dynamic power). Else,
-	// all the constant dynamic power is accounted for, add nothing.
-	if(p->sys.scaling_coefficients[CONST_DYNAMICN] > cnst_dyn)
-		sample_cmp_pwr[CONST_DYNAMICP] = (p->sys.scaling_coefficients[CONST_DYNAMICN]-cnst_dyn);
+    powerfile << "gpu_avg_threads_per_warp = "
+                << avg_threads_per_warp_tot / (double)kernel_sample_count
+                << std::endl;
 
-	proc_power+=sample_cmp_pwr[CONST_DYNAMICP];
+    for (unsigned i = 0; i < num_perf_counters; ++i) {
+      powerfile << "gpu_tot_" << perf_count_label[i] << " = "
+                << kernel_cmp_perf_counters[i].avg
+                << std::endl;
+    }
 
-	double sum_pwr_cmp=0;
-	for(unsigned i=0; i<num_pwr_cmps; i++){
-		sum_pwr_cmp+=sample_cmp_pwr[i];
-	}
-	bool check=false;
-	check=sanity_check(sum_pwr_cmp,proc_power);
-	assert("Total Power does not equal the sum of the components\n" && (check));
+    powerfile << std::endl << "Kernel Maximum Power Data:" << std::endl;
+    powerfile << "kernel_max_power = " << kernel_power.max << std::endl;
+    for (unsigned i = 0; i < num_pwr_cmps; ++i) {
+      powerfile << "gpu_max_" << pwr_cmp_label[i] << " = "
+                << kernel_cmp_pwr[i].max << std::endl;
+    }
+    for (unsigned i = 0; i < num_perf_counters; ++i) {
+      powerfile << "gpu_max_" << perf_count_label[i] << " = "
+                << kernel_cmp_perf_counters[i].max << std::endl;
+    }
 
-}
+    powerfile << std::endl << "Kernel Minimum Power Data:" << std::endl;
+    powerfile << "kernel_min_power = " << kernel_power.min << std::endl;
+    for (unsigned i = 0; i < num_pwr_cmps; ++i) {
+      powerfile << "gpu_min_" << pwr_cmp_label[i] << " = "
+                << kernel_cmp_pwr[i].min << std::endl;
+    }
+    for (unsigned i = 0; i < num_perf_counters; ++i) {
+      powerfile << "gpu_min_" << perf_count_label[i] << " = "
+                << kernel_cmp_perf_counters[i].min << std::endl;
+    }
 
-void gpgpu_sim_wrapper::compute()
-{
-	proc->compute();
+    powerfile << std::endl
+              << "Accumulative Power Statistics Over Previous Kernels:"
+              << std::endl;
+    powerfile << "gpu_tot_avg_power = "
+              << gpu_tot_power.avg / total_sample_count << std::endl;
+    powerfile << "gpu_tot_max_power = " << gpu_tot_power.max << std::endl;
+    powerfile << "gpu_tot_min_power = " << gpu_tot_power.min << std::endl;
+    powerfile << std::endl << std::endl;
+    powerfile.flush();
+
+    if (print_trace) {
+      print_trace_files();
+    }
+  }
 }
-void gpgpu_sim_wrapper::print_power_kernel_stats(double gpu_sim_cycle, double gpu_tot_sim_cycle, double init_value, const std::string & kernel_info_string, bool print_trace)
-{
-	   detect_print_steady_state(1,init_value);
-	   if(g_power_simulation_enabled){
-
-		   powerfile<<kernel_info_string<<std::endl;
-
-		   sanity_check((kernel_power.avg*kernel_sample_count), kernel_tot_power);
-		   powerfile<<"Kernel Average Power Data:"<<std::endl;
-		   powerfile<<"kernel_avg_power = " << kernel_power.avg << std::endl;
-
-		   for(unsigned i=0; i<num_pwr_cmps; ++i){
-				powerfile<<"gpu_avg_"<<pwr_cmp_label[i]<<" = "<<kernel_cmp_pwr[i].avg/kernel_sample_count<<std::endl;
-		   }
-		   for(unsigned i=0; i<num_perf_counters; ++i){
-				powerfile<<"gpu_avg_"<<perf_count_label[i]<<" = "<<kernel_cmp_perf_counters[i].avg/kernel_sample_count<<std::endl;
-		   }
-
-		   powerfile<<std::endl<<"Kernel Maximum Power Data:"<<std::endl;
-		   powerfile<<"kernel_max_power = "<< kernel_power.max<<std::endl;
-		   for(unsigned i=0; i<num_pwr_cmps; ++i){
-			   powerfile<<"gpu_max_"<<pwr_cmp_label[i]<<" = "<<kernel_cmp_pwr[i].max<<std::endl;
-		   }
-		   for(unsigned i=0; i<num_perf_counters; ++i){
-				powerfile<<"gpu_max_"<<perf_count_label[i]<<" = "<<kernel_cmp_perf_counters[i].max<<std::endl;
-		   }
-
-		   powerfile<<std::endl<<"Kernel Minimum Power Data:"<<std::endl;
-		   powerfile<<"kernel_min_power = "<< kernel_power.min<<std::endl;
-		   for(unsigned i=0; i<num_pwr_cmps; ++i){
-			   powerfile<<"gpu_min_"<<pwr_cmp_label[i]<<" = "<<kernel_cmp_pwr[i].min<<std::endl;
-		   }
-		   for(unsigned i=0; i<num_perf_counters; ++i){
-				powerfile<<"gpu_min_"<<perf_count_label[i]<<" = "<<kernel_cmp_perf_counters[i].min<<std::endl;
-		   }
-
-		   powerfile<<std::endl<<"Accumulative Power Statistics Over Previous Kernels:"<<std::endl;
-		   powerfile<<"gpu_tot_avg_power = "<< gpu_tot_power.avg/total_sample_count<<std::endl;
-		   powerfile<<"gpu_tot_max_power = "<<gpu_tot_power.max<<std::endl;
-		   powerfile<<"gpu_tot_min_power = "<<gpu_tot_power.min<<std::endl;
-		   powerfile<<std::endl<<std::endl;
-		   powerfile.flush();
-
-		   if(print_trace){
-			   print_trace_files();
-		   }
-	   }
-
-}
-void gpgpu_sim_wrapper::dump()
-{
-	if(g_power_per_cycle_dump)
-		proc->displayEnergy(2,5);
+void gpgpu_sim_wrapper::dump() {
+  if (g_power_per_cycle_dump) proc->displayEnergy(2, 5);
 }
 
-void gpgpu_sim_wrapper::print_steady_state(int position, double init_val){
-	double temp_avg = sample_val / (double)samples.size() ;
-	double temp_ipc = (init_val-init_inst_val)/ (double) (samples.size()*gpu_stat_sample_freq);
-
-	if((samples.size() > gpu_steady_min_period)){ // If steady state occurred for some time, print to file
-		has_written_avg=true;
-		gzprintf(steady_state_tacking_file,"%u,%d,%f,%f,",sample_start,total_sample_count,temp_avg,temp_ipc);
-		for(unsigned i=0; i<num_perf_counters; ++i){
-			gzprintf(steady_state_tacking_file,"%f,", samples_counter.at(i)/((double)samples.size()));
-		}
-		gzprintf(steady_state_tacking_file,"\n");
-	}else{
-		if(!has_written_avg && position)
-			gzprintf(steady_state_tacking_file,"ERROR! Not enough steady state points to generate average\n");
-	}
-
-	sample_start = 0;
-	sample_val = 0;
-	init_inst_val=init_val;
-	samples.clear();
-	samples_counter.clear();
-	pwr_counter.clear();
-	assert(samples.size() == 0);
-}
-
-void gpgpu_sim_wrapper::detect_print_steady_state(int position, double init_val)
-{
-	// Calculating Average
-    if(g_power_simulation_enabled && g_steady_power_levels_enabled){
-    	steady_state_tacking_file = gzopen(g_steady_state_tracking_filename,"a");
-		if(position==0){
-			if(samples.size() == 0){
-				// First sample
-				sample_start = total_sample_count;
-				sample_val = proc->rt_power.readOp.dynamic;
-				init_inst_val=init_val;
-				samples.push_back(proc->rt_power.readOp.dynamic);
-				assert(samples_counter.size() == 0);
-				assert(pwr_counter.size() == 0);
-
-				for(unsigned i=0; i<(num_perf_counters); ++i){
-					samples_counter.push_back(sample_perf_counters[i]);
-				}
-
-				for(unsigned i=0; i<(num_pwr_cmps); ++i){
-					pwr_counter.push_back(sample_cmp_pwr[i]);
-				}
-				assert(pwr_counter.size() == (double)num_pwr_cmps);
-				assert(samples_counter.size() == (double)num_perf_counters);
-			}else{
-				// Get current average
-				double temp_avg = sample_val / (double)samples.size() ;
-
-				if( abs(proc->rt_power.readOp.dynamic-temp_avg) < gpu_steady_power_deviation){ // Value is within threshold
-					sample_val += proc->rt_power.readOp.dynamic;
-					samples.push_back(proc->rt_power.readOp.dynamic);
-					for(unsigned i=0; i<(num_perf_counters); ++i){
-						samples_counter.at(i) += sample_perf_counters[i];
-					}
-
-					for(unsigned i=0; i<(num_pwr_cmps); ++i){
-						pwr_counter.at(i) += sample_cmp_pwr[i];
-					}
-
-				}else{	// Value exceeds threshold, not considered steady state
-					print_steady_state(position, init_val);
-				}
-			}
-		}else{
-			print_steady_state(position, init_val);
-		}
-		gzclose(steady_state_tacking_file);
+void gpgpu_sim_wrapper::print_steady_state(int position, double init_val) {
+  double temp_avg = sample_val / (double)samples.size();
+  double temp_ipc = (init_val - init_inst_val) /
+                    (double)(samples.size() * gpu_stat_sample_freq);
+
+  if ((samples.size() >
+       gpu_steady_min_period)) {  // If steady state occurred for some time,
+                                  // print to file
+    has_written_avg = true;
+    gzprintf(steady_state_tacking_file, "%u,%d,%f,%f,", sample_start,
+             total_sample_count, temp_avg, temp_ipc);
+    for (unsigned i = 0; i < num_perf_counters; ++i) {
+      gzprintf(steady_state_tacking_file, "%f,",
+               samples_counter.at(i) / ((double)samples.size()));
     }
+    gzprintf(steady_state_tacking_file, "\n");
+  } else {
+    if (!has_written_avg && position)
+      gzprintf(steady_state_tacking_file,
+               "ERROR! Not enough steady state points to generate average\n");
+  }
+
+  sample_start = 0;
+  sample_val = 0;
+  init_inst_val = init_val;
+  samples.clear();
+  samples_counter.clear();
+  pwr_counter.clear();
+  assert(samples.size() == 0);
 }
 
-void gpgpu_sim_wrapper::open_files()
-{
-    if(g_power_simulation_enabled){
-        if (g_power_trace_enabled ){
-            power_trace_file = gzopen(g_power_trace_filename,  "a");
-            metric_trace_file = gzopen(g_metric_trace_filename, "a");
+void gpgpu_sim_wrapper::detect_print_steady_state(int position,
+                                                  double init_val) {
+  // Calculating Average
+  if (g_power_simulation_enabled && g_steady_power_levels_enabled) {
+    steady_state_tacking_file = gzopen(g_steady_state_tracking_filename, "a");
+    if (position == 0) {
+      if (samples.size() == 0) {
+        // First sample
+        sample_start = total_sample_count;
+        sample_val = proc->rt_power.readOp.dynamic;
+        init_inst_val = init_val;
+        samples.push_back(proc->rt_power.readOp.dynamic);
+        assert(samples_counter.size() == 0);
+        assert(pwr_counter.size() == 0);
+
+        for (unsigned i = 0; i < (num_perf_counters); ++i) {
+          samples_counter.push_back(sample_perf_counters[i]);
         }
+
+        for (unsigned i = 0; i < (num_pwr_cmps); ++i) {
+          pwr_counter.push_back(sample_cmp_pwr[i]);
+        }
+        assert(pwr_counter.size() == (double)num_pwr_cmps);
+        assert(samples_counter.size() == (double)num_perf_counters);
+      } else {
+        // Get current average
+        double temp_avg = sample_val / (double)samples.size();
+
+        if (abs(proc->rt_power.readOp.dynamic - temp_avg) <
+            gpu_steady_power_deviation) {  // Value is within threshold
+          sample_val += proc->rt_power.readOp.dynamic;
+          samples.push_back(proc->rt_power.readOp.dynamic);
+          for (unsigned i = 0; i < (num_perf_counters); ++i) {
+            samples_counter.at(i) += sample_perf_counters[i];
+          }
+
+          for (unsigned i = 0; i < (num_pwr_cmps); ++i) {
+            pwr_counter.at(i) += sample_cmp_pwr[i];
+          }
+
+        } else {  // Value exceeds threshold, not considered steady state
+          print_steady_state(position, init_val);
+        }
+      }
+    } else {
+      print_steady_state(position, init_val);
     }
+    gzclose(steady_state_tacking_file);
+  }
 }
-void gpgpu_sim_wrapper::close_files()
-{
-    if(g_power_simulation_enabled){
-  	  if(g_power_trace_enabled){
-  		  gzclose(power_trace_file);
-  		  gzclose(metric_trace_file);
-  	  }
-  	 }
 
+void gpgpu_sim_wrapper::open_files() {
+  if (g_power_simulation_enabled) {
+    if (g_power_trace_enabled) {
+      power_trace_file = gzopen(g_power_trace_filename, "a");
+      metric_trace_file = gzopen(g_metric_trace_filename, "a");
+    }
+  }
+}
+void gpgpu_sim_wrapper::close_files() {
+  if (g_power_simulation_enabled) {
+    if (g_power_trace_enabled) {
+      gzclose(power_trace_file);
+      gzclose(metric_trace_file);
+    }
+  }
 }
diff --git a/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.h b/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.h
index 9d060925c4..73d69f572b 100644
--- a/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.h
+++ b/design/gpgpu/gpgpu-sim/src/gpuwattch/gpgpu_sim_wrapper.h
@@ -28,120 +28,191 @@
 #ifndef GPGPU_SIM_WRAPPER_H_
 #define GPGPU_SIM_WRAPPER_H_
 
-#include "processor.h"
+#include <assert.h>
 #include <stdio.h>
 #include <stdlib.h>
-#include <assert.h>
-#include <string>
-#include <iostream>
-#include <fstream>
-#include <zlib.h>
 #include <string.h>
-
+#include <zlib.h>
+#include <fstream>
+#include <iostream>
+#include <string>
+#include "processor.h"
 
 using namespace std;
 
 template <typename T>
-struct avg_max_min_counters{
-	T avg;
-	T max;
-	T min;
+struct avg_max_min_counters {
+  T avg;
+  T max;
+  T min;
+
+  avg_max_min_counters() {
+    avg = 0;
+    max = 0;
+    min = 0;
+  }
+};
 
-	avg_max_min_counters(){avg=0; max=0; min=0;}
+#ifndef COEFF_STRUCT
+#define COEFF_STRUCT
+
+struct PowerscalingCoefficients{
+    double int_coeff;
+    double int_mul_coeff;
+    double int_mul24_coeff;
+    double int_mul32_coeff;
+    double int_div_coeff;
+    double fp_coeff;
+    double dp_coeff;
+    double fp_mul_coeff;
+    double fp_div_coeff;
+    double dp_mul_coeff;
+    double dp_div_coeff;
+    double sqrt_coeff;
+    double log_coeff;
+    double sin_coeff;
+    double exp_coeff;
+    double tensor_coeff;
+    double tex_coeff;
 };
 
+#endif
+
 class gpgpu_sim_wrapper {
-public:
-	gpgpu_sim_wrapper(bool power_simulation_enabled, char* xmlfile);
-	~gpgpu_sim_wrapper();
-
-	void init_mcpat(char* xmlfile, char* powerfile, char* power_trace_file,char* metric_trace_file,
-			char * steady_state_file,bool power_sim_enabled,bool trace_enabled,bool steady_state_enabled,
-			bool power_per_cycle_dump,double steady_power_deviation,double steady_min_period,int zlevel,
-			double init_val,int stat_sample_freq);
-	void detect_print_steady_state(int position, double init_val);
-	void close_files();
-	void open_files();
-	void compute();
-	void dump();
-	void print_trace_files();
-	void update_components_power();
-	void update_coefficients();
-	void reset_counters();
-	void print_power_kernel_stats(double gpu_sim_cycle, double gpu_tot_sim_cycle, double init_value, const std::string & kernel_info_string, bool print_trace);
-	void power_metrics_calculations();
-	void set_inst_power(bool clk_gated_lanes, double tot_cycles, double busy_cycles, double tot_inst, double int_inst, double fp_inst, double load_inst, double store_inst, double committed_inst);
-	void set_regfile_power(double reads, double writes, double ops);
-	void set_icache_power(double accesses, double misses);
-	void set_ccache_power(double accesses, double misses);
-	void set_tcache_power(double accesses, double misses);
-	void set_shrd_mem_power(double accesses);
-	void set_l1cache_power(double read_accesses, double read_misses, double write_accesses, double write_misses);
-	void set_l2cache_power(double read_accesses, double read_misses, double write_accesses, double write_misses);
-	void set_idle_core_power(double num_idle_core);
-	void set_duty_cycle_power(double duty_cycle);
-	void set_mem_ctrl_power(double reads, double writes, double dram_precharge);
-	void set_exec_unit_power(double fpu_accesses, double ialu_accesses, double sfu_accesses);
-	void set_active_lanes_power(double sp_avg_active_lane, double sfu_avg_active_lane);
-	void set_NoC_power(double noc_tot_reads, double noc_tot_write);
-	bool sanity_check(double a, double b);
-
-private:
-
-	void print_steady_state(int position, double init_val);
-
-	Processor* proc;
-	ParseXML * p;
-    // power parameters
-    double const_dynamic_power;
-    double proc_power;
-
-    unsigned num_perf_counters; // # of performance counters
-    unsigned num_pwr_cmps; // # of components modelled
-    int kernel_sample_count; // # of samples per kernel
-    int total_sample_count; // # of samples per benchmark
-
-    std::vector< avg_max_min_counters<double> > kernel_cmp_pwr; // Per-kernel component power avg/max/min values
-    std::vector< avg_max_min_counters<double> > kernel_cmp_perf_counters; // Per-kernel component avg/max/min performance counters
-
-    double kernel_tot_power; // Total per-kernel power
-    avg_max_min_counters<double> kernel_power; // Per-kernel power avg/max/min values
-    avg_max_min_counters<double> gpu_tot_power; // Global GPU power avg/max/min values (across kernels)
-
-    bool has_written_avg;
-
-    std::vector<double> sample_cmp_pwr; // Current sample component powers
-    std::vector<double> sample_perf_counters; // Current sample component perf. counts
-    std::vector<double> initpower_coeff;
-    std::vector<double> effpower_coeff;
-
-    // For calculating steady-state average
-    unsigned sample_start;
-    double sample_val;
-    double init_inst_val;
-    std::vector<double> samples;
-    std::vector<double> samples_counter;
-    std::vector<double> pwr_counter;
-
-    char *xml_filename;
-    char *g_power_filename;
-    char *g_power_trace_filename;
-    char *g_metric_trace_filename;
-    char * g_steady_state_tracking_filename;
-    bool g_power_simulation_enabled;
-    bool g_steady_power_levels_enabled;
-    bool g_power_trace_enabled;
-    bool g_power_per_cycle_dump;
-	double   gpu_steady_power_deviation;
-	double   gpu_steady_min_period;
-	int g_power_trace_zlevel;
-    double gpu_stat_sample_frequency;
-    int gpu_stat_sample_freq;
-
-    std::ofstream powerfile;
-    gzFile power_trace_file;
-    gzFile metric_trace_file;
-    gzFile steady_state_tacking_file;
+ public:
+  gpgpu_sim_wrapper(bool power_simulation_enabled, char* xmlfile, int power_simulation_mode, bool dvfs_enabled);
+  ~gpgpu_sim_wrapper();
+
+  void init_mcpat(char* xmlfile, char* powerfile, char* power_trace_file,
+                  char* metric_trace_file, char* steady_state_file,
+                  bool power_sim_enabled, bool trace_enabled,
+                  bool steady_state_enabled, bool power_per_cycle_dump,
+                  double steady_power_deviation, double steady_min_period,
+                  int zlevel, double init_val, int stat_sample_freq, int power_sim_mode, 
+                  bool dvfs_enabled, unsigned clock_freq, unsigned num_shaders);
+  void init_mcpat_hw_mode(unsigned gpu_sim_cycle);
+  void detect_print_steady_state(int position, double init_val);
+  void close_files();
+  void open_files();
+  void compute();
+  void dump();
+  void print_trace_files();
+  void update_components_power();
+  double calculate_static_power();
+  void update_coefficients();
+  void reset_counters();
+  void print_power_kernel_stats(double gpu_sim_cycle, double gpu_tot_sim_cycle,
+                                double init_value,
+                                const std::string& kernel_info_string,
+                                bool print_trace);
+  void power_metrics_calculations();
+  void set_model_voltage(double model_voltage);
+  void set_inst_power(bool clk_gated_lanes, double tot_cycles,
+                      double busy_cycles, double tot_inst, double int_inst,
+                      double fp_inst, double load_inst, double store_inst,
+                      double committed_inst);
+  void set_regfile_power(double reads, double writes, double ops);
+  void set_icache_power(double accesses, double misses);
+  void set_ccache_power(double accesses, double misses);
+  void set_tcache_power(double accesses, double misses);
+  void set_shrd_mem_power(double accesses);
+  void set_l1cache_power(double read_accesses, double read_misses,
+                         double write_accesses, double write_misses);
+  void set_l2cache_power(double read_accesses, double read_misses,
+                         double write_accesses, double write_misses);
+  void set_num_cores(double num_core);
+  void set_idle_core_power(double num_idle_core);
+  void set_duty_cycle_power(double duty_cycle);
+  void set_mem_ctrl_power(double reads, double writes, double dram_precharge);
+  void set_exec_unit_power(double fpu_accesses, double ialu_accesses,
+                           double sfu_accesses);
+  void set_int_accesses(double ialu_accesses, double imul24_accesses, 
+                        double imul32_accesses, double imul_accesses, 
+                        double idiv_accesses);
+  void set_dp_accesses(double dpu_accesses, double dpmul_accesses, 
+                       double dpdiv_accesses);
+  void set_fp_accesses(double fpu_accesses, double fpmul_accesses, 
+                       double fpdiv_accesses);
+  void set_trans_accesses(double sqrt_accesses, double log_accesses, 
+                       double sin_accesses, double exp_accesses);
+  void set_tensor_accesses(double tensor_accesses);
+  void set_tex_accesses(double tex_accesses);
+  void set_avg_active_threads(float active_threads);
+  void set_active_lanes_power(double sp_avg_active_lane,
+                              double sfu_avg_active_lane);
+  void set_NoC_power(double noc_tot_acc);
+  bool sanity_check(double a, double b);
+
+  PowerscalingCoefficients * get_scaling_coeffs();
+
+ private:
+  void print_steady_state(int position, double init_val);
+
+  Processor* proc;
+  ParseXML* p;
+  // power parameters
+  double const_dynamic_power;
+  double avg_threads_per_warp_tot;
+  double proc_power;
+  double num_cores;
+  double num_idle_cores;
+  unsigned num_perf_counters;  // # of performance counters
+  unsigned num_pwr_cmps;       // # of components modelled
+  int kernel_sample_count;     // # of samples per kernel
+  int total_sample_count;      // # of samples per benchmark
+
+  std::vector<avg_max_min_counters<double> >
+      kernel_cmp_pwr;  // Per-kernel component power avg/max/min values
+  std::vector<avg_max_min_counters<double> >
+      kernel_cmp_perf_counters;  // Per-kernel component avg/max/min performance
+                                 // counters
+
+  double kernel_tot_power;  // Total per-kernel power
+  avg_max_min_counters<double>
+      kernel_power;  // Per-kernel power avg/max/min values
+  avg_max_min_counters<double>
+      gpu_tot_power;  // Global GPU power avg/max/min values (across kernels)
+
+  bool has_written_avg;
+
+  std::vector<double> sample_cmp_pwr;  // Current sample component powers
+  std::vector<double>
+      sample_perf_counters;  // Current sample component perf. counts
+  std::vector<double> initpower_coeff;
+  std::vector<double> effpower_coeff;
+
+  // For calculating steady-state average
+  unsigned sample_start;
+  double sample_val;
+  double init_inst_val;
+  double tot_sfu_accesses;
+  double tot_fpu_accesses;
+  double modeled_chip_voltage;
+  unsigned avg_threads_per_warp;
+  std::vector<double> samples;
+  std::vector<double> samples_counter;
+  std::vector<double> pwr_counter;
+
+  char* xml_filename;
+  char* g_power_filename;
+  char* g_power_trace_filename;
+  char* g_metric_trace_filename;
+  char* g_steady_state_tracking_filename;
+  bool g_power_simulation_enabled;
+  int g_power_simulation_mode;
+  bool g_dvfs_enabled;
+  bool g_steady_power_levels_enabled;
+  bool g_power_trace_enabled;
+  bool g_power_per_cycle_dump;
+  double gpu_steady_power_deviation;
+  double gpu_steady_min_period;
+  int g_power_trace_zlevel;
+  double gpu_stat_sample_frequency;
+  int gpu_stat_sample_freq;
+
+  std::ofstream powerfile;
+  gzFile power_trace_file;
+  gzFile metric_trace_file;
+  gzFile steady_state_tacking_file;
 };
-
 #endif /* GPGPU_SIM_WRAPPER_H_ */
diff --git a/design/gpgpu/gpgpu-sim/src/intersim2/interconnect_interface.cpp b/design/gpgpu/gpgpu-sim/src/intersim2/interconnect_interface.cpp
index 1bb31b1931..d7039f4b07 100644
--- a/design/gpgpu/gpgpu-sim/src/intersim2/interconnect_interface.cpp
+++ b/design/gpgpu/gpgpu-sim/src/intersim2/interconnect_interface.cpp
@@ -149,7 +149,8 @@ void InterconnectInterface::Push(unsigned input_deviceID, unsigned output_device
   // it should have free buffer
   assert(HasBuffer(input_deviceID, size));
 
-  GPGPUSIM_DPRINTF(INTERCONNECT, "Sent %d bytes from %d to %d", size, input_deviceID, output_deviceID);
+  // FIXME
+  // GPGPUSIM_DPRINTF(INTERCONNECT, "Sent %d bytes from %d to %d", size, input_deviceID, output_deviceID);
   
   int output_icntID = _node_map[output_deviceID];
   int input_icntID = _node_map[input_deviceID];
diff --git a/design/gpgpu/gpgpu-sim/src/option_parser.cc b/design/gpgpu/gpgpu-sim/src/option_parser.cc
index 4fa2343ee5..48b3260013 100644
--- a/design/gpgpu/gpgpu-sim/src/option_parser.cc
+++ b/design/gpgpu/gpgpu-sim/src/option_parser.cc
@@ -239,7 +239,7 @@ public:
          }
          if (optionFound == false) {
             fprintf(stderr, "\n\nGPGPU-Sim ** ERROR: Unknown Option: '%s' \n", argv[i]);
-            exit(1);
+            // exit(1);
          }
       }
    }
diff --git a/design/gpgpu/gpgpu-sim/src/stream_manager.cc b/design/gpgpu/gpgpu-sim/src/stream_manager.cc
index 7757e24256..9ad65f42c9 100644
--- a/design/gpgpu/gpgpu-sim/src/stream_manager.cc
+++ b/design/gpgpu/gpgpu-sim/src/stream_manager.cc
@@ -26,10 +26,11 @@
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #include "stream_manager.h"
-#include "gpgpusim_entrypoint.h"
-#include "gpu/gpgpu-sim/cuda_gpu.hh"
 #include "cuda-sim/cuda-sim.h"
 #include "gpgpu-sim/gpu-sim.h"
+#include "gpgpusim_entrypoint.h"
+#include "gpu/gpgpu-sim/cuda_gpu.hh"
+#include "../libcuda_sim/gpgpu_context.h"
 
 unsigned CUstream_st::sm_next_stream_uid = 0;
 
@@ -184,8 +185,9 @@ bool stream_operation::do_operation( gpgpu_sim *gpu )
     case stream_event: {
         printf("event update\n");
         time_t wallclock = time((time_t *)NULL);
+        // TODO
         // m_event->update( gpu_tot_sim_cycle, wallclock );
-        m_event->update( gpu_tot_sim_cycle, wallclock, gpu->gem5CudaGPU->getCurTick() );
+        m_event->update( gpu->gpu_tot_sim_cycle, wallclock, gpu->gem5CudaGPU->getCurTick() );
         if (m_event->needs_unblock()) {
             gpu->gem5CudaGPU->unblockThread(NULL);
             m_event->reset();
@@ -306,19 +308,19 @@ bool stream_manager::register_finished_kernel(unsigned grid_uid)
 }
 
 void stream_manager::stop_all_running_kernels(){
-    // pthread_mutex_lock(&m_lock);
+  // pthread_mutex_lock(&m_lock);
 
-    // Signal m_gpu to stop all running kernels
-    m_gpu->stop_all_running_kernels();
+  // Signal m_gpu to stop all running kernels
+  m_gpu->stop_all_running_kernels();
 
-    // Clean up all streams waiting on running kernels
-    int count=0;
-    while(check_finished_kernel()){
-        count++;
-    }
+  // Clean up all streams waiting on running kernels
+  int count=0;
+  while(check_finished_kernel()){
+    count++;
+  }
 
-    // If any kernels completed, print out the current stats
-    if(count > 0)
+  // If any kernels completed, print out the current stats
+  if(count > 0)
         m_gpu->print_stats();
 
     // pthread_mutex_unlock(&m_lock);
@@ -326,20 +328,20 @@ void stream_manager::stop_all_running_kernels(){
 
 stream_operation stream_manager::front()
 {
-    // called by gpu simulation thread
-    stream_operation result;
+  // called by gpu simulation thread
+  stream_operation result;
 //    if( concurrent_streams_empty() )
-    m_service_stream_zero = true;
-    if( m_service_stream_zero ) {
-        if( !m_stream_zero.empty() && !m_stream_zero.busy() ) {
-                result = m_stream_zero.next();
-                if( result.is_kernel() ) {
-                    unsigned grid_id = result.get_kernel()->get_uid();
-                    m_grid_id_to_stream[grid_id] = &m_stream_zero;
-                }
-        } else {
-            m_service_stream_zero = false;
-        }
+  m_service_stream_zero = true;
+  if( m_service_stream_zero ) {
+    if( !m_stream_zero.empty() && !m_stream_zero.busy() ) {
+      result = m_stream_zero.next();
+      if( result.is_kernel() ) {
+        unsigned grid_id = result.get_kernel()->get_uid();
+        m_grid_id_to_stream[grid_id] = &m_stream_zero;
+      }
+    } else {
+      m_service_stream_zero = false;
+      }
     }
     if(!m_service_stream_zero)
     {
@@ -362,7 +364,7 @@ stream_operation stream_manager::front()
             }
         }
     }
-    return result;
+  return result;
 }
 
 bool stream_manager::ready()
@@ -476,30 +478,38 @@ void stream_manager::print_impl( FILE *fp)
         m_stream_zero.print(fp);
 }
 
-void stream_manager::push( stream_operation op )
-{
-    struct CUstream_st *stream = op.get_stream();
-
-    // pthread_mutex_lock(&m_lock);
-    if(!m_gpu->cycle_insn_cta_max_hit()) {
-        // Accept the stream operation if the maximum cycle/instruction/cta counts are not triggered
-        if( stream && !m_cuda_launch_blocking ) {
-            stream->push(op);
-        } else {
-            op.set_stream(&m_stream_zero);
-            m_stream_zero.push(op);
-        }
-    }else {
-        // Otherwise, ignore operation and continue
-        printf("GPGPU-Sim API: Maximum cycle, instruction, or CTA count hit. Skipping:");
+void stream_manager::push( stream_operation op ) {
+  struct CUstream_st *stream = op.get_stream();
+
+  // block if stream 0 (or concurrency disabled) and pending concurrent
+  // operations exist
+  bool block = !stream || m_cuda_launch_blocking;
+  while (block) {
+    pthread_mutex_lock(&m_lock);
+    block = !concurrent_streams_empty();
+    pthread_mutex_unlock(&m_lock);
+  };
+
+  // pthread_mutex_lock(&m_lock);
+  if(!m_gpu->cycle_insn_cta_max_hit()) {
+    // Accept the stream operation if the maximum cycle/instruction/cta counts
+    // are not triggered
+    if ( stream && !m_cuda_launch_blocking ) {
+      stream->push(op);
+    } else {
+      op.set_stream(&m_stream_zero);
+      m_stream_zero.push(op);
+    }
+  }else {
+    // Otherwise, ignore operation and continue
+    printf("GPGPU-Sim API: Maximum cycle, instruction, or CTA count hit. Skipping:");
         op.print(stdout);
         printf("\n");
-    }
-    if(g_debug_execution >= 3)
-       print_impl(stdout);
+  }
+  if(g_debug_execution >= 3) print_impl(stdout);
 
-    // TODO schi fixme
-    if (ready()) {
+  // TODO schi fixme
+  if (ready()) {
         m_gpu->gem5CudaGPU->scheduleStreamEvent();
     }
 }
diff --git a/design/gpgpu/gpgpu-sim/src/stream_manager.h b/design/gpgpu/gpgpu-sim/src/stream_manager.h
index 70e57762f2..fa512e8024 100644
--- a/design/gpgpu/gpgpu-sim/src/stream_manager.h
+++ b/design/gpgpu/gpgpu-sim/src/stream_manager.h
@@ -28,11 +28,11 @@
 #ifndef STREAM_MANAGER_H_INCLUDED
 #define STREAM_MANAGER_H_INCLUDED
 
-#include "abstract_hardware_model.h"
 #include "cpu/thread_context.hh"
-#include <list>
 #include <pthread.h>
 #include <time.h>
+#include <list>
+#include "abstract_hardware_model.h"
 
 using namespace gem5;
 
@@ -48,15 +48,14 @@ using namespace gem5;
 
 struct CUevent_st {
 public:
-   CUevent_st( bool blocking )
-   {
-      m_uid = ++m_next_event_uid;
-      m_blocking = blocking;
-      m_updates = 0;
-      m_wallclock = 0;
-      m_gpu_tot_sim_cycle = 0;
-      m_issued = 0;
-      m_done = false;
+  CUevent_st(bool blocking) {
+    m_uid = ++m_next_event_uid;
+    m_blocking = blocking;
+    m_updates = 0;
+    m_wallclock = 0;
+    m_gpu_tot_sim_cycle = 0;
+    m_issued = 0;
+    m_done = false;
 
       // TODO schi
       m_needs_unblock = false;
@@ -97,16 +96,16 @@ private:
 };
 
 enum stream_operation_type {
-    stream_no_op,
-    stream_memcpy_host_to_device,
-    stream_memcpy_device_to_host,
-    stream_memcpy_device_to_device,
-    stream_memcpy_to_symbol,
-    stream_memcpy_from_symbol,
-    stream_kernel_launch,
-    stream_event,
-    stream_wait_event,
-    stream_memset
+  stream_no_op,
+  stream_memcpy_host_to_device,
+  stream_memcpy_device_to_host,
+  stream_memcpy_device_to_device,
+  stream_memcpy_to_symbol,
+  stream_memcpy_from_symbol,
+  stream_kernel_launch,
+  stream_event,
+  stream_wait_event,
+  stream_memset
 };
 
 class stream_operation {
@@ -230,107 +229,110 @@ public:
         launchTime = curTick();
     }
 
-    bool is_kernel() const { return m_type == stream_kernel_launch; }
-    bool is_mem() const {
-        return m_type == stream_memcpy_host_to_device ||
-               m_type == stream_memcpy_device_to_host ||
-               m_type == stream_memcpy_host_to_device;
-    }
-    bool is_noop() const { return m_type == stream_no_op; }
-    bool is_done() const { return m_done; }
-    kernel_info_t *get_kernel() { return m_kernel; }
-    bool do_operation( gpgpu_sim *gpu );
-    void print( FILE *fp ) const;
-    struct CUstream_st *get_stream() { return m_stream; }
-    void set_stream( CUstream_st *stream ) { m_stream = stream; }
+  bool is_kernel() const { return m_type == stream_kernel_launch; }
+  bool is_mem() const {
+    return m_type == stream_memcpy_host_to_device ||
+           m_type == stream_memcpy_device_to_host ||
+           m_type == stream_memcpy_host_to_device;
+  }
+  bool is_noop() const { return m_type == stream_no_op; }
+  bool is_done() const { return m_done; }
+  kernel_info_t *get_kernel() { return m_kernel; }
+  bool do_operation( gpgpu_sim *gpu );
+  void print( FILE *fp ) const;
+  struct CUstream_st *get_stream() { return m_stream; }
+  void set_stream( CUstream_st *stream ) { m_stream = stream; }
 
-    // TODO schi remove it in next step
-    // For handling the gem5 thread context
-    void setThreadContext(ThreadContext *_tc) { tc = _tc; }
+  // TODO schi remove it in next step
+  // For handling the gem5 thread context
+  void setThreadContext(ThreadContext *_tc) { tc = _tc; }
 
 private:
-    struct CUstream_st *m_stream;
+  struct CUstream_st *m_stream;
 
-    bool m_done;
+  bool m_done;
 
-    stream_operation_type m_type;
-    size_t      m_device_address_dst;
-    size_t      m_device_address_src;
-    void       *m_host_address_dst;
-    const void *m_host_address_src;
-    size_t      m_cnt;
+  stream_operation_type m_type;
+  size_t      m_device_address_dst;
+  size_t      m_device_address_src;
+  void       *m_host_address_dst;
+  const void *m_host_address_src;
+  size_t      m_cnt;
 
-    const char *m_symbol;
-    size_t m_offset;
-    int m_write_value;
+  const char *m_symbol;
+  size_t m_offset;
+  int m_write_value;
 
-    bool m_sim_mode;
-    kernel_info_t *m_kernel;
-    struct CUevent_st *m_event;
-    Tick launchTime;
+  bool m_sim_mode;
+  kernel_info_t *m_kernel;
+  struct CUevent_st *m_event;
+  Tick launchTime;
 
-    // TODO schi remove it 
-    // The gem5 thread context executing this stream
-    ThreadContext *tc;
+  // TODO schi remove it 
+  // The gem5 thread context executing this stream
+  ThreadContext *tc;
 
 };
 struct CUstream_st {
 public:
-    CUstream_st(); 
-    bool empty();
-    bool busy();
-    void synchronize();
-    void push( const stream_operation &op );
-    void record_next_done();
-    stream_operation next();
-    void cancel_front(); //front operation fails, cancle the pending status
-    stream_operation &front() { return m_operations.front(); }
-    void print( FILE *fp );
-    unsigned get_uid() const { return m_uid; }
+  CUstream_st(); 
+  bool empty();
+  bool busy();
+  void synchronize();
+  void push(const stream_operation &op );
+  void record_next_done();
+  stream_operation next();
+  void cancel_front(); //front operation fails, cancle the pending status
+  stream_operation &front() { return m_operations.front(); }
+  void print( FILE *fp );
+  unsigned get_uid() const { return m_uid; }
     // For handling the gem5 thread context
     void setThreadContext(ThreadContext *_tc) { tc = _tc; }
     ThreadContext *getThreadContext() { return tc; }
 
 private:
-    unsigned m_uid;
-    static unsigned sm_next_stream_uid;
+  unsigned m_uid;
+  static unsigned sm_next_stream_uid;
 
-    std::list<stream_operation> m_operations;
-    bool m_pending; // front operation has started but not yet completed
+  std::list<stream_operation> m_operations;
+  bool m_pending; // front operation has started but not yet completed
 
-    pthread_mutex_t m_lock; // ensure only one host or gpu manipulates stream operation at one time
+  pthread_mutex_t m_lock; // ensure only one host or gpu manipulates stream operation at one time
     // The gem5 thread context executing this stream
-    ThreadContext *tc;
+  ThreadContext *tc;
 };
 
 class stream_manager {
 public:
-    stream_manager( gpgpu_sim *gpu, bool cuda_launch_blocking );
-    bool register_finished_kernel(unsigned grid_uid  );
-    bool check_finished_kernel(  );
-    stream_operation front();
-    bool ready();
-    void add_stream( CUstream_st *stream );
-    void destroy_stream( CUstream_st *stream );
-    bool concurrent_streams_empty();
-    bool empty_protected();
-    bool empty();
-    void print( FILE *fp);
-    void push( stream_operation op );
-    void pushCudaStreamWaitEventToAllStreams( CUevent_st *e, unsigned int flags );
-    bool operation(bool * sim);
-    void stop_all_running_kernels();
+  stream_manager( gpgpu_sim *gpu, bool cuda_launch_blocking );
+  bool register_finished_kernel(unsigned grid_uid  );
+  bool check_finished_kernel(  );
+  stream_operation front();
+  bool ready();
+  void add_stream( CUstream_st *stream );
+  void destroy_stream( CUstream_st *stream );
+  bool concurrent_streams_empty();
+  bool empty_protected();
+  bool empty();
+  void print( FILE *fp);
+  void push( stream_operation op );
+  void pushCudaStreamWaitEventToAllStreams( CUevent_st *e, unsigned int flags );
+  bool operation(bool * sim);
+  void stop_all_running_kernels();
+  unsigned size() { return m_streams.size(); };
+  bool is_blocking() { return m_cuda_launch_blocking; };
+
 private:
-    void print_impl( FILE *fp);
+  void print_impl( FILE *fp);
 
-    bool m_cuda_launch_blocking;
-    gpgpu_sim *m_gpu;
-    std::list<CUstream_st *> m_streams;
-    std::map<unsigned,CUstream_st *> m_grid_id_to_stream;
-    CUstream_st m_stream_zero;
-    bool m_service_stream_zero;
-    pthread_mutex_t m_lock;
-    std::list<struct CUstream_st*>::iterator m_last_stream;
+  bool m_cuda_launch_blocking;
+  gpgpu_sim *m_gpu;
+  std::list<CUstream_st *> m_streams;
+  std::map<unsigned,CUstream_st *> m_grid_id_to_stream;
+  CUstream_st m_stream_zero;
+  bool m_service_stream_zero;
+  pthread_mutex_t m_lock;
+  std::list<struct CUstream_st*>::iterator m_last_stream;
 };
 
 #endif
diff --git a/design/gpgpu/gpgpu-sim/src/trace.cc b/design/gpgpu/gpgpu-sim/src/trace.cc
index 54a9d9e051..ff6fb4b04d 100644
--- a/design/gpgpu/gpgpu-sim/src/trace.cc
+++ b/design/gpgpu/gpgpu-sim/src/trace.cc
@@ -30,27 +30,27 @@
 
 namespace Trace_gpgpu {
 
-
 #define TS_TUP_BEGIN(X) const char* trace_streams_str[] = {
 #define TS_TUP(X) #X
-#define TS_TUP_END(X) };
+#define TS_TUP_END(X) \
+  }                   \
+  ;
 #include "trace_streams.tup"
 #undef TS_TUP_BEGIN
 #undef TS_TUP
 #undef TS_TUP_END
 
-    bool enabled = false;
-    int sampling_core = 0;
-    int sampling_memory_partition = -1;
-    bool trace_streams_enabled[NUM_TRACE_STREAMS] = {false};
-    const char* config_str;
+bool enabled = false;
+int sampling_core = 0;
+int sampling_memory_partition = -1;
+bool trace_streams_enabled[NUM_TRACE_STREAMS] = {false};
+const char* config_str;
 
-    void init()
-    {
-        for ( unsigned i = 0; i < NUM_TRACE_STREAMS; ++i ) {
-            if ( strstr( config_str, trace_streams_str[i] ) != NULL ) {
-                trace_streams_enabled[ i ] = true;
-            }
-        }
+void init() {
+  for (unsigned i = 0; i < NUM_TRACE_STREAMS; ++i) {
+    if (strstr(config_str, trace_streams_str[i]) != NULL) {
+      trace_streams_enabled[i] = true;
     }
-} 
+  }
+}
+}  // namespace Trace
diff --git a/design/gpgpu/gpgpu-sim/src/trace.h b/design/gpgpu/gpgpu-sim/src/trace.h
index c29297d11f..e45a944f1d 100644
--- a/design/gpgpu/gpgpu-sim/src/trace.h
+++ b/design/gpgpu/gpgpu-sim/src/trace.h
@@ -31,9 +31,6 @@
 #ifndef __TRACE_H__
 #define __TRACE_H__
 
-extern unsigned long long  gpu_sim_cycle;
-extern unsigned long long  gpu_tot_sim_cycle;
-
 namespace Trace_gpgpu {
 
 #define TS_TUP_BEGIN(X) enum X {
@@ -44,37 +41,49 @@ namespace Trace_gpgpu {
 #undef TS_TUP
 #undef TS_TUP_END
 
-    extern bool enabled;
-    extern int sampling_core;
-    extern int sampling_memory_partition;
-    extern const char* trace_streams_str[];
-    extern bool trace_streams_enabled[NUM_TRACE_STREAMS];
-    extern const char* config_str;
-
-    void init();
+extern bool enabled;
+extern int sampling_core;
+extern int sampling_memory_partition;
+extern const char* trace_streams_str[];
+extern bool trace_streams_enabled[NUM_TRACE_STREAMS];
+extern const char* config_str;
 
-} // namespace Trace_gpgpu
+void init();
 
+}  // namespace Trace
 
 #if TRACING_ON
 
 #define SIM_PRINT_STR "GPGPU-Sim Cycle %llu: %s - "
 #define GPGPUSIM_DTRACE(x) ((Trace_gpgpu::trace_streams_enabled[Trace_gpgpu::x]) && Trace_gpgpu::enabled)
-#define GPGPUSIM_DPRINTF(x, ...) do {\
-    if (GPGPUSIM_DTRACE(x)) {\
-        printf( SIM_PRINT_STR,\
-                gpu_sim_cycle + gpu_tot_sim_cycle,\
-                Trace_gpgpu::trace_streams_str[Trace_gpgpu::x] );\
-        printf(__VA_ARGS__);\
-    }\
-} while (0)
-
-
-#else 
+#define GPGPUSIM_DPRINTF(x, ...)                                                      \
+  do {                                                                       \
+    if (GPGPUSIM_DTRACE(x)) {                                                         \
+      printf(SIM_PRINT_STR, m_gpu->gpu_sim_cycle + m_gpu->gpu_tot_sim_cycle, \
+             Trace_gpgpu::trace_streams_str[Trace_gpgpu::x]);                            \
+      printf(__VA_ARGS__);                                                   \
+    }                                                                        \
+  } while (0)
+
+#define GPGPUSIM_DPRINTFG(x, ...)                                       \
+  do {                                                         \
+    if (GPGPUSIM_DTRACE(x)) {                                           \
+      printf(SIM_PRINT_STR, gpu_sim_cycle + gpu_tot_sim_cycle, \
+             Trace_gpgpu::trace_streams_str[Trace_gpgpu::x]);              \
+      printf(__VA_ARGS__);                                     \
+    }                                                          \
+  } while (0)
+
+#else
 
 #define GPGPUSIM_DTRACE(x) (false)
-#define GPGPUSIM_DPRINTF(x, ...) do {} while (0)
+#define GPGPUSIM_DPRINTF(x, ...) \
+  do {                  \
+  } while (0)
+#define GPGPUSIM_DPRINTFG(x, ...) \
+  do {                   \
+  } while (0)
 
-#endif  
+#endif
 
-#endif 
+#endif
